<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>杨力祥老师的编程哲学-把程序写成东西</title>
    <link href="/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/"/>
    <url>/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h2 id="把程序写成东西"><a href="#把程序写成东西" class="headerlink" title="把程序写成东西"></a>把程序写成东西</h2><p>感谢今生遇见杨力祥老师！！！活了20多年了，第一次遇到一位将面向对象思想讲解这么透彻的老师。</p><p>老师一上来的课程就非常硬核，告诉我们，<strong>面向对象就是把程序写成东西</strong></p><p>以麦当劳炸薯条的例子，教会我们区分面向过程和面向对象。面向过程就是流程化，第一步做什么第二部做什么。。。最后一步做什么。规则、时间、火候、尺寸必须一点都不能差。</p><p>而面向对象是将工作的各个流程打包成几个“东西”，任何人只要学会使用这个东西，就可以炸出麦当劳家的高质量薯条。</p><h2 id="复用"><a href="#复用" class="headerlink" title="复用"></a>复用</h2><p>为什么要学面向对象？有什么好处？</p><p>答案就是：<strong>复用</strong>，程序写一次，在不修改的情况下大量拷贝使用。而复用的本质是<strong>共性</strong>，程序只有写成东西才容易找到共性。</p><p>杨老师还从人类思维认知起源给我们讲起，人类最初学会的工作就是<code>分类</code>，引申出类和对象的关系。将分类比做一个多叉树，在实际思考时，是自下而上抽取共性，抽象就是脱离形式提取共性，但写的时候从上到下派生。</p><p>为什么把程序写成东西就容易复用？</p><p>因为人类祖先最早学会的技能就是分类，根据东西的<strong>共性</strong>进行分类，面向对象根本目的是提高复用率。</p><p>程序写成东西的唯一目的就是复用。复用就是自己可以作为贡献者也可以作为创新者。</p><p>不要做“万能工具箱类”，万能工具箱类不可以进行程序的复用，谁都可以拿来用。</p><h2 id="共性与个性"><a href="#共性与个性" class="headerlink" title="共性与个性"></a>共性与个性</h2><p>共性+差异&#x3D;个性</p><p>面向对象提高效率的理论基础与使用依据：<strong>共性远远远远大于差异，但是差异体现价值</strong>，自然界的事物亦如此。</p><p>继承派生的过程中加入了多态的思想，于是产生了个性的差异价值；如果只是单纯的继承派生无法体现出个性的差异价值。</p><p>虚函数是技术、多态是思想。</p><p>越能加入差异反而复用率越多，越能体现差异越能找到共性。</p><h2 id="创新思维"><a href="#创新思维" class="headerlink" title="创新思维"></a>创新思维</h2><p>创新力与创新思维不是天生的，一定是要经过后天大量培训锻炼出来的。分析问题要抓住本质，这句话很空，但是很难做到。</p><p>作为未来的计算机从业者，凡事用数据和计算的思维去思考。</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>雁栖一年，科研三年</title>
    <link href="/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/"/>
    <url>/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="开学"><a href="#开学" class="headerlink" title="开学"></a>开学</h2><p>开学参加了很多次的入学讲座和培训，听了很多老师的演讲和致辞。</p><p>曹亚男老师结合她自己的经历，告诉我们要<code>做有品位的研究</code>。针对科学研究，提出了两大方向：针对开放问题的开创探讨和针对封闭数据的优先探索。问题的价值作为核心，与个人兴趣、资源、能力、知识储备紧密结合。做有意义的课题，要平衡好“我喜欢”和“我的能力”。在雁栖湖的一年，学好专业课程的同时，一定要定好自己的研究点(至少3个)，学硕开题前至少要投出去一篇。</p><p>林政老师在开学典礼的致辞中也告诫我们，研究生的学习过程中，要<code>多关注why而不是how</code>。对于新技术、新方法，主要要弄懂为什么这样用？为什么是这个结构？至于说怎么用，那是学习过程中需要解决的问题。林老师也提醒到，学习课程的同时，不要忘记培养自己的科研能力与科学素养。</p><h2 id="关于选课"><a href="#关于选课" class="headerlink" title="关于选课"></a>关于选课</h2><p>新学期选课工作全部结束啦，抢到了自己心仪的课程。新学期还是立足本专业，结合李老师的建议侧重数据库和人工智能相关。</p><p>核心课选了晓飞老师的机器学习、曹亚男老师的自然语言处理、沙老师的大数据管理与分析。专业课选了黄晶老师的大数据技术、计算机苏老师的数据库新技术。</p><p>希望自己好好利用在雁栖湖的这一年时光，多学点真本领、多培养一些科学素养、多积累一些科研经验。上好课是基本的态度，此外还要在立足上好专业课的同时多读读经典论文，毕竟自己是学术型硕士，科学研究的基本素养与方法论还是要多多去积累。</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>开学一周，结实了身边很多优秀的同学。依稀记得，第一次开班会的时候，大家在做自我介绍，每个人都会说自己的本科毕业院校是哪里。我留心关注了一下，河北大学应该是“倒数第三好”。说实话，本科院校说出来其实是不自信的，在我上台前我也犹豫过、摇摆过，到底要不要说。最终，我还是大大方方、很坦然的向大家做了介绍。此时此刻，彷佛就像刚步入高中，步入沧州市第一中学那个优秀的环境中。但是，唯一不同的是，我的心态和那时有了很大的变化。</p><p>刚从乡镇走出来，步入一中的时候，那时候身边都是比自己优秀的人。说实话，那三年过的非常迷茫和压抑。初中的时候，你在学校是“呼风唤雨的”，但是到了一个更大的环境中、一个竞争更激烈的环境中，你彷佛就迷失了自己。那几年你会发现，你好像无论怎么努力、怎么使劲学都没办法赶超周围的人，你一直在盲目的进行分数的“攀比”。学业学业跟不上、擅长的体育也不再是特长、个人感情不顺利、二叔车祸离世家庭带来创伤…真就想掉入深渊一样，周围全是黑暗，看不到一丝希望与光亮。</p><p>本科的时候，你考了一个相对来说正常发挥的分数，以专业第一的身份进入大学。在前三年没有懈怠，因为不装逼的说，当时确实还是有遗憾。推免完成后，顺利完成学业。这四年你的表现不错，通过积极体育锻炼、综合素质锻炼、遇到了一群好的室友，你的性格、心态有了很大改变。唯一重要的是，这四年，我觉着你一直在<code>找自己</code>。找自己就是客观坦然的面对自己，从容地面对自己的优点和缺点，三人行必有我师焉，你也在时时刻刻向身边优秀的人学习。真诚、勇敢、向上社交。</p><p>好了，话说回来了，来到研究生阶段，又是到了一个更大的环境中、一个竞争更激烈的环境中，彷佛是轮回。我想，此时的你一定不同于高中时期的你，你的心里一定多了几分淡定与从容。就随着这份淡定与从容，继续向前吧！千磨万击还坚劲，任尔东西南北风。走好自己的路，心安即强大。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>雁栖一年，科研三年。前路漫漫，忘君珍重、珍惜。</p><p>​                                                                                                                                                    2024年8月30日于国科大雁栖湖</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DataWhale AI夏令营-LLM实训</title>
    <link href="/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/"/>
    <url>/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/</url>
    
    <content type="html"><![CDATA[<h2 id="day01-baseline搭建"><a href="#day01-baseline搭建" class="headerlink" title="day01-baseline搭建"></a>day01-baseline搭建</h2><div class="note note-success">            <p>首先注册魔塔社区帐号，免费领取魔塔GPU算力资源</p>          </div><p>新建GPU算力环境，下载相关第三方库与拉取镜像资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 拉取git镜像</span></span><br>git lfs install<br>git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 安装第三方库</span></span><br>pip install streamlit==1.24.0<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 启动demo</span></span><br>streamlit run AICamp_yuan_baseline/Task\ 1：零基础玩转源大模型/web_demo_2b.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure><h2 id="day02-RAG原理与实践"><a href="#day02-RAG原理与实践" class="headerlink" title="day02-RAG原理与实践"></a>day02-RAG原理与实践</h2><p>检索增强生成 <strong>(Retrieval Augmented Generation,RAG)</strong> 是一种使用来自私有或专用数据源的信息来辅助文本生成的技术。它将检索模型(设计用于搜索大型数据集或知识库)和生成模型(例如大型语言模型 (LLM))，此类模型会使用检 索到的信息生成可供阅读的文本回复)结合在一起。</p><h3 id="LLM局限性"><a href="#LLM局限性" class="headerlink" title="LLM局限性"></a>LLM局限性</h3><p><a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p><p>这篇文章由来自Facebook AI Research、University College London、New York University三大科研教育机构的12名作者 (Patrick Lewis等)共同完成。文章主要介绍了一种新颖的检索增强生成(RAG)模型，该模型旨在解决预训练语言模型 在知识密集型NLP任务中的局限性，RAG技术被首次提出。</p><p>文章中阐述了传统大模型的局限性:传统的大型预训练模型虽然拥有存储大量事实知识的能力，但在 (query accuracy)和更 (knowledge updates)时存在不足。</p><p>同样，在实际业务场景中，通用的基础大模型可能存在无法满足我们需求的情况，主要有以下几方面原因：</p><ul><li><p>知识局限性：大模型的知识来源于训练数据，而这些数据主要来自于互联网上已经公开的资源，对于一些实时性的或者非公开的，由于大模型没有获取到相关数据，这部分知识也就无法被掌握。</p></li><li><p>数据安全性：为了使得大模型能够具备相应的知识，就需要将数据纳入到训练集进行训练。然而，对于企业来说，数据的安全性至关重要，任何形式的数据泄露都可能对企业构成致命的威胁。</p></li><li><p>大模型幻觉：由于大模型是基于概率统计进行构建的，其输出本质上是一系列数值运算。因此，有时会出现模型“一本正经地胡说八道”的情况，尤其是在大模型不具备的知识或不擅长的场景中。</p></li></ul><h3 id="RAG基本步骤"><a href="#RAG基本步骤" class="headerlink" title="RAG基本步骤"></a>RAG基本步骤</h3><p><img src="/img/post/rag-buzhou.png" alt="基本步骤"></p><ul><li><p>索引：将文档库分割成较短的 <strong>Chunk</strong>，即文本块或文档片段，然后构建成向量索引。</p></li><li><p>检索：计算问题和 Chunks 的相似度，检索出若干个相关的 Chunk。</p></li><li><p>生成：将检索到的Chunks作为背景信息，生成问题的回答。</p></li></ul><h3 id="RAG完整链路图"><a href="#RAG完整链路图" class="headerlink" title="RAG完整链路图"></a>RAG完整链路图</h3><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><p>图片来源:(<a href="https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png">https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png</a>)</p><p>用户进行query查询后，RAG会先进行检索，之后将检索到的 <strong><code>Chunks</code></strong> 和 <strong><code>query</code></strong> 一并输入到大模型，进而回答用户的问题。</p><p>为了完成检索，需要离线将文档（ppt、word、pdf等）经过解析、切割甚至OCR转写，然后进行向量化存入数据库(vector database)中。</p><h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h3><p>知识库中包含了多种类型的文件，如pdf、word、ppt等，这些 <code>文档</code>（Documents）需要提前被解析，然后切割成若干个较短的 <code>Chunk</code>，并且进行清洗和去重。</p><p>然后，我们会将知识库中的所有 <code>Chunk</code> 都转成向量，这一步也称为 <code>向量化</code>（Vectorization）或者 <code>索引</code>（Indexing）。<code>向量化</code> 需要事先构建一个 <code>向量模型</code>（Embedding Model），它的作用就是将一段 <code>Chunk</code> 转成 <code>向量</code>（Embedding）。</p><p>随着新知识的不断存储，向量的数量也会不断增加。这就需要将这些向量存储到 <code>数据库</code> （DataBase）中进行管理。</p><h3 id="在线计算"><a href="#在线计算" class="headerlink" title="在线计算"></a>在线计算</h3><p>在实际使用RAG系统时，当给定一条用户 <code>查询</code>（Query），需要先从知识库中找到所需的知识，这一步称为 <code>检索</code>（Retrieval）。在 <code>检索</code> 过程中，用户查询首先会经过向量模型得到相应的向量，然后与 <code>数据库</code> 中所有 <code>Chunk</code> 的向量计算相似度，最简单的例如  <code>余弦相似度</code>，然后得到最相近的一系列 <code>Chunk</code> 。</p><p>由于向量相似度的计算过程需要一定的时间，尤其是 <code>数据库</code> 非常大的时候。可以在检索之前进行 <code>召回</code>（Recall），即从 <code>数据库</code> 中快速获得大量大概率相关的 <code>Chunk</code>，然后只有这些 <code>Chunk</code> 会参与计算向量相似度。这样，计算的复杂度就从整个知识库降到了非常低。</p><p>随着知识库的增大，除了检索的速度变慢外，检索的效果也会出现退化。这是由于 <code>向量模型</code> 能力有限，而随着知识库的增大，已经超出了其容量，因此准确性就会下降。在这种情况下，相似度最高的结果可能并不是最优的。</p><p>为了解决这一问题，提升RAG效果，研究者提出增加一个二阶段检索——<code>重排</code> (Rerank)，即利用 <code>重排模型</code>（Reranker），使得越相似的结果排名更靠前。这样就能实现准确率稳定增长，即数据越多，效果越好（如上图中紫线所示）。</p><p>通常，为了与 <code>重排</code> 进行区分，一阶段检索有时也被称为 <code>精排</code> 。而在一些更复杂的系统中，在 <code>召回</code> 和 <code>精排</code> 之间还会添加一个 <code>粗排</code> 步骤，这里不再展开，感兴趣的同学可以自行搜索。综上所述，在整个 <code>检索</code> 过程中，计算量的顺序是 <code>召回</code> &gt; <code>精排</code> &gt; <code>重排</code>，而检索效果的顺序则是 <code>召回</code> &lt; <code>精排</code> &lt; <code>重排</code> 。</p><p>至此，一个完整的RAG链路就构建完毕了。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks </p><p>[2] Gao, Yunfan, et al. “Retrieval-augmented generation for large language models: A survey.” <strong>arXiv preprint arXiv:2312.10997</strong> (2023).</p><p>[3] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewriting for retrieval-augmented large language models,” <strong>arXiv preprint arXiv:2305.14283</strong>, 2023.</p><p>[4] QAnything: <a href="https://github.com/netease-youdao/QAnything">https://github.com/netease-youdao/QAnything</a></p><p>[5] When Large Language Models Meet Vector Databases: A Survey <a href="https://doi.org/10.48550/arXiv.2402.01763">https://doi.org/10.48550/arXiv.2402.01763</a> </p><h2 id="RAG技术实践"><a href="#RAG技术实践" class="headerlink" title="RAG技术实践"></a>RAG技术实践</h2><p>前置条件：使用day01搭建好的baseline环境</p><p>下载环境所需的任务包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git lfs install<br>git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-built_in">cp</span> AICamp_yuan_baseline/Task\ 3：源大模型RAG实战/* .<br></code></pre></td></tr></table></figure><p>双击打开<code>Task 3：源大模型RAG实战.ipynb</code>，然后运行所有单元格。</p><p>在环境中安装<code>streamlit</code>,为了后续进行模型微调以及Demo搭建(day01已经安装完毕)。</p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>在RAG实战过程中，需要构建一个向量模型。向量模型通常是一个BERT架构，是一个Transformer Encoder。</p><p>在本次学习中，选用基于BERT架构的向量模型 <code>bge-small-zh-v1.5</code>，它是一个4层的BERT模型，最大输入长度512，输出的向量维度也为512。</p><p>向量模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&quot;AI-ModelScope/bge-small-zh-v1.5&quot;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Yuan大模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>构造向量索引，分装一个向量模型类EmbeddingModel：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EmbeddingModel</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for EmbeddingModel</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(path)<br><br>        <span class="hljs-variable language_">self</span>.model = AutoModel.from_pretrained(path).cuda()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading EmbeddingModel from <span class="hljs-subst">&#123;path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">self, texts: <span class="hljs-type">List</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate embedding for text list</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        encoded_input = <span class="hljs-variable language_">self</span>.tokenizer(texts, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>        encoded_input = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()&#125;<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            model_output = <span class="hljs-variable language_">self</span>.model(**encoded_input)<br>            sentence_embeddings = model_output[<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>]<br>        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> sentence_embeddings.tolist()<br></code></pre></td></tr></table></figure><p>通过传入模型路径，新建一个 <code>EmbeddingModel</code> 对象 <code>embed_model</code>。初始化时自动加载向量模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create embedding model...&quot;</span>)<br>embed_model_path = <span class="hljs-string">&#x27;./AI-ModelScope/bge-small-zh-v1___5&#x27;</span><br>embed_model = EmbeddingModel(embed_model_path)<br></code></pre></td></tr></table></figure><p><code>EmbeddingModel</code> 类还有一个 <code>get_embeddings()</code> 函数，它可以获得输入文本的向量表示。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>为了实现向量检索，定义一个向量库索引类 <code>VectorStoreIndex</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量库索引类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorStoreIndex</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for VectorStoreIndex</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, doecment_path: <span class="hljs-built_in">str</span>, embed_model: EmbeddingModel</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.documents = []<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(doecment_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>):<br>            line = line.strip()<br>            <span class="hljs-variable language_">self</span>.documents.append(line)<br><br>        <span class="hljs-variable language_">self</span>.embed_model = embed_model<br>        <span class="hljs-variable language_">self</span>.vectors = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings(<span class="hljs-variable language_">self</span>.documents)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.documents)&#125;</span> documents for <span class="hljs-subst">&#123;doecment_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarity</span>(<span class="hljs-params">self, vector1: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], vector2: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate cosine similarity between two vectors</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dot_product = np.dot(vector1, vector2)<br>        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> magnitude:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> dot_product / magnitude<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, k: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        question_vector = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings([question])[<span class="hljs-number">0</span>]<br>        result = np.array([<span class="hljs-variable language_">self</span>.get_similarity(question_vector, vector) <span class="hljs-keyword">for</span> vector <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vectors])<br>        <span class="hljs-keyword">return</span> np.array(<span class="hljs-variable language_">self</span>.documents)[result.argsort()[-k:][::-<span class="hljs-number">1</span>]].tolist() <br></code></pre></td></tr></table></figure><p>类似地，通过传入知识库文件路径，新建一个 <code>VectorStoreIndex</code> 对象 <code>index</code>。初始化时会自动读取知识库的内容，然后传入向量模型，获得向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create index...&quot;</span>)<br>doecment_path = <span class="hljs-string">&#x27;./knowledge.txt&#x27;</span><br>index = VectorStoreIndex(doecment_path, embed_model)<br></code></pre></td></tr></table></figure><p>上文提到 <code>get_embeddings()</code> 函数支持一次性传入多条文本，但由于GPU的显存有限，输入的文本不宜太多。</p><p>所以，如果知识库很大，需要将知识库切分成多个batch，然后分批次送入向量模型。</p><p><code>VectorStoreIndex</code> 类还有一个 <code>get_similarity()</code> 函数，它用于计算两个向量之间的相似度，这里采用了余弦相似度。<code>VectorStoreIndex</code> 类的入口，即查询函数 <code>query()</code>。传入用户的提问后，首先会送入向量模型获得其向量表示，然后与知识库中的所有向量计算相似度，最后将 <code>k</code> 个最相似的文档按顺序返回，<code>k</code>默认为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">question = <span class="hljs-string">&#x27;介绍一下广州&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Question:&#x27;</span>, question)<br><br>context = index.query(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Context:&#x27;</span>, context)<br></code></pre></td></tr></table></figure><h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3><p>为了实现基于RAG的生成，我们还需要定义一个大语言模型类 <code>LLM</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义大语言模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLM</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for Yuan2.0 LLM</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat tokenizer...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat model...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading Yuan2.0 model from <span class="hljs-subst">&#123;model_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, context: <span class="hljs-type">List</span></span>):<br>        <span class="hljs-keyword">if</span> context:<br>            prompt = <span class="hljs-string">f&#x27;背景：<span class="hljs-subst">&#123;context&#125;</span>\n问题：<span class="hljs-subst">&#123;question&#125;</span>\n请基于背景，回答问题。&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            prompt = question<br><br>        prompt += <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>        inputs = <span class="hljs-variable language_">self</span>.tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>        outputs = <span class="hljs-variable language_">self</span>.model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">1024</span>)<br>        output = <span class="hljs-variable language_">self</span>.tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>这里我们传入 <code>Yuan2-2B-Mars</code> 的模型路径，新建一个 <code>LLM</code> 对象 <code>llm</code>。初始化时自动加载源大模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create Yuan2.0 LLM...&quot;</span>)<br>model_path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br>llm = LLM(model_path)<br></code></pre></td></tr></table></figure><p><code>LLM</code> 类的入口是生成函数 <code>generate()</code>，它有两个参数：</p><ul><li><code>question</code>: 用户提问，是一个str</li><li><code>context</code>: 检索到的上下文信息，是一个List，默认是[]，代表没有使用RAG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Without RAG:&#x27;</span>)<br>llm.generate(question, [])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; With RAG:&#x27;</span>)<br>llm.generate(question, context)<br></code></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">&gt;</span> <span class="hljs-string">Without RAG:</span><br><span class="hljs-attr">广州大学（Guangzhou</span> <span class="hljs-string">University）是广东省内一所综合性大学，位于中国广东省广州市。广州大学成立于1952年，前身为广州工学院，是中华人民共和国成立后创建的第一所高等工科院校。</span><br><span class="hljs-attr">广州大学坐落在广州市海珠区，占地面积广阔，校园环境优美。学校拥有多个校区，其中主校区位于广州市番禺区，其他校区分布在广州市的其他地区。学校占地面积约4000亩，拥有现代化的教学、实验和生活设施。</span><br><span class="hljs-attr">广州大学以培养人才为宗旨，注重理论与实践相结合的教学模式。学校开设了多个学院和专业，涵盖了工学、理学、文学、法学、经济学、管理学、艺术学等多个领域。学校现有本科专业近300个，研究生专业涵盖科学、工程、管理、文学、法学、艺术等多个领域。</span><br><span class="hljs-attr">广州大学注重国际交流与合作，积极推进国际化办学。学校与许多国际知名大学建立了合作关系，开展学术交流和合作研究。此外，学校还鼓励学生参与国际交流项目，提供海外实习和留学机会，提升学生的国际视野和能力。</span><br><span class="hljs-attr">广州大学一直以来致力于为学生提供优质的教育环境和丰富的学习资源。学校拥有先进的教学设施和实验室，以及图书馆、体育场馆、艺术工作室等丰富的学生课外活动设施。</span><br><span class="hljs-attr">广州大学以其优秀的教学质量、领先的科研水平和培养优秀学生的能力而闻名。学校致力于培养具有创新精神和社会责任感的高素质人才，为地方经济发展和社会进步做出贡献。&lt;eod&gt;</span><br><span class="hljs-attr">&gt;</span> <span class="hljs-string">With RAG:</span><br><span class="hljs-attr">广州大学是一所位于广东省广州市的全日制普通高等学校，实行省市共建、以市为主的办学体制。学校的办学历史可以追溯到1927年创办的私立广州大学，后来在1951年并入华南联合大学。1984年定名为广州大学。2000年，广州大学经过教育部批准，与广州教育学院、广州师范学院、华南建设学院西院、广州高等师范专科学校合并组建新的广州大学。&lt;eod&gt;</span><br></code></pre></td></tr></table></figure><h2 id="day03-微调技术原理与实践"><a href="#day03-微调技术原理与实践" class="headerlink" title="day03-微调技术原理与实践"></a>day03-微调技术原理与实践</h2><p>模型微调也被称为指令微调（Instruction Tuning）或者有监督微调（Supervised Fine-tuning, SFT），该方法利用成对的任务输入与预期输出数据，训练模型学会以问答的形式解答问题，从而解锁其任务解决潜能。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p><p>指令微调并非无中生有地传授新知，而是更多地扮演着催化剂的角色，激活模型内在的潜在能力，而非单纯地灌输信息。</p><p>相较于预训练所需的海量数据，指令微调所需数据量显著减少，从几十万到上百万条不等的数据，均可有效激发模型的通用任务解决能力。</p><h3 id="轻量化微调"><a href="#轻量化微调" class="headerlink" title="轻量化微调"></a>轻量化微调</h3><p>由于大模型的参数量巨大， 进行全量参数微调需要消耗非常多的算力。为了解决这一问题，研究者提出了参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning），这些方法通过训练极少的模型参数，同时保证微调后的模型表现可以与全量微调相媲美。</p><p>常用的轻量化微调技术有<code>LoRA</code>、<code>Adapter</code> 和 <code>Prompt Tuning</code>。</p><p>LoRA:<a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a></p><p>大模型轻量级微调（LoRA）：训练速度、显存占用分析:<a href="https://zhuanlan.zhihu.com/p/666000885">https://zhuanlan.zhihu.com/p/666000885</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#模型下载</span><br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br><span class="hljs-comment"># 导入环境</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer<br><span class="hljs-comment"># 读取数据</span><br>df = pd.read_json(<span class="hljs-string">&#x27;./data.json&#x27;</span>)<br>ds = Dataset.from_pandas(df)<br><span class="hljs-comment"># 加载 tokenizer</span><br>path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br>tokenizer.pad_token = tokenizer.eos_token<br><span class="hljs-comment"># 定义数据处理函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_func</span>(<span class="hljs-params">example</span>):<br>    MAX_LENGTH = <span class="hljs-number">384</span>    <span class="hljs-comment"># Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性</span><br><br>    instruction = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;input&#x27;</span>]&#125;</span>&lt;sep&gt;&quot;</span>)<br>    response = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&lt;eod&gt;&quot;</span>)<br>    input_ids = instruction[<span class="hljs-string">&quot;input_ids&quot;</span>] + response[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>    attention_mask = [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(input_ids) <br>    labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(instruction[<span class="hljs-string">&quot;input_ids&quot;</span>]) + response[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-comment"># instruction 不计算loss</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(input_ids) &gt; MAX_LENGTH:  <span class="hljs-comment"># 做一个截断</span><br>        input_ids = input_ids[:MAX_LENGTH]<br>        attention_mask = attention_mask[:MAX_LENGTH]<br>        labels = labels[:MAX_LENGTH]<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids,<br>        <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>        <span class="hljs-string">&quot;labels&quot;</span>: labels<br>    &#125;<br><span class="hljs-comment"># 处理数据集</span><br>tokenized_id = ds.<span class="hljs-built_in">map</span>(process_func, remove_columns=ds.column_names)<br>tokenized_id<br><span class="hljs-comment"># 数据检查</span><br>tokenizer.decode(tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>tokenizer.decode(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x != -<span class="hljs-number">100</span>, tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;labels&quot;</span>])))<br><span class="hljs-comment"># 模型加载</span><br>model = AutoModelForCausalLM.from_pretrained(path, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>)<br>model<br>model.enable_input_require_grads() <span class="hljs-comment"># 开启gradient_checkpointing时，要执行该方法</span><br><span class="hljs-comment"># 配置Lora</span><br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model<br><br>config = LoraConfig(<br>    task_type=TaskType.CAUSAL_LM, <br>    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>, <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],<br>    inference_mode=<span class="hljs-literal">False</span>, <span class="hljs-comment"># 训练模式</span><br>    r=<span class="hljs-number">8</span>, <span class="hljs-comment"># Lora 秩</span><br>    lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># Lora alaph，具体作用参见 Lora 原理</span><br>    lora_dropout=<span class="hljs-number">0.1</span><span class="hljs-comment"># Dropout 比例</span><br>)<br>config<br><span class="hljs-comment"># 构建PeftModel</span><br>model = get_peft_model(model, config)<br>model<br><span class="hljs-comment"># 设置训练参数</span><br>args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./output/Yuan2.0-2B_lora_bf16&quot;</span>,<br>    per_device_train_batch_size=<span class="hljs-number">12</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">1</span>,<br>    logging_steps=<span class="hljs-number">1</span>,<br>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    num_train_epochs=<span class="hljs-number">3</span>,<br>    learning_rate=<span class="hljs-number">5e-5</span>,<br>    save_on_each_node=<span class="hljs-literal">True</span>,<br>    gradient_checkpointing=<span class="hljs-literal">True</span>,<br>    bf16=<span class="hljs-literal">True</span><br>)<br><span class="hljs-comment"># 初始化Trainer</span><br>trainer = Trainer(<br>    model=model,<br>    args=args,<br>    train_dataset=tokenized_id,<br>    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="hljs-literal">True</span>),<br>)<br><span class="hljs-comment"># 模型训练</span><br>trainer.train()<br><span class="hljs-comment"># 定义生成函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">prompt</span>):<br>    prompt = prompt + <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>    inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>    outputs = model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">256</span>)<br>    output = tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br>    <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br>input_str = <span class="hljs-string">&#x27;张三，汉族，金融学硕士。&#x27;</span><br>prompt = template.replace(<span class="hljs-string">&#x27;input_str&#x27;</span>, input_str).strip()<br>generate(prompt)<br>&#123;<span class="hljs-string">&quot;姓名&quot;</span>: [<span class="hljs-string">&quot;张三&quot;</span>], <span class="hljs-string">&quot;国籍&quot;</span>: [<span class="hljs-string">&quot;汉族&quot;</span>], <span class="hljs-string">&quot;职位&quot;</span>: [<span class="hljs-string">&quot;金融学硕士&quot;</span>]&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型应用开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>部署</tag>
      
      <tag>微调技术</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据技术学习笔记</title>
    <link href="/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="大数据技术路线总揽"><a href="#大数据技术路线总揽" class="headerlink" title="大数据技术路线总揽"></a>大数据技术路线总揽</h2><h2 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h2><h3 id="Google-files-system"><a href="#Google-files-system" class="headerlink" title="Google files system"></a>Google files system</h3><p>GFS直接以Linux为基础存储层，并且设计模式为单master模式。</p><p>另外一方面，GFS 还是采用了 Checkpoints、操作日志（Operation Logs）、影子Master（Shadow Master）等一系列的工程手段，来尽可能地保障整个系统的“可恢复（Recoverable）”，以及读层面的“可用性（Availability）”。</p>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>数仓</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习基础</title>
    <link href="/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="Machine-Learning-Notebook"><a href="#Machine-Learning-Notebook" class="headerlink" title="Machine Learning-Notebook"></a>Machine Learning-Notebook</h1><center>Andrew Ng-吴恩达&copy;</center><center>Standford ONLINE & DeepLearning.AI</center><center>Mungeryang-杨桂淼总结</center><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>“Field of study that gives computers the ability to learn without being explicitly programmed.”——Arthur Samuel(1995)</p><p>  Practical advice for applying learning algorithm</p><h2 id="Basic-conception-cookbook"><a href="#Basic-conception-cookbook" class="headerlink" title="Basic conception cookbook"></a>Basic conception cookbook</h2><p>Data set:</p><p>Training set:</p><p>Test set:</p><p>Cost function:</p><p>Gradient:</p><p>Gradient descent:</p><p>Recall:</p><p>Precision:</p><h2 id="Supervised-learning-监督学习"><a href="#Supervised-learning-监督学习" class="headerlink" title="Supervised learning-监督学习"></a>Supervised learning-监督学习</h2><p> Learns from being given “<strong>right answers</strong>(labels)”</p><img src="/img/fig/1.1.png" alt="s" style="text-align: center;"/><p>In all of these applications, we will first train our model with examples of inputs <strong>x</strong> and right answers, that is the labels <strong>y</strong>. After the model has learned from these input, output, or x and y pairs, they can take a brand new input x, something it has never seen before, and try to produce the appropriate corresponding output y.  </p><img src="/img/fig/1.2.jpg" alt="s" style="text-align: center;" /><p>  The task of the supervised learning algorithm is to produce more of these right answers based on labels.  </p><p>Classification is to predict categories,Regression is to predict a number. </p><img src="/img/fig/1.3.png" alt="s" style="text-align: center;" /><h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning-无监督学习"></a>Unsupervised learning-无监督学习</h2><p>Learns from being given “<strong>no-right answers</strong>(unlabeled data)”, data only comes with inputs x, but not output labels y. Algorithm has to find structure automatically in the data and automatically figure out whether the major types of individuals.</p><img src="/img/fig/1.4.png" style="text-align: center;" /><h2 id="Linear-Regression-with-one-variable"><a href="#Linear-Regression-with-one-variable" class="headerlink" title="Linear Regression with one variable"></a>Linear Regression with one variable</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Linear regression means fitting a <code>straight line</code> to the data. -&gt;linear regression  </p><img src="/img/fig/2.1.jpg" alt="s" style="text-align: center;"/><p>Regression model is to predict numbers</p><p>Classification model is to predicts categories</p><h3 id="Terminology-in-ML"><a href="#Terminology-in-ML" class="headerlink" title="Terminology in ML"></a><strong>Terminology in ML</strong></h3><p><code>Training dataset</code>-&gt;data used to train the model</p><p>$x&#x3D;$“input”variables feature</p><p>$y&#x3D;$“output ”variables or “target”variables</p><p>$(x,y)&#x3D;$single training example</p><p>$(x^{(i)},y^{(i)})&#x3D;i^{th}$ training example not exponent</p><img src="/img/fig/2.2.jpg" alt="s" style="text-align: center;"/><p>In the linear regression, we instantly believe the function is a linear function as follow:<br>$$<br>f_{w,b}(X)&#x3D;wX+b \<br>f(X)&#x3D;wX+b<br>$$<br>when we give a “x”as a input variable, we can get a “y-hat”variable as a result.</p><h3 id="triangular-ruler-Cost-function"><a href="#triangular-ruler-Cost-function" class="headerlink" title=":triangular_ruler:Cost function"></a>:triangular_ruler:Cost function</h3><p>squared error<br>$$<br>\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2<br>$$<br><img src="/img/fig/2.3.jpg" alt="s" style="text-align: center;" /></p><p>Model: $f_{w,b}(X)&#x3D;wX+b$.$w(slope),b(intersects)$ are parameters.<br>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{w.b}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2 \<br>&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2<br>$$<br>Our goal is to minimize the parameters to fit the model:<br>$$<br>\underset{w.b}{\min}J\left( w,b \right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#define cost function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    cost_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i]) ** <span class="hljs-number">2</span><br>        cost_sum += cost<br>    total_cost = (<span class="hljs-number">1</span>/(<span class="hljs-number">2</span>*m))*cost_sum<br>    <span class="hljs-keyword">return</span> total_cost<br></code></pre></td></tr></table></figure><h3 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent-梯度下降"></a>Gradient descent-梯度下降</h3><p><strong>Simultaneous update</strong> the parameters w and b until the cost function is <code>convergence</code>:</p><p>Simultaneous update the parameters is significant, we must focus on the order about the algorithm.</p><p>Correct order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>\<br>w&#x3D;tmp_w<br>\<br>b&#x3D;tmp_b<br>$$<br>Incorrect order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right) \<br>w&#x3D;tmp_w \<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right) \<br>b&#x3D;tmp_b \<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#compute gradient</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradient</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_dw = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_dw += dj_dw_i<br>        dj_db += dj_db_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></td></tr></table></figure><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>The choice of learning rate, alpha($\alpha$) will have a huge impact on the efficiency of our implementation of Gradient descent.<br>$$<br>w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>$$<br><img src="/img/fig/2.6.jpg" alt="s" style="text-align: center;" /></p><p>If we chose the alpha is too large, the fit efficiency is not very well. We can design the algorithm to decrease the alpha($\alpha$) following by the w and b.</p><img src="/img/fig/cost1.png" alt="s" style="text-align: center;" /><h2 id="Multiple-linear-regression"><a href="#Multiple-linear-regression" class="headerlink" title="Multiple linear regression"></a>Multiple linear regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>【Model】<br>$$<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n<br>$$<br><code>Dot product(inner product)</code> of two vectors about $w$ and $b$.<br>$$<br>\vec{w}&#x3D;\left[ w_1,w_2···,w_n \right]<br>\<br>\vec{x}&#x3D;\left[ x_1,x_2···,x_n \right]<br>$$<br>$x_j&#x3D;j^{th} features$</p><p>$n &#x3D; $ numbers of features</p><p>$\vec{x}^{(i)}$ &#x3D; features of $i^{th}$ training example</p><p>$\vec{x}^{(i)}_j$ &#x3D; value of feature j in  $i^{th}$ training example</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>We contrast the two process between vectorization and without vectorization.</p><p>without vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n):<br>    f = f + w[i]*x[i]<br>f = f + b<br></code></pre></td></tr></table></figure><p>If n is infinite, the algorithm is time consuming.</p><p>with vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>f = np.dot(w,b)<br></code></pre></td></tr></table></figure><p> Without vectorization, we just only use the for loop to calculate the results one by one. In the contrast, we can use the function numpy.dot() to calculate the result. Using numpy can decrease the time complexity and improve the algorithm efficiency. Numpy can use <code>parallel process</code> hardware to carry out the data.</p><p>Dot product of two vectors:<br>$$<br>a·b&#x3D;\underset{i&#x3D;0}{\overset{n-1}{\varSigma}}a_ib_i&#x3D;\left[ \begin{array}{l}<br>    ,, a_0\<br>    ,, a_1\<br>    ,,···\<br>    a_{n-1}\<br>\end{array} \right] \left[ \begin{array}{l}<br>    ,, b_0\<br>    ,, b_1\<br>    ,,···\<br>    b_{n-1}\<br>\end{array} \right] &#x3D;\left[ a_0b_0+a_1b_1+···+a_{n-1}b_{n-1} \right]<br>$$</p><h3 id="Gradient-descent-in-Multiple-regression"><a href="#Gradient-descent-in-Multiple-regression" class="headerlink" title="Gradient descent in Multiple regression"></a>Gradient descent in Multiple regression</h3><p>Parameters include $w_{1},w_{2},···w_{n}$ and $b$</p><p>Model is $f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n+b$</p><p>Cost function is $J\left( w_{1},w_{2},···w_{n},b \right)$</p><p>Gradient descent:</p><pre><code class="hljs">repeat&#123;</code></pre><p>$$<br>w_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>}</p><p><code>for i in range(1,n)</code>:<br>$$<br>\frac{\partial}{\partial w_{1}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{1}^{\left( i \right)} \<br>\frac{\partial}{\partial w_{n}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{n}^{\left( i \right)} \<br>\frac{\partial}{\partial b}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right)<br>$$<br>Normal equation:</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations</li></ul><p>Disadvantages:</p><ul><li>Does not generalize to other learning algorithm</li><li>Slow when number of features is large(n&gt;10000)</li></ul><h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The technology of Features scaling will enable Gradient descent to run much faster.</p><p>Z-score normalization:<br>$$<br>x_i&#x3D;\frac{x_i-\mu}{\sigma}<br>$$<br>$\mu$ is the sample average value, and $\sigma$ is standard error of sample.</p><p>Aim for about $-1\leqslant x_j\leqslant 1$ for $x_{j}$.</p><p>The objective of Gradient descent in Multiple regression is:<br>$$<br>\underset{\vec{w}.b}{\min},,J\left(\vec{w},b \right)<br>$$<br><img src="/img/fig/2.7.jpg" alt="s" style="text-align: center;" /></p><p>As the calculate process, $y&#x3D;J\left(\vec{w},b \right)$ is decreased until a low score. If we can get the similar the result in the experiment, the cost function is <code>convergence</code> and we can find the min $J\left(\vec{w},b \right)$.</p><p>【<strong>Skill</strong>】</p><p>At the beginning, we can set the learning rate($\alpha$) in a small value like 0.001. As running the algorithm, we can update the parameter $\alpha$ gradually.(0.001-&gt;0.01-&gt;0.1)</p><p><strong>Feature Engineering</strong> : Using <code>intuition</code> to design new features, by transforming or combining original features.</p><blockquote><p>[Wiki]:<strong>Feature engineering</strong> or <strong>feature extraction</strong> or <strong>feature discovery</strong> is the process of extracting features (characteristics, properties, attributes) from raw data.</p></blockquote><img src="/img/fig/2.8.jpg" style="text-align: center;" /><p>Except for using linear regression, we can also use <code>polynomial regression</code> to fit data. Through finding the segment of the data distribution(scatter), we can create special features($\sqrt{x}\quad x^3$) to fit data successfully.<br>$$<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+b\quad②<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+b\quad①<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2\sqrt{x}+b\quad③<br>$$</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>It turns out that, linear regression is not the good algorithm for classification. For classification, the output is not a continuous number. In the fact, it is a class variable like <strong>0(false-negative class)</strong> or <strong>1(true-positive class)</strong>.</p><img src="/img/fig/3.1.jpg" alt="s" style="text-align: center;" /><p>Sometimes, if we want to classify the output, the method of linear regression maybe not fit. Through the figure, the result of predict can be changed when we add a sample. The original fit result is the blue line, but, the new result is the green line. The standard of classification can be changed as the sample we adding.</p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>In this chapter, we will learn a useful algorithm model——<code>Logistic Regression</code> to solve the classification problem.</p><p>With linear regression method, the model is $f_{\vec{w},b}\left(x^{i} \right) &#x3D;\vec{w}·x^{i}+b$, to predict $y$ given $x$. However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</p><p>This can be accomplished by using a <code>&quot;sigmoid function&quot;</code> which maps all input values to values between 0 and 1.</p><p>【sigmoid function】</p><img src="/img/fig/3.2.jpg" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;\frac{1}{1+e^{-Z}},0&lt;g\left( Z \right) &lt;1<br>\<br>f_{w,b}\left( X^{\left( i \right)} \right) &#x3D;g\left( w·X^{\left( i \right)}+b \right)<br>\<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;g\left( \vec{w}·\vec{x}+b \right) &#x3D;\frac{1}{1+e^{-\left( \vec{w}·\vec{x}+b \right)}}<br>$$</p><p>We can calculate the Z-score to classify the predict value by the probability.</p><p>$$<br>P\left( y&#x3D;0 \right) +P\left( y&#x3D;1 \right) &#x3D;1<br>$$</p><img src="/img/fig/3.3.jpg" style="text-align: center;"/><p>$$<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;1|\vec{X};\vec{w},b \right)<br>\<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;0|\vec{X};\vec{w},b \right)<br>$$</p><p>Probability that $y&#x3D;1$  or $y&#x3D;0$, given input $\vec{x}$, parameters $w,b$.</p><p>If $y&#x3D;f_{\vec{w},b}\left( \vec{X} \right) &gt; 0.5$ -&gt; $g(Z)&gt;0.5$ -&gt; $Z&gt;0$ -&gt; $\vec{w}·\vec{x}+b &gt; 0$ &#x3D;&#x3D; YES $Y&#x3D;1$.</p><p>Else, NO $y &#x3D; 0$.</p><h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><img src="/img/fig/3.4.jpg" alt="s" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+b \right)<br>\<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b \right)\<br>boundary1&#x3D;w_1x_1+w_2x_2+b&#x3D;0<br>\<br>boundary2&#x3D;w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b&#x3D;0<br>$$</p><h3 id="Cost-function-for-regularized-logistic-regression"><a href="#Cost-function-for-regularized-logistic-regression" class="headerlink" title="Cost function for regularized logistic regression"></a>Cost function for regularized logistic regression</h3><p>Cost function:</p><p>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\frac{1}{2}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2&#x3D;L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right)<br>$$</p><p>Our goal is to minimize the parameters to fit the model:</p><p>$$<br>\underset{w.b}{\min},,J\left( w,b \right)<br>$$</p><p><strong>Simplified</strong> loss function:</p><p>$$<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;\begin{cases}<br>    -\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;1\<br>    -\log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;0\<br>\end{cases}\ \Longrightarrow \<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;-y^{\left( i \right)}\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) -\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right)<br>$$</p><p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}  \sum_{i&#x3D;0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2<br>$$</p><p>where:</p><p>$$<br>f_{\mathbf{w},b}(\mathbf{x}^{(i)}) &#x3D; sigmod(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)<br>$$</p><p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}\sum_{i&#x3D;0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]<br>$$</p><p>As was the case in linear regression above, the difference is the regularization term, which is   $\frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2$ </p><p>Gradient descent:</p><p>repeat{</p><p>$$<br>tw_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>tb&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>w&#x3D;tw_{j}\<br>b&#x3D;tb<br>$$</p><p>}</p><img src="/img/fig/gridant1.png" style="text-align: center;" /><p>To provide the overfitting problem, we apply the regularized method to add the penalty coefficient. That is why the we add the $\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}$ at the end of the formula.</p><img src="/img/fig/3.5.png" alt="s" style="text-align: center;" /><p><strong>Regularized</strong>:</p><p>$$<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}<br>\<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}b_{j}^{2}<br>$$</p><p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$.</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;&#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\ \<br>$$</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial b}  &#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})<br>$$</p><ul><li><p>m is the number of training examples in the data set      </p></li><li><p>$f_{\mathbf{w},b}(x^{(i)})$ is the model’s prediction, while $y^{(i)}$ is the target</p></li><li><p>For a  <span style="color:blue"> <strong>linear</strong> </span> regression model<br>  $f_{\mathbf{w},b}(x) &#x3D; \mathbf{w} \cdot \mathbf{x} + b$  </p></li><li><p>For a <span style="color:blue"> <strong>logistic</strong> </span> regression model<br>  $z &#x3D; \mathbf{w} \cdot \mathbf{x} + b$ \ $f_{\mathbf{w},b}(x) &#x3D; g(z)$<br>  where $g(z)$ is the sigmoid function:</p></li></ul><p>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#cpmpute cost function through itertion process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function</span>):<br>    w = copy.deepcopy(w_in)<br>    J_history = []<br>    p_history = []<br>    b = b_in <br>    w = w_in<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw,dj_db = gradient_function(x,y,w,b)<br>        <span class="hljs-comment"># update the parameters </span><br>        b = b - alpha * dj_db<br>        w = w - alpha * dj_dw<br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">100000</span>:<br>            J_history.append(cost_function(x,y,w,b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i % math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteraton <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> w,b,J_history,p_history<br></code></pre></td></tr></table></figure><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><h3 id="How-to-install-TensorFlow"><a href="#How-to-install-TensorFlow" class="headerlink" title="How to install TensorFlow"></a>How to install TensorFlow</h3><ul><li>conda install tensorflow</li><li>pip install tensorflow</li></ul><h3 id="How-the-brain-works"><a href="#How-the-brain-works" class="headerlink" title="How the brain works"></a>How the brain works</h3><p>The following picture shows the basic structure about neutral network. Like human&#96;s neutral cell, it passes information by layers, which every cell includes a logistic function.</p><img src="/img/fig/4.2.jpg" style="text-align: center;" /><h3 id="Data-format-in-Tensorflow"><a href="#Data-format-in-Tensorflow" class="headerlink" title="Data format in Tensorflow"></a>Data format in Tensorflow</h3><p><code>matrix and tensor</code></p><p>Numpy, a standard library created in 1970s, is used to calculate linear algebra in python(data analysis). Tensorflow is a machine learning framework created by Google.</p><p>In Tensorflow, the input format must <strong>a matrix</strong>. We should focus on the special characteristic in our work.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#一维数组</span><br>b = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<span class="hljs-comment">#一维矩阵</span><br>x = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">14</span>],<br>])<span class="hljs-comment">#3X3矩阵</span><br>Z = np.matmul(A_in,W) + B <span class="hljs-comment">#input matrix to simplify for loop</span><br></code></pre></td></tr></table></figure><h3 id="Build-a-Tensorflow"><a href="#Build-a-Tensorflow" class="headerlink" title="Build a Tensorflow"></a>Build a Tensorflow</h3><ol><li>build the structure of the model</li><li>compile the model</li><li>input training data</li><li>fit the model</li><li>predict the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span><br>layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_3 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1,layer_2,...layer_n])<br>----------------------------<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>])<br><span class="hljs-number">2.</span><br>model.<span class="hljs-built_in">compile</span>()<br><span class="hljs-number">3.</span><br>x = np.array([[<span class="hljs-number">0.</span>..,<span class="hljs-number">245</span>,...,<span class="hljs-number">17</span>],[<span class="hljs-number">0.</span>..,<span class="hljs-number">200</span>,...,<span class="hljs-number">284</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>])<br><span class="hljs-number">4.</span><br>model.fit<br><span class="hljs-number">5.</span><br>model.predict(x_new)<br></code></pre></td></tr></table></figure><h3 id="Implementation-of-the-preceding-communication"><a href="#Implementation-of-the-preceding-communication" class="headerlink" title="Implementation of the preceding communication"></a>Implementation of the preceding communication</h3><img src="/img/fig/4.4.png" alt="s" style="text-align: center;"/>$$\vec{w}_{1}^{\left[ 1 \right]}=\left[ \begin{array}{c}    1\\    2\\\end{array} \right] ,\vec{w}_{2}^{\left[ 1 \right]}=\left[ \begin{array}{c}    -3\\    4\\\end{array} \right] ,\vec{w}_{3}^{\left[ 1 \right]}=\left[ \begin{array}{c}    5\\    -6\\\end{array} \right] $$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([<br>    [<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,<span class="hljs-number">5</span>],<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,-<span class="hljs-number">6</span>]<br>]) <br>b = np.array([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>a_in = np.array([-<span class="hljs-number">2</span>,<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>Implement the coding of dense function with python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in,W,b,g</span>):<br>    <span class="hljs-comment">#units equals to the numbers of cols of W</span><br>    units = W.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment">#Create a matrix with the same size of units--[0,0,0]</span><br>    a_out = np.zeros(units)<br>    <span class="hljs-comment">#compete the g(z)</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x,W1,b1)<br>    a2 = dense(a1,W2,b2)<br>    a3 = desne(a2,W3,b3)<br>    a4 = dense(a3,W4,b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br>    <br>    <br></code></pre></td></tr></table></figure><h3 id="Choose-activation-function"><a href="#Choose-activation-function" class="headerlink" title="Choose activation function"></a>Choose activation function</h3><p>Three types activation function:</p><img src="/img/fig/4.3.jpg" alt="s" style="text-align: center;" /><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">activation=<span class="hljs-string">&quot;linear&quot;</span><br>activation=<span class="hljs-string">&quot;sigmoid&quot;</span><br>activation=<span class="hljs-string">&quot;relu&quot;</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>type y</th><th>0&#x2F;1</th><th>+&#x2F;-</th><th>0&#x2F;+</th></tr></thead><tbody><tr><td>sigmoid</td><td>binary classfication</td><td></td><td></td></tr><tr><td>linear</td><td></td><td>regression</td><td></td></tr><tr><td>relu</td><td></td><td></td><td>regression</td></tr></tbody></table><p>[why we need activation function?]</p><p>If we use the linear activation function in  hidden layers all the time, the predict results of neutral network equal to linear regression. that&#96;s why we should use sigmoid or relu function as the activation function to build the network.</p><h3 id="Soft-max-regression"><a href="#Soft-max-regression" class="headerlink" title="Soft-max regression"></a>Soft-max regression</h3><p>Softmax function has been used to solve the  multi-class problem. Essentially, it is still a <strong>classification problem</strong> <code>based on probability</code>. The expression of Softmax function as follow:<br>$$<br>z_j&#x3D;\vec{w}_j·\vec{x}+b_j\<br>a_j&#x3D;\frac{e^{z_j}}{\underset{k&#x3D;1}{\overset{n}{\varSigma}}e^{z_k}}&#x3D;P\left( y&#x3D;j|\vec{x} \right)<br>$$<br>a_j is the possibility of predict result.</p><p>The cost function of Soft-max function is:<br>$$<br>a_N&#x3D;\frac{e^{Z_N}}{e^{Z_1}+e^{Z_2}+\cdot \cdot \cdot +e^{Z_N}}&#x3D;P\left( y&#x3D;N|\vec{x} \right)<br>$$</p><p>$$<br>loss\left( a_1,…,a_N,y \right) &#x3D;\begin{cases}<br>    -\log a_1,,if,,y&#x3D;1\<br>    -\log a_2,,if,,y&#x3D;2\<br>    \vdots\<br>    -\log a_n,,if,,y&#x3D;n\<br>\end{cases}<br>$$<br>The result is <code>one of</code> the loss functions. </p><h3 id="Output-of-the-soft-max"><a href="#Output-of-the-soft-max" class="headerlink" title="Output of the soft-max"></a>Output of the soft-max</h3><img src="/img/fig/4.5.jpg" style="text-align: center;" /><p>The outcome of soft-max classification is multiply. Every outcome will be competed a score to predict the right answer. </p><p>Carefully, the loss function we must choose the <code>SparseCategoricalCrossentropy</code>.<br>$$<br>a_{N}^{\left[ l \right]}&#x3D;\frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>\<br>-\log a_{N}^{\left[ l \right]}\ne -\log \frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>$$<br>The difference of competing order can lead to the different outcome, which has different model accurancy.</p><h3 id="Improve-the-soft-max-model"><a href="#Improve-the-soft-max-model" class="headerlink" title="Improve the soft-max model"></a>Improve the soft-max model</h3><p>In order to improve the accuracy of calculations, we have made the following improvements to the model algorithm:</p><ul><li>In soft-max layer, we adopt the <code>linear</code> function as the activation.</li><li>In the process of compiling model, we add a parameter to improve the accurancy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#linear function as the activation function in the soft-max layer</span><br>model = sequential([<br>    Dense(units = <span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>)<br>])<br><span class="hljs-comment">#add parameter:from_logits=True </span><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br>logit = model(X)<br>f_x = tf.nn.sigmoid(logit)<br></code></pre></td></tr></table></figure><h3 id="MNIST-Adam"><a href="#MNIST-Adam" class="headerlink" title="MNIST Adam"></a>MNIST Adam</h3><h2 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h2><h3 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h3><p>Model fits the training data well(over-fit) but will fail to generalize to new examples not in the training set.</p><p>Hence, we need to <strong>partition the dataset</strong> to test the accurancy of the model.</p><p>The data set was divided into <code>test set</code> and <code>train set</code>.</p><p>【regression】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m_{train}}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{test}}\left[ \underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) -y</em>{test}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>compute train error:</p><p>$$<br>J_{train}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{train}}\left[ \underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) -y</em>{train}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>【train】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;-\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left[ y^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) \right] +\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2}<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}&#x3D;-\frac{1}{m_{test}}\underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left[ y_{test}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) \right)<br>+\left( 1-y</em>{test}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{test}^{\left( i \right)} \right) \right) \right]<br>$$</p><p>compute train error:</p><p>$$<br>-\frac{1}{m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left[ y_{train}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) \right) +\left( 1-y</em>{train}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{train}^{\left( i \right)} \right) \right) \right]<br>$$</p><h3 id="Model-selection-and-cross-validation"><a href="#Model-selection-and-cross-validation" class="headerlink" title="Model selection and cross validation"></a>Model selection and cross validation</h3><p>If we want to fit a function to predict a problem or classification, we often use test error $J_{test}$ to judge the accurancy of the model. But, the J test is likely to be an <code>optimistic estimate</code> of generalization error. Because, when we choose the degree of parameter d in polynomial fit.,This fit of J test may lower than the actual estimate. The optimistic estimate can lead to a low score of J_test.</p><p>So, we need to partition the dataset as three parts to avoid optimistic estimate:</p><ul><li>training set</li><li>cross validation set</li><li>test set</li></ul><p>compute cross validation error:</p><p>$$<br>J_{cv}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{cv}}\left[ \underset{i&#x3D;1}{\overset{m_{cv}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{cv}^{\left( i \right)} \right) -y</em>{cv}^{\left( i \right)} \right) ^2 \right]<br>$$</p><h3 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h3><p>Have idea-Train model-Diagnose bias and variance</p><p> $J_{train}$ reflects bias, and  $J_{cv}$ reflects variance. A perfect model has low  $J_{train}$ and low  $J_{cv}$.</p><p>As the increasing of degree of d, $J_{train}$ will typically go down. Meanwhile, $J_{cv}$ will also go down and then it will increase.$J_{cv}$ will have a min value for different degree of d.</p><img src="/img/fig/4.6.jpg" alt="s" style="text-align: center;" /><p>How do you tell if our algorithm has a bias or variance problem?</p><ul><li>High bias(under fit): $J_{train}$ will be high($J_{train}\approx J_{cv}$)</li><li>High variance(over fit): $J_{cv}$&gt;&gt;$J_{train}$($J_{train}$ may be low)</li><li>High bias and High variance $J_{train}$ will be high and $J_{cv}$&gt;&gt;$J_{train}$</li></ul><h3 id="Regularization-and-bias-variance"><a href="#Regularization-and-bias-variance" class="headerlink" title="Regularization and bias&#x2F;variance"></a>Regularization and bias&#x2F;variance</h3><p>A large neutral network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_1 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&quot;sigmoid&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>model = Sequential([layer_1,layer_2,layer_3])<br></code></pre></td></tr></table></figure><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>When we increase the size of training set, the train error will increase. But, the error of cross validation will decrease. As the increasing of sample points. a regression function(a line or a curve) cannot fit all the point.</p><img src="/img/fig/4.7.jpg" style="text-align: center;" /><p>If a algorithm suffers from high variance, getting more training data is <strong>likely</strong> to help.</p><p>If a algorithm suffers from high bias, getting more training data <strong>will not</strong> help much.</p><p>【Debugging algorithm】</p><p>we have implemented the regularized linear regression:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><table><thead><tr><th align="center">operation</th><th>what should do</th></tr></thead><tbody><tr><td align="center">get more training examples</td><td>fixes high variance</td></tr><tr><td align="center">try smaller sets of features</td><td>fixes high variance</td></tr><tr><td align="center">try getting additional features</td><td>fixes high bias</td></tr><tr><td align="center">try adding polynomial features</td><td>fixes high bias</td></tr><tr><td align="center">try decreasing  $\lambda$</td><td>fixes high bias</td></tr><tr><td align="center">try increasing  $\lambda$</td><td>fixes high variance</td></tr></tbody></table><p><code>Trade-off</code></p><p>Simple model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+b$) will get high bias VS complex model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+w_3x^3+w_4x^4+b$) will get high variance.</p><img src="/img/fig/4.8.jpg" style="text-align: center;" /><h3 id="Cycle-of-machine-learning"><a href="#Cycle-of-machine-learning" class="headerlink" title="Cycle of machine learning"></a>Cycle of machine learning</h3><p>The cycle of ML process:</p><img src="/img/fig/4.9.jpg" style="text-align: center;" /><img src="/img/fig/4.10.jpg" style="text-align: center;" /><p>How to apply the ML model to solve the actual problem in software engineering design?</p><img src="/img/fig/4.11.jpg" alt="s" style="text-align: center;" /><p>ML model is collected in the inference server. we use mobile app through API call to achieve these function.</p><h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><strong>Precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.</p><p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that were retrieved. </p><p>We design a <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><p>In the trade-off  between Precision(P) and Recall(R), we use F1 score to evaluate the efficiency about the model.</p><p>the trade-off  between Precision(P) and Recall(R) has shown in the figure:</p><img src="/img/fig/4.13.jpg" style="text-align: center;" /><p>$$<br>F1&#x3D;\frac{1}{\frac{1}{2}\left( \frac{1}{P}+\frac{1}{R} \right)}&#x3D;\frac{2PR}{P+R}<br>$$</p><h2 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h2><p>The structure of a decision tree:</p><img src="/img/fig/5.1.jpg" style="text-align: center;" /><h3 id="Methods-chosen"><a href="#Methods-chosen" class="headerlink" title="Methods chosen"></a>Methods chosen</h3><p>For Signal Decision tree, we should focus on the problem is that the data features.</p><p>If the data is  discrete(just like 0 or 1), we can build the Signal Decision tree model. But,a row data may includes more than two classes, in this situation we should use <code>one-hot encoding</code>.</p><blockquote><p>one-hot encoding only fit for the decision tree model.</p></blockquote><p>If the data has continuous data(not only just like 0 or 1), we should split on a continuous variance.</p><p>For Multiple trees, we can use <strong>Random Forest</strong> and <strong>XGboost</strong> algorithm to solve.</p><img src="/img/fig/5.3.jpg" style="text-align: center;" /><h3 id="Purity-entropy"><a href="#Purity-entropy" class="headerlink" title="Purity(entropy)"></a>Purity(entropy)</h3><p>$p_{1}$ &#x3D; fraction of examples that are True.</p><img src="/img/fig/5.4.png" style="text-align: center;" /><p>$$<br>H\left( p_1 \right) &#x3D;-p_1\log \left( p_1 \right) -p_0\log \left( p_0 \right)<br>\<br>&#x3D;-p_1\log \left( p_1 \right) -\left( 1-p_1 \right) \log \left( 1-p_1 \right)<br>$$</p><img src="/img/fig/5.5.jpg" style="text-align: center;" /><p>In this figure, $w^{left}$&#x3D;2&#x2F;5、 $w^{right}$&#x3D;3&#x2F;5、$p_{1}^{left}$&#x3D;5&#x2F;10、$p_{2}^{left}$&#x3D;5&#x2F;10.</p><p>Information Purity</p><p>$$<br>Information Purity &#x3D;H\left( p_{1}^{root} \right) -\left( w^{left}H\left( p_{1}^{left} \right) +w^{right}H\left( p_{1}^{right} \right) \right)<br>$$</p><p>We should choose the <code>max</code> value of the Information Purity to <strong>recursive</strong> the decision tree model, which is called <code>Information Gain</code>.</p><p>In the process of split on a continuous variance(<strong>Regression tree</strong>), we also choose the max decreasing variance result as a good fit model.</p><img src="/img/fig/5.2.jpg" alt="s" style="text-align: center;"/><p>The purity of regression tree(equal to information gain):</p><p>$$<br>D&#x3D;V^{root}-\left( w^{left}V^{left}+w^{right}V^{right} \right)<br>$$</p><p>V instead of <code>variance</code>.</p><h3 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h3><ul><li><p>Start with all examples at the root node</p></li><li><p>Calculate information gain for all features, and pick the one with the highest information gain</p></li><li><p>Split dataset according to selected features, and create left and right branches of the tree</p></li><li><p>Keep repeating splitting process until stopping criteria is met:</p><pre><code class="hljs">      <figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">when a node <span class="hljs-keyword">is</span> <span class="hljs-number">100</span>% one <span class="hljs-keyword">class</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">splitting</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">will</span> <span class="hljs-symbol">result</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">tree</span> <span class="hljs-symbol">exceeding</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">maximum</span> <span class="hljs-symbol">depth</span><br><br><span class="hljs-symbol">Information</span> <span class="hljs-symbol">gain</span> <span class="hljs-symbol">from</span> <span class="hljs-symbol">additional</span> <span class="hljs-symbol">splits</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">less</span> <span class="hljs-symbol">than</span> <span class="hljs-symbol">threshold</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">number</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">examples</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">below</span>  <span class="hljs-symbol">a</span> <span class="hljs-symbol">threshold</span><br></code></pre></td></tr></table></figure></code></pre></li></ul><h2 id="Decision-tree-VS-Neutral-network"><a href="#Decision-tree-VS-Neutral-network" class="headerlink" title="Decision tree VS Neutral network"></a>Decision tree VS Neutral network</h2><h3 id="Decision-tree-1"><a href="#Decision-tree-1" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul><li>Works well on tabular(structured) data</li><li>Not recommended for unstructured data(images,audios,text)</li><li>Small decision tree may be human interpretable</li></ul><h3 id="Neutral-network"><a href="#Neutral-network" class="headerlink" title="Neutral network"></a>Neutral network</h3><ul><li>Works well on all types of data,including tabular(structured) data and unstructured data(images,audios,text)</li><li>May be slower than decision tree</li><li>Works with transfer learning</li><li>When building a system of multiple models working together, it might be easier to string together multiple neutral network</li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>本科毕业论文致谢</title>
    <link href="/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
    <url>/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>时光匆匆，转眼间四年的本科生活即将接近尾声。在此，请允许我以一篇致谢，表达在这四年时光里最真挚的谢意！</p><p>感谢论文指导老师杨国庆教授的悉心指导。从2021年参加数学建模比赛与老师结缘，杨老师严谨的治学态度和科学的研究方法给了我极大的帮助和影响，由衷感谢杨老师对我的关心和指导！</p><p>同时，由衷感谢教过我的河北大学国际学院信管专业全体ISEC教师：杨秀丹老师、吴树芳老师、徐杰老师、史海燕老师、郝杰老师、史江兰老师、贾金英老师、宇文姝丽老师、郭海玲老师、崔广志老师、吴利明老师、韩倩老师、陈婷老师、张鑫老师(排名不分先后)，以及其他全部科任老师。</p><p>感谢我的教练吕旭老师。大一期间将我纳入冰雪运动队，从此便与体育运动结下不解之缘。吕旭老师积极的心态与干练的作风对我产生了深远影响。从教练身上，我也更加懂得了什么是责任，什么叫标杆。</p><p>感谢我的辅导员曹永姝老师。四年的本科生活，曹老师对我照顾有加，老师的隐忍与坚持对我产生了很大的影响。她了解我的脾气与性格，尊重我的选择。</p><p>感恩在河北大学遇到的老师们，师恩天大，永记心间。</p><p>感谢我的父亲杨海龙先生、母亲刘秋菊女士。二十四年的成长之路，你们敢于放手，让我自由生长、大胆试错。每当我面临人生抉择的时候，你们都是那样义无反顾地支持我的选择。</p><p>感谢我的舅舅刘运陶先生，他对于我的成长是特别的。舅舅是我们家族中的第一位本科生、研究生，他潜移默化地培养了我的阅读习惯、学习习惯，并用他四十多年走过的弯路为我规避错误，他一直是我的榜样。</p><p>感谢我的姥姥邵长芬女士、姥爷刘增才先生。从四岁上幼儿园开始我便离开父母跟随姥姥、姥爷生活，直到我十八岁离开小镇去沧州市第一中学读高中。十四年的时光大部分都是和姥姥、姥爷度过的，姥姥、姥爷的勤劳、隐忍、坚韧、厚道对我的性格塑造影响至深。感谢我的奶奶刘绪巧女士、爷爷杨春林先生。2017年的仲夏，我二叔的车祸离世对两位老人以及全家人造成了沉痛打击，我曾一度认为两位老人会无法走出晚年丧子之痛。但是时过境迁，二老以顽强不屈、坚忍不拔的精神面貌给儿孙们以希望。</p><p>感恩我的家人们，让我有足够的勇气去面对生活中的任何挫折与苦难。</p><p>最后，感谢这四年的自己。前路漫漫，未来还会有很长的路要走，还会有更多的挑战、磨难需要去面对，还会有更多的责任需要去承担。会遇到很多人，会经历更多的事。但是无论怎样，请不要丢掉良心和理想，都不要忘记抽时间回忆回忆在河北大学这四年的美好时光。加油，祝好！</p><p>感谢百忙之中参加答辩的各位领导、老师们！</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell学习笔记</title>
    <link href="/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量定义的格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">变量名=变量值 #注意=左右两边不可以有空格<br><span class="hljs-meta prompt_">#</span><span class="language-bash">直接赋值</span><br>username=&quot;ygm&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">键盘赋值</span><br>read username<br><span class="hljs-meta prompt_">#</span><span class="language-bash">执行的命令结果赋值</span><br>str=$(pwd)<br>str=$(ll)<br>str=`ps -ef`<br></code></pre></td></tr></table></figure><p>变量的访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>echo $name<br>echo $echo&#123;name&#125;<br></code></pre></td></tr></table></figure><p>特殊变量</p><table><thead><tr><th>变量名</th><th>定义</th></tr></thead><tbody><tr><td>$#</td><td>命令行参数的个数</td></tr><tr><td>$?</td><td>前一个命令或函数的返回码</td></tr><tr><td>$n</td><td>$1表示第一个参数</td></tr><tr><td>$0</td><td>当前程序的名称</td></tr><tr><td>$*</td><td>以“参数1，参数2···”保存所有参数</td></tr></tbody></table><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>结论：推荐编程的时候使用双引号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">str=hello<br>str=`hello`<br>str=&quot;hello&quot;<br><br>str2=&#x27;I am $&#123;str&#125;&#x27;#单引号不会解释字符串里面的变量<br>str2=&quot;I am $&#123;str&#125;&quot;#双引号可以解释字符串里面的变量<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出字符串的长度</span><br>echo $&#123;#name&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">提取子字符串</span><br>echo $&#123;string:a:b&#125;#从索引(索引从0开始)为a个位置开始截取长度为b的子字符串<br></code></pre></td></tr></table></figure><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><p>算数运算在shell中要遵守严格的规范格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo `expr 2 + 3`#必须是反引号包裹、加号与数字之间留有空格<br>echo `expr 2 \* 3`#注意乘法和除法必须使用转义符号\前缀才会起作用<br>echo `expr 2 \% 4`<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此外，算数运算还可以用$(())和$[]表示-推荐方式</span><br>a=2<br>b=3<br>echo $((a+b))<br>echo $(($a+$b))<br>echo $[a+b]<br>echo $[$a+$b]<br></code></pre></td></tr></table></figure><h3 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h3><p>数字比较</p><ul><li>-eq：比较两个数是否相等，相等返回true</li><li>-ne：比较两个数是否不想等，不想等返回true</li><li>-gt：检测左边的数是否大于右边，若是返回true</li><li>-lt：检测左边的数是够小于右边，若是返回true</li><li>-ge：检测左边的数是否大于等于右边，若是返回true</li><li>-le：检测左边的数是否小于等于右边，若是返回true</li></ul><p>字符串比较：</p><ul><li>-z STRING：字符串长度为0</li><li>-n STRING：字符串长度不为0</li><li>&#x3D;：判断字符串长度是否相等</li><li>！&#x3D;：判断字符串长度是否不想等</li></ul><p>文件：</p><p>-f：存在且普通的文件</p><p>-e：文件存在</p><p>-d：存在且是目录</p><p>-h：存在且是链接</p><p>-r：存在且是只读</p><p>-w：存在且是可写</p><p>-x：存在且是可执行</p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组用小括号表示，中间元素用空格隔开，也可以直接定义数组中的每个元素的值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">array=(1 2 &quot;hello&quot; ygm)<br>array[4]=&quot;xyc&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取数组元素</span><br>echo $&#123;array[index]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取整个数组</span><br>echo $&#123;array[*]&#125;<br>echo $&#123;array[@]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">获取数组长度</span><br>echo $&#123;#array[*]&#125;<br><br></code></pre></td></tr></table></figure><h3 id="shell命令"><a href="#shell命令" class="headerlink" title="shell命令"></a>shell命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">逻辑运算符&amp;&amp;和||<br><br>&amp;&amp; 表示与，|| 表示或<br>二者具有短路原则：<br>expr1 &amp;&amp; expr2：当expr1为假时，直接忽略expr2<br>expr1 || expr2：当expr1为真时，直接忽略expr2<br>表达式的exit code为0，表示真；为非零，表示假。（与C/C++中的定义相反）<br><br></code></pre></td></tr></table></figure><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><p>if判断语句范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">if condition<br>then <br>···<br>else<br>···<br>fi<br><br>if[ &quot;a&quot; -lt &quot;b&quot; ] &amp;&amp; [ &quot;a&quot; -gt 2]<br>then <br>echo $&#123;a&#125;在范围内<br>fi<br><br>if[ $a -eq 2]<br>then <br>echo $&#123;a&#125;等于2<br>elif [ $a -eq 3]<br>then<br>echo $&#123;a&#125;等于3<br>else<br>echo 其他<br>fi<br></code></pre></td></tr></table></figure><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><p>for范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">for v in var1 var2 var3<br>do<br>echo $v<br>done<br><br>for flie in `ls`<br>do <br>echo $file<br>done<br><br>for i in $(seq 1 10)<br>do<br>echo $i<br>done<br><br>for ((i = 1;i&lt;10;i++))<br>do<br>echo $i<br>done<br></code></pre></td></tr></table></figure><p>while循环范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">while read name<br>do <br>echo $name<br>done<br><br>until [&quot;$&#123;word&#125;&quot; == &quot;yes&quot;] || [&quot;$&#123;word&#125;&quot; == &quot;YES&quot;]<br>do <br>read -p &quot;please input yse or YES to stop this program:&quot; word<br>done<br></code></pre></td></tr></table></figure><p>PS3使用方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;what is your favourite OS?&quot;<br>PS3=&quot;please enter your chose:&quot;<br>select var in &quot;linux&quot; &quot;windowns&quot; &quot;unix&quot;<br>do<br>break;<br>done<br>echo &quot;you have selected $var&quot;<br></code></pre></td></tr></table></figure><h2 id="一键安装JDK"><a href="#一键安装JDK" class="headerlink" title="一键安装JDK"></a>一键安装JDK</h2><p>使用shell脚本实现自动化部署</p><p>jdk_install.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">提示安装jdk</span><br>echo &quot;开始安装jdk&quot;<br>sleep 1<br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除自带的jdk</span><br>oldjdk=$(rpm -qa | grep jdk)<br>for old in $&#123;oldjdk&#125;<br>do<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-variable">$old</span></span><br>rpm -e --nodes $old<br>done<br><span class="hljs-meta prompt_">#</span><span class="language-bash">创建安装目录</span><br>java_path=$(/export/server)<br>if[ !-d $java_path]<br>then<br> mkdir -p $java_path<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">解压jdk安装包</span><br>tar -zxvf /export/softwore/jdk-8u241-linux-x64.tar.gz -C $java_path<br><span class="hljs-meta prompt_">#</span><span class="language-bash">设置环境变量</span><br>JAVA_HOME=&quot;/export/server/jdk1.8.0_241&quot;<br>grep &quot;JAVA_HOME&quot; /etc/profile<br>if[ #? -ne 0]<br>then<br><span class="hljs-meta prompt_">#</span><span class="language-bash">JAVA_HOME</span><br>echo &quot;---------JAVA_HOME-----------&quot;<br>echo `export JAVA_HOME=/export/server/jdk1.8.0_241` &gt;&gt; /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">PATH</span><br>echo &quot;----------PATH-----------&quot;<br>echo `export PATH=:$JAVA_HOME/bin:$PATH` &gt;&gt; /etc/profile<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">加载环境变量</span><br>sleep 1<br>source /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装完成提示</span><br>echo &quot;恭喜您jdk安装成功！&quot;<br>java -version<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>云计算</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cloud computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/08/05/hello-world/"/>
    <url>/2024/08/05/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! </p><p>值得纪念的日子，今天把之前的博客内容全部成功迁移到了新的平台上面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; welcome to hexo! &lt;&lt; endl;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>张宇智慧箴言</title>
    <link href="/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/"/>
    <url>/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>宝剑锋从磨砺出，梅花香自苦寒来</p>          </div><h2 id="别害怕"><a href="#别害怕" class="headerlink" title="别害怕"></a>别害怕</h2><p>不要试图掌控自己的奋斗过程，完美自己的各项计划，你会失落的，因为事实上它们并不听你的，你越跟它较劲，它越不听话。<br>不过这没什么，每一个成功，都是源于一段不完美的甚至是很狼狈的奋斗。<br>你只要，摈弃杂念，放下功利，全神贯注，尽力就好，学不完又怎样，考的上就行。好孩子，听话。</p><h2 id="拼搏"><a href="#拼搏" class="headerlink" title="拼搏"></a>拼搏</h2><p>年轻的时候，闯一闯，拼一拼，不要怕什么。实在扛不住了，哭一哭，喊一喊。大不了，回家。<br>不要把感情和情绪放在社会上、网络上，真正值得你在乎的，是表面上希望你出人头地，对你百般挑剔，但在心中却只为你祈求平安健康快乐的家人。<br>孩子们，听话，你逼自己的方法，可能不太对。放下包袱，放下功利，带着爱，带着对这个世界的好奇，去拼搏一下。累了，咱们回家就是了。<br><strong>健健康康，内心阳光</strong>，这八个字，可别弄丢了，丢了，会悔恨终身的。</p><h2 id="最大的敌人是自己"><a href="#最大的敌人是自己" class="headerlink" title="最大的敌人是自己"></a>最大的敌人是自己</h2><p>笛卡尔说，征服你自己，而不是征服世界。孩子们，晚安。</p><h2 id="嗜欲深者天机浅天机浅"><a href="#嗜欲深者天机浅天机浅" class="headerlink" title="嗜欲深者天机浅天机浅"></a>嗜欲深者天机浅天机浅</h2><p>古有云，<strong>无欲则刚</strong>。心思繁杂，欲望太多，则易焦虑，徘徊，无己见，恐非做事之正道。孩子们，务必静心，放下功利，用心为之，方可成之。</p><h2 id="基本功"><a href="#基本功" class="headerlink" title="基本功"></a>基本功</h2><p>有人问基础阶段以什么标准来看自己是不是真的懂了，这个问题我前面也回答过，这里详细说几句。我在课上提到一个人，叫做费曼，他是诺贝尔物理学奖获得者，人们试图研究费曼为什么对问题的理解总是透彻深刻，深得其精髓要义，事实上，费曼自己提出过一个观点：把你所学到的理解到的概念性质和方法讲给一个比如小孩子听（这里是为了确保聆听的人对你所讲述的知之甚少），而且让他尽量听懂，这时你无法用那些复杂的专业性的表达来叙述，在寻找简单通俗易懂的语言的时候，你的大脑在不断的深化对于知识的认识，迫使自己在更深层次上理解它们，这样你一定会停滞在很多地方，而这些地方就是你没有理解的地方。通过努力，把你的知识传递给这个孩子并让他听懂，就是你真的懂了，这就是过关。<br>我前面说到，你一定要会复述我课上对知识的讲解，甚至比我讲的还要简要精炼，这是你迅速融入这个学科思维状态的最佳途径。各位，我再写一会书，你们好好思考下我上面这段话，我是真的希望你们能够懂得如何学习，不要再功利的去卷了，真学东西，学真东西，学会怎么学，不仅为了考研，更为了自己的一生。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>前路漫漫亦灿灿</title>
    <link href="/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/"/>
    <url>/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>不要辜负这份好运气——写在顺利推免后读任正非先生<a href="https://baike.baidu.com/item/%E6%88%91%E7%9A%84%E7%88%B6%E4%BA%B2%E6%AF%8D%E4%BA%B2/6610022">《我的父亲母亲》</a>有感</p>          </div><p>2023年9月29日，下午14:50，故事开始了新的篇章。</p><p>从2023年2月15日离家返校，一直到10月1号，整整228天，家门未踏入一步。一直想带一个好的结果回家，经历磕磕绊绊，最终如愿以偿。</p><p>想说的东西、想表达的东西有好多，但却难以启齿、无从下笔。</p><p>回首这段岁月，感谢自己没有放弃自己，感谢暑假留校踏踏实实读书学习的沉淀。暑假留校两个月，自己在图书馆五楼靠窗户占了一整张桌子，五楼人烟稀少，八月份安安静静在那里度过了一个月充实的时光。早上八点半开馆准时到达，晚上在漆黑的五楼写读书笔记。如今再进入五楼，往事历历如过眼云烟般浮现在眼前。上一次有这种感觉，还是因为19年底口罩事情高三开学延期，20年初居家复习的岁月。这两段时光交织在一起，给我的反思就是，要想做成一件事情，首先要把自己调整成静音模式，踏踏实实忘我般的投入进去。想成事，不是每天炸炸呼呼，一天八条朋友圈，恨不能别人不知道我干啥。而是要沉淀、要安静、要忍耐、要坚持。</p><p>人性的弱点之一就是<strong>只看结果，不注重过程</strong>。</p><p>你考上研了，考上公了，周围都是欢呼、赞美之词。”牛逼、大佬、交给朋友吧…”诸如此类的话，会让你沉迷享受，会让你飘飘然忘乎所以。但是，当你没考上、没惊起波澜，周围人并不会关注，甚至会有讥讽、嘲笑不绝于耳。”他不行，他光玩，他就不是那块料…”。所以，自强不息、厚德载物，永远都是法宝，让自己强大起来，让自己内心强大起来，才会在面对这些情况的时候，做到多一份的从容、淡定与坦然。</p><p>带着好结果回家过节，家人们由衷地为我感到高兴、自豪。感谢家人，没有家人的理解与支持，我没有勇气走好今天的路。</p><p>我的父亲杨海龙，1976年出生于河北省沧州市一个普普通通的农村家庭。小时候学习成绩不错，凭借自己的努力一直读到了高中毕业。中考成绩很好，但是因为家里穷要继续供给我二叔、三叔读书。于是，父亲放弃了去县里读高中的机会，选择了在镇上完成学业，很多年前，他仍然自嘲:“以我当年的成绩，要是去县里上，考过河北工业绰绰有余”。高中毕业后在我二爷家工厂里工作，然后和爷爷、叔叔们创业。</p><p>我的母亲刘秋菊，1978年出生于河北省沧州市一个普普通通的农村家庭。小的时候学习成绩一般，肯定没有我父亲学习好，想考中专，结果也是为了弟弟妹妹的学业而放弃了自己的梦想。初中毕业后进入农村信用社当会计，后来在我姥爷的厂子里当会计，一干就是半辈子。母亲是一个聪明的人，积极、阳光、热爱学习，她算账、算数没出过错，爱写读书笔记，有阅读习惯，这些都润物细无声般影响着我。</p><p>两个有着相同命运路径的人走到了一起，养育了我、培养了我。特别是，今年因为面试忙于各个城市奔波，报辅导机构，发paper。以上种种，当我需要钱的时候，老父亲总是那么斩钉截铁地转给我，让我没有任何顾虑。钱是好东西，而且钱是干净的，脏的是人心。走正道，好好赚钱、多赚钱永远都是幸福的。</p><p>这些年，二叔的去世一直也是我前进的动力。我很少和外人去谈我二叔因为车祸去世的事情，我认为这是家族之殇。我作为家族中的长子长孙，在我17岁那年，他的死，使我对我的家庭、对我自己有了新的理解。挫折都是发人深省的，我庆幸自己没有被生活的重创打垮，我也庆幸，家里的老人没有因为生活的苦难而使他们丧失对生活的希望。在经历苦难之后，我看到的依然是他们积极的生活态度，没有怨天尤人、没有一蹶不振，永远都给子女树立了好的榜样。其实，这就是我遇到无数困难后，依然有勇气面对的底气来源。</p><p>今年还要感谢张宇(宇爹)，不确定的日子里一直在通过备考高数舒缓。激发了我对数学的热情，重塑了学科的理解。</p><p>没有轻舟已过万重山，唯有前路漫漫亦灿灿。逝去的已经逝去，活着的仍要前行。</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>致谢</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
