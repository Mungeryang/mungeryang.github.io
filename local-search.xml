<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>精心排版了一本书</title>
    <link href="/2026/01/18/%E7%B2%BE%E5%BF%83%E6%8E%92%E7%89%88%E4%BA%86%E4%B8%80%E6%9C%AC%E4%B9%A6/"/>
    <url>/2026/01/18/%E7%B2%BE%E5%BF%83%E6%8E%92%E7%89%88%E4%BA%86%E4%B8%80%E6%9C%AC%E4%B9%A6/</url>
    
    <content type="html"><![CDATA[<p>最近在读王教授的这本写于1994年的日记，收获很多。网络上不难找到很多PDF的电子版，但是发现所有书籍的排版质量均不佳，严重影响阅读体验。于是，在我自己阅读本书的同时，产生了这样一个想法，精选一些好的篇章，使用latex重新精排一下。</p><p>第一次听说这本书，是在沧州一中就读期间(高一)，政治老师刘欣湄在课堂上提到过。依稀记得老师当时在课堂上说，这个作者很牛逼，很多社会现象都前瞻性的预测出来了。</p><p>当时没上心，转眼间与本书再次结缘，已经是9年后的今天了。我没有大包大揽地对每一篇日记进行详细的梳理，因为很多涉及政治和经济的论述我自己读不懂。更多地，我是摘录了很多关于对生活、社会现象的一些思考，希望能引出我们的思考。</p><p>我本人这段时期也处在迷茫期，对于工作还是继续硕博连读的选择，也在犹豫和挣扎中，读这本书也是想收获一些内心的平静，以及启发自己对当下以及未来关于自己发展的一些思考。</p><p>比起王教授这样的学术路线，我觉着还是做技术能让我更快乐些；倘若都是做冷板凳，我觉着做在冷板凳上钻研学习新技术要比探索新理论更能激发我自己的学习兴趣与学习热情。</p><p>我也不知道我的选择是对是错，反正对读博的路慢慢变得祛魅了；但是趁着年轻，做点感兴趣的事我觉着是没错的，最起码我自己是不后悔的。</p><p>那什么能让我感兴趣呢？——持续学习本领域内前沿新技术，不断夯实自己的基础本领，积极投入生产实践，做出产品做出成果，是我感兴趣的。</p><iframe src='../img/jianli/zzlife.pdf' height=850 width=90% frameborder=0  allowfullscreen "></iframe>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年度总结</title>
    <link href="/2026/01/01/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    <url>/2026/01/01/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>一篇一篇地去翻看自己这一年每个月的月度总结，发现时间过的真快。这一年感觉做了很多事情，也感觉没做很多事情。</p><h2 id="收获还是蛮多的"><a href="#收获还是蛮多的" class="headerlink" title="收获还是蛮多的"></a>收获还是蛮多的</h2><p>专业层面：</p><ul><li><p>系统地掌握的LLM基本原理</p></li><li><p>系统地复现了一系列LLM&#x2F;VLM源码结构设计细节</p></li><li><p>发了一篇CCF-C论文</p></li><li><p>发了一篇专利</p></li></ul><p>性格方面：</p><p>更加开朗、更加主动了</p><p>身体方面：</p><p>系统性开始健身，比去年更壮了，体脂变低了，快有腹肌了(加油💪)</p><p>财富方面：</p><p>国科大和科学院的薪资挺高的，开始慢慢挣工资了，能攒下钱了</p><hr><p>24年的总结里面说要买相机学摄影；25年相机没买成，但是摄影技术确实比之前提高了很多。</p><p>24年说要买辆公路车，走遍北京角落；25年回所后没买公路车，买了辆小电驴，海淀这一块算是逛遍了。</p><p>2024年年度总结里面，给自己写了一个美好的标题：<strong>永远相信美好的事情即将发生</strong>。在展望部分里面，告诫自己<strong>做时间的朋友、实践的朋友</strong>。年度照片，选了蔡国庆老师的烟花作品。</p><p>2025年年度总结里面，没有借鉴华丽的标题词藻，而是分成了不同的小标题。年度照片，选了风骚律师中的经典镜头。</p><p><img src="https://pbs.twimg.com/media/FaNnpIpXgAEUZL_.jpg" alt="better call saul"></p><h2 id="今年，我变了"><a href="#今年，我变了" class="headerlink" title="今年，我变了"></a>今年，我变了</h2><p>变得更外向了</p><p>变得不拧巴了：该摆烂摆烂，该休息休息</p><p>变得开始认真收拾自己了</p><p>同时，</p><p>也变得没那么专注了</p><p>也变得懒惰了</p><p>所以，</p><p>要变得勤奋些了</p><p>要变得有目标些了</p><h2 id="玩遍了京城，爱上了内蒙"><a href="#玩遍了京城，爱上了内蒙" class="headerlink" title="玩遍了京城，爱上了内蒙"></a>玩遍了京城，爱上了内蒙</h2><p>25年不像24年那样去了那么多地方，今年主要还是在北京的时间多一些。京城的大小角落基本上都去玩过了，海淀“闭着眼”都知道去哪里该咋走了。</p><p>清明节去了乌兰察布，端午节自驾去了二连浩特大环线，爱上内蒙了，喜欢那种旷野、无拘无束、自由的感觉。</p><p>26年有机会想去躺满洲里和大兴安岭地区。</p><h2 id="平静地接受任何事情的发生"><a href="#平静地接受任何事情的发生" class="headerlink" title="平静地接受任何事情的发生"></a>平静地接受任何事情的发生</h2><p>年终岁尾，要开始考虑要不要硕博连读了，对于自己的人生而言，这也是一次重要的选择，如果有机会，我还是想继续读下去的。</p><p>年终岁尾，奶奶突然被检查出来肾癌，要把整个右肾全部割下去了。我不知道更不确定，奶奶距离生命的终点还有多长的距离，还剩多长的时间。26年如果假期允许的话，多回家陪陪老人们吧，毕竟有生之年，能在他们身边的日子不多了。</p><h2 id="尽可能，敬所无能"><a href="#尽可能，敬所无能" class="headerlink" title="尽可能，敬所无能"></a>尽可能，敬所无能</h2><p>转博、择业、陪伴。。。种种事情，自己想做的，要积极主动地去争取，有机会一定要把握住。控制能控制的过程，顺应不能控制或不合预期的结果。努力不一定会带来回报，但是如果有机会，该拼还是要拼，该争取还是要争取。</p><hr><p>2025年还是很丰富的一年，仅仅用着不到1000字去记录是远远不够的，最重要的，还是要用心去感受。安分守己、逆来顺受。</p><p>今天只是今天，未来也只是今天的未来。</p><p>爱你所爱，行你所行，听从你心，无问西东。</p><p>以感恩之心，迎接2026的到来！😊</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>年度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年12月总结与展望</title>
    <link href="/2025/12/29/2025%E5%B9%B412%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/12/29/2025%E5%B9%B412%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="焦虑"><a href="#焦虑" class="headerlink" title="焦虑"></a>焦虑</h2><p>为啥坐立难安，因为科研上实在没有进展了。。。💔</p><p>这个月挺难的，每天在工位，浑浑噩噩地就过去了。可能到冬天了，一到工位浑身就不自在，犯困🥱</p><p>好在26号晚上和秦老师聊了一个多小时，稍微喘了口气。能感觉到秦老师一直在强调稳扎稳打，好工作还是要有扎实的积累和沉淀。</p><p>珍惜机会，积极主动。</p><h2 id="读了两本传记，一本日记"><a href="#读了两本传记，一本日记" class="headerlink" title="读了两本传记，一本日记"></a>读了两本传记，一本日记</h2><p>这个月淘了两本近乎绝版的书，一本是温总理写的《地质笔记》，一本李明博的个人传记《经营未来》。两本书都基本完整真实地记录了两位东亚领袖的奋斗过程。</p><p>这个月读了一本特别的日记，是现政协王主席当年的在复旦教书时深夜写的日记《政治的人生》。网上的影印版质量都太差了，于是乎，我一边读并一边用latex重新排版了一遍，待我整理好会发布到Blog上面供大家阅读。</p><p>三本书，三段奋斗历程，读起来毫无鸡汤感，而是实打实地做事情，多学习，多积累的过程(无论是做事还是思考)。</p><p><strong>网络是煽动者的回音壁，现实是实干家的大舞台</strong>。我是推崇实践的，实事求是始终是指导思想。但是，平心而论，我认为自己这个阶段还处在每天信息输入过载，而实操过少的阶段，和本科恰好有点相反。</p><p>首先，输入信息过载首当其冲地是由于每天需要读大量的论文，但是阅读过程中就很难保持长期的专注，总会偏离预定的轨道去看些主题之外的东西，这是表现在读论文上。其次，生活中的碎片化信息过多，抖音、小红书这两个APP我认为消耗了自己多部分的注意力，本科期间我是很少刷抖音的。最后，多处的环境不同。本科期间可能无忧无虑不用考虑那么多事情，但是读研以后，各地方面临的压力都比较大，为了了解情况并解决问题，需要去看信息源，这样以来难免就会导致信息过载了。同时，在北京读研这几年，圈子也比本科期间大了不少。可以说，本科期间除了和室友、龙哥以及滑雪队的老师朋友们外，我圈子还是小的很。老北京以后，收获了很多搭子，健身、吃饭、篮球、游玩。。。</p><p>对自己注意力的分配与调整策略的思考，我认为可以把它作为2026年的开年第一件事情，如何平衡、如何取舍需要我自己多想想了。同样的，习惯很重要，执行更重要。</p><p>资质一般，那就要在别人玩的时候，自己多下下功夫。这不是明目张胆的搞内卷，而是在自己感兴趣的领域内，客观的认识到自己目前的能力与想做成事情之间的不匹配程度有多大。</p><h2 id="初雪与LiveHouse"><a href="#初雪与LiveHouse" class="headerlink" title="初雪与LiveHouse"></a>初雪与LiveHouse</h2><p>这个月，严老师带着我和lxc去看了人生第一次LiveHose，是白皮书乐队2025年的最后一站巡演。</p><p>12号北京下雪了，正赶上党支部组织活动，参观了卢沟桥和宛平城，以及中国人民抗日战争纪念馆。13号，我们又去了故宫，第一次去下雪后的故宫，别有一番韵味。又和天华、国栋在北海公园汇合，一起去胡同里探索了一家涮肉店。</p><p>最近看了不少电影：一战再战、悲情城市、一一、你行你上、帝国的毁灭、疯狂动物城2、用武之地</p><h2 id="不好的预兆"><a href="#不好的预兆" class="headerlink" title="不好的预兆"></a>不好的预兆</h2><p>27号晚上按照惯例和姥姥视频通话，一开始我没接着。后来打回去，我老弟给接了，告诉我奶奶因为肾结石要手术了，爸妈竟然和我只字未提。</p><p>晚上在海底捞给老班过生日的时候，又给老妈打了个视频，她那天正好在医院陪床。聊了一会就挂了，老妈和我打字又说了实情。奶奶原来不是肾结石，是肾上腺和肾内部长了肿瘤。我当时反应很快，直接和老妈说：这不就是<strong>肾癌</strong>嘛。老妈说你自己清楚就好，不要和爷爷奶奶说。</p><p>对于两位老人，子女们都在骗他们说是肾结石，小手术没啥毛病。但是实际上，奶奶这次手术是要把整个右肾和右肾上腺全部切除，相当于是器官摘除。</p><p>28号晚上，和我爸打了将近一个小时的视频，印象中的第一次我们爷俩为数不多俩了这么长时间。以为之前都是每两周和老妈打一次视频，每次都会聊一个小时，他要是不出去应酬喝酒就会也陪着聊会，但是在的次数不多。老爸把核磁结果都给我念了一遍，自己也在吐槽着很多术语都不懂，在这件事情上，他一开始是不想让我知道的。</p><p>对于死亡的经历，我自认为我比同龄人的经历要多些。24年，我经历了大姥爷、三姥爷、二姥爷的葬礼；亲历了我二叔从尸体被拉回来到入土为安的全过程。我把这些和死亡近距离接触的经历视为一笔财富，因为它让我学会了更加珍惜眼前人的道理，这个道理看似浅显且被说烂了，但是能做到且坚持做到的人很少。</p><p>现在是北京时间2025年12月29日下午15:31，昨天和我爸打视频，他说是今天下午两点开始手术。此刻，孙子远在两百多公里外的北京，而奶奶正在手术床上昏迷进行着右肾切割手术。我也只能默默祈祷，祝愿奶奶手术一切顺利，保你平安～🙏🙏🙏</p><p>未来会怎样，说不好。</p><p>也许我会在26年里平常的一天，突然接到我爸妈的电话，让我赶紧回家奔丧；</p><p>也许没准未来好几年都无事发生，一个肾也能维持好后面的生活，在她有生之间还能看到我把孙媳妇领回家的那一天呢。</p><p>但是，也许不是我能说了算的。</p><p>我能做的，就是做完手术后，元旦放假抽1-2天回医院看看她；</p><p>我能做的，就是今年春节回家给她买点好吃的，多和她拍拍照，陪陪她；</p><p>我能做的，就是春节放假回去教会她接听微信视频通话，26年多和她打打视频；</p><p>我能做的，也许只有这些了罢。</p><p>亲情和子女、子孙们的意义就在于，无论你是大富大贵、青史留名的“人物”，还是籍籍无名、普普通通的百姓，在你走后，总会有人记住你、挂念你。</p><p>爷爷奶奶、姥姥姥爷这一辈子，很不容易的。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年11月总结与展望</title>
    <link href="/2025/11/30/2025%E5%B9%B411%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/11/30/2025%E5%B9%B411%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>11月留下的文字不多，因为11月大部分时间都在代码实战，收获还是蛮多的。</p><h2 id="越来越文艺了"><a href="#越来越文艺了" class="headerlink" title="越来越文艺了"></a>越来越文艺了</h2><p>一直觉着自己是个粗人，活得很粗糙。我其实是不太注重自己衣着打扮这些的，但是来到科学院读研究生以来，慢慢的开始有所改变了。朋友们大多都是非常体面、立整的人，都会把自己收拾的干干净净的，向优秀的人看齐，把自己收拾立整了，也是对自己最大的尊重。习惯了干净，生活才会变的干净。</p><p>我和lcx都是比较闷、娱乐活动比较少的人；从国庆以后，严老师带领我们一起看了很多电影，他是一个阅片无数的人。因为他说他自己高考以后，已经没有心气去耐心读完一整本书了，所以疯狂看电影、看展来弥补。</p><p>今年我确实很多了很多电影、美剧和书，阅读量比往年都要大很多。艺术的熏陶对人还是蛮重要的。我感觉，对我最大的益处无非是让自己学会放松、学会松弛、学会无所事事。该歇就歇，不想干就不干，而不是强撑着。</p><p>看完风骚律师后，我拍照的技术也进步了很多。这个月我们去了奥森公园、去了798园区、去了什刹海和大大小小的老北京胡同。。。</p><h2 id="科研仍然没有进展"><a href="#科研仍然没有进展" class="headerlink" title="科研仍然没有进展"></a>科研仍然没有进展</h2><p>11月感觉过的很快，没干啥就过去了。</p><p>这个月主要把colpali模型熟悉了一遍，踏踏实实把源代码看了一遍。</p><p>在colpali的基础之上，训练了Colqwen3模型，并发布到了Hugging Face上。</p><p>另一个重要的收获是学习了Qwen-VL模型的系列架构，对VLM架构细节有了深入了解。</p><p>科研上仍然在探索，漫无目的的探索，其实挺迷茫的～</p><p>12月份要好好开始准备下一篇论文的投稿方向了。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Qwen_VL复现与源码剖析</title>
    <link href="/2025/11/26/Qwen-VL%E5%A4%8D%E7%8E%B0%E4%B8%8E%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"/>
    <url>/2025/11/26/Qwen-VL%E5%A4%8D%E7%8E%B0%E4%B8%8E%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="复刻架构图"><a href="#复刻架构图" class="headerlink" title="复刻架构图"></a>复刻架构图</h2><p>通过阅读源代码，画出qwenVL组件架构图：</p><img src="/img/fig/qwenvl1.jpg"/><img src="/img/fig/qwenvl4.jpg"/><img src="/img/fig/qwenvl3.jpg"/>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Colpali 复现与源码剖析</title>
    <link href="/2025/11/19/Colpali%E5%A4%8D%E7%8E%B0/"/>
    <url>/2025/11/19/Colpali%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="写在前面——谈谈我认为的论文复现"><a href="#写在前面——谈谈我认为的论文复现" class="headerlink" title="写在前面——谈谈我认为的论文复现"></a>写在前面——谈谈我认为的论文复现</h2><p>绝对不是按着GitHub上READ文件跑一遍，而是仔细去阅读源代码，真正弄清楚代码的设计逻辑》前提是你复现的工作作者非常的良心，譬如这篇colpali工作，基本都很透明。</p><p>这篇工作我是真去下功夫去理解复现的，前前后后奖奖将近两周多一点。实验记录我整理的非常详细，每一张架构图、类图都是我通过阅读源代码精心制作的，希望能让大家弄清楚colpali的结构设计。</p><p>侯捷老师说过，<strong>天下大事必作于细</strong>。要想好下手并作出修改，仔细阅读源码的功力和锻炼过程比不能少！这样才能避免遇到什么问题就问GPT，GPT修改，再报错，问GPT…周而复始的死循环当中。</p><h2 id="复刻论文架构图"><a href="#复刻论文架构图" class="headerlink" title="复刻论文架构图"></a>复刻论文架构图</h2><img src="/img/fig/colpali.png"/><h2 id="核心环境配置"><a href="#核心环境配置" class="headerlink" title="核心环境配置"></a>核心环境配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 1️创建虚拟环境 ----</span><br>conda create -n colpali python=3.10 -y<br>conda activate colpali<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 2️安装核心依赖 ----</span><br>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121<br>pip install transformers accelerate datasets sentencepiece bitsandbytes flash-attn faiss-gpu<br>pip install peft einops omegaconf pytorch-lightning matplotlib tqdm<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 3️配置 Hugging Face 镜像 ----</span><br>export HF_ENDPOINT=https://hf-mirror.com<br>export TRANSFORMERS_OFFLINE=0<br>export HF_HUB_DISABLE_SYMLINKS_WARNING=1<br>echo &quot;已设置 Hugging Face 镜像源：$HF_ENDPOINT&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 4️克隆源码 ----</span><br>git clone https://github.com/illuin-tech/colpali.git<br>cd colpali<br>pip install -e .<br>cd ..<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 5️下载模型权重（通过镜像）----</span><br>mkdir -p models<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">ColPali (PaliGemma-3B)</span><br>wget https://hf-mirror.com/illuin-tech/colpali/resolve/main/pytorch_model.bin -O models/colpali/pytorch_model.bin<br>wget https://hf-mirror.com/illuin-tech/colpali/resolve/main/config.json -O models/colpali/config.json<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">ColQwen2-VL-2B</span><br>wget https://hf-mirror.com/illuin-tech/colqwen2-vl/resolve/main/pytorch_model.bin -O models/colqwen2/pytorch_model.bin<br>wget https://hf-mirror.com/illuin-tech/colqwen2-vl/resolve/main/config.json -O models/colqwen2/config.json<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">---- 6️下载 ViDoRe 数据集 ----</span><br>mkdir -p data/vidore &amp;&amp; cd data/vidore<br>wget https://hf-mirror.com/datasets/illuin-tech/vidore/resolve/main/vidore.tar.gz<br>tar -xvzf vidore.tar.gz &amp;&amp; rm vidore.tar.gz<br>cd ../../<br><br>echo &quot;环境与数据准备完成，可直接运行评测与源码阅读。&quot;<br></code></pre></td></tr></table></figure><h2 id="colqwen2训练"><a href="#colqwen2训练" class="headerlink" title="colqwen2训练"></a>colqwen2训练</h2><h3 id="前期准备工作"><a href="#前期准备工作" class="headerlink" title="前期准备工作"></a>前期准备工作</h3><p>修改train_colqwen2_model.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">config:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.trainer.colmodel_training.ColModelTrainingConfig</span><br>  <span class="hljs-attr">output_dir:</span> <span class="hljs-type">!path</span> <span class="hljs-string">/mnt/data1/ygm/models/colqwen2-cesmoothmax-5e-2604</span><br>  <span class="hljs-attr">processor:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.utils.transformers_wrappers.AllPurposeWrapper</span><br>    <span class="hljs-attr">class_to_instanciate:</span> <span class="hljs-type">!ext</span> <span class="hljs-string">colpali_engine.models.ColQwen2Processor</span><br>    <span class="hljs-attr">pretrained_model_name_or_path:</span>  <span class="hljs-string">&quot;/mnt/data1/ygm/models/colqwen2-base&quot;</span><br>    <span class="hljs-attr">max_num_visual_tokens:</span> <span class="hljs-number">1024</span><br>    <span class="hljs-attr">size:</span><br>      <span class="hljs-attr">shortest_edge:</span> <span class="hljs-number">448</span><br>      <span class="hljs-attr">longest_edge:</span> <span class="hljs-number">448</span><br><br>  <span class="hljs-attr">model:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.utils.transformers_wrappers.AllPurposeWrapper</span><br>    <span class="hljs-attr">class_to_instanciate:</span> <span class="hljs-type">!ext</span> <span class="hljs-string">colpali_engine.models.ColQwen2</span><br>    <span class="hljs-attr">pretrained_model_name_or_path:</span> <span class="hljs-string">&quot;/mnt/data1/ygm/models/colqwen2-base&quot;</span><br>    <span class="hljs-attr">torch_dtype:</span>  <span class="hljs-type">!ext</span> <span class="hljs-string">torch.bfloat16</span><br>    <span class="hljs-attr">use_cache:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">attn_implementation:</span> <span class="hljs-string">&quot;flash_attention_2&quot;</span><br><br>  <span class="hljs-attr">train_dataset:</span> <br>    <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.utils.dataset_transformation.load_train_set</span><br>  <span class="hljs-attr">eval_dataset:</span> <span class="hljs-type">!import</span> <span class="hljs-string">../data/test_data.yaml</span><br><br>  <span class="hljs-comment"># max_length: 50</span><br>  <span class="hljs-attr">run_eval:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">loss_func:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.loss.late_interaction_losses.ColbertPairwiseCELoss</span><br>  <span class="hljs-attr">tr_args:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">transformers.training_args.TrainingArguments</span><br>    <span class="hljs-attr">output_dir:</span> <span class="hljs-literal">null</span><br>    <span class="hljs-attr">overwrite_output_dir:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">num_train_epochs:</span> <span class="hljs-number">1</span><br>    <span class="hljs-attr">per_device_train_batch_size:</span> <span class="hljs-number">16</span><br>    <span class="hljs-attr">gradient_checkpointing:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">gradient_checkpointing_kwargs:</span> &#123; <span class="hljs-attr">&quot;use_reentrant&quot;:</span> <span class="hljs-literal">false</span> &#125;<br>    <span class="hljs-comment"># 6 x 8 gpus = 48 batch size</span><br>    <span class="hljs-comment"># gradient_accumulation_steps: 4</span><br>    <span class="hljs-attr">per_device_eval_batch_size:</span> <span class="hljs-number">8</span><br>    <span class="hljs-attr">eval_strategy:</span> <span class="hljs-string">&quot;steps&quot;</span><br>    <span class="hljs-attr">dataloader_num_workers:</span> <span class="hljs-number">0</span><br>    <span class="hljs-comment"># bf16: true</span><br>    <span class="hljs-attr">save_steps:</span> <span class="hljs-number">500</span><br>    <span class="hljs-attr">logging_steps:</span> <span class="hljs-number">10</span><br>    <span class="hljs-attr">eval_steps:</span> <span class="hljs-number">100</span><br>    <span class="hljs-attr">warmup_steps:</span> <span class="hljs-number">100</span><br>    <span class="hljs-attr">learning_rate:</span> <span class="hljs-number">2e-4</span><br>    <span class="hljs-attr">save_total_limit:</span> <span class="hljs-number">1</span><br>    <span class="hljs-comment"># resume_from_checkpoint: true</span><br>    <span class="hljs-comment"># optim: &quot;paged_adamw_8bit&quot;</span><br>    <span class="hljs-comment"># wandb logging</span><br>    <span class="hljs-comment"># wandb_project: &quot;colqwen2&quot;</span><br>    <span class="hljs-comment"># run_name: &quot;colqwen2-ba32-nolora&quot;</span><br>    <span class="hljs-comment"># report_to: &quot;wandb&quot;</span><br><br><br>  <span class="hljs-attr">peft_config:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">peft.LoraConfig</span><br>    <span class="hljs-attr">r:</span> <span class="hljs-number">32</span><br>    <span class="hljs-attr">lora_alpha:</span> <span class="hljs-number">32</span><br>    <span class="hljs-attr">lora_dropout:</span> <span class="hljs-number">0.1</span><br>    <span class="hljs-attr">init_lora_weights:</span> <span class="hljs-string">&quot;gaussian&quot;</span><br>    <span class="hljs-attr">bias:</span> <span class="hljs-string">&quot;none&quot;</span><br>    <span class="hljs-attr">task_type:</span> <span class="hljs-string">&quot;FEATURE_EXTRACTION&quot;</span><br>    <span class="hljs-attr">target_modules:</span> <span class="hljs-string">&#x27;(.*(model).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(custom_text_proj).*$)&#x27;</span><br></code></pre></td></tr></table></figure><p>修改scripts&#x2F;configs&#x2F;data&#x2F;test_data.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">syntheticDocQA_energy:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/syntheticDocQA_energy_test/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">syntheticDocQA_healthcare_industry:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/syntheticDocQA_healthcare_industry_test/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">syntheticDocQA_artificial_intelligence_test:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/syntheticDocQA_artificial_intelligence_test/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">syntheticDocQA_government_reports:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/syntheticDocQA_government_reports_test/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">infovqa_subsampled:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/infovqa_test_subsampled/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">docvqa_subsampled:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/docvqa_test_subsampled/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">arxivqa_subsampled:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/arxivqa_test_subsampled/data/test-00000-of-00001.parquet</span>  <span class="hljs-comment"># &lt;-- 已修正</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">tabfquad_subsampled:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/tabfquad_test_subsampled/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">tatdqa:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/tatdqa_test/data/test-00000-of-00002.parquet</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/tatdqa_test/data/test-00001-of-00002.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br><br><span class="hljs-attr">shift_project:</span><br>  <span class="hljs-string">():</span> <span class="hljs-string">colpali_engine.data.dataset.ColPaliEngineDataset</span><br>  <span class="hljs-attr">pos_target_column_name:</span> <span class="hljs-string">&quot;image&quot;</span><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-string">():</span> <span class="hljs-string">datasets.load_dataset</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;parquet&quot;</span><br>    <span class="hljs-attr">data_files:</span><br>      <span class="hljs-attr">test:</span> <span class="hljs-string">/mnt/data1/ygm/colpali/data_dir/shiftproject_test/data/test-00000-of-00001.parquet</span><br>    <span class="hljs-attr">split:</span> <span class="hljs-string">&quot;test&quot;</span><br></code></pre></td></tr></table></figure><h3 id="终端训练"><a href="#终端训练" class="headerlink" title="终端训练"></a>终端训练</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">(colpali) [ygm@localhost colpali]$ CUDA_VISIBLE_DEVICES=0,1 accelerate launch scripts/train/train_colbert.py scripts/configs/qwen2/train_colqwen2_model.yaml<br></code></pre></td></tr></table></figure><img src="/img/fig/traincolqwen2.png"/><h2 id="基于训练好的colqwen进行测试"><a href="#基于训练好的colqwen进行测试" class="headerlink" title="基于训练好的colqwen进行测试"></a>基于训练好的colqwen进行测试</h2><p>需要提前下载好数据集到本地服务器，下面展示一个数据集测试的例子，10个数据集是同样的操作。</p><blockquote><p>vidore&#x2F;docvqa_test_subsampled数据集</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install &quot;vidore-benchmark[colpali-engine]&quot;<br><br>vidore-benchmark evaluate-retriever \<br>--model-class colpali \<br>--model-name /mnt/data1/ygm/models/colqwen2 \<br>--dataset-name vidore/...--&gt;对应的数据集名称 \<br>--dataset-format qa \<br>--split test<br></code></pre></td></tr></table></figure><img src="/img/fig/docvqa.png"/><h2 id="Colpali中的Vision-LLM"><a href="#Colpali中的Vision-LLM" class="headerlink" title="Colpali中的Vision LLM"></a>Colpali中的Vision LLM</h2><h3 id="PaliGemma骨干"><a href="#PaliGemma骨干" class="headerlink" title="PaliGemma骨干"></a>PaliGemma骨干</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># PaliGemmaModel 部分源代码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PaliGemmaModel</span>(<span class="hljs-title class_ inherited__">PaliGemmaPreTrainedModel</span>):<br>    _checkpoint_conversion_mapping = &#123;<span class="hljs-string">&quot;language_model.model&quot;</span>: <span class="hljs-string">&quot;language_model&quot;</span>&#125;<br>    <span class="hljs-comment"># we are filtering the logits/labels so we shouldn&#x27;t divide the loss based on num_items_in_batch</span><br>    accepts_loss_kwargs = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: PaliGemmaConfig</span>):<br>        <span class="hljs-built_in">super</span>().__init__(config)<br>        <span class="hljs-comment"># siglip_vision_model 作为 Vision Encoder</span><br>        <span class="hljs-variable language_">self</span>.vision_tower = AutoModel.from_config(config=config.vision_config)<br>        <span class="hljs-comment"># PaliGemmaMultiModalProjector：vision encoder 与 LLM 之间的投影层 </span><br>        <span class="hljs-comment"># colpali中VisionLLM中的第一个Proj</span><br>        <span class="hljs-variable language_">self</span>.multi_model_projector = PaliGemmaMultiModalProjector(config)<br>        <span class="hljs-variable language_">self</span>.vocab_size = config.text_config.vocab_size<br><br>        <span class="hljs-comment"># Gemma-2B 作为 LLM</span><br>        language_model = AutoModel.form_config(config=config.text_config)<br>        <span class="hljs-variable language_">self</span>.language_model = language_model<br><br>        <span class="hljs-variable language_">self</span>.pad_token_id = <span class="hljs-variable language_">self</span>.config.pad_token_id <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.pad_token_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.text_config_dtype = <span class="hljs-variable language_">self</span>.config.get_text_config().dtype <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.dtype<br>        <span class="hljs-variable language_">self</span>.post_init()<br></code></pre></td></tr></table></figure><h3 id="两个Proj层搞清楚"><a href="#两个Proj层搞清楚" class="headerlink" title="两个Proj层搞清楚"></a>两个Proj层搞清楚</h3><h4 id="第一个"><a href="#第一个" class="headerlink" title="第一个"></a>第一个</h4><p>Vision LLM中的第一个proj是<strong>模态对齐层，</strong>SigLIP 看到的“像素语言”(image_features)是 1152 维的，Gemma 思考的“文本语言”是 2048 维(hidden_states)。这个层的作用就是把“像素语言”(image_features)翻译成 Gemma 能接受的向量维度，让 Gemma 自己读到了文本 Embedding。</p><h4 id="第二个"><a href="#第二个" class="headerlink" title="第二个"></a>第二个</h4><p>经过Vision LLM的LLM后输出的语义特征需要再经过第二个Proj进行特征压缩，以便后续存储并加速Late interaction的计算，这部分代码体现在&#x2F;colpali&#x2F;colpali_engine&#x2F;models&#x2F;paligemma&#x2F;colpali&#x2F;modeling_colpali.py</p><h4 id="Online与Offine阶段的不同输入搞清楚"><a href="#Online与Offine阶段的不同输入搞清楚" class="headerlink" title="Online与Offine阶段的不同输入搞清楚"></a>Online与Offine阶段的不同输入搞清楚</h4><p>两阶段的输入都是在.&#x2F;colpali_engine&#x2F;models&#x2F;paligemma&#x2F;colpali&#x2F;modeling_colpali.py中Colpali类中完成的，只是输入时数据不同。</p><p><strong>Offline</strong>：输入inputs_ids(包含图像占位符) + pixel_values -&gt; 融合 -&gt; Gemma -&gt; Proj -&gt; 128维向量</p><p><strong>Online</strong>：仅输入input_ids (纯文本) -&gt; Gemma -&gt; Proj -&gt; 128维向量</p><h2 id="colpali的类继承关系"><a href="#colpali的类继承关系" class="headerlink" title="colpali的类继承关系"></a>colpali的类继承关系</h2><p>(点击图片即可放大)通过阅读源代码，根据自己的理解我画出了colpali的类图：</p><img src="/img/fig/paliclass.png"/><h2 id="colqwen2的类继承关系"><a href="#colqwen2的类继承关系" class="headerlink" title="colqwen2的类继承关系"></a>colqwen2的类继承关系</h2><p>(点击图片即可放大)通过阅读源代码，根据自己的理解我画出了colqwen2的类图：</p><img src="/img/fig/colqwenclass.png"/>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从ColBERT到Colpali</title>
    <link href="/2025/11/09/%E4%BB%8EColBERT%E5%88%B0Colpali/"/>
    <url>/2025/11/09/%E4%BB%8EColBERT%E5%88%B0Colpali/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>从ColBERT到Colpali，唯一不变的核心本质是<strong>Late interaction</strong>——晚交互机制。</p>          </div><h2 id="理解Late-interaction"><a href="#理解Late-interaction" class="headerlink" title="理解Late interaction"></a>理解Late interaction</h2><p>梳理一下Late interaction之前所有的“Query-Document”交互方式：</p><img src="/img/fig/lateinter.png"/><ul><li><strong>双塔结构</strong>：将query和文档各自编码为一个向量，然后通过简单的点积或余弦相似度计算分数。</li></ul><blockquote><p> 缺点：强行将长文档中丰富的信息压缩到一个定长的向量中，导致大量细节丢失</p></blockquote><ul><li><strong>交叉编码</strong>：将 Query 和 Document 拼接在一起输入到 BERT 中，让 Query 的词和 Document 的词在每一层网络中都进行的交互。</li></ul><blockquote><p>缺点：Document 的编码依赖于 Query ，无法预先计算文档向量。而且对于每一个查询，都要把全部的候选文档重新跑一遍 BERT，计算成本非常大</p></blockquote><ul><li><strong>全对全交互&#x2F;all-to-all</strong>：Query 和 Document 分别编码，保留所有 Token 向量。然后构建一个交互矩阵去计算 Query 中每个词和 Document 中每个词的相似度。最后通过一个可学习的深层网络去学习这个矩阵，提取匹配特征，最后输出一个分数。</li></ul><blockquote><p>缺点：交互矩阵占大量内存空间，相似度是通过构建的深层网络学出来的，很难利用现有的向量检索引擎进行索引加速</p></blockquote><img src="/img/fig/colbert.png"/><p><strong>Late interaction</strong>：Query 和 Document 分别通过 BERT，但<strong>不压缩</strong>成单独的一个向量，而是保留所有 Token 的 Embedding 列表。将BERT模型输出的 Embedding 通过MaxSim 操作进行细粒度匹配。</p><p>模型输入：query 和 Document</p><p>模型输出：一个MaxSim Score，query与每个文档都计算出一个文档分数，根据这个分数从大到小排序，分数最高的就是最相关的结果。 </p><div class="note note-danger">            <p>[例子] query &#x3D; “苹果 价格”  Document A &#x3D; “红 苹果 好吃” Document B &#x3D; “香蕉 便宜”</p>          </div><p>假设BERT已经跑完拿到了以下的Embedding结果(得到query和Document的Token级编码)：</p><p>q1(苹果): [1.0, 2.0]</p><p>q2(价格): [-1.0, 1.0]</p><p>Document A</p><p>d1a(红): [0.5, 0.5]</p><p>d2a(苹果): [1.0, 2.1] </p><p>d3a(好吃): [-0.5, 0.5]</p><p>Document B</p><p>d1b(香蕉): [2.0, -1.0]</p><p>d2b(便宜): [0.0, 0.1]</p><hr><p>计算query与Document A分数：</p><p>q1(苹果)·d1a(红) &#x3D; 1.5</p><p>q1(苹果)·d2a(苹果) &#x3D; 5.2</p><p>q1(苹果)·d3a(好吃) &#x3D; 0.5</p><p>q2(价格)·d1a(红) &#x3D; 0</p><p>q2(价格)·d2a(苹果) &#x3D; 1.1</p><p>q2(价格)·d3a(好吃) &#x3D; 1.0</p><p>Score(query,Document A) &#x3D; 5.2 + 1.1 &#x3D; 6.3</p><hr><p>计算query与Document B分数：</p><p>q1(苹果)·d1b(香蕉) &#x3D; 0</p><p>q1(苹果)·d2b(便宜) &#x3D; 0.2</p><p>q2(价格)·d1b(香蕉) &#x3D; -3.0</p><p>q2(价格)·d2b(便宜) &#x3D; 0.1</p><p>Score(query,Document B) &#x3D; 0.2 + 0.1 &#x3D; 0.3 </p><hr><p>排序结果：</p><ol><li>Document A</li><li>Document B</li></ol>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年10月总结与展望</title>
    <link href="/2025/10/31/2025%E5%B9%B410%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/10/31/2025%E5%B9%B410%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="摸索、状态差"><a href="#摸索、状态差" class="headerlink" title="摸索、状态差"></a>摸索、状态差</h2><p>国庆节放假回来，终于把第一篇小论文的全部流程走完了；提交camera-ready版本、注册缴费。。。过程一波三折，完全是自己一步步摸索出来的路子。好在老师们都很配合、很给力，Qi老师主动转发了好几次通讯的邮件，非常上心；斌导最后贡献了一张自己的Visa卡，最后经费也是直接卡里直接扣了。</p><p>但是从国庆放假回来，感觉自己就像是一个无头苍蝇一般，每天生活非常无聊。早上来到工位就往那一坐，然后就是漫无目的的瞎看论文，东看一篇、西看一篇，完全没有任何想法和动力，又一次陷入了迷茫的状态。而且我发现，人的生活一旦失去了目标和方向感，自然而然就会变得漫无目的和无比懈怠。</p><p>因为10月几乎每天睡的都很晚，也可能是晚上健身的缘故，反正睡不好。当然也和自己的压力情况有关，因为迷茫、没有方向感，所以才没有动力去做事情。今天看了一篇公众号推文挺有感触的，持续的轻度压力转台会使人造成拖延和成就感幻想，导致无法正常开展工作。我觉着说的没毛病，还是要学会压力的转换。</p><p>不过虽然生活上像个无头苍蝇一样，思绪也跟着飞来飞去的，但是还是做了一些事情的。</p><p>这个月继续剖析阅读了transformers库的源代码，在上个月完成了GPT-2、Llama以及Qwen架构设计的基础之上，10月份又继续梳理了Vision Transformer和CLIP的源代码，并且画图详解了一下模型的架构设计细节，真正吃透设计细节。相关的学习心得体会与自己画的模型架构图都发在了小红书上，欢迎Follow～</p><h2 id="送别"><a href="#送别" class="headerlink" title="送别"></a>送别</h2><p>🕯️🕯️🕯️</p><p>10月18号，伟大的物理学家，俺们老杨家的大名人，无比尊敬的杨振宁先生逝世了。杨先生的学习方法无疑对我产生了巨大影响，<strong>学习就是修正错误的直觉</strong>，这句话会永记心间。先生一路走好，吾辈自强！</p><p>但愿人长久，千里共同途。</p><p><img src="https://i.ytimg.com/vi/94zEK4bTWto/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&rs=AOn4CLBlupVioEqL0LlWpFmhnqx0y7qL7Q" alt="缅怀"></p><h2 id="转折点"><a href="#转折点" class="headerlink" title="转折点"></a>转折点</h2><p>这个月在最后的时候出现了转机，终于不再是一只无头苍蝇了。</p><p>斌导在月底的时候让我联系了Qin老师，我在10月20号加上了Qin老师微信，后续科研上慢慢进入了正轨。</p><p>现在每周二都在参加线上讨论班，Qin的科研能力还是很强的，对我自己而言，这是一次无比宝贵的机会和机遇，无比好好珍惜，多和Qin交流讨论，和Qin老师学习如何去做科研。</p><p>Qin也很热情，上来就甩给我了20多篇的Ppaer List，任务量还是蛮大的，不过我很激动和欣喜，因为总算有任务干了，不会在想月初那样不知道看啥、不知道做啥了。</p><p>现在的研究方向有在一点点明确，每天也感觉有在充满目标和方向感。有时候不得不感慨，人真是很贱的物种，因为有了明确的指导和方向，一个月，发生了两种截然不同的状态，尝到甜头的感觉是真好啊。😅</p><p>希望能一直有好状态，就像Saul一样。</p><p><img src="https://pbs.twimg.com/media/FaNnpIpXgAEUZL_.jpg" alt="better call saul"></p><h2 id="Better-call-Saul"><a href="#Better-call-Saul" class="headerlink" title="Better call Saul"></a>Better call Saul</h2><p>上个月看完Breaking Bad，这个月有续上了Better Call Saul。</p><p>我宣布，风骚律师这部剧，无论是角色塑造、剧本、还有摄影镜头画面，无疑是都是无可逾越的大山，比毒师系列各方面都更成熟。艾美奖真是眼瞎，呸，大SB！🥵</p><p>学习Saul，不屈不饶，无所畏惧。🫵</p><h2 id="11月"><a href="#11月" class="headerlink" title="11月"></a>11月</h2><ul><li>继续梳理源码架构：旋转位置编码、QwenVL系列、Flash Attention</li><li>按部就班推进科研任务，招收准备第二篇小论文，锚定高质量提升</li><li>充满动力，复苏！</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepSeek-OCR innovates Long-Context Compression</title>
    <link href="/2025/10/21/DeepSeek-OCR-innovates-Long-Context-Compression/"/>
    <url>/2025/10/21/DeepSeek-OCR-innovates-Long-Context-Compression/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepSeek-OCR-Contexts-Optical-Compression"><a href="#DeepSeek-OCR-Contexts-Optical-Compression" class="headerlink" title="DeepSeek-OCR: Contexts Optical Compression"></a>DeepSeek-OCR: Contexts Optical Compression</h1><p>记得25年上半年在雁栖湖校区读书的时候看了B站up主EZConder讲了DeeoSeek-V3和R1的系列视频，Top-Down的授课方式给我留下了很深的印象。EZ老师在视频里强调，知识就像是水流一样，后面他基于这种“水流”的思想把DeepSeek发展的来龙去脉很快就梳理出来了，非常值得学习的方式。</p><p>昨天DeepSeek团队推出了新的模型：DeepSeek-OCR，一经发布便火遍全网，也正值自己前一阵子在关注Prompt Compression与Long Contexts Compression的相关内容，所以也想以这种“水流”的思想把论文串一下。Good idea的出现绝非偶然，不仅要关注当前这篇文章的创新点，同时也要关注当前的这篇文章前期工作有什么，思路是从哪些文章延伸过来的，以及基于现有知识可以做出哪些创新。</p><h2 id="上下文压缩时间线"><a href="#上下文压缩时间线" class="headerlink" title="上下文压缩时间线"></a>上下文压缩时间线</h2><p>第一部分将 DeepSeek-OCR 的“光学压缩”思想放入上下文压缩技术发展的时间线中，清晰地构建出一条从“处理”长上下文到“压缩”长上下文的演进脉络。</p><h3 id="Stage1-RAG早期"><a href="#Stage1-RAG早期" class="headerlink" title="Stage1:RAG早期"></a>Stage1:RAG早期</h3><p>这个阶段的核心思想是“不直接让LLM处理全部的上下文，而是在输入前进行筛选和精简”。</p><p>本阶段的核心问题是：<strong>LLM上下文窗口有限且处理成本昂贵。</strong></p><p>解决方案：</p><ol><li><strong>后检索过滤</strong>，通过计算查询与检索文档的向量相似度，丢弃掉不那么相关的文档。[EmbeddingsFilter]</li><li><strong>后检索摘要</strong>，通过计算查询（Query）与检索文档的向量相似度，丢弃掉不那么相关的文档。[LLMChainExtractor]</li></ol><h3 id="Stage2-提示工作与蒸馏"><a href="#Stage2-提示工作与蒸馏" class="headerlink" title="Stage2:提示工作与蒸馏"></a>Stage2:提示工作与蒸馏</h3><p>这个阶段的核心思想是不改变LLM架构，而是学习一种更为精简的的‘soft prompt’来代替冗长的原始上下文。</p><p>核心问题：如何让压缩后的信息能被LLM高效理解。</p><p>解决方案：将长上下文“蒸馏”成密集向量的形式。</p><ol><li>通过训练，让模型学会将长指令或上下文的“要点”（gist）压缩到几个特殊的 gist tokens 中。[Gist]</li><li>训练一个小型模型，专门用来识别并删除原始提示中的“不重要”词元（tokens），同时保证压缩后的提示能让 LLM 做出同样的回答。[LLMLingua系列]</li></ol><h3 id="Stage3-专有压缩器与自编码器"><a href="#Stage3-专有压缩器与自编码器" class="headerlink" title="Stage3:专有压缩器与自编码器"></a>Stage3:专有压缩器与自编码器</h3><p>这个阶段的核心思想是构建一个独立于特定任务、可学习的压缩器，将任意长上下文编码为固定长度的向量表示，是的压缩上下文成为一个通用的、可插拔式的模块组件。</p><p>核心问题：如何实现一种高效且通用的上下文压缩方法？</p><p>解决方案：引入自编码器思想，训练一个 Encoder 将长文本压缩成一组“记忆向量”，然后要求一个 Decoder（通常是固定的 LLM）能够基于这些向量完美地重构原文。[ICAE]</p><h3 id="Stage4-提升压缩效率与可扩展性"><a href="#Stage4-提升压缩效率与可扩展性" class="headerlink" title="Stage4:提升压缩效率与可扩展性"></a>Stage4:提升压缩效率与可扩展性</h3><p>这一阶段的核心思想是在保证压缩质量的同时，将压缩过程的效率推向极致，并解决超长序列的训练难题。</p><p>核心问题：上一阶段的压缩器虽然有效，但压缩本身还是太慢，并且难以在真正超长的文本上进行训练。</p><p>解决方案：优化压缩器架构和训练策略。</p><ol><li>压缩器的瓶颈在于自注意力机制，创新性地仅使用<strong>交叉注意力</strong>。让一小组可学习的“摘要词元”作为Query，去原文中提取信息。直接将压缩的复杂度从O(n²) 降到了**线性复杂度 O(n)**。[In-Context Former]</li><li>使用分块处理和优化采样策略是的模型在极长的序列上进行训练而不会爆显存，将压缩结果直接注入到LLM的KV-Cache使得可扩展性得以提高。[CCF]</li></ol><h2 id="OCR长文档处理"><a href="#OCR长文档处理" class="headerlink" title="OCR长文档处理"></a>OCR长文档处理</h2><p>多模态大语言模型处理长上下文是面临的挑战：计算成本巨大、视觉编码效率低下、长文本采样受限。</p><p>Vision Transformer（ViT）模型通常需要将图像调整为固定分辨率，会丢失原始图像的<strong>长宽比信息</strong>，对于非方形的图像（如图表或文档）尤其次优。</p><p>处理<strong>文档级的高密度文本场景时，</strong>CLIP-style的视觉词汇表难以高效地将所有视觉信息编码到固定数量的Token中，容易导致信息损失。</p><table><thead><tr><th><strong>阶段</strong></th><th><strong>核心论文</strong></th><th><strong>核心思想&#x2F;技术</strong></th><th><strong>解决的局限性</strong></th></tr></thead><tbody><tr><td><strong>视觉词汇表扩展</strong></td><td><strong>Vary</strong></td><td><strong>扩展视觉词汇表（Scaling up the Vision Vocabulary）</strong>：提出<strong>Vary</strong>，为LVLMs增加<strong>新的、针对文档和图表等人工图像</strong>（<strong>Artificial Images</strong>）的视觉词汇表。使用<strong>自回归（Autoregression）</strong>方法训练新的视觉编码器（基于SAM-ViTDet）。</td><td>解决了CLIP-ViT在处理<strong>高分辨率、非英文OCR、文档&#x2F;图表理解</strong>等特殊场景时出现的“视觉词汇表不足（Vision Out-of-Vocabulary）”问题。</td></tr><tr><td><strong>精细化焦点与长上下</strong></td><td><strong>Fox</strong></td><td><strong>聚焦任意位置（Focus Anywhere）</strong>：提出<strong>Fox</strong>，通过<strong>位置感知提示（Position-aware Prompts）</strong>（如点、颜色、方框）来实现细粒度的文档理解，如区域OCR&#x2F;翻译&#x2F;总结。并扩展到<strong>多页面文档</strong>。</td><td>实现了对文档内容的<strong>细粒度交互</strong>（类似于“阅读笔”功能），并且在保持高压缩比的同时，支持了<strong>跨页面的上下文理解</strong>。</td></tr><tr><td><strong>DeepSeek-OCR：核心思路的最终体现</strong></td><td><strong>DeepSeek-OCR</strong></td><td><strong>上下文光学压缩（Contexts Optical Compression）</strong>：将文本处理为图像，使用<strong>DeepEncoder</strong>（包含窗口注意力、卷积压缩、全局注意力）<strong>高效地将图像Token压缩到极少量</strong>，然后由MoE解码器解码。</td><td>实现了极高压缩比（如10倍压缩率）和高解码精度（如97%），从而<strong>根本上解决了大模型处理长上下文的计算瓶颈</strong>。</td></tr></tbody></table><h2 id="DeepSeek-OCR"><a href="#DeepSeek-OCR" class="headerlink" title="DeepSeek-OCR"></a>DeepSeek-OCR</h2><h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>核心思想：探索一种全新的长上下文压缩方法，即上下文光学压缩(Contexts Optical Compression)。传统的大语言模型（LLMs）在处理长文本时，会因为序列长度的增加而面临巨大的计算挑战。</p><p>实现思路：<strong>利用视觉模态作为文本信息的高效压缩媒介</strong>。将文本内容渲染成图片，然后利用视觉模型（VLM）来处理这张图片。由于一张图片可以用远少于等效数字文本的vision tokens来表示，因此可以实现极高的信息压缩率。</p><h3 id="架构细节"><a href="#架构细节" class="headerlink" title="架构细节"></a>架构细节</h3><p><img src="/./img/fig/deepseekocr1.jpg" alt="deepseekocr"></p><p><strong>编码器：DeepEncoder</strong>：负责从输入图像中提取特征、进行标记化（tokenizing）并压缩视觉表示。</p><p>组成：</p><ul><li>**感知部分 (SAM)**：采用一个8000万参数的SAM（Segment Anything Model）模型，它以窗口注意力（window attention）为主，负责处理高分辨率图像的局部感知特征。</li><li>**16倍卷积压缩器 (Conv 16x)**：在感知部分和知识部分之间，使用一个双层卷积模块，对视觉Token进行16倍的降采样。这极大地减少了进入后续全局注意力模块的Token数量，从而有效控制了计算量和内存占用。</li><li>**知识部分 (CLIP)**：采用一个3亿参数的CLIP模型，它以密集的全局注意力为主，负责提取图像的全局知识性特征。</li></ul><p><strong>解码器：DeepSeek-3B-MoE Decoder</strong>：解码器采用了DeepSeek-3B-MoE模型，这是一个拥有30亿参数但激活参数仅为5.7亿的混合专家（Mixture-of-Experts）模型。它的作用是接收由DeepEncoder生成的、被高度压缩的视觉Tokens，并根据用户的prompt生成最终的文本结果。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>天空不设限</title>
    <link href="/2025/10/11/%E5%A4%A9%E7%A9%BA%E4%B8%8D%E8%AE%BE%E9%99%90/"/>
    <url>/2025/10/11/%E5%A4%A9%E7%A9%BA%E4%B8%8D%E8%AE%BE%E9%99%90/</url>
    
    <content type="html"><![CDATA[<p>国庆假期刚回来，实在没心气读论文🥲。正赶上今天还是周六调休，在工位摸鱼，把邓予恒的《天空不设限》、《天空不设限2》读完了。作为创一代，很佩服他的勇气与一路的坚持，当然也离不开必要的运气成分。</p><p>邓予恒、孙宇晨都是我非常欣赏的创业一代，他们的三观和对财富、对社会的认知总能不断地帮助我刷新认知。今天读了邓予恒的两本著作，做一些书摘记录下来，方便日后自己反复阅读与体会。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>成长认知</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>口述历史之姥姥姥爷史</title>
    <link href="/2025/10/05/%E5%8F%A3%E8%BF%B0%E5%8E%86%E5%8F%B2%E4%B9%8B%E5%A7%A5%E5%A7%A5%E5%A7%A5%E7%88%B7%E5%8F%B2/"/>
    <url>/2025/10/05/%E5%8F%A3%E8%BF%B0%E5%8E%86%E5%8F%B2%E4%B9%8B%E5%A7%A5%E5%A7%A5%E5%A7%A5%E7%88%B7%E5%8F%B2/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/life/laolaolaoye.jpg"></p><p>照片中的帅哥是我的姥爷，姓名刘增才，今年69岁，是河北省沧州市—–村人。照片中的美女是我的姥姥，姓名邵长芬，今年72岁，是河北省沧州市河间市—–村人。</p><p>为什么要写他们两个呢？因为今天晚上我们仨人在一起吃晚饭，聊起来很多很多的往事，这顿晚饭大概吃了40多分钟，期间我听到了很多过去的事，这些事有些我小的时候经历过，有些是他们小时候的事，这种陌生与好奇的感觉不禁让我想要赶紧动手记下来。</p><p>我从上幼儿园以来，我妈就把我放在了镇上的姥姥家，我从这是读完了小学、初中、高中；可以说，我人生前24年陪伴最多的人就是我的姥姥姥爷，对他们的感情也是最深厚的。今天晚饭，他们和我讲了很多很多往事，我之间没有听说过的事，我想记下来，因为我觉着这对我而言是一笔财富。只有清楚地了解我家庭的过去，才能更有勇气走好未来的漫漫人生路。五千年前的事情我们心里门儿清，但是五十年前的事情我们一无所知。</p><p>今天的记录是客观的，客观在就是原原本本的记录；可能某些往事尚未盖棺定论，写在里面显得敏感突兀，但是我觉着不能避之不谈。我记录的不是这件事、那件事的真相，我记录的只是从她们口中最原汁原味的表述。</p><hr><p>我：小时候你们俩都是什么“学历”啊？</p><p>姥爷：我那时候是高小毕业，放到现在应该算是初中毕业。那时候我在班里是体育委员，我脑子挺好使的，就是没赶上高考。</p><p>姥姥：我小学毕业，考上初中了，但是上初中需要去隔壁镇上上学，家里都说女孩子家没有出村上学的，索性我也不上了；虽然我小学毕业，但是我认字、识字不少，不是文盲，当年背毛主席语录顶数我背的最快。</p><p>我：小时候你们是不是都当红卫兵啊？</p><p>姥爷：当，那时候天天运动。</p><p>姥姥：别提了，那时候真是太热闹了。我们那时候白天踢正步、伏地行进、跑步训练；到了晚上，只要一听到号声，甭管几点必须起来跑操，一跑操就是几里地远，可折腾人了。当年地主们都挨批斗，那些年轻的男红卫兵们半夜把人家关在一间黑屋子里，当着我们面，用新长出来的木条抽人家，抽的人家浑身衣服包着血丝。我记得有一个我们村的地主，最后实在忍不了，用皮带在自己屋子上吊死了。给人家带高帽子，那帽子有近一米长，脖子上挂着大牌子，整体拉着全村游行。那地主家的女的都裹小脚，感觉小脚支撑不住身体，走路都一晃一晃的。</p><p>我：那你们家庭没受影响吗？</p><p>姥爷：我们是下中农，没影响。</p><p>姥姥：我爷爷当年管粮食，但是我们家不是富农，是上中农，没受影响。当年赶集去，路口都有红卫兵看着，当面让你背毛主席语录，背不出来不让赶集。我当年背的最快记得最准，但是因为我们家是上中农，我评优秀红小兵的时候不能参选。</p><p>我：这老哥几个不容易啊！</p><p>姥姥：这一辈子可算是吃过苦过来的啊！</p><p>姥爷：我兄弟四个，这里面最有文化的是我大哥、你大姥爷。他年轻的时候教书，之后在镇上乡镇企业站当站长，管理全镇上的国有企业，因为有文化，外号叫“刘罗锅”。毛笔字写得好，懂风水生辰八字，会看时辰；你这名字就是你大姥爷给你起的。</p><p>姥姥：二姥爷年轻的时候在北京杀猪卖猪肉，当时在北京回来赚了钱在镇上开了第一家信用社，当年你妈妈不上学了就在里面当会计。这一大家子里面，就是三姥爷家婆媳关系一直不和，总是闹矛盾，现在也不说话，哎没法说。你以后得记着，不管怎么样，首先要家和，要不什么也干不好。</p><p>我：当年是怎么一步步闯过来的？</p><p>姥爷：我毕业以后在国品厂当会计，给供销社送货，一天挣一块五。后面在种子站卖种子、农药，后来种子站就没了。再往后，塑料在咱们这兴起来了，就开始和你大舅舅一起卖塑料原料，这一干就是一辈子。</p><p>姥姥：那些年受的苦别提了，但是我这辈子最骄傲的事就是培养了你和你老舅这两位大学生，走到哪我都特别有面子。</p><hr><p><strong>历史知识补充：</strong></p><ul><li>1951年10月中央人民政府政务院公布《关于改革学制的决定》，全日制小学学制未改动，增加了对成人实施小学教育的“工农速成初等学校”、“业余初等学校”和识字学校。1959年3月中共中央、国务院关于试验改革学制，即“学制要缩短、教育要革命”“开门办学”，从1960年开始，部分地区实施了小学五年一贯制、中学五年一贯制、中小学十年一贯制或初中小学九年一贯制、高中再分文理科等，这一时期由于小学师资普及，基本取消了初小、高小划分，普及了完全小学。[wiki百科]</li><li><strong>红卫兵</strong>，是中华人民共和国在文化大革命时期对部分特殊人群的一种称呼，红卫兵在文革中所扮演的角色并不相同。总体而言，红卫兵是受极左思潮影响下成立的群众组织，也指其成员。[wiki百科]</li><li><strong>乡镇企业</strong>是在乡镇举办的，以农村集体经济组织或者农民投资为主的企业。在中华人民共和国，乡镇企业由原来的社队企业发展而来。[wiki百科]</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>家族史</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年9月总结与展望</title>
    <link href="/2025/10/04/2025%E5%B9%B49%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/10/04/2025%E5%B9%B49%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/life/huijia.jpg"></p><h2 id="拖欠"><a href="#拖欠" class="headerlink" title="拖欠"></a>拖欠</h2><p>九月基本上没有更新blog内容，其实不是没有写东西，而是一直在整理。</p><p>这个月阅读了大量的经典工作的源代码，把GPT-2、Llama-2、Qwen2&amp;3的源代码都阅读复现完了，进行了架构细节的梳理，后续会同步更新。架构细节对比图是我自己根据源代码的设计细节自己手动绘制实现的，网上没有见过的细节对比梳理，首次尝试手绘，希望大家批评指正。</p><p>多模态方面阅读了CLIP以及ViT的论文以及源代码，完成了复现与架构设计。</p><p>Prompt压缩的最新文章一直在跟进阅读，但是由于阅读源代码，论文阅读耽搁了很久没有推进，<strong>十月份要在论文上赶一赶进度了</strong>。</p><h2 id="玩和吃"><a href="#玩和吃" class="headerlink" title="玩和吃"></a>玩和吃</h2><p>魁圣阁的烧麦真不错，味真足，真心推荐！</p><p>华联地下一层的麻辣香锅差评！！！我才点了多少啊，花了老子71块大洋😭，再也不吃了。。。</p><p>严骥带着我和刘熙宸去看了IMAX的F1，真好看，位列本年度最佳毋庸置疑；后面要多去看IMAX了，争取每周去看一场电影。</p><p>去年的地坛书市错过了，今年赶上了，人好多和赶集一样，体验一般。</p><h2 id="回忆涌上心头"><a href="#回忆涌上心头" class="headerlink" title="回忆涌上心头"></a>回忆涌上心头</h2><p>九月的最后一天和茂然回河大了，校园的一草一木都是那样的亲切，好几座新的大楼拔地而起，熟悉又陌生；应了那句古话：“雕栏玉砌应犹在，只是朱颜改”啊！</p><p>逛了宿舍、学院教学楼、图书馆，一幕幕满载着无数的青春回忆，如过眼云烟般在脑海中涌现。</p><p>国庆第一天去任丘参加了马哥瑶姐的婚礼，晚上带着茂然和老于在沧州市里逛了逛，吃了晚饭，难得相聚😋</p><h2 id="略带疲惫，回家充电"><a href="#略带疲惫，回家充电" class="headerlink" title="略带疲惫，回家充电"></a>略带疲惫，回家充电</h2><p>九月总是熬夜，睡眠不是很好；国庆回家后这几天睡的真香，我要把九月失去的觉都补回来～</p><p>十月希望能延续好在家里调整好的作息吧！🙏</p><p>十月要多读论文了，有了九月打下的源码基础，尽快找到能做的点子来，加油，晚安！✌️</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈GPT-2、LlaMA与Qwen架构的对比分析</title>
    <link href="/2025/09/21/%E6%B5%85%E8%B0%88GPT-2%E3%80%81LlaMA%E4%B8%8EQwen%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"/>
    <url>/2025/09/21/%E6%B5%85%E8%B0%88GPT-2%E3%80%81LlaMA%E4%B8%8EQwen%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>九月份投递完第一篇论文初稿以后，认真把三个经典大模型架构的源代码阅读、复现了一遍。动机出于以下因素：首先，暑假期间读完了<a href="https://sebastianraschka.com/">Sebastian Raschka</a>大佬写的Build a Large Language Model (From Scratch)这本书，把GPT-2的完整架构熟悉了一遍，但是一些具体的细节设计和GPT源代码还是有些区别。其次，考虑到后续论文复现的工作大多是建立在transformers库中的源代码基础之上，索性多花费了一些时间把“LLM三驾马车”的架构设计对比学习一下，知道后续在此基础之上如何去修改、动哪里。</p><p>全文是我在认真阅读并复现了GPT-2、LlaMA-2与Qwen3的transformers源代码后撰写的架构设计对比分析，如有失偏颇，烦请给位读者大佬们批评指正！</p><h2 id="Top-Down"><a href="#Top-Down" class="headerlink" title="Top-Down"></a>Top-Down</h2><p>开篇先宏观理一下我对三个架构设计的对比分析：</p><ul><li>整体架构设计上LLaMA的架构设计要比GPT架构更<strong>简洁</strong>，LLaMA舍弃了GPT架构中首位的Dropout层。</li><li>相比于GPT-2的<code>绝对位置编码</code>，Llama使用<code>ROPE旋转位置编码</code>。</li><li>相比于GPT-2中的<code>LayerNorm层归一化</code>，Llama使用的是<code>RMSNorm层归一化</code>，并且RMSNorm层主要用在LlamaDecoder<strong>后</strong>、LlamaAttention块<strong>前后</strong>、以及最后经过MLP<strong>输出后</strong>。</li><li>在MLP层的设计上，LLaMA使用的<code>SwiGELU激活+门控制方法</code>，GPT使用<code>GELU激活+Conv1D线性变换</code>的方式。</li><li>Qwen2的源代码基本上和LLaMA源代码保持一致，相比于GPT-2和Llama架构增加了<strong>self.sliding_window滑动窗口注意力</strong>部分。</li><li>Qwen3的源代码基本上和Qwen2源代码保持一致，只是相比于Llama和Qwen2，在计算注意力的时候增加了self.q_norm和self.k_norm的RMSNorm归一化层，提升训练的稳定性。</li></ul><h2 id="具体展开"><a href="#具体展开" class="headerlink" title="具体展开"></a>具体展开</h2><h3 id="LlaMA的简洁如何体现？"><a href="#LlaMA的简洁如何体现？" class="headerlink" title="LlaMA的简洁如何体现？"></a>LlaMA的简洁如何体现？</h3><p>在GPT-2中，输入的词嵌入（Token Embeddings）和位置嵌入（Position Embeddings）相加后，会立刻经过一个nn.Dropout层，然后再进入Transformer的Decoder层堆栈。Llama直接将词嵌入送入Decoder层，省去了这个初始的Dropout。</p><p>通过阅读transformers库中的源代码，我绘制了GPT-2与LLaMA的模型架构对比图，如下所示：</p><p><img src="/img/fig/tgpt.jpg" alt="gpt-2"></p><p><img src="/img/fig/tllama.jpg" alt="llama"></p><p>观察上述架构设计图可知，实际上GPT-2和LlaMA的架构设计细节与《Attention is All You Need》、《Improving Language Understanding by Generative Pre-Training》论文里面绘制的模型架构图是有所区别的。</p><p>实际的数据转换过程应该是，经过位置编码后，进入模型的主体部分，GPT-2的数据流动过程为DropOut层 -&gt; GPT Blocks -&gt; Final LayerNorm，而LlaMA简洁就体现在去掉了GPT-2经过绝对位置编码后的Dropout层，使得输入数据经过旋转位置编码后，直接进入LlaMADecoderBlocks层，所以Llama模型的数据流动过程只有LlaMADecoderBlocks -&gt; LlamaRMSNorm。</p><h3 id="DecoderBlock"><a href="#DecoderBlock" class="headerlink" title="DecoderBlock"></a>DecoderBlock</h3><p>在DecoderBlocks里面，GPT-2主要先经过了多头注意力层，之后再经过多头掩码注意力块，之后输入到FFN网络中；而Llama的DecoderBlocks层中，只有一个多头注意力层计算块，之后便输入到LlamaRMSNorm层中；Qwen的DecoderBlocks基本上就是“继承”了Llama的源代码，只是注意力块与Llama模型有所不同。</p><p>通过阅读源代码，描绘了三个模型的DecoderBlock架构设计对比图如下所示：</p><p><img src="/img/fig/deocderblock.jpg" alt="decoderblock"></p><p>GPT-2使用标准的<strong>LayerNorm</strong>，而Llama的RMSNorm省去了LayerNorm中的“减去均值”这一步，只保留了“除以方差”的缩放，计算量更小，实验证明在性能上几乎没有损失。</p><p>三个模型的DecoderBlock层源代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#下面是GPT的前向传播逻辑</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT2Block</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, layer_idx=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        hidden_size = config.hidden_size<br>        inner_dim = config.n_inner <span class="hljs-keyword">if</span> config.n_inner <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-number">4</span> * hidden_size<br><br>        <span class="hljs-variable language_">self</span>.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)<br>        <span class="hljs-variable language_">self</span>.attn = GPT2Attention(config=config, layer_idx=layer_idx)<br>        <span class="hljs-variable language_">self</span>.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)<br><br>        <span class="hljs-keyword">if</span> config.add_cross_attention:<br>            <span class="hljs-variable language_">self</span>.crossattention = GPT2Attention(config=config, is_cross_attention=<span class="hljs-literal">True</span>, layer_idx=layer_idx)<br>            <span class="hljs-variable language_">self</span>.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)<br><br>        <span class="hljs-variable language_">self</span>.mlp = GPT2MLP(inner_dim, config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        ...</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-built_in">tuple</span>[torch.Tensor], <span class="hljs-type">Optional</span>[<span class="hljs-built_in">tuple</span>[torch.Tensor, <span class="hljs-built_in">tuple</span>[torch.FloatTensor, ...]]]]:<br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.ln_1(hidden_states)<br>        attn_output, self_attn_weights = <span class="hljs-variable language_">self</span>.attn(<br>            ...<br>        )<br>        <span class="hljs-comment"># residual connection</span><br>        hidden_states = attn_output + residual<br><br>        <span class="hljs-keyword">if</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># add one self-attention block for cross-attention</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">&quot;crossattention&quot;</span>):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">f&quot;If `encoder_hidden_states` are passed, <span class="hljs-subst">&#123;self&#125;</span> has to be instantiated with &quot;</span><br>                    <span class="hljs-string">&quot;cross-attention layers by setting `config.add_cross_attention=True`&quot;</span><br>                )<br>            residual = hidden_states<br>            hidden_states = <span class="hljs-variable language_">self</span>.ln_cross_attn(hidden_states)<br>            cross_attn_output, cross_attn_weights = <span class="hljs-variable language_">self</span>.crossattention(<br>                ...<br>            )<br>            <span class="hljs-comment"># residual connection</span><br>            hidden_states = residual + cross_attn_output<br><br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.ln_2(hidden_states)<br>        feed_forward_hidden_states = <span class="hljs-variable language_">self</span>.mlp(hidden_states)<br>        <span class="hljs-comment"># residual connection</span><br>        hidden_states = residual + feed_forward_hidden_states<br><br>        outputs = (hidden_states,)<br>        <span class="hljs-keyword">if</span> output_attentions:<br>            outputs += (self_attn_weights,)<br>            <span class="hljs-keyword">if</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                outputs += (cross_attn_weights,)<br><br>        <span class="hljs-keyword">return</span> outputs<br>    <br><span class="hljs-comment">#----------------------</span><br><span class="hljs-comment">#下面是LlaMA的前向传播逻辑</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaDecoderLayer</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: LlamaConfig, layer_idx: <span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_size = config.hidden_size<br><br>        <span class="hljs-variable language_">self</span>.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)<br><br>        <span class="hljs-variable language_">self</span>.mlp = LlamaMLP(config)<br>        <span class="hljs-variable language_">self</span>.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>        <span class="hljs-variable language_">self</span>.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        ...</span><br><span class="hljs-params">    </span>) -&gt; torch.Tensor:<br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.input_layernorm(hidden_states)<br>        <span class="hljs-comment"># Self Attention</span><br>        hidden_states, _ = <span class="hljs-variable language_">self</span>.self_attn(<br>            ...<br>        )<br>        hidden_states = residual + hidden_states<br><br>        <span class="hljs-comment"># Fully Connected</span><br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.post_attention_layernorm(hidden_states)<br>        hidden_states = <span class="hljs-variable language_">self</span>.mlp(hidden_states)<br>        hidden_states = residual + hidden_states<br>        <span class="hljs-keyword">return</span> hidden_states<br>      <br><span class="hljs-comment">#----------------------</span><br><span class="hljs-comment">#下面是Qwen的前向传播逻辑</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3DecoderLayer</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Qwen3Config, layer_idx: <span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_size = config.hidden_size<br><br>        <span class="hljs-variable language_">self</span>.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)<br><br>        <span class="hljs-variable language_">self</span>.mlp = Qwen3MLP(config)<br>        <span class="hljs-variable language_">self</span>.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>        <span class="hljs-variable language_">self</span>.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>        <span class="hljs-variable language_">self</span>.attention_type = config.layer_types[layer_idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        ...</span><br><span class="hljs-params">    </span>) -&gt; torch.Tensor:<br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.input_layernorm(hidden_states)<br>        <span class="hljs-comment"># Self Attention</span><br>        hidden_states, _ = <span class="hljs-variable language_">self</span>.self_attn(<br>            hidden_states=hidden_states,<br>            attention_mask=attention_mask,<br>            position_ids=position_ids,<br>            past_key_values=past_key_values,<br>            use_cache=use_cache,<br>            cache_position=cache_position,<br>            position_embeddings=position_embeddings,<br>            **kwargs,<br>        )<br>        hidden_states = residual + hidden_states<br><br>        <span class="hljs-comment"># Fully Connected</span><br>        residual = hidden_states<br>        hidden_states = <span class="hljs-variable language_">self</span>.post_attention_layernorm(hidden_states)<br>        hidden_states = <span class="hljs-variable language_">self</span>.mlp(hidden_states)<br>        hidden_states = residual + hidden_states<br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure><h3 id="MLP层"><a href="#MLP层" class="headerlink" title="MLP层"></a>MLP层</h3><p>在MLP层的设计上，LLaMA与Qwen使用的<code>SwiGELU激活+门控制方法</code>，GPT使用<code>GELU激活+Conv1D线性变换</code>的方式。阅读源代码，将两种设计方式绘制了架构图进行对比：</p><p><img src="/img/fig/tmlp.jpg" alt="mlp"></p><p>GPT-2的MLP是一个标准的结构：一个线性层将维度放大，经过<strong>GELU</strong>激活函数，再由另一个线性层将维度缩小。Llama采用了更先进的<strong>SwiGLU</strong>，使用<strong>三个</strong>线性层 (gate_proj, up_proj, down_proj) 而不是两个，gate_proj和up_proj并行作用于输入，然后将gate_proj的输出经过SiLU&#x2F;Swish激活函数后，作为“门”与up_proj的输出进行逐元素相乘。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#GPT的MLP层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT2MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, intermediate_size, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        embed_dim = config.hidden_size<br>        <span class="hljs-variable language_">self</span>.c_fc = Conv1D(intermediate_size, embed_dim)<br>        <span class="hljs-variable language_">self</span>.c_proj = Conv1D(embed_dim, intermediate_size)<br>        <span class="hljs-variable language_">self</span>.act = ACT2FN[config.activation_function]<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(config.resid_pdrop)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">tuple</span>[torch.FloatTensor]]</span>) -&gt; torch.FloatTensor:<br>        hidden_states = <span class="hljs-variable language_">self</span>.c_fc(hidden_states)<br>        hidden_states = <span class="hljs-variable language_">self</span>.act(hidden_states)<br>        hidden_states = <span class="hljs-variable language_">self</span>.c_proj(hidden_states)<br>        hidden_states = <span class="hljs-variable language_">self</span>.dropout(hidden_states)<br>        <span class="hljs-keyword">return</span> hidden_states<br>      <br><span class="hljs-comment">#----------------------------</span><br><span class="hljs-comment">#GPT的MLP层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3MLPandLlamaMLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <span class="hljs-variable language_">self</span>.hidden_size = config.hidden_size<br>        <span class="hljs-variable language_">self</span>.intermediate_size = config.intermediate_size<br>        <span class="hljs-variable language_">self</span>.gate_proj = nn.Linear(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.up_proj = nn.Linear(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.down_proj = nn.Linear(<span class="hljs-variable language_">self</span>.intermediate_size, <span class="hljs-variable language_">self</span>.hidden_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.act_fn = ACT2FN[config.hidden_act]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        down_proj = <span class="hljs-variable language_">self</span>.down_proj(<span class="hljs-variable language_">self</span>.act_fn(<span class="hljs-variable language_">self</span>.gate_proj(x)) * <span class="hljs-variable language_">self</span>.up_proj(x))<br>        <span class="hljs-keyword">return</span> down_proj<br></code></pre></td></tr></table></figure><h3 id="Attention计算的区别"><a href="#Attention计算的区别" class="headerlink" title="Attention计算的区别"></a>Attention计算的区别</h3><p>Qwen3相比于Llama和Qwen2，在计算注意力的时候增加了self.q_norm和self.k_norm，起作用是在生成query和key的时候使用，将生成的q和k要经过一次归一化。</p><p><img src="/img/fig/tattn.jpg" alt="attention"></p><p>self.q_norm和self.k_norm通常是RMSNorm层。它们被应用在Query和Key向量经过RoPE之后<strong>，但在它们进行点积运算（Q @ K.T）</strong>之前。</p><p>在FP16或BF16等低精度浮点数训练下，经过多层计算后，向量中可能会出现极大的或极小的值，导致数值不稳定。Q和K的点积对向量的“长度”（L2范数）非常敏感。如果某些Q或K向量的范数异常大，它们会在Softmax中产生极端接近于1的概率，导致梯度消失，使得模型学习变得困难。</p><p>同时，在标准的自回归模型中，每个新的token需要关注前面<strong>所有</strong>的token。当序列长度达到几万甚至几十万时，计算量和内存占用会变得巨大（复杂度为O(序列长度²)），这使得处理超长上下文非常困难。滑动窗口注意力提出，一个token的语义通常与它<strong>最邻近</strong>的上下文关系最密切。因此，没有必要让每个token都关注从开头到现在的全部历史Token。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以GPT-2为base model，Llama和Qwen系列模型在架构上基本相似；但是Llama和Qwen系列模型与GPT-2模型在架构细节上有明显的区别。</p><p>Llama与Qwen的LayerNorm归一化层都是使用的RMSNorm归一化函数，MLP层的设计上Llama和Qwen系列都使用了门控制机制并使用SwiGELU函数作为激活函数，在数据经过Attention与MLP之前都要经过归一化层，最后经过RMSNorm层后输出。</p><p>而GPT-2模型是在位置编码完后经过Dropout层，模型最后输出前也要经过Dropout层，Llama和Qwen系列省去了Dropout层，在架构设计上相比于GPT-2更为简洁。</p><p>GPT-2和Llama主要使用的全量注意力方式计算前向传播过程中的Attention；而Qwen2和Qwen3在注意力计算上进行了改进，使用了混合注意力机制；Qwen2引入滑动窗口机制，Qwen3在滑动窗口基础之上，又在计算点积之前加入了query和key的RMSNorm层。</p><p><img src="/img/fig/tfinal.jpg" alt="summary"></p><h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><div class="note note-success">            <p>Qwen与LlaMA中的MLP层和FFN层的作用是什么？两者是一个东西吗？</p>          </div><p><strong>在Llama和Qwen等Transformer模型的语境下，它们基本上指的是同一个东西。</strong>FFN源自开创性论文《Attention Is All You Need》的<strong>官方术语</strong>。它特指在自注意力（Self-Attention）子层之后那个由两个线性层和一个激活函数组成的模块。</p><p><strong>MLP (Multi-Layer Perceptron)</strong>: 这是一个更<strong>通用、更古老</strong>的神经网络术语，泛指任何由多个全连接层（通常带有非线性激活函数）组成的网络。</p><p>FFN与MLP的作用是：</p><ol><li>增加非线性，提升模型表达能力，Transformer模型中，自注意力计算和线性投影本质上都是线性的（Softmax除外）。如果没有MLP层中的非线性激活函数（如GELU, SiLU&#x2F;Swish），那么整个Transformer堆叠起来也只不过相当于一个巨大的线性变换，其表达能力将极其有限，无法学习复杂的模式。<strong>非线性是“深度”学习之所以强大的根本。</strong></li><li>MLP层通常采用一种“<strong>扩展-压缩</strong>”的结构；在原始的 hidden_size 空间中可能难以分离或理解的复杂特征，在被映射到更高维的 intermediate_size 空间后，可能就变得线性可分或更容易处理了。</li></ol><div class="note note-success">            <p>GPT模型MLP组件中的Conv1D？</p>          </div><p>一个kernel_size&#x3D;1的Conv1D在数学上等价于一个全连接的线性层。为了能<strong>直接加载原始的TensorFlow权重文件</strong>，他们实现了一个Conv1D来模拟线性层，因为TensorFlow的全连接层权重矩阵的存储方式恰好与PyTorch中Conv1D的权重存储方式（在特定排列下）相匹配，并且与PyTorch的nn.Linear层的权重<strong>互为转置</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年8月总结与展望</title>
    <link href="/2025/09/03/2025%E5%B9%B48%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/09/03/2025%E5%B9%B48%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>今天是2025年9月3日，特地把八月份的总结放在了今天去写。</p><p>今天是<strong>中国人民抗日战争暨世界反法西斯战争</strong>胜利80周年纪念日，天安们广场举办了盛大的阅兵式，正义必胜、和平必胜、人名必胜！</p><center>  <img src="https://x0.ifengimg.com/res/2025/14FA726210F67D0464195FD01C15A0DBCEE9EBC8_size136_w1080_h720.jpg" style="zoom:50%;" /></center><h2 id="爱党、爱国永记心间"><a href="#爱党、爱国永记心间" class="headerlink" title="爱党、爱国永记心间"></a>爱党、爱国永记心间</h2><p>今年9·3阅兵从头看到尾，认认真真地把新兵种和新式尖端武器都了解了下，我军的信息化能力绝对是“世界一流”水平；当DF-61开过天安门广场，国威军威尽显，民族自豪感溢满屏幕！</p><p>踏踏实实过日子吧，就这些武器一亮相，用不着我们整天关心啥国家大事。真理就在大炮的射程之内，好巧不巧，我们的东风打击范围覆盖全球🇨🇳</p><p>身为一名D员，我觉着爱党爱国不是整天挂在嘴边的，少说空话多做实事才是王道。其实我挺看不惯pyq疯狂转发直播链接这种行为的，现在信息这么发达，老子想看啥又用不着你丫刷屏转发。还有一些人，阅兵前发些找三不着两、稀奇古快的东西，真到活动开始了，咱就瞅它这个爱国劲儿啊，比丫谁都足。言论自由这是肯定的，我也不反对，但是我就是看不惯丫一套套的形式主义的劲儿！</p><p>有事需要咱就狠狠往前冲，没事少bb别给社会添堵，这就是鄙人最大的诚意！</p><h2 id="平淡"><a href="#平淡" class="headerlink" title="平淡"></a>平淡</h2><p>这两个字很有杀伤力～</p><p>回所一个月了，总体过的还是很快乐的；这一个月基本上的作息是晚上0:30睡觉，早上8:30起床；九点多到工位十一点半吃饭，上午在岗俩半小时。中午13:30上工，下午17:30下工，在岗4小时。晚上八点多健身完后再回工位学到23:00结束一天的打工人生活，全天累计在工位9小时。</p><p>平心而论，我还是属于比较吃苦耐劳类型的，唯一需要自己注意的，就是自己的工作效率。有多少时间刷手机，上厕所，走神儿。。。</p><p>平淡的时间就要自己想事去做，找事去做；转变自己的思维认知和心态，老板给的活，别管什么活用心去做，都要从每一项工作中有所得。</p><p>回所一个月生活还是很充实的，周末可以一起去吃大餐、喝大酒，平时努力工作，晚上健身打球跑步，四个字总结目前的生活：还行，挺好！</p><p>平淡其实最能杀死一切，因为平淡意味着你安于现状，对周围一切不再有好奇心。</p><p>其实反思自己目前的状态，最能直观感受到的就是睡的愈来愈晚，起的愈来愈晚😭，其实我一直觉着早上早起一会多挤出一些时间来可以做很多很多的事情。一直想下定决心有所改变，但是始终没有任何进展😑</p><p>希望九月自己能有所改变；同样的，老话常说无利不起早，想想六月底、七月初那会，每天11:40睡觉，早上6:10起床，为的就是赶进度写论文，那一个月真是拼，也是熬过来了。</p><h2 id="保持兴趣与好奇"><a href="#保持兴趣与好奇" class="headerlink" title="保持兴趣与好奇"></a>保持兴趣与好奇</h2><p>八月份刷完了《从零构建大模型》、《图解大模型》两部大部头，收获颇丰；周围朋友们现在也都开始陆续出去实习了，最初的热闹新鲜过后，随之而来的就是平淡与乏味。</p><p>想想自己真正想要什么，现在这个阶段对什么仍然感兴趣，还想并且还能(有足够的精力)去持续学一些什么，这是目前最重要的。(自从来北京读研以后，心态确实发生了很大的转变，慢慢开始有更多的生活了，不会逼着自己每天都要学习，算是和自己和解了很多，允许自己不够好，但是要求自己还要努力💪)</p><p>围绕大模型相关领域能不能深入学进去，读到研究生了，该有一门安生立命的本事了，不然进入社会咋混啊！下定决心排除万难也要争取学好，学精！</p><h2 id="想做的"><a href="#想做的" class="headerlink" title="想做的"></a>想做的</h2><p>近期想围绕Prompt压缩内容继续展开，开启第二篇小论文的工作，最近把硬提示和软提示里面的经典代表作都读了读，收获颇丰；但是还是要继续找点子、找idea。</p><p>九月份在突破大模型架构的基础上，继续把预训练和微调搞清楚；结合ICAE这篇论文，认认真真完完整整地把指令微调的细节搞懂突破掉！加油！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零构建大模型</title>
    <link href="/2025/08/24/%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2025/08/24/%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="从零构建大模型"><a href="#从零构建大模型" class="headerlink" title="从零构建大模型"></a>从零构建大模型</h1><h2 id="GPT-2架构"><a href="#GPT-2架构" class="headerlink" title="GPT-2架构"></a>GPT-2架构</h2><p>GPT-2是基于Transformer的Decoder架构，细节实现如下：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*jbcwhhB8PEpJRk781rML_g.png" alt="GPT-2"></p><h2 id="组件代码"><a href="#组件代码" class="headerlink" title="组件代码"></a>组件代码</h2><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> d_out % num_heads == <span class="hljs-number">0</span>, <span class="hljs-string">&quot;d_out must be divisible by num_heads&quot;</span><br><br>        <span class="hljs-variable language_">self</span>.d_out = d_out<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-variable language_">self</span>.head_dim = d_out // num_heads  <span class="hljs-comment"># Reduce the projection dim to match desired output dim</span><br><br>        <span class="hljs-variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.out_proj = nn.Linear(d_out, d_out)  <span class="hljs-comment"># Linear layer to combine head outputs</span><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;mask&quot;</span>, torch.triu(torch.ones(context_length, context_length), diagonal=<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        b, num_tokens, d_in = x.shape<br><br>        keys = <span class="hljs-variable language_">self</span>.W_key(x)  <span class="hljs-comment"># Shape: (b, num_tokens, d_out)</span><br>        queries = <span class="hljs-variable language_">self</span>.W_query(x)<br>        values = <span class="hljs-variable language_">self</span>.W_value(x)<br><br>        <span class="hljs-comment"># We implicitly split the matrix by adding a `num_heads` dimension</span><br>        <span class="hljs-comment"># Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)</span><br>        keys = keys.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br>        values = values.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br>        queries = queries.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br><br>        <span class="hljs-comment"># Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span><br>        keys = keys.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        queries = queries.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        values = values.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span><br>        attn_scores = queries @ keys.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># Dot product for each head</span><br><br>        <span class="hljs-comment"># Original mask truncated to the number of tokens and converted to boolean</span><br>        mask_bool = <span class="hljs-variable language_">self</span>.mask.<span class="hljs-built_in">bool</span>()[:num_tokens, :num_tokens]<br><br>        <span class="hljs-comment"># Use the mask to fill attention scores</span><br>        attn_scores.masked_fill_(mask_bool, -torch.inf)<br><br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>]**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br>        attn_weights = <span class="hljs-variable language_">self</span>.dropout(attn_weights)<br><br>        <span class="hljs-comment"># Shape: (b, num_tokens, num_heads, head_dim)</span><br>        context_vec = (attn_weights @ values).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span><br>        context_vec = context_vec.contiguous().view(b, num_tokens, <span class="hljs-variable language_">self</span>.d_out)<br>        context_vec = <span class="hljs-variable language_">self</span>.out_proj(context_vec)  <span class="hljs-comment"># optional projection</span><br><br>        <span class="hljs-keyword">return</span> context_vec<br></code></pre></td></tr></table></figure><h3 id="LayerNorm-层归一化"><a href="#LayerNorm-层归一化" class="headerlink" title="LayerNorm - 层归一化"></a>LayerNorm - 层归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, emb_dim</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.eps = <span class="hljs-number">1e-5</span><br>        <span class="hljs-variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))<br>        <span class="hljs-variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        mean = x.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        var = x.var(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>, unbiased=<span class="hljs-literal">False</span>)<br>        norm_x = (x - mean) / torch.sqrt(var + <span class="hljs-variable language_">self</span>.eps)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.scale * norm_x + <span class="hljs-variable language_">self</span>.shift<br></code></pre></td></tr></table></figure><h3 id="GELU-激活函数"><a href="#GELU-激活函数" class="headerlink" title="GELU - 激活函数"></a>GELU - 激活函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GELU</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * x * (<span class="hljs-number">1</span> + torch.tanh(<br>            torch.sqrt(torch.tensor(<span class="hljs-number">2.0</span> / torch.pi)) *<br>            (x + <span class="hljs-number">0.044715</span> * torch.<span class="hljs-built_in">pow</span>(x, <span class="hljs-number">3</span>))<br>        ))<br></code></pre></td></tr></table></figure><h3 id="FeedForward-全连接前馈网络"><a href="#FeedForward-全连接前馈网络" class="headerlink" title="FeedForward - 全连接前馈网络"></a>FeedForward - 全连接前馈网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.Sequential(<br>            nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], <span class="hljs-number">4</span> * cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>]),<br>            GELU(),<br>            nn.Linear(<span class="hljs-number">4</span> * cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>]),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.layers(x)<br></code></pre></td></tr></table></figure><h3 id="Transformer块"><a href="#Transformer块" class="headerlink" title="Transformer块"></a>Transformer块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.att = MultiHeadAttention(<br>            d_in=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            d_out=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            context_length=cfg[<span class="hljs-string">&quot;context_length&quot;</span>],<br>            num_heads=cfg[<span class="hljs-string">&quot;n_heads&quot;</span>],<br>            dropout=cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>],<br>            qkv_bias=cfg[<span class="hljs-string">&quot;qkv_bias&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.ff = FeedForward(cfg)<br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Shortcut connection for attention block</span><br>        shortcut = x<br>        x = <span class="hljs-variable language_">self</span>.norm1(x)<br>        x = <span class="hljs-variable language_">self</span>.att(x)   <span class="hljs-comment"># Shape [batch_size, num_tokens, emb_size]</span><br>        x = <span class="hljs-variable language_">self</span>.drop_shortcut(x)<br>        x = x + shortcut  <span class="hljs-comment"># Add the original input back</span><br><br>        <span class="hljs-comment"># Shortcut connection for feed-forward block</span><br>        shortcut = x<br>        x = <span class="hljs-variable language_">self</span>.norm2(x)<br>        x = <span class="hljs-variable language_">self</span>.ff(x)<br>        x = <span class="hljs-variable language_">self</span>.drop_shortcut(x)<br>        x = x + shortcut  <span class="hljs-comment"># Add the original input back</span><br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="GPT-Model"><a href="#GPT-Model" class="headerlink" title="GPT Model"></a>GPT Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;context_length&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br><br>        <span class="hljs-variable language_">self</span>.trf_blocks = nn.Sequential(<br>            *[TransformerBlock(cfg) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cfg[<span class="hljs-string">&quot;n_layers&quot;</span>])])<br><br>        <span class="hljs-variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.out_head = nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, in_idx</span>):<br>        batch_size, seq_len = in_idx.shape<br>        tok_embeds = <span class="hljs-variable language_">self</span>.tok_emb(in_idx)<br>        pos_embeds = <span class="hljs-variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))<br>        x = tok_embeds + pos_embeds  <span class="hljs-comment"># Shape [batch_size, num_tokens, emb_size]</span><br>        x = <span class="hljs-variable language_">self</span>.drop_emb(x)<br>        x = <span class="hljs-variable language_">self</span>.trf_blocks(x)<br>        x = <span class="hljs-variable language_">self</span>.final_norm(x)<br>        logits = <span class="hljs-variable language_">self</span>.out_head(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型面试200问</title>
    <link href="/2025/08/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%95200%E9%97%AE/"/>
    <url>/2025/08/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%95200%E9%97%AE/</url>
    
    <content type="html"><![CDATA[<center>  <h1>大模型面试200问</h1>  问题原创作者：李博杰(本书译者)</center><center>  整理作答：杨桂淼</center><div class="note note-success">            <p>所有问题的作答没有标准答案，全部都是由本人通过阅读《图解大模型：生成式 AI 原理与实战》之后的所学所得，如有任何不妥之处，烦请诸位读者大佬批评斧正🙏！</p>          </div><h2 id="第-1-章-大模型简介"><a href="#第-1-章-大模型简介" class="headerlink" title="第 1 章　大模型简介"></a>第 1 章　大模型简介</h2><p>Q1：Transformer 中的编码器和解码器有什么区别，只有编码器或者只有解码器的模型是否有用？</p><p>Q2：GPT 跟原始 Transformer 论文的模型架构有什么区别？</p><p>Q3：仅编码器（BERT 类）、仅解码器（GPT 类）和完整编码器 - 解码器架构各有什么优缺点？</p><p>Q4：为什么说 Transformer 的自注意力机制相对于早期 RNN 中的注意力机制是一个显著的进步？</p><p>Q5：大模型为什么有最长上下文长度的概念？为什么它是指输入和输出的总长度？</p><p>Q6：大模型的首字延迟、输入吞吐量、输出吞吐量分别是如何计算的？不同应用场景对首字延迟、输入吞吐量和输出吞吐量的需求分别是什么？</p><p>Q7：预训练和微调的两步范式为什么如此重要？基础模型通过预训练获得了哪些核心能力？微调在引导模型遵循指令、回答问题和对齐人类价值观方面起到什么作用？</p><p>Q8：Llama-3 8B 的综合能力比 Llama-1 70B 的能力还强，是如何做到的？</p><h2 id="第-2-章-词元和嵌入"><a href="#第-2-章-词元和嵌入" class="headerlink" title="第 2 章　词元和嵌入"></a>第 2 章　词元和嵌入</h2><p>Q9：大模型的分词器和传统的中文分词有什么区别？对于一个指定的词表，一句话是不是只有一种唯一的分词方式？</p><p>Q10：为什么传统 BM25 检索对中文分词的质量很敏感，而大模型对分词器的选取不敏感？</p><p>Q11：GPT-4、Llama 等现代大模型采用的字节级 BPE 分词器相比传统的 BPE 分词器有什么优点？</p><p>Q12：国内预训练的大模型与海外模型相比，是如何做到用相对更少的词元表达中文语料的？</p><p>Q13：大模型是如何区分聊天历史中用户说的话和 AI 说的话的？</p><p>Q14：大模型做工具调用的时候，输出的工具调用参数是如何与文本回复区分开来的？</p><p>Q15：参考章节中用播放列表数据训练歌曲嵌入的案例，设计一个使用嵌入技术解决电子商务产品推荐的系统。使用什么数据作为“句子”的等价物？如何将用户行为融入嵌入模型？</p><p>Q16：word2vec 的训练过程中，负例的作用是什么？</p><p>Q17：传统的静态词嵌入（如 word2vec）与大模型产生的与上下文相关的嵌入相比，有什么区别？有了与上下文相关的嵌入，静态词嵌入还有什么价值？</p><p>Q18：与上下文相关的嵌入是如何解决一词多义问题的，如技术语境下，英文 token 可能表示词元、代币、令牌，而中文“推理”可能表示 reasoning 或 inference ？</p><p>Q19：在 word2vec 等词嵌入空间中，存在 king - man + woman ≈ queen 的现象，这是为什么？大模型的词元嵌入空间是否也有类似的属性？</p><h2 id="第-3-章-LLM-的内部机制"><a href="#第-3-章-LLM-的内部机制" class="headerlink" title="第 3 章　LLM 的内部机制"></a>第 3 章　LLM 的内部机制</h2><p>Q20：大模型怎么知道它的输出该结束了？</p><p>Q21：训练时如何防止模型看到未来的词元？</p><p>Q22：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头是只关注一个词元吗？ softmax 之前为什么要除以 $\sqrt{d_{k}}$ ？</p><p>Q23：Q 和 K 在注意力的表达式里看起来是对称的，但 KV 缓存里为什么只有 KV，没有 Q ？</p><p>Q24：如果没有 KV 缓存，推理性能会降低多少？</p><p>Q25：为什么 Transformer 中需要残差连接？</p><p>Q26：Transformer 中的 LayerNorm 跟 ResNet 中的 BatchNorm 有什么区别，为什么 Llama-3 换用了 RMSNorm ？</p><p>Q27：Transformer 中前馈神经网络的作用是什么？注意力层中已经有 softmax 非线性层，那么前馈神经网络是否必要？</p><p>Q28：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数？</p><p>Q29：大模型在数学计算时，为什么经常不准确？</p><p>Q30：模型深度（层数）与宽度（隐藏维度大小）、注意力头数量、上下文长度等参数之间是如何相互影响的？如果要训练一个比当前模型参数规模大 10 倍的模型，你会如何调整这些参数？</p><p>Q31：以一个你熟悉的开源模型为例，介绍模型中每个矩阵的大小和形状。</p><p>Q32：大模型推理过程中，内存带宽和算力哪个是瓶颈？以一个你熟悉的开源模型为例，计算输入批次大小达到多少时，能够平衡利用内存带宽和算力？</p><p>Q33：从统计学角度看，Transformer 输出层假设词元符合什么分布？</p><p>Q34：给定一个支持 8K 上下文的开源模型，如何把它扩展成支持 32K 上下文的模型？上下文长度增加后对 KV 缓存会带来什么挑战？</p><p>Q35：为什么注意力机制需要多个头？ GQA、MQA 优化跟简单减少注意力头的数量相比，有什么不同？ GQA、MQA 优化的是训练阶段还是推理阶段？</p><p>Q36：Flash Attention 并不能减少计算量，为什么能实现加速？ Flash Attention 是如何实现增量计算 softmax 的？</p><p>Q37：RoPE（旋转位置嵌入）相比 Transformer 论文中的绝对位置编码有什么优点？ RoPE 在长上下文外推时会面临什么挑战？</p><p>Q38：由于训练样本长度往往小于最大上下文长度，把多个训练样本放到同一个上下文中训练时，如何避免它们互相干扰？</p><p>Q39：如何利用一个小规模的大模型提升大规模模型的推理性能，并尽量不影响大模型的推理结果？推测解码并没有减少计算量，为什么能提升推理性能？</p><h2 id="第-4-章-文本分类"><a href="#第-4-章-文本分类" class="headerlink" title="第 4 章　文本分类"></a>第 4 章　文本分类</h2><p>Q40：如何基于表示模型生成的嵌入向量实现文本分类？</p><p>Q41：使用嵌入向量实现分类和使用生成模型直接分类的方法相比，有什么优缺点？</p><p>Q42：如果没有标注数据，如何基于嵌入模型实现文本分类？如何优化标签描述来提高零样本分类的准确率？</p><p>Q43：书中嵌入模型 + 逻辑回归的分类方式获得了 0.85 的 F1 分数，而零样本分类方式获得了 0.78 的 F1 分数，如果有标注数据，什么情况下会选择零样本分类？</p><p>Q44：Transformer 为什么比朴素贝叶斯分类器效果好很多？朴素贝叶斯分类器的条件独立性假设有什么问题？</p><p>Q45：掩码语言建模与 BERT 的掩蔽策略相比有何不同？这种预训练方式如何帮助模型在下游的文本分类任务中获得更好的性能？</p><p>Q46：假设你有一个包含 100 万条客户评论的数据集，但只有 1000 条带有标签的数据，同时利用有标签和无标签数据，结合表示模型和生成模型的优势，构建一个分类系统？</p><p>Q47：使用生成模型进行文本分类时，以下三个提示词哪个会更有效？</p><ul><li><p>“Is the following sentence positive or negative?”</p></li><li><p>“Classify the sentiment of this movie review as positive or negative.”</p></li><li><p>“You are a sentiment analysis expert. Given a movie review, determine if it expresses a positive or negative opinion. Return only the label ‘positive’ or ‘negative’.”</p></li></ul><h2 id="第-5-章-文本聚类和主题建模"><a href="#第-5-章-文本聚类和主题建模" class="headerlink" title="第 5 章　 文本聚类和主题建模"></a>第 5 章　 文本聚类和主题建模</h2><p>Q48：有了强大的生成式大模型，嵌入模型还有什么用？请举一个适合嵌入模型但不适合生成模型的例子。（提示：推荐系统）</p><p>Q49：给定大量的文档，如何把它们聚类成几簇，并总结出每一簇的主题？</p><p>Q50：词袋法和文档嵌入在实现原理上有什么区别？词袋法是不是一无是处了？</p><p>Q51：BERTopic 中的 c-TF-IDF 与传统 TF-IDF 有何不同？这种差异如何帮助改进主题表示的质量？</p><p>Q52：LDA、BTM、NMF、BERTopic、Top2Vec 等主题模型有什么优缺点？对长文档、短文档、高质量需求的垂直领域分别应使用何种模型？</p><p>Q53：基于质心的和基于密度的文本聚类算法有什么优缺点？</p><p>Q54：为什么在主题建模流程中，将聚类和主题表示这两个步骤分开处理是有益的？</p><p>Q55：在一个主题建模项目中，你发现生成的主题中有大量重叠的关键词，如何使用本章介绍的技术来改进主题之间的区分度？</p><p>Q56：在使用 BERTopic 时，如果很大比例的文档被归类为离群值，这可能是什么原因导致的？如何调整聚类参数？</p><p>Q57：在新闻或社交媒体推荐系统中，主题往往随时间快速演化，如何检测新兴主题？</p><p>Q58：如何构建一个内容平台的推荐系统，冷启动时通过文本聚类和主题建模提供推荐，有一定量用户交互数据后又能利用这些数据提升推荐效果？</p><h2 id="第-6-章-提示工程"><a href="#第-6-章-提示工程" class="headerlink" title="第 6 章　提示工程"></a>第 6 章　提示工程</h2><p>Q59：针对翻译类任务、创意写作类任务、头脑风暴类任务，temperature 和 top_p 分别该怎么设置？如何验证你选择的参数设置是否最优？</p><p>Q60：为什么一些模型把温度设置成 0，输出的内容仍然有一定的不确定性？（提示：推测解码）</p><p>Q61：对于指定的大模型，如何通过提示词减少其幻觉？</p><p>Q62：一个专业的提示词模板应该由哪几部分构成？为什么提示词中需要描述角色定义？</p><p>Q63：对于一个复杂的提示词，如何测试其中哪些部分是有用的，哪些部分是无用的？</p><p>Q64：如何设计提示词模板，尽量防止提示词注入？如何在系统层面检测提示词注入攻击？</p><p>Q65：如果把用户信息放在系统提示词中，但在对话轮数较多后，大模型经常忘记用户信息，如何解决？</p><p>Q66：如何让 ChatGPT 输出它自己的系统提示词？</p><p>Q67：在没有推理模型之前，如何让模型先思考后回答？思维链、自洽性、思维树等几种技术有什么优缺点？</p><p>Q68：在创意写作任务中，如何让模型生成多个可能输出，再从中选取一个最好的？</p><p>Q69：如果需要模型遵循指定的格式输出，提示词应该怎么写？</p><p>Q70：如何保证模型的输出一定是合法的 JSON 格式？（提示：限制采样）</p><p>Q71：将大模型用于分类任务时，如何保证其输出一定是几个类别之一，不会输出无关内容？（提示：限制采样）</p><p>Q72：如果做一个学英语的应用，如何保证它说的话一定在指定的词汇表中，绝不会出现超纲的生词？（提示：限制采样）</p><h2 id="第-7-章-高级文本生成技术与工具"><a href="#第-7-章-高级文本生成技术与工具" class="headerlink" title="第 7 章　高级文本生成技术与工具"></a>第 7 章　高级文本生成技术与工具</h2><p>Q73：如果我们需要生成小说的标题、角色描述和故事梗概，单次模型调用生成效果不佳时，如何分步生成？</p><p>Q74：如果用户跟模型对话轮次过多，超出了模型的上下文限制，但又希望尽可能保留用户的对话信息，该怎么办？</p><p>Q75：在角色扮演场景中，用户跟模型对话轮次过多后（但没有超过上下文限制），模型经常没有注意到过去对话中发生过的关键事件，怎么办？</p><p>Q76：用户跟模型对话轮数较多后，处理输入词元的预填延迟升高，应该如何解决？（提示：持久化 KV 缓存）</p><p>Q77：如何编写一个智能体（agent），让它像 OpenAI Deep Research 一样，能够自主思考下一步该搜索什么关键词，浏览哪个网页？</p><p>Q78：如何编写一个智能体，帮助用户规划一次包含机票预订、酒店安排和景点游览的旅行？需要配置哪些工具？如何确保系统在面对不完整或矛盾信息时仍能提供合理建议？</p><p>Q79：如果单一智能体的提示词过长，导致性能下降，如何将其拆分为多个智能体，并在合适的时机调用不同的智能体？不同智能体间如何进行有效的上下文传递和结果整合？</p><p>Q80：不同基础模型在不同任务上的表现不同，如何基于任务特性自动选择最合适的模型？</p><p>Q81：如果一个工具的调用时间较长，如何让智能体在等待工具调用返回前能够持续与用户交互或调用其他工具，并在工具调用返回时及时做出下一步动作？</p><p>Q82：对于角色扮演场景下的持续对话任务，如何缓存角色设定和历史对话，降低输入词元的成本和延迟？</p><p>Q83：智能体如何处理记忆中的时间信息，例如“昨天讨论的问题”？如何在用户长时间不回复时，主动询问用户？</p><p>Q84：多个智能体在同一房间里讨论时，如何防止多个智能体互相抢话，又避免冷场？</p><p>Q85：支持实时语音的智能体如何既保持低延迟，又避免与用户抢话？</p><p>Q86：支持语音输入的智能体，如何用非声学方法，通过语义理解用户是在对旁边人说话还是对它说话？</p><p>Q87：PTQ 和 QAT 量化方法的区别是什么，有什么优缺点？</p><h2 id="第-8-章-语义搜索与-RAG"><a href="#第-8-章-语义搜索与-RAG" class="headerlink" title="第 8 章　语义搜索与 RAG"></a>第 8 章　语义搜索与 RAG</h2><p>Q88：在 RAG 中，为什么要把文档划分成多个块进行索引？如何解决文档分块后，内容上下文缺失的问题？如何处理跨片段的依赖关系？</p><p>Q89：如果发现向量相似度检索的匹配效果不佳，除了更换嵌入模型，还有哪些办法？</p><p>Q90：向量相似度检索不能实现关键词的精确匹配，传统关键词检索不能匹配语义相近的词，如何解决这对矛盾？</p><p>Q91：向量相似度检索已经是根据语义相似度匹配，为什么还需要重排序模型？</p><p>Q92：为什么要在向量相似度检索前，对用户输入的话进行改写？</p><p>Q93：RAG 系统检索的文档可能包含冲突信息或过时数据，如何在生成回答时防止被这些信息误导？</p><p>Q94：如何使检索模块能够从生成模块获得反馈并动态调整检索策略，例如给不同的文档标注可信度？</p><p>Q95：如何提升 RAG 系统的可解释性，包括清晰标注生成内容的来源，以及量化展示系统对回答的确信度？</p><p>Q96：智能体如何把处理企业任务的经验总结到知识库中，并在后续任务中引入知识库中的经验？如何保证经验不断积累，而不是简单用新的经验覆盖已有的经验？</p><p>Q97：如果需要根据一本长篇小说的内容回答问题，小说长度远远超出上下文限制，应该如何综合利用摘要总结和 RAG 技术，使其能同时回答故事梗概和故事细节？</p><p>Q98：如何将 RAG 系统从纯文本扩展到多模态，支持检索图像、视频、图文并茂的文档等多模态信息，并在生成回答时以多模态形式呈现，例如包含原始文档中的图表和视频？</p><p>Q99：如果需要设计一个 AI 智能伴侣，每天记录用户说过的所有话、做过的所有事，持续多个月，如何在需要的时候快速检索出相关的记忆，让 AI 能够根据记忆回答问题？综合对话历史窗口化、摘要总结、RAG 等技术。</p><h2 id="第-9-章-多模态大模型"><a href="#第-9-章-多模态大模型" class="headerlink" title="第 9 章　多模态大模型"></a>第 9 章　多模态大模型</h2><p>Q100：为什么 ViT 不能简单地像处理文本词元那样，为每个图像块分配一个唯一的、离散的 ID，而是必须采用线性投影生成连续的嵌入向量？</p><p>Q101：在 CLIP 训练过程中，为什么需要同时最大化匹配图文对的相似度和最小化非匹配对的相似度？</p><p>Q102：BLIP-2 采用了冻结预训练 ViT 和 LLM，仅训练 Q-Former 的策略。这种设计的核心动机和优势是什么？</p><p>Q103：BLIP-2 是如何连接预训练的图片编码器和预训练 LLM 的？为何不直接将视觉编码器的输出连接到语言模型，而要引入 Q-Former 这一中间层结构？</p><p>Q104：将多模态特征映射到文本特征空间时不可避免会产生信息损失，交叉注意力、Q-Former 和线性映射等方法的信息保留能力有什么区别？</p><p>Q105：BLIP-2 模型的图像 - 文本对比学习、图像 - 文本匹配、基于图像的文本生成三个任务分别是什么作用？与今天的 Qwen-VL 等多模态模型有什么区别？</p><p>Q106：基于已经预训练好的模态编码器、模态解码器、文本大模型做多模态模型，多模态预训练和多模态微调两个阶段分别需要什么数据，需要冻结模型的哪些参数？</p><p>Q107：CLIP 和 BLIP-2 在处理图像时，都会将其预处理成固定尺寸。如何处理长宽差异巨大的图像？</p><p>Q108：在 BLIP-2 实现视觉问答（VQA）时，模型是如何同时处理输入的图像和文本问题的？</p><p>Q109：以一个你熟悉的开源多模态模型为例，输入一张 512×512 的图片和一个 100 词元的问题，其首字延迟大约是多少，其中模态编码器、Q-Former 和 LLM 部分各占多少？</p><p>Q110：能够操作计算机图形界面的多模态大模型每步操作的延迟通常需要几秒，延迟的构成是什么？</p><p>Q111：人类对不熟悉的界面操作较慢，但对熟悉的界面操作很快。如何让多模态模型像人类一样快速操作熟悉的界面？</p><p>Q112：现有一个能力较弱的多模态模型和一个能力较强的文本模型（如 DeepSeek-R1），如何结合两者的能力，回答多模态问题？</p><p>Q113：如果一个垂直领域（如医学）的图文对训练数据极为有限，如何为该领域构建多模态大模型？</p><p>Q114：如何构建一个 AI 照片助手，能够索引用户的上万张照片，根据用户的查询高效地检索到相关照片？</p><p>Q115：端到端语音模型中，语音是如何转换成词元表示的？</p><p>Q116：端到端语音模型是如何实现在工具调用进行过程中，继续与用户实时语音交互的？工具调用的结果与用户的语音输入在模型的上下文中如何区分？</p><p>Q117：图像生成模型（如 Stable Diffusion）与图像理解模型（如 CLIP、BLIP-2）在技术路线上有什么异同？为什么扩散模型在推理时需要噪声，而自回归模型不需要？</p><h2 id="第-10-章-构建文本嵌入模型"><a href="#第-10-章-构建文本嵌入模型" class="headerlink" title="第 10 章　构建文本嵌入模型"></a>第 10 章　构建文本嵌入模型</h2><p>Q118：为什么通过对比（相似 &#x2F; 不相似样本）学习通常比仅学习相似样本能更有效地捕捉文本的语义或特定任务特征？</p><p>Q119：如何生成负例以提升模型性能？如何构建高质量的难负例？</p><p>Q120：双编码器和交叉编码器有什么区别？假设你需要构建一个大规模语义搜索引擎，你会优先选择哪种架构来计算查询与文档的相似度，为什么？如果任务变为对少量候选对进行精确重排序，你的选择会改变吗？</p><p>Q121：多负例排序损失（MNR）、余弦相似度损失和 softmax 损失在训练嵌入模型时有哪</p><p>些优缺点？在什么场景下，余弦相似度损失可能比 MNR 损失更合适？</p><p>Q122：为什么 TSDAE 选择使用特殊词元而非平均池化作为句子表征？</p><p>Q123：相比有监督方法，TSDAE 这类无监督预训练方法在处理领域外数据或进行领域适配时有何优缺点？</p><p>Q124：MTEB 相比基础的语义相似度测试（STSB）有哪些改进？其中包括哪些类别的嵌入任务？</p><p>Q125：如何根据用户偏好反馈数据，持续提升 RAG 系统的重排序模型性能？</p><p>Q126：如果一个 RAG 系统没有人类用户，仅供 AI agent 使用，如何自动收集 AI agent 的反馈，持续提升 RAG 系统的重排序模型性能？</p><p>Q127：如果要构建一个类似 Google 图片搜索的文本嵌入模型，根据输入图片找到相似图片，应该如何训练？</p><p>Q128：如果要构建一个非自然语言垂直领域（如氨基酸序列、集成电路设计）的语义搜索系统，但该领域标注数据极少，应该如何训练嵌入模型？</p><p>Q129：随着新数据和新概念的不断产生，如何检测何时需要更新文本嵌入模型，实现增量的持续学习？</p><h2 id="第-11-章-为分类任务微调表示模型"><a href="#第-11-章-为分类任务微调表示模型" class="headerlink" title="第 11 章　为分类任务微调表示模型"></a>第 11 章　为分类任务微调表示模型</h2><p>Q130：在微调任务中，应该冻结哪些层的权重？微调编码器前几层、编码器后几层、前馈神经网络层有什么区别？</p><p>Q131：如果有标注的训练数据很少，如何扩增训练数据的数量？（提示：SetFit）</p><p>Q132：SetFit 在训练分类头之前，会先利用对比学习微调 Sentence Transformer。为什么这个微调步骤对于在极少标注样本下取得高性能至关重要？</p><p>Q133：相比直接使用一个冻结的通用 Sentence Transformer 提取嵌入向量再训练分类器，SetFit 的对比学习微调方法能让嵌入向量学习到哪些更适用于下游分类任务的特性？</p><p>Q134：在继续预训练时，如何在保证模型获得特定领域知识的同时，最大程度保留其通用能力？</p><p>Q135：请比较以下三种方案在垂直领域文本分类任务上的优缺点：(a) 直接使用通用 BERT 模型微调；(b) 在医疗文本上继续预训练 BERT 后再微调；(c) 从头开始用医疗文本预训练模型再微调。</p><p>Q136：在基于掩码语言建模的继续预训练中，应该如何设计掩码出现的位置和概率？</p><p>Q137：在微调过程中，为什么模型对学习率等超参数通常比预训练阶段更敏感？</p><p>Q138：在命名实体识别任务中，当 BERT 将单词拆分成多个词元时，如何解决标签对齐问题？</p><p>Q139：如何用领域数据训练一个在嵌入式设备上使用的小模型，同时处理文本分类、命名实体识别和语义搜索三个任务？</p><p>Q140：假设一个嵌入模型的训练语料主要由英文构成，其中文表现不佳，如何用较低的继续预训练成本，提升其中文能力？</p><p>Q141：对于一个关键场景的分类任务，例如将“严重不良反应”误分类为“轻微不良反应”比反向错误更危险，如何选择评估指标，解决数据集类别不平衡的问题，并修改损失函数？</p><h2 id="第-12-章-微调生成模型"><a href="#第-12-章-微调生成模型" class="headerlink" title="第 12 章　微调生成模型"></a>第 12 章　微调生成模型</h2><p>Q142：在 Llama-3 70B 开源模型基础上，如何微调模型以使其输出风格更简洁、更像微信聊天，并保证输出的内容符合中国的大模型安全要求？你认为需要准备多少数据，用多少GPU 训练多长时间？</p><p>Q143：有人声称一篇文章是用 DeepSeek-R1 生成的，并给了你生成所用的完整提示词，你应该如何证实或证伪这个说法？如何量化计算这个提示词生成这篇文章的概率？（提示：利用困惑度）</p><p>Q144：计算一个拥有 96 个 Transformer 块，且每个块有 12 288 × 12 288 权重矩阵的模型，使用秩为 8 的 LoRA 后，需要微调的参数量是多少？微调过程中的每一步需要多少计算量？相比全量微调减少了多少？</p><p>Q145：QLoRA 中的分块量化如何解决了普通量化导致的信息损失问题？</p><p>Q146：现有一个若干篇文章组成的企业知识库，希望通过 SFT 方法让模型记住，如何将其转换成适合 SFT 的数据集？如何确定 SFT 所需数据集的大小？</p><p>Q147：如果微调数据模板中缺少了结束标记 </s> 会产生什么影响？</p><p>Q148：微调模型时，学习率、LoRA alpha、LoRA rank 等超参数通常应该如何设置？应该如何决定模型何时停止训练，是不是验证集损失函数越低效果就越好？</p><p>Q149：在微调过程中，损失函数应该仅计算输出部分，还是同时计算输入和输出部分？两种方案各有什么优缺点？</p><p>Q150：微调后的模型上线后发现一些反复出错的用例，应当怎样修改 SFT 数据集？</p><p>Q151：模型对话轮次较多后，出现模型重复用户的提问或者之前轮次的回答等“复读机”问题，应该怎样通过微调方法解决？</p><p>Q152：目前最流行的几个模型分别在什么领域表现较好？为什么有些模型在排行榜中表现</p><p>突出，但在实际使用中表现不佳？</p><p>Q153：Chatbot Arena 的模型评估方法相比固定测试集有什么优缺点？</p><p>Q154：PPO 和 DPO 在计算效率上、实现复杂度上、训练稳定程度上有什么区别？</p><p>Q155：如果现有人类偏好数据集质量高但数量有限，应该用 PPO 还是 DPO ？</p><p>Q156：PPO 中的 Proximal（近端）是什么意思？如何防止模型在微调数据集以外的问题上泛化能力下降？如何防止模型收敛到单一类型高奖励回答？</p><p>Q157：PPO 中演员模型、评论家模型、奖励模型、参考模型的作用分别是什么？</p><p>Q158：PPO 是如何解决 RL 中经典的稀疏奖励和奖励黑客（reward hacking）问题的？</p><p>Q159：PPO 中的归一化优势函数、值函数剪裁、熵正则化等关键技巧有什么作用？</p><p>Q160：DPO 中 beta 参数是什么意思，增大或减小它会有什么影响？</p><p>Q161：设想一个网站上都是 AI 生成的内容，统计了每篇内容的平均用户停留时长，如何将其转化为 DPO 所需的偏好数据？对于小红书和知乎两种类型的网站，处理方式有什么区别？</p><p>Q162：对一个 ChatGPT 类型的网站，如何把用户行为转化为 DPO 数据？例如点赞点踩、重新生成、复制、分享、后续追问等。</p><p>Q163：什么是大模型的对齐问题？如何避免大模型输出训练语料中的个人隐私信息？</p><p>Q164：如何通过模型微调，尽量解决提示词注入的问题？</p><p>Q165：现有 100 条回答用户问题的规则，完全放在提示词中指令遵循效果不佳，如何构建微调数据集和利用 RL 训练，让模型微调后能够遵从这 100 条规则？</p><h2 id="图解推理大模型"><a href="#图解推理大模型" class="headerlink" title="图解推理大模型"></a>图解推理大模型</h2><p>Q166：根据缩放定律，如何估算训练一个特定规模的大模型所需的预训练数据集大小和所需算力？</p><p>Q167：从大模型原理的角度说明，为什么 Llama-3 70B 模型不可能在不输出思维链的前提下，可靠地解决 24 点问题。（即输入 24 点的问题描述和 4 个 100 以内的整数，要求立即输出一个单词 Yes 或 No）</p><p>Q168：通过“let’s think step by step”提示词触发的思维链模式，与推理模型的原理有什么不同？同样是测试时计算，为什么推理模型的上限更高？</p><p>Q169：推理模型的 RL 与非推理模型的 RLHF 有什么区别？</p><p>Q170：根据 AlphaZero 玩桌游的研究，训练时计算和测试时计算的算力最优配比是多少？</p><p>Q171：如果需要针对垂直领域微调推理模型，过程奖励模型（PRM）和结果奖励模型（ORM）分别适合什么场景？</p><p>Q172：在 MCTS 方法中，如何平衡探索和利用？探索和利用分别使用什么方式来评估？</p><p>Q173：STaR 方法是如何让模型通过自我生成的推理数据来改进自身的？它有什么优缺点？</p><p>Q174：推理模型在后训练过程中，思维链会越来越长，这样结果的准确率提升了，但响应延迟也增加了。如何处理推理深度与响应延迟的权衡？</p><p>Q175：如何让推理模型根据问题复杂度、用户需求和系统负载自动调整推理深度？</p><p>Q176：为什么推理模型每个输出词元的成本一般高于架构和参数量相同的非推理模型？</p><p>Q177：在实时语音对话应用中，如何利用推理模型，又不让用户忍受过高的响应延迟？</p><p>Q178：如何用 RL 方法提升一个大模型的工具调用能力？如何训练模型，使其能够智能地决定何时依靠内部推理能力以及何时调用外部工具，例如写一段代码来解决复杂的推理问题，而不是在输出的推理过程中穷举所有可能？</p><p>Q179：提示工程、RAG、SFT、RL、RLHF 方法应该分别在什么场景下应用？例如：快速迭代基本能力（提示工程）、用户个性化记忆（提示工程）、案例库和事实知识（RAG）、输出格式和语言风格（SFT）、领域基础能力（SFT）、领域深度思考能力（RL）、领域工具调用能力（RL）、根据用户反馈持续优化（RLHF）?</p><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><p>Q180：DeepSeek-R1 与 DeepSeek-R1-Zero 的训练过程有什么区别，各自有什么优缺点？既然 R1-Zero 生成的推理过程可读性差，在非推理任务上的表现也不如 R1，R1-Zero 存在的价值是什么？ R1 训练过程是如何解决 R1-Zero 的上述问题的？</p><p>Q181：为什么说 DeepSeek-R1-Zero 可能开启了一条让模型智力水平超越人类的路径？</p><p>Q182：为什么 DeepSeek-R1 在创意写作任务中，只需较短的思考过程，就能写出比DeepSeek-V3 基座模型有趣很多的内容？</p><p>Q183：DeepSeek-R1 为什么没有使用 PRM、MCTS、集束搜索等方法?</p><p>Q184：DeepSeek-R1 使用的 GRPO 与 PPO 有什么区别？优势值归一化是如何解决传统 PPO 算法中的值函数估计问题的？</p><p>Q185：GRPO 中的 KL 惩罚项有什么作用？为什么过大或过小的 KL 惩罚项会影响训练效果？</p><p>Q186：DeepSeek-R1 在 SFT 阶段，为什么要加入 20 万条与推理无关的训练样本？</p><p>Q187：DeepSeek 是如何把 R1 的推理能力蒸馏到较小的模型中的？如果我们要自己蒸馏一个较小的垂直领域模型，如何尽可能保留 R1 在特定领域的能力？</p><p>Q188：DeepSeek MLA 相比 MQA 占用的 KV 缓存事实上更多，那么 MLA 为什么比 MQA 更好？ MLA 是对哪个维度做了低秩压缩？</p><p>Q189：DeepSeek MLA 是如何解决 RoPE 位置编码与低秩 KV 不兼容的问题的？如果采用其他基于注意力偏置的位置编码，会有什么问题？</p><p>Q190：DeepSeek MoE 模型为什么前 3 层采用稠密连接而后续采用 MoE ？如果所有层都使用 MoE，会有什么影响？</p><p>Q191：DeepSeek MoE 和 Mixtral MoE 有什么区别？ DeepSeek MoE 的细粒度专家分割和共享专家隔离有什么优点？</p><p>Q192：DeepSeek MoE 中的专家负载均衡是如何解决路由崩溃问题的？</p><p>Q193：从大模型对语言中概念建模的角度分析，为什么 R1-Zero 的思维链会出现多语言混杂现象？</p><p>Q194：R1-Zero 的方法主要适用于有明确验证机制的任务（如数学、编程），如何将这一方法扩展到更主观的领域（如创意写作或战略分析）？</p><p>Q195：如果要在一个非推理模型基础上通过 RL 后训练出一个 1000 以内整数四则运算错误率低于 1% 的模型，基座模型预计最少需要多大，RL 过程预计需要多少 GPU 训练多长时间？（提示：TinyZero）</p><p>Q196：在 QwQ-32B 推理模型基础上，通过 RL 在类似 OpenAI Deep Research 的场景中强化垂直领域能力，如何构建训练数据集，如何设计奖励函数？</p><p>Q197：DeepSeek-R1 不支持多模态，如果要在 R1 基础上支持图片推理，例如学会走迷宫、根据照片推断地理位置，如何构建训练数据集，如何设计奖励函数？</p><p>Q198：DeepSeek-V3 的多词元预测方法在样本利用效率和推理效率方面相比一次预测一个词元，有什么优势？</p><p>Q199：DeepSeek-V3 的混合精度训练在哪些矩阵计算中使用了 FP8 量化？为了减少对模型精度的影响，DeepSeek-V3 是如何对激活值和权重做分组量化的？</p><p>Q200：DeepSeek 的 DualPipe 并行训练算法相比传统流水线并行有什么优势？它如何与专家并行协同工作，以解决 MoE 模型的负载均衡问题？</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年7月总结与展望</title>
    <link href="/2025/08/04/2025%E5%B9%B47%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/08/04/2025%E5%B9%B47%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://www.ifensi.com/uploads/ueditor/20250715/3-250G5104F0437.jpg" alt="你行!你上!"></p><h2 id="新体验"><a href="#新体验" class="headerlink" title="新体验"></a>新体验</h2><p>七月多半个月都在上班通勤，也是切身体会了上班通勤的经历。因为之前都没有感受，所以打算辛苦些，自己去体验体验这种上班通勤的经历。每天早上六点多起床六点四十准时做上房山线，八点到所里，五点半下班。牛马不是好做的，当个合格的牛马也给出不容易，劳动人民最光荣啊！</p><h2 id="你行你上"><a href="#你行你上" class="headerlink" title="你行你上"></a>你行你上</h2><p>时隔七年，老姜又出新作品了；作为老姜的粉丝，无论作品咋样都要支持一下。老姜的作品总是能给人思考和启迪，以一个不同的视角让我们看到一些不一样的。</p><h2 id="心头病"><a href="#心头病" class="headerlink" title="心头病"></a>心头病</h2><p>熬了半学期的小论文终于投出去了，心里一块石头终于落地了。算是自己真正意义上的第一篇学术文章，无论结果如何都收获满满，一步一步来吧。</p><p>回所以后读书的时间缩短了很多很多，有空都是在听书，碎片化阅读太多了，还是要调整调整，静下心来学读一些东西。</p><h2 id="Breaking-Bad"><a href="#Breaking-Bad" class="headerlink" title="Breaking Bad"></a>Breaking Bad</h2><p>卧槽，最近一直在追绝命毒师，已经看到第四季了，算是二刷上瘾了；之前这么痴迷的追剧还是神探夏洛克。。。</p><h2 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h2><p>最近梳理了几个感兴趣的新方向，想着慢慢梳理深入一下：</p><ul><li>多模态检索：GME: Improving Universal Multimodal Retrieval by Multimodal LLMs(<strong>CVPR2025</strong>)</li><li>提示压缩：Leveraging Attention to Effectively Compress Prompts for Long-Context LLMs(<strong>AAAI2025</strong>)</li><li>KV Cache压缩：Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques</li><li>多模态Tokens压缩：Token Sequence Compression for Efficient Multimodal Computing(<a href="https://arxiv.org/pdf/2504.17892">https://arxiv.org/pdf/2504.17892</a> Stanford University)</li></ul><p>多模态大模型的源代码要多读一读了；牢记：学习的第一性原理不是积累知识，而是<strong>实践</strong>！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt Compression</title>
    <link href="/2025/07/29/Prompt-Compression/"/>
    <url>/2025/07/29/Prompt-Compression/</url>
    
    <content type="html"><![CDATA[<h1 id="Prompt-Compression-for-LLM-提示压缩"><a href="#Prompt-Compression-for-LLM-提示压缩" class="headerlink" title="Prompt Compression for LLM - 提示压缩"></a>Prompt Compression for LLM - 提示压缩</h1><p>提示词压缩提出的背景：大语言模型处理复杂的NLP任务需要详细的长格式提示信息</p><ul><li><a href="https://aclanthology.org/2025.naacl-long.368.pdf">Prompt Compression for Large Language Models: A Survey</a> ACL2025</li></ul><p>提出问题：大模型进行复杂的NLP任务通常需要长上下文进行详细的提示，这会导致内存使用量和推理成本增加，提示压缩技术可以缓解长山下文所带来的挑战</p><p>主要方法：软提示(Soft Prompt)和硬提示(Hard Prompt)</p><p>提示压缩的机制：注意力最优化、参数的高效微调、模态融合、新合成语言视角</p><p>未来方向：优化压缩编码器、软硬提示结合方法、多模态见解</p><p>硬提示：移除低信息词元或进行同义词改写</p><p>软提示：将文本压缩成更少的特殊词元</p><p>提高大语言模型效率的方法大致分为两类：以模型为中心的方法和以提示为中心的方法；提示压缩最佳架构和潜在机制仍然不明确，凸显出了进一步研究的必要性。</p><p>提示压缩的基本机制有：注意力机制的修改、参数的高效微调、模态整合和新的合成语言</p><p>提示的结构：instruction, input, output format(指令-输入-输出格式)和context, question, answer format(上下文-问题-答案格式)</p><p>提示压缩主要的两种方法：删除不必要的或低信息量的内容 以及 在嵌入空间中学习提示信息的连续表示。</p><p><img src="https://moonlight-paper-snapshot.s3.ap-northeast-2.amazonaws.com/arxiv/prompt-compression-for-large-language-models-a-survey-2.png" alt="prompt"></p><h2 id="硬提示"><a href="#硬提示" class="headerlink" title="硬提示"></a>硬提示</h2><p>从原始提示中移除不必要的词元，同时保持使用紫檀语言的词语和子词。这种方法对于仅接受自然语言输入的大语言模型特别有用，而不是词嵌入。</p><p>三种典型的硬提示方法：</p><ul><li>过滤的SelectiveContext</li><li>LLMLingua</li><li>用于改写 paraphrasing 的 Nano-Capsulator</li></ul><p><img src="https://github.com/ZongqianLi/Prompt-Compression-Survey/blob/main/Figures/hard_prompt_methods.png?raw=true" alt="hard-prompt"></p><h3 id="SelectiveContext"><a href="#SelectiveContext" class="headerlink" title="SelectiveContext"></a><strong>SelectiveContext</strong></h3><p>Compressing Context to Enhance Inference Efficiency of Large Language Models：<a href="https://arxiv.org/pdf/2310.06201">https://arxiv.org/pdf/2310.06201</a></p><p>使用自信息量化词汇单元的信息量，识别并删除输入提示中的冗余或较少的信息部分。为了保持文本的连贯性，使用Spacy句法解析功能将单独词元分词为名词短语。SelectiveContext不依赖任何外部模型和额外参数，可以适用于任何模型架构。</p><p>SelectiveContext三个主要缺点：</p><ul><li>过滤后的提示必须由大语言模型重新编码，限制了效率</li><li>依赖于Spacy的准确短语边界检测</li><li>目前没有合并动词短语的方法</li></ul><h3 id="LLMLingua"><a href="#LLMLingua" class="headerlink" title="LLMLingua"></a><strong>LLMLingua</strong></h3><p>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models：<a href="https://arxiv.org/pdf/2310.05736">https://arxiv.org/pdf/2310.05736</a></p><p>使用一个较小的语言模型(如GPT-2)来计算自信息和困惑度并在自然语言提示被输入到LLM之前删除冗余的词元。</p><p>LLMLingua对结构为{Instruction,Input,Question}的提示进行操作，根据困惑度得分选择关键的示例，然后在提示上应用词元级别过滤，允许将词语拆分成子词单元并避免名词短语合并。</p><p>对于数字和单位等关键元素，LLMLingua 结合了词元保留算法，优先考虑这些元素在指令和问题中的位置。</p><p>LLMLingua两大局限性：</p><ul><li>较小的语言模型需要额外的内存，并且可能使用与较大 LLM 不同的词元分析器。</li><li>并非所有提示都包含大量上下文示例，因此区分提示压缩和上下文示例选择非常重要。</li></ul><h3 id="Nano-Capsulator"><a href="#Nano-Capsulator" class="headerlink" title="Nano-Capsulator"></a><strong>Nano-Capsulator</strong></h3><p>本质上是微调一个Vicuna-7B作为摘要模型</p><p>将原始提示总结成简洁的自然语言版本，然后输入到 LLM。此过程会删除不相关信息，并将提示重 构为流畅的句子。</p><p>LongLLMLingua，它通过应用文档重排序和子序列恢复，具有比 LLMLingua 更长的压缩窗口</p><p>AdaComp根据查询复杂性和检索质量动态选择相关文档</p><p>LLMLingua-2 使用数据蒸馏创建压缩数据集，并训练分类器以保留关键词元</p><p>CPC 使用上下文感知嵌入对句子相关性进行排序</p><p>TCRA-LLM 使用嵌入进行摘要和语义压缩</p><h2 id="软提示"><a href="#软提示" class="headerlink" title="软提示"></a>软提示</h2><p>处理向量的编码-解码架构</p><p>软提示由编码-解码架构组成：一个将提示压缩成较短的连续特殊标记序列的编码器，以及处理压缩后的提示以生成相应响应的解码器。</p><p>经典的软提示压缩架构：CC、GIST、Auto-Compressor、ICAE、500xCompressor、UniICL</p><p><img src="https://github.com/ZongqianLi/Prompt-Compression-Survey/blob/main/Figures/soft_prompt_methods.png?raw=true" alt="soft-prompt"></p><h3 id="CC"><a href="#CC" class="headerlink" title="CC"></a><strong>CC</strong></h3><p>是一种Decoder-Only方法，最小化跨词元序列的 Kullback-Leibler（KL）散度，训练一个较短的软提示以近似自然语言提示的输出分布，从而与期望的响应模式对齐。</p><p>缺点：每个软提示都是针对特定的自然语言提示进行唯一训练的，限制了 CC 的泛化能力，因为新提示需要从头开始重新训练。</p><h3 id="GIST"><a href="#GIST" class="headerlink" title="GIST"></a><strong>GIST</strong></h3><p>修改了LLM的注意力机制，一系列新的可训练词元，被追加在原始提示词元之后。新生成的词元只能关注压缩词元，从而在注意力流中实现分离。</p><p>原始提示→(经过编码器)压缩词元→(经过解码器)新词元</p><p>缺点：由于微调数据集中的提示较短，其受到最大压缩提示长度的限制。此外，压缩词元无法与原 始未微调的 LLM 一起使用，限制了其更广泛的应用。</p><h3 id="Auto-Compressor"><a href="#Auto-Compressor" class="headerlink" title="Auto-Compressor"></a><strong>Auto-Compressor</strong></h3><p>原始提示被分为几个子提示，每次迭代中，子提示被压缩成一组少量的词元，然后将这些词元与一个新的子提示一起传递到下一次迭代进行进一步压缩。</p><p>缺点：训练过程耗时较长，且压缩后的词元无法被原始未调优的 LLM 使用。</p><p><strong>问题</strong>：为什么压缩后的词元无法被LLM使用？？？</p><h3 id="ICAE"><a href="#ICAE" class="headerlink" title="ICAE"></a><strong>ICAE</strong></h3><p>增加了压缩长度，并将冻结的 LLM作为解码器。它将长而信息丰富的上下文压缩成少量的词元，用于问答。问题本身保持未压缩，答案是基于压缩后的上下文和未压缩的问题生成的。</p><p>与 GIST 不同，GIST 压缩的是大约 30 个词元的低信息文本，专注于问题和指令，而 ICAE可以处理详细且复杂的上下文。</p><p>由于解码器冻结，压缩后的词元可以直接与原始LLM一起使用，无需微调。</p><p>缺点：压缩率相比GIST 有所下降，ICAE 在 Pile 数据集上进行训练和测试，该数据集可能与 LLM 的训练语料库重叠，引发了关于潜在数据泄露以及从 LLM 内存中检索答案的可能性的担忧。</p><h3 id="500xCompressor"><a href="#500xCompressor" class="headerlink" title="500xCompressor"></a><strong>500xCompressor</strong></h3><p>探索了在高压缩比下的提示压缩，在编码器中使用可训练的 LoRA 参数，同时在解码器中保持原始 LLM 冻结。</p><p>与 ICAE 不同的是，500xCompressor 向解码器提供压缩词元的 K V 值，而不是词元本身，证明了K V值比嵌入向量保留了更多的详细信息，尤其是在高压缩比条件下。</p><h3 id="xRAG"><a href="#xRAG" class="headerlink" title="xRAG"></a><strong>xRAG</strong></h3><p>使用一个冻结的嵌入模型作为编码器，仅在编码器和解码器大语言模型之间放置一个适配器，包含可训练参数。</p><p>xRAG 证明了当前的嵌入模型可以将信息压缩到单个词元中用于问答任务。虽然原始论文中测试了几种嵌入模型，但最终选择的是SFR-Embedding-Mistra(仍然基于大语言模型,需要大量内存)</p><p>缺点：xRAG 需要加载两个大语言模型和一个投影器，而 ICAE 和 500xCompressor 只需加载一个大语言模型和一组 LoRA 参数。</p><h3 id="UniICL"><a href="#UniICL" class="headerlink" title="UniICL"></a><strong>UniICL</strong></h3><p>UniICL中唯一可训练的组件是位于编码器和解码器之间的投影器。它的编码器和解码器都使用的相同的冻结的大语言模型，从而在训练过程中减少梯度计算并节省加载大型语言模型的内存。</p><p>UniICL 中的压缩词元可以被视为各种上下文示例的嵌入，从而在选择上下文示例时无需额外的嵌入过程。</p><p>其它方法：COCOM、LLoCO、QGC</p><p>COCOM 和 LLoCO 使用微调的编码器-解码器设置，而 QGC 采用冻结的解码器。COCOM 和 LLoCO都是为 RAG 任务设计的：COCOM 将多个文档压缩成上下文嵌入组，并将它们一起输入解码器，而 LLoCO 存储并检索特定的 LoRA 参数以使解码器适应特定的文本类型和任务。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs336:Language Modeling from Scratch</title>
    <link href="/2025/07/14/cs336-study-note/"/>
    <url>/2025/07/14/cs336-study-note/</url>
    
    <content type="html"><![CDATA[<center>  <h1>CS336: Language Modeling from Scratch</h1></center><center>  <h1>Stanford / Spring 2025</h1></center><center>  学习笔记整理：杨桂淼</center><center>  课程主页：https://stanford-cs336.github.io/spring2025/</center><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>This course is designed to provide students with a comprehensive understanding of language models by walking them through the entire process of developing their own. Drawing inspiration from operating systems courses that create an entire operating system from scratch, we will lead students through every aspect of language model creation, including data collection and cleaning for pre-training, transformer model construction, model training, and evaluation before deployment.</p><p><img src="https://stanford-cs336.github.io/spring2025-lectures/images/design-decisions.png" alt="design-decisions"></p><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><p>Tokenizers是将字符串和整数序列之间进行相互转换，四类主要的分词器：character_tokenizer、byte_tokenizer、word_tokenizer、bpe_tokenizer。</p><p><img src="https://stanford-cs336.github.io/spring2025-lectures/images/tokenized-example.png" alt="Tokenization"></p><p>character_tokenizer：每个字符可以通过<code>ord</code>函数完成字符到ASCII码(Unicode)的相互转换。但是带来的问题就是构成的词表非常大，许多字符生僻少用导致词汇使用效率低。</p><p>byte_tokenizer：Unicode字符可以被表示为一个使用0-255整数表示的<strong>字节序列</strong>。但是由于一个字节只能被256个整数所表示，这就造成了经过Tokenization后的序列长度会变得非常长。</p><p>word_tokenizer：NLP领域内常用的方法是将字符串分割为单词，利用正则表达式构建分词器，将基于分词器得到的segments映射为整数，构建出每个segment的整数映射。但是存在的问题是单词的数量巨大，无法提供固定的词汇量；训练期间未使用过的新词会被标记为UNK，扰乱困惑度计算。</p><p>💡联想语言模型的输出：LLM的输出本质上就是在一个大的词表上预测单词的概率分布，如果词表非常大，那么计算了可想而知是非常恐怖的，而且也会影响token的预测。上面介绍的三种Tokenization都各有特点，但是也都存在致命的缺点。LLM基于上述编码的劣势，最终选择采用BPE算法作为模型的Tokenization方法。</p><h3 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h3><p>BPE算法是在1994年发表的“A New Algorithm for Data Compression”提出的一种新型<strong>数据压缩</strong>算法。核心思想就是将序列中常见的一对<strong>相邻的数据单元</strong>替换为数据中没有出现过的新单元，反复迭代直至满足终止条件。</p><p>bpe_tokenizer：基本思路是常见的字符序列被单个token表示，罕见的序列被多个tokens表示。</p><p>BPE的基本计算路程如下所示：</p><img src="/img/fig/bpe.jpg"/><h2 id="Pytorch-and-Resource-accounting"><a href="#Pytorch-and-Resource-accounting" class="headerlink" title="Pytorch and Resource accounting"></a>Pytorch and Resource accounting</h2><h3 id="tensors-memory"><a href="#tensors-memory" class="headerlink" title="tensors_memory"></a>tensors_memory</h3><p>Higher precision: more accurate&#x2F;stable, more memory, more compute</p><p>Lower precision: less accurate&#x2F;stable, less memory, less compute</p><ul><li>float32：单精度浮点数是tensor的默认数据类型，4个字节表示一个浮点数。1位符号位， 8位指数位， 23位小数位。神经网络训练时，创建tensor默认的数据类型。</li></ul><p>当符号位为0，指数位为11111110，尾数位为全1时，float32的最大值为 $2^{127} \times (1+\frac{2^{23}-1}{2^{23}}&#x3D;3.40e38$；</p><p>当符号位为1，指数位为11111110，尾数位为全1时，float32的最小值为$-2^{127} \times (1+\frac{2^{23}-1}{2^{23}}&#x3D;-3.40e38$。</p><ul><li>float16：半精度浮点数，两个字节表示一个浮点数。1位符号位， 5位指数位， 10位小数位。相比于float32所占内存更小，但是会出现<strong>溢出问题</strong>。</li></ul><p>当符号位为0，指数位为11110，尾数位为全1时，float16的最大值为 $2^{15} \times (1+\frac{1023}{1024})&#x3D;65504$；</p><p>当符号位为1，指数位为11110，尾数位为全1时，float16的最小值为$-2^{15} \times (1+\frac{1023}{1024}) &#x3D; -65504$。</p><ul><li>bfloat16：与float16所占内存一致，但是具有float32的存储范围。1位符号位， 8位指数位， 7位小数位。</li></ul><p>计算<code>矩阵乘法</code>与<code>前向传播</code>时推荐使用bfloat16数据类型，计算<code>Attention注意力</code>时推荐使用float32数据类型。</p><p>指数的偏置：</p><ul><li><code>float32</code> 的 Bias 为 127</li><li><code>float16</code> 的 Bias 为 15</li><li><code>bfloat16</code> 的 Bias 为 127</li></ul><p><img src="https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/guides/performance_improving/images/float.png?raw=true" alt="FLOPS"></p><p>计算实例如下，以float16为例：</p><img src="/img/fig/float16.jpg"/><h3 id="tensors-on-gpus"><a href="#tensors-on-gpus" class="headerlink" title="tensors_on_gpus"></a>tensors_on_gpus</h3><p>为了利用好GPUs的并行优势，我们需要将数据从CPU中的RAM移动到GPU中的DRAM中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.cuda.is_available():<br>  <span class="hljs-keyword">return</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_gpus):<br>  properties = torch.cuda.get_device_properties(i)  <span class="hljs-comment"># @inspect properties</span><br> <br>y = x.to(<span class="hljs-string">&quot;cuda:0&quot;</span>)<br><span class="hljs-keyword">assert</span> x.device == torch.device(<span class="hljs-string">&quot;cuda&quot;</span>,<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>PyTorch中的tensors就是存储在内存中的指针，存储形式如下图所示：</p><p><img src="https://stanford-cs336.github.io/spring2025-lectures/var/files/image-97aa05a6701b46521cb8a7c1e096c7e7-https_martinlwx_github_io_img_2D_tensor_strides_png" alt="tensor-pointer"></p><p>strides[1]表示列指针，strides[0]表示行指针；上图中二维格式为我们习惯上画出的存储格式，但是在内存条中存储的格式本质上是下图中的线性格式。如果要定位一个元素的位置(查找一个元素)，需要利用连个指针进行辅助：$pos&#x3D;1 * strides[1] + 4 * strides[0]$。</p><p>以元素10为例(2行3列)，$pos_{10}&#x3D;1 * 3 + 4 * 2 &#x3D; 11$，那么内存中横向第11个位置(从1计数)对应的元素即是10。</p><h3 id="tensor-operations"><a href="#tensor-operations" class="headerlink" title="tensor_operations"></a>tensor_operations</h3><p>Pytorch对于tensor的操作中，<code>view</code>是free，无须占用内存；但是copy操作(<code>contiguous and reshape</code>)是需要内存开销的。</p><p>⚠️<strong>einops</strong>是之前没有接触过的操作，主要作用是高效操作张量的维度，需要留心学习关注下。推荐一篇博文：<a href="https://zhuanlan.zhihu.com/p/342675997">https://zhuanlan.zhihu.com/p/342675997</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 矩阵乘法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tensor_matmul</span>():<br>  x = torch.ones(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>)<br>  w = torch.ones(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>)<br>  <br>y = x @ w<br><span class="hljs-keyword">assert</span> y.size() == torch.Size([<span class="hljs-number">16</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-comment"># 定义两个张量:</span><br>x: Float[torch.Tensor, <span class="hljs-string">&quot;batch seq1 hidden&quot;</span>] = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># @inspect x</span><br>y: Float[torch.Tensor, <span class="hljs-string">&quot;batch seq2 hidden&quot;</span>] = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># @inspect y</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tensor_einops</span>():<br>  <span class="hljs-comment">#einops主要是rearrange, reduce, repeat这3个方法</span><br>  z = x @ y.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># batch, sequence, sequence  @inspect z --&gt; Old way</span><br>  z = einsum(x, y, <span class="hljs-string">&quot;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&quot;</span>)  <span class="hljs-comment"># @inspect z --&gt; New (einops) way</span><br>  z = einsum(x, y, <span class="hljs-string">&quot;... seq1 hidden, ... seq2 hidden -&gt; ... seq1 seq2&quot;</span>)  <span class="hljs-comment"># @inspect z</span><br>  <span class="hljs-comment"># reduce</span><br>  x: Float[torch.Tensor, <span class="hljs-string">&quot;batch seq hidden&quot;</span>] = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># @inspect x</span><br>  y = x.mean(dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># @inspect y --&gt; Old way</span><br>  y = reduce(x,<span class="hljs-string">&quot;...hidden -&gt; ...&quot;</span>,<span class="hljs-string">&quot;sum&quot;</span>) --&gt; New (einops) way<br>  <span class="hljs-comment"># rearrange</span><br>  x: Float[torch.Tensor, <span class="hljs-string">&quot;batch seq total_hidden&quot;</span>] = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>)  <span class="hljs-comment"># @inspect x</span><br>  w: Float[torch.Tensor, <span class="hljs-string">&quot;hidden1 hidden2&quot;</span>] = torch.ones(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br>  <br>  x = rearrange(x, <span class="hljs-string">&quot;... (heads hidden1) -&gt; ... heads hidden1&quot;</span>, heads=<span class="hljs-number">2</span>)  <span class="hljs-comment"># @inspect x</span><br>  x = einsum(x, w, <span class="hljs-string">&quot;... hidden1, hidden1 hidden2 -&gt; ... hidden2&quot;</span>)  <span class="hljs-comment"># @inspect x</span><br></code></pre></td></tr></table></figure><h3 id="Computational-cost"><a href="#Computational-cost" class="headerlink" title="Computational cost"></a>Computational cost</h3><p>LLM里的计算基本上都是浮点运算(FLOP)，类似于加法运算和乘法运算。</p><p>容易混淆的概念：</p><ul><li>FLOPs: 浮点运算次数</li><li>FLOP&#x2F;s：每秒钟浮点计算次数，衡量硬件指标</li></ul><p>对 m x n 矩阵进行元素运算需要 O(mn) FLOPs；两个 m x n 矩阵的加法需要 mn 次 FLOPs。一般来说，没有任何运算比矩阵乘法更耗时。</p><p>我们用 B 表示数据点的个数，(D K)是参数大小，2 (# tokens) (# parameters)代表前想传播的FLOPs、4 (# data points) (# parameters) 反向传播FLOPs、6 (# data points) (# parameters)代表一次训练的FLOPs。</p><p>一个重要指标：Model FLOPs utilization - 模型FLOPs利用率，计算公式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">mfu = actual_flop_per_sec / promised_flop_per_sec  <span class="hljs-comment"># @inspect mfu</span><br></code></pre></td></tr></table></figure><p>FLOP&#x2F;s取决于硬件(H100 &gt;&gt; A100) 和数据类型(bfloat16 &gt;&gt; float32)。</p><h3 id="Parameter-initialization"><a href="#Parameter-initialization" class="headerlink" title="Parameter initialization"></a>Parameter initialization</h3><p>初始化语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = nn.Parameter(torch.randn(input_dim))<br>w = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=<span class="hljs-number">1</span> / np.sqrt(input_dim), a=-<span class="hljs-number">3</span>, b=<span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure><p>维度缩放的原因：为了防止前向传播过程中发生梯度爆炸，导致训练不稳定；将输入进行1&#x2F;sqrt(input_dim) 缩放，这样每一步都会得到基于正太分布的输出，保证了数据的稳定性。这个道理在注意力机制的计算中同样适用。</p><p>⚠️数据加载的注意事项：Don’t want to load the entire data into memory at once！大语言模型的输入是整数序列，可以使用numpy数组加载这些数据，使用memap延迟加载部分数据到内存中。</p><p>optimizer的组成：</p><ul><li>momentum &#x3D; SGD + exponential averaging of grad</li><li>AdaGrad &#x3D; SGD + averaging by grad$^2$</li><li>RMSProp &#x3D; AdaGrad + exponentially averaging of grad$^2$</li><li>Adam &#x3D; RMSProp + momentum</li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年6月总结与展望</title>
    <link href="/2025/07/01/2025%E5%B9%B46%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/07/01/2025%E5%B9%B46%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>今天是2025年7月1号，党的生日，祝党生日快乐！</p><h2 id="离别"><a href="#离别" class="headerlink" title="离别"></a>离别</h2><p>2025年6月16号，正式和雁栖湖道别啦，但是没有说再见👋，我相信我们还会再见的。雁子，我们还会再见的对吗？！</p><p>雁栖湖的一年和梦一样，这一年太过于充实和美好。</p><p>从2024年8月底来到这里，2025年将近6月底离开，这一年收获满满，很快乐、很充实。感觉就像是保研后老天给自己的奖励一样，让自己慢下来，好好去享受一下生活。</p><p>这一年认识了很多朋友，也在多方面锻炼了自己的能力；元旦晚会上跳了舞、冬天滑了雪、爬了一次来了无数次北京都没去过的长城和香山、逛遍了大小胡同、吃了角落里藏着的美食、作为主讲人参加了几次庄重的汇报、划了龙舟、打了数不尽场的篮球。。。</p><p>我想我们还会再见的，再见，我一定会换个身份和你再见。雁子，让我们各自努力，顶峰相见！</p><h2 id="打工人"><a href="#打工人" class="headerlink" title="打工人"></a>打工人</h2><p>“没跑路啊xdm，干了15天了。”</p><p>哈哈哈哈，最近抖音上这个哥们魔性的开场白给我洗脑了，同为打工人，现在也算是深有体会了。6月16号离校后便搬到了舅舅房山的家里，舅舅一家工作日住海淀家里，周一到周五房山的房子没人住，我便自己开始在房山开启了爽住模式。</p><p>老话说：“无利不起早”，老话说得没错。</p><p>记得从四月份开始写阅读总结的时候就说自己很懒了，早上根部起不来；这下好了，由于房山的房子离我们单位比较远，每天地铁来回通勤就要接近3个小时。现在，根本不需要担心自己早上起不来了，每天6点准时起床挤地铁。第一天起得晚，七点半上的地铁，卧槽，给我挤得窒息了，第二天开始便提前半小时。现在每天晚上11:30睡觉，早上六点闹钟，也适应了。</p><p>目前完全是上班节奏，晚上下班在单位吃完饭，下地铁基本上就7点了，到家收拾一下就去健身，留给自己自由支配的时间很少了。但是我感觉挺充实的，每天都有在地铁上利用好时间，最近在通勤过程中一直在学西湖大学赵世钰老师的《强化学习的数学原理》，没错，我就是卷王哈哈哈。我是真感兴趣，赵老师讲的真通透，打算正式回所前，要把这个网课系统的刷完，做好笔记。</p><p>上个月把马伯庸老师的短篇《长安的荔枝》看完了，这个月接着看《太白金星有点烦》，打工人的宿命在这一刻交相呼应。</p><p>我想，热爱自己所从事的工作胜过一切吧，虽然辛苦，但是我挺快乐的，这两点不矛盾！！！</p><h2 id="欠债还钱天经地义"><a href="#欠债还钱天经地义" class="headerlink" title="欠债还钱天经地义"></a>欠债还钱天经地义</h2><p>由于这半年一直在根据idea去做实验，我自己的博客空间已经欠下了n多篇文章都没有梳理了💔</p><p>接下来的这一月，我务必会把在雁栖湖的这一年里全部文章认认真真梳理完善完毕，把知识体系与技术路线都建立好，对自己负责、对为数不多的读者们负责。</p><p>进度差的多的文章有：五一五更系列、强化学习、线性代数的本质；重点关注完善。</p><p>PS：写这篇文章的时候，其实我正在工位摸鱼哈哈哈哈，7月挺好，事少，好好珍惜；2025的下半场了，加油呀！👊</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习:从入门到放弃</title>
    <link href="/2025/06/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E5%BC%80%E5%A7%8B%E5%88%B0%E6%94%BE%E5%BC%83/"/>
    <url>/2025/06/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E5%BC%80%E5%A7%8B%E5%88%B0%E6%94%BE%E5%BC%83/</url>
    
    <content type="html"><![CDATA[<center><h1>强化学习:从入门到放弃</h1></center><center>Munger的强化学习笔记</center><center>课程:强化学习的数学原理</center><center>西湖大学·赵世钰</center><center>学习笔记整理：杨桂淼</center><h2 id="引"><a href="#引" class="headerlink" title="引"></a>引</h2><p>OK,咱就是说；深度学习还没完全整透彻呢，强化学习快马加鞭地就来了。因为夏季小学期选了晓飞老师的《强化学习》课程，奈何网课我是实在听不进去，而且每天还要在所里通勤干活，只能拿出挤地铁的三个小时来看网课去学咯。。。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>社畜牛马生活初体验</title>
    <link href="/2025/06/17/%E7%A4%BE%E7%95%9C%E7%89%9B%E9%A9%AC%E7%94%9F%E6%B4%BB%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <url>/2025/06/17/%E7%A4%BE%E7%95%9C%E7%89%9B%E9%A9%AC%E7%94%9F%E6%B4%BB%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="see-you雁栖湖"><a href="#see-you雁栖湖" class="headerlink" title="see you雁栖湖"></a>see you雁栖湖</h2><p>匆匆忙忙告别了雁栖湖，也没赶上16号晚上学院举办的离湖草地音乐节😭</p><p>从正门走的，拍了一眼钟楼和自己的小窝</p><p>在老舅家房山这边自己爽住一个月，感恩舅舅舅妈收留孩子🥹</p><p>每天从西南五环通勤西北五环，地铁往返仨小时。早上六点起床开始挤地铁，要窒息了，给孩子真挤死了。体验社畜生活，必须亲自体验！</p><p>在印象城办了健身卡，下班健身，练，狠狠地练！</p><h2 id="早起半小时，生活更美好"><a href="#早起半小时，生活更美好" class="headerlink" title="早起半小时，生活更美好"></a>早起半小时，生活更美好</h2><p>第一天起床晚了，七点半在长阳上的地铁。尼玛，一上车就给我干窒息了。。。人是真多啊，比肩继踵在那天早上具象化了。</p><p>在郭公庄一般会下去一大波人，然后在东管头换乘后人依旧很多，国家图书馆也是大的换乘站，人会减少一大部分；晚上从农大南路回来，运气好的话坐到东管头会捡一个座位，运气不好就只能站一路了。。。不过从东管头到长阳还是可以抢上座位的，因为东管头是房山线的始发站。</p><p>就这样每天来回通勤3小时，持续将近一个月的时间，自己体验体验吧，感受一下打工人的生活状态。</p><p>虽然辛苦些，但是感觉一天还是蛮充实的；因为这样的模式会强制让自己早睡早起。早上一般六点就要起床了，晚上七点到家，休息一会去健身，回来就已经很累了。然后利用来回通勤的时间在地铁上看会书，感觉相当不错。😌</p>]]></content>
    
    
    <categories>
      
      <category>生活体验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>打工人</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数的本质</title>
    <link href="/2025/06/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8/"/>
    <url>/2025/06/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8/</url>
    
    <content type="html"><![CDATA[<p>最近一个月每天都要挤地铁，通勤来回要三个小时，所以总得找点事干。</p><p>这两天在地铁上断断续续把《线性代数的本质》系列学完了，做个总结，构建一下核心知识体系。</p><h2 id="矩阵与线性变换"><a href="#矩阵与线性变换" class="headerlink" title="矩阵与线性变换"></a>矩阵与线性变换</h2><h2 id="行列式的几何意义"><a href="#行列式的几何意义" class="headerlink" title="行列式的几何意义"></a>行列式的几何意义</h2><h2 id="逆矩阵、秩"><a href="#逆矩阵、秩" class="headerlink" title="逆矩阵、秩"></a>逆矩阵、秩</h2><h2 id="特征值和特征向量"><a href="#特征值和特征向量" class="headerlink" title="特征值和特征向量"></a>特征值和特征向量</h2>]]></content>
    
    
    <categories>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高等数学</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年5月总结与展望</title>
    <link href="/2025/06/01/2025%E5%B9%B45%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/06/01/2025%E5%B9%B45%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>赶在端午假期，5月份结束了。</p><h2 id="牛仔很忙"><a href="#牛仔很忙" class="headerlink" title="牛仔很忙"></a>牛仔很忙</h2><p>五月很忙，忙的一团糟。因为有几门课节课考试还有节课汇报，再加上跑实验效果不佳，整个五月过的就是尼玛一坨屎💩。</p><p>晚睡晚起已经成为了五月份的主旋律😢</p><h2 id="遗憾-祸兮福之所倚"><a href="#遗憾-祸兮福之所倚" class="headerlink" title="遗憾-祸兮福之所倚"></a>遗憾-祸兮福之所倚</h2><p>五月，球队科苑杯止步淘汰赛首轮；科苑杯全国总决赛，信工所2分惜败，无冕之王，难受😭</p><p>很难想，24年后半程有多顺，25年上半年就有多衰；祸兮福之所倚，福祸相依；不找借口，还要继续沉淀！</p><h2 id="收获-福兮祸之所伏"><a href="#收获-福兮祸之所伏" class="headerlink" title="收获-福兮祸之所伏"></a>收获-福兮祸之所伏</h2><p>这个月虽然衰，但是这个月开始跟着天华和国栋规律性健身了。可能感觉晚睡晚起让整个人都松松垮垮的，没精神状态，想着要改变改变了💪</p><p>这个月赶在月末端午假期，和好朋友们租车自驾去了祖国北大门-内蒙古二连浩特，特种兵的两天内蒙行，太有意义了！</p><h2 id="要分别了"><a href="#要分别了" class="headerlink" title="要分别了"></a>要分别了</h2><p>六月啦，马上就要离开雁栖湖了；感觉这一年真快，学了很多又感觉没学到啥，没荒废又感觉有些荒废；珍惜了又感觉没珍惜，说不清。。。但是，事实是马上就要走了。</p><p>珍惜吧，珍惜～</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五一五更之五：手撸DeepSeek V3</title>
    <link href="/2025/05/05/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%BA%94%EF%BC%9A%E6%89%8B%E6%92%B8DeepSeek-V3-R1/"/>
    <url>/2025/05/05/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%BA%94%EF%BC%9A%E6%89%8B%E6%92%B8DeepSeek-V3-R1/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepSeek-V3"><a href="#DeepSeek-V3" class="headerlink" title="DeepSeek-V3"></a>DeepSeek-V3</h1><p>DeepSeek-V3 Technical Report：<a href="https://arxiv.org/pdf/2412.19437">https://arxiv.org/pdf/2412.19437</a></p><p><img src="https://raw.githubusercontent.com/deepseek-ai/deepseek-v3/master/figures/benchmark.png" alt="DeepSeek-V3"></p><h2 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h2><p>由于是自回归语言模型，下一步的输出结果依赖于当前状态(及以前的状态)的结果，采用空间换时间的思想，将当前状态(及以前的状态)计算好的K和V存储到GPU中，加速下一步attention的计算。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*g0kJ90z4LW4ZgtCKur4gTg.png" alt="attention"></p><p>手绘了一下KV Cache的缓存流程如下：</p><img src="/img/fig/kv.jpg"/><p>如上图所示，KV Cache在执行过程中会造成GPU大量的空间消耗，使得GPU memory消耗非常大。</p><p>两大流派解决空间消耗问题：1. quantization；2. 减小K和V大小</p><h2 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h2><p>Positional Embedding主要包括两大类：</p><ul><li><p>Absolute positional embedding - 绝对位置编码 </p></li><li><p>Relative positional embedding - 相对位置编码</p></li></ul><p>绝对位置编码是指在输入序列经过词嵌入后的第𝑘个token向量中加入位置向量p_k，相对位置编码并不是直接建模每个输入token的位置信息，而是在计算注意力矩阵时考虑当前向量与待交互向量的位置的相对距离。</p><p>Rotary positional embedding - 旋转位置编码</p><p>RoPE通过绝对位置编码的方式实现相对位置编码，综合了绝对位置编码和相对位置编码的优点。主要就是对attention中的q, k向量注入了绝对位置信息，然后用更新的q,k向量做attention中的内积就会引入相对位置信息了。</p><center>  <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_10.38.41_AM.png" alt="RoPE" style="zoom:60%;" /></center><img src="/img/fig/pe.jpg"/><h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><center>  <img src="https://towardsdatascience.com/wp-content/uploads/2025/01/15MvV9YDPmc37axJe60w8Ag.png" alt="MLA" style="zoom:70%;" /></center><h3 id="MLA的设计动机"><a href="#MLA的设计动机" class="headerlink" title="MLA的设计动机"></a>MLA的设计动机</h3><p>对于拥有数千亿参数的超大规模模型,推理过程中的主要挑战之一是巨大的KV缓存(KV Cache)。在自回归生成文本时，模型需要缓存先前所有词元的键(Key)和值(Value)向量，以便后续词元的生成能够关注到历史上下文。</p><h3 id="基本思想与方法"><a href="#基本思想与方法" class="headerlink" title="基本思想与方法"></a>基本思想与方法</h3><p>核心方法是矩阵的<strong>低秩分解</strong>。</p><p>设计一个低维度的向量或者矩阵去取代最原始的KV Cache里面的K和V。GPU不在直接缓存K和V，而是去缓存一个低维度的矩阵$\mathbf{c}_{t}^{Q}$。</p><p>K和V可以通过 $\mathbf{c}_{t}^{KV}$ 乘以对应的参数矩阵进行还原：</p><p>$$<br>\mathbf{c} &#x3D; \mathbf{W}^{DK}\mathbf{h}_{t}<br>$$</p><p>$$<br>\mathbf{K} &#x3D; \mathbf{W}^{UK}\mathbf{c}<br>$$</p><p>所以只需要把$\mathbf{c}_{t}^{KV}$进行cache即可。</p><p>只引入 $\mathbf{c}_{t}$ 无法对输入嵌入进行位置编码，后续便会引入Rotary Positional Embedding(RoPE)问题。</p><p>MLA解决思路就是引入了额外的head。如上图所示，在Attention中引入一些额外的multi-head，使这些head做最原始的attention流程($\mathbf{q}_{t}^{C}$)，之后再将其与经过RoPE编码后的向量进行拼接。</p><p>以下是对于MLA的推导梳理过程如下：</p><img src="/img/fig/mla.jpg"/><h2 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h2><p>MoE是一个经济实惠的架构，因为训练的时候无需所有的Experts参与训练。</p><p>DeepSeek对于MoE架构的改进如下所示：</p><ul><li>Fine-Grained Expert Segmentation：把之前大的expert变成更小的expert，把少量的大expert变成更多的小expert。</li><li>Shared Expert Isolation：单独设立一些共享的expert，存储common knowledge，解决不同问题时会使用到(应对不同任务时都会用到的)。Shared Expert相当于通才，Routed expert相当于专才。</li></ul><p><img src="https://substackcdn.com/image/fetch/$s_!ZC1d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e45d3d4-d64a-40e5-bda0-1df7ff842983_1183x768.png" alt="deepseek-MoE"></p><p> 每一个expert由两个MLP层组成，deepseekMoE做法是把这两个MLP中间层维度缩小，使得MLP整体参数量减少，但是增加了其数量，使得参数总量不变的前提下，增加了expert数量，相当于model ensemble降低variance。</p><p>💡前面的KV Cache是空间换时间的思想，这里的MoE架构则是数量换质量的思想。</p><p>科学研究的思路选取方法：极端折中法。对于MoE架构而言，一些paper选择做FFN，一些paper选择做MoE；那么比较好的想法是按照DeepSeek这样折中的办法——Shared Expert。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
      <tag>DeepSeek</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五一五更之四：动手学GPT</title>
    <link href="/2025/05/04/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E5%9B%9B%EF%BC%9A%E5%8A%A8%E6%89%8B%E5%AD%A6GPT/"/>
    <url>/2025/05/04/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E5%9B%9B%EF%BC%9A%E5%8A%A8%E6%89%8B%E5%AD%A6GPT/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>学习知识就像是水流一样，一定要找到知识的源头和知识的流向；也就是说，不仅仅要关注当前这篇文章的创新点，同时也要关注当前的这篇文章前期工作有什么，思路是从哪些文章延伸过来的；而流向就是基于现有知识作出的创新。</p>          </div><h2 id="为什么现在的大语言模型都是Transformer架构？"><a href="#为什么现在的大语言模型都是Transformer架构？" class="headerlink" title="为什么现在的大语言模型都是Transformer架构？"></a>为什么现在的大语言模型都是Transformer架构？</h2><div class="note note-danger">            <p>Transformer模型本身里面的<code>inductive bias</code>比较小。</p>          </div><h3 id="NLP与CV领域模型的演进历程"><a href="#NLP与CV领域模型的演进历程" class="headerlink" title="NLP与CV领域模型的演进历程"></a>NLP与CV领域模型的演进历程</h3><p>NLP：human rule - statistical model - RNN - Transformer</p><p>CV：hard-crafed + SVM  - CNN - Vision Transformer(ViT)</p><p>CV和NLP领域的发展最终都不约而同的满满发展到了Transformer架构。因为随着数据量的增长，计算量也随之变大。CNN和RNN架构在数据量和计算量增大后会遇到性能上的瓶颈，但是Transformer模型随着数据量和计算量的增大性能会进一步提高，鲜有出现性能瓶颈的问题。</p><p>–》  这背后的主要原因是<code>inductive bias</code>。</p><p>如何理解<code>inductive bias</code>？其实就是人类给出计算机先验知识的过程。</p><p>人类主动注入一些对于某一领域事物的特征偏见，使得计算机利用这些特征偏见去做识别、分类任务。这里就包含比较多的<code>inductive bias</code>。</p><p>如果是一个现成的领域数据集，比如ImageNet，里面已经包含了多种分类任务所需要的特征，这里面就包含比较少的<code>inductive bias</code>。</p><p>经典例子就是我们考试不知道如何复习？之后我们就会去问助教，助教会给我们考试范围和几篇往年的复习真题，这里的考试范围和几篇往年的复习真题其实就是人类的<code>inductive bias</code>。我们最后突击的时候，就会利用这个<code>inductive bias</code>使得复习更简单。但是助教给出的<code>inductive bias</code>也不一定完全正确，因为老师很可能会出一些范围以外的题目，所以对于学习最好的方式就是不要去看助教给出的范围，而是多花时间和精力去系统完整地学习所有的知识。</p><h3 id="Trade-off"><a href="#Trade-off" class="headerlink" title="Trade off"></a>Trade off</h3><p>CNN和RNN就是一类<code>inductive bias</code>比较强的模型，因为人类设计这类模型的时候就已经注入了大量的先验知识。以分类任务为例：模型在训练的时候，构建的数据集基本上都是带有标签数据，在构建好的数据集上进行监督训练，之后在测试集上进行测试验证，计算损失反向传播更新模型的权重参数。</p><p>Transformer模型的本质其实是一个<code>Attention + MLP</code>架构，比起CNN和RNN，Transformer模型训练里面的<code>inductive bias</code>更小。相比于神经网络的训练，Transformer模型设计之初是用来完成机器翻译任务而非分类任务。其训练过程更像是非监督学习，随意输入给模型一个文本，通过注意力机制计算文本中词与词之间的Correlation，通过Encoder-Decoder架构完成序列转换，序列的转换过程就是通过注意力机制计算得来的。</p><p>所以，对比两种模型可以发现，相比于CNN和RNN网络，Transformer模型训练过程中数据的<code>inductive bias</code>是更小的。</p><h2 id="为什么大语言模型领域会反复强调scale-up？"><a href="#为什么大语言模型领域会反复强调scale-up？" class="headerlink" title="为什么大语言模型领域会反复强调scale up？"></a>为什么大语言模型领域会反复强调scale up？</h2><p>The Bitter lesson by Rich Sutton：<a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf</a></p><p>Rich Sutton观点：Search和Learning是人工智能研究中使用大量计算的两种最重要的方法，越general的方法越容易scale up，从而会带来模型性能上的提高。核心思想是开发的方法是没有使用人类的<code>inductive bias</code>知识，然后将这种方法加单粗暴的scale up从而大幅度提高模型的性能。</p><p><img src="https://pbs.twimg.com/media/GB5f0-ybEAEGtN7.jpg" alt="bitter-lesson"></p><p>Rich Sutton想法提出的原因是因为摩尔定律对于单位计算成本持续呈现指数级别的下降，并且human-knowledge的方法不太适合去利用好计算的<code>general methods</code>。</p><p><strong>The Bitter lesson</strong>基于的历史观察：</p><ul><li>AI researchers总是尝试去在他们的智能系统中建立知识</li><li>建立知识在短期是起作用的，但是长远来看是停滞的</li><li>突破性进展往往是基于搜索和学习的计算方法来实现</li></ul><p>从the bitter lesson中吸取的教训：可用计算量增大的同时，通用方法会随着计算量的增加而扩展，搜索和学习是可以任意扩展的两种方法；心智是极其复杂的，与其探索复杂性，构建捕获复杂性的元方法更为重要。(有些深奥了昂。。。)</p><h2 id="GPT发展史"><a href="#GPT发展史" class="headerlink" title="GPT发展史"></a>GPT发展史</h2><div class="note note-success">            <p>预训练-监督微调 -&gt; 无监督训练 -&gt; 上下文学习 -&gt; RLHF</p>          </div><p>大模型架构发展之初的三个流派(如下图所示)：Enc-only、Enc-Dec、Dec-only</p><p>Enc-only代表：BERT</p><p>Dec-only代表：GPT</p><img src="https://static-beesseek.oss-cn-hangzhou.aliyuncs.com/forum/202304/28/171122eqgvvkqoc4czgc2w.jpeg" alt="llm-tree" style="zoom:67%;" /><p>GPT模型最初的灵感来源是OpenAI的研究员Radford，他使用爬虫抓取了一些网络数据。利用这些数据，将Transformer模型的Decoder训练成了一个简易QA系统。之后Ilya觉着这种方法非常有前景，就在这个基于Transformer-Decoder架构之上，进一步改进成了GPT模型。</p><p>总结BERT和GPT训练的不同点：BERT是完形填空、GPT是续写接龙，具体区别见下图：</p><img src="/img/fig/gpt1.png"/><p>梳理GPT模型的发展路线：</p><img src="/img/fig/gpt2.png"/><p>结合上文的scale up部分，梳理一下GPT模型训练时慢慢“变大”的过程：</p><p>GPT-1：117M｜1GB</p><p>GPT-2：1542M｜40GB</p><p>GPT-3：175B｜580GB</p><p>PaLM：540B</p><p>DeepSeek LLM：67B</p><p>DeepSeek Math：145B</p><p>DeepSeekV2：236B</p><p>DeepSeekV3：671B</p><h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p><strong>Improving Language Understanding by Generative Pre-Training</strong>：<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><p>GPT在微调时会根据任务特性对输入进行特定的转换，同时网络结构仅需微小的更改。对所有参数进行微调以后即可将GPT应用于不用的下游任务。通过这种预训练-微调方式，无须大量与特定任务相关的结构更改，便可以快速适应于各种NLP任务。</p><p>由于没有编码器，GPT去掉了原始Transformer解码器中交叉多头注意力，仅保留了带掩码的多头自注意力和前馈神经网络。</p><p>GPT模型架构图如下：</p><center>  <img src="/img/fig/gpt3.jpg" style="zoom:37%;" /></center><p>在预训练阶段，GPT使用了标准的语言模型目标函数。给定序列的前i-1个词元，去预测第i个词元。给定无标注预料中的文本序列：<br>$$<br>U&#x3D;{u_1,u_2,…u_n}<br>$$<br>语言模型的目标函数极大化似然函数为：<br>$$<br>\mathcal{L}(\mathcal{U})&#x3D;\sum_{i}logP(u_{i}|u_{i-k},…,u_{i-1};\theta)<br>$$</p><h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><p><strong>Language Models are Unsupervised Multitask Learners</strong>：<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p><p>对于GPT的核心改进思想：通过语言模型的框架，使用无监督模型直接去实现各种下游的有监督任务，将任务的相关信息也作为语言模型的输入。</p><p>GPT模型虽然可以通过预训练-微调框架在多种下游任务中取得好的效果，但是微调阶段仍然需要收集特定的数据集并对模型进行监督训练。为了实现真正的通用NLP模型，GPT-2实现了无须微调直接利用<strong>无监督训练</strong>的模型去做各种不同的下游任务(多任务的无监督学习)。在训练时，输入构建成类<code>Question-Answer</code>的形式，并扩大数据集继续Scaling Up。</p><p>P(output | input) -&gt; P(output | input, task)</p><img src="/img/fig/gpt4.jpg"/><h3 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h3><p><strong>Language Models are Few-Shot Learners</strong>：<a href="https://arxiv.org/pdf/2005.14165">https://arxiv.org/pdf/2005.14165</a></p><p>对于GPT-2改进：进一步增加语料集数量和模型参数量、使用上下文学习方式提高处理不同任务的通用型。</p><p>上下文学习，指使用自然语言的任务提示(<strong>prompt</strong>)和少量演示示例，来学习对不同任务的处理。</p><p>上下文学习与微调的最大区别在于，微调需要利用大量下游任务数据对模型进行训练，更新模型参数；而上下文学习无需大量下游任务数据，也不需要进行梯度更新。</p><img src="/img/fig/gpt5.jpg"/><ul><li>小样本学习(few-shot learning)：在输入文本中，向模型提供描述任务的自然语言提示，以及<strong>一些</strong>任务相关的示例演示。</li><li>单样本学习(one-shot learning)：在输入文本中，除了向模型提供描述任务的自然语言提示，仅提供<strong>一个</strong>任务相关的示例演示。</li><li>零样本学习(zero-shot learning)：在输入文本中，只向模型提供描述任务的自然语言提示，<strong>不提供</strong>任务相关的示例演示。</li></ul><h3 id="InstructGPT"><a href="#InstructGPT" class="headerlink" title="InstructGPT"></a>InstructGPT</h3><p><strong>Training language models to follow instructions with human feedback</strong>：<a href="https://arxiv.org/pdf/2203.02155">https://arxiv.org/pdf/2203.02155</a></p><p>Open AI发现，在GPT-3的基础上直接扩大模型规模(Scaling Up)并不一定能让模型达到更好的效果。如果用户的问题是数据集中没有包含的，那么大模型无法给出正确的答案。</p><p>InstructGPT使用了基于人类反馈的强化学习(Reinforcement Learning from Human Feedback, RLHF)技术，使InstructGPT的输出结果贴近人类习惯。</p><img src="/img/fig/gpt6.jpg"/><p>RLHF三阶段：</p><ol><li>监督微调(Supervised fine-tuning)：使用人工给出的示范性数据，监督训练模型。人工标注问题的答案，然后将问题的回答拼接形成<code>Q-A</code>形式，收集大量对话文本数据集作为提示数据集，问题将作为模型的提示信息，回答是人工写的，作为模型期望输出。-&gt;微调的目的是让GPT-3学习到什么样的回答更加符合人类习惯。</li><li>Human Feedback：使用人工排序的对比性数据，训练奖励模型。人工讲SFT阶段得到的回答进行排序，将排序数据整理为一个数据集用于训练奖励模型。-&gt;奖励模型输入是问题和回答的文本，优化目标是让奖励模型判断的分数排序后要接近人工顺序。</li><li>Reinforcement Learning：使用奖励模型，通过强化学习训练模型。用第二步生成的奖励模型对有SFT模型的输出结果进行评估，通过强化学习训练后，使模型的输出结果的奖励尽量高。</li></ol><h2 id="从零构建大语言模型"><a href="#从零构建大语言模型" class="headerlink" title="从零构建大语言模型"></a>从零构建大语言模型</h2><p>参见文章《从零构建大模型》</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><p>Build A Large Language Model (From Scratch), Sebastian Raschka, <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a></p></li><li><p>EZencoder. DeepSeek-R1 论文详解 part 3：GPT发展史 | scaling law | 训练范式 | 涌现, <a href="https://www.bilibili.com/video/BV1bUA8eYEHJ/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1bUA8eYEHJ/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五一五更之三：动手学BERT</title>
    <link href="/2025/05/03/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%B8%89%EF%BC%9A%E5%8A%A8%E6%89%8B%E5%AD%A6BERT/"/>
    <url>/2025/05/03/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%B8%89%EF%BC%9A%E5%8A%A8%E6%89%8B%E5%AD%A6BERT/</url>
    
    <content type="html"><![CDATA[<h1 id="基于Bert-base-chinese的中文文本分类"><a href="#基于Bert-base-chinese的中文文本分类" class="headerlink" title="基于Bert-base-chinese的中文文本分类"></a>基于Bert-base-chinese的中文文本分类</h1><p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding：<a href="https://arxiv.org/pdf/1810.04805">https://arxiv.org/pdf/1810.04805</a></p><h2 id="安装必要的第三方库"><a href="#安装必要的第三方库" class="headerlink" title="安装必要的第三方库"></a>安装必要的第三方库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@autodl-container-7a964b9870-ea0546ee:~# pip install pandas transformers<br></code></pre></td></tr></table></figure><p><img src="/img/fig/image-20250625224311571.png" alt="库安装"></p><h2 id="加载代码与数据集"><a href="#加载代码与数据集" class="headerlink" title="加载代码与数据集"></a>加载代码与数据集</h2><p>使用git从GitHub拉取镜像仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@autodl-container-7a964b9870-ea0546ee:~# git <span class="hljs-built_in">clone</span> https://github.com/QijingGJ/Bert-Chinese-Text-Classification.git<br>root@autodl-container-7a964b9870-ea0546ee:~# <span class="hljs-built_in">ls</span><br>Bert-Chinese-Text-Classification  autodl-pub  autodl-tmp  miniconda3  tf-logs<br></code></pre></td></tr></table></figure><img src="/img/fig/image-20250625222119585.png" style="zoom:75%;" /><h2 id="下载google-bert-bert-base-chinese模型"><a href="#下载google-bert-bert-base-chinese模型" class="headerlink" title="下载google-bert&#x2F;bert-base-chinese模型"></a>下载google-bert&#x2F;bert-base-chinese模型</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@autodl-container-7a964b9870-ea0546ee:~# git <span class="hljs-built_in">clone</span> https://hf-mirror.com/google-bert/bert-base-chinese<br>root@autodl-container-7a964b9870-ea0546ee:~/bert-base-chinese# <span class="hljs-built_in">ls</span><br>README.md  config.json  flax_model.msgpack  model.safetensors  pytorch_model.bin  tf_model.h5  tokenizer.json  tokenizer_config.json  vocab.txt<br></code></pre></td></tr></table></figure><p><img src="/img/fig/image-20250625222817754.png" alt="模型加载"></p><h2 id="修改数据参数"><a href="#修改数据参数" class="headerlink" title="修改数据参数"></a>修改数据参数</h2><p>修改必要参数路径前，先把模型文件整合一下，并创建好<code>bert_checkpoint</code>文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@autodl-container-7a964b9870-ea0546ee:~/bert-base-chinese# <span class="hljs-built_in">cd</span> ../<br>root@autodl-container-7a964b9870-ea0546ee:~# <span class="hljs-built_in">ls</span><br>Bert-Chinese-Text-Classification  autodl-pub  autodl-tmp  bert-base-chinese  miniconda3  tf-logs<br><br>root@autodl-container-7a964b9870-ea0546ee:~# <span class="hljs-built_in">mv</span> bert-base-chinese/ Bert-Chinese-Text-Classification/<br><br>root@autodl-container-7a964b9870-ea0546ee:~# <span class="hljs-built_in">ls</span><br>Bert-Chinese-Text-Classification  autodl-pub  autodl-tmp  miniconda3  tf-logs<br><br>root@autodl-container-7a964b9870-ea0546ee:~# <span class="hljs-built_in">cd</span> Bert-Chinese-Text-Classification/<br>root@autodl-container-7a964b9870-ea0546ee:~/Bert-Chinese-Text-Classification# <span class="hljs-built_in">ls</span><br>README  THUCNews  bert-base-chinese  bert_get_data.py  bert_test.py  bert_train.py  bert_tuili.py<br><br>root@autodl-container-7a964b9870-ea0546ee:~/Bert-Chinese-Text-Classification# <span class="hljs-built_in">mkdir</span> -p bert_checkpoint<br><br>root@autodl-container-7a964b9870-ea0546ee:~/Bert-Chinese-Text-Classification# <span class="hljs-built_in">ls</span><br>README  THUCNews  bert-base-chinese  bert_checkpoint  bert_get_data.py  bert_test.py  bert_train.py  bert_tuili.py<br></code></pre></td></tr></table></figure><h3 id="bert-get-data-py"><a href="#bert-get-data-py" class="headerlink" title="bert_get_data.py"></a>bert_get_data.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#修改位置1:</span><br>bert_name = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/bert-base-chinese&#x27;</span><br>tokenizer = BertTokenizer.from_pretrained(bert_name)<br><br><span class="hljs-comment">#修改位置2:</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GenerateData</span>(<span class="hljs-params">mode</span>):<br>    train_data_path = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/THUCNews/data/train.txt&#x27;</span><br>    dev_data_path = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/THUCNews/data/dev.txt&#x27;</span><br>    test_data_path = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/THUCNews/data/test.txt&#x27;</span><br><br>    ...代码逻辑保持不变<br></code></pre></td></tr></table></figure><h3 id="bert-train-py与bert-test-py"><a href="#bert-train-py与bert-test-py" class="headerlink" title="bert_train.py与bert_test.py"></a>bert_train.py与bert_test.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#修改位置：</span><br>save_path = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/bert_checkpoint&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="bert-tuili-py"><a href="#bert-tuili-py" class="headerlink" title="bert_tuili.py"></a>bert_tuili.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">bert_name = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/bert-base-chinese&#x27;</span><br>tokenizer = BertTokenizer.from_pretrained(bert_name)<br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>save_path = <span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/bert_checkpoint&#x27;</span><br>model = BertClassifier()<br>model.load_state_dict(torch.load(os.path.join(save_path, <span class="hljs-string">&#x27;best.pt&#x27;</span>)))<br>model = model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br><br>real_labels = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/root/Bert-Chinese-Text-Classification/THUCNews/data/class.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    ...代码逻辑保持不变<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p><img src="/img/fig/image-20250625235024801.png" alt="训练"></p><h2 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h2><p><img src="/img/fig/image-20250625235414672.png" alt="测试"></p><h2 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h2><p><img src="/img/fig/image-20250625235727682.png" alt="推理"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五一五更之二：注意力机制的起源与发展</title>
    <link href="/2025/05/02/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%BA%8C%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%B5%B7%E6%BA%90%E4%B8%8E%E5%8F%91%E5%B1%95/"/>
    <url>/2025/05/02/%E4%BA%94%E4%B8%80%E4%BA%94%E6%9B%B4%E4%B9%8B%E4%BA%8C%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%B5%B7%E6%BA%90%E4%B8%8E%E5%8F%91%E5%B1%95/</url>
    
    <content type="html"><![CDATA[<h1 id="注意力机制的起源与发展"><a href="#注意力机制的起源与发展" class="headerlink" title="注意力机制的起源与发展"></a>注意力机制的起源与发展</h1><p>注意力是我们人类特有的功能之一，这半年读周岭老师的《认知觉醒》、《认知驱动》时候对注意力这个词产生了很深的印象。</p><p>人类情绪和能力的优劣差异来自于对自身注意力关注方式的差异。注意力是一种<strong>深度沉浸</strong>的能力，从大范围看，沉浸能力强的人时常处于支配层，沉浸能力弱的人时常处于被支配层。</p><p>在聚精会神的状态下，人类大脑的前额叶皮层就会自动沿着神经通路传递信号，这些信息会奔向与我们思考内容相关的各个脑区，将它们连起来。</p><div class="note note-success">            <p>海量的数据训练本身就是一个刻意练习的过程，如果在刻意练习的同时，将人类的注意力机制赋予到机器身上，是不是可以也让机器变得更加聪明、专注呢？这就是本文主要想要说明的一个问题，特别是当机器被用在处理语言和视觉的问题上。</p><p>Attention的本质就是在计算Token之间的Correlation。</p>          </div><h2 id="深度学习中注意力的本质"><a href="#深度学习中注意力的本质" class="headerlink" title="深度学习中注意力的本质"></a>深度学习中注意力的本质</h2><p>注意力机制允许模型在处理信息时模仿人类认知系统，有选择地聚焦于输入数据的相关部分，从而做出更精准的预测或生成更相关的输出 。这一机制的引入，极大地克服了早期模型在处理长序列数据时信息丢失或稀释的局限性 。注意力机制的广泛应用遍及自然语言处理（NLP）（如机器翻译、文本摘要、问答系统）、计算机视觉（CV）（如图像描述、目标检测）以及语音识别等多个领域 。</p><p>在注意力机制出现之前，主流的编码器-解码器架构通常试图将整个输入序列的所有信息压缩到一个<strong>固定长度</strong>的向量中，但是这种做法在处理长序列时尤为捉襟见肘，关键信息的丢失无法难免(最终有用的信息会溢出丢失)。注意力机制通过允许解码器在生成输出的每一步“回顾”并动态地关注输入序列的不同部分，直接解决了这一瓶颈问题。这不仅仅是性能上的提升，更是神经网络内部信息流动范式的一次革新。</p><p>注意力机制的目标就是关注必要信息，根据该信息进行时序转换。</p><center>  <img src="https://shangzhih.github.io/images/2018-03-30-13-47-09.jpg" alt="attention" style="zoom:80%;" /></center><h2 id="起初：Seq2Seq中的早期萌芽"><a href="#起初：Seq2Seq中的早期萌芽" class="headerlink" title="起初：Seq2Seq中的早期萌芽"></a>起初：Seq2Seq中的早期萌芽</h2><p><a href="https://arxiv.org/pdf/1606.01933">https://arxiv.org/pdf/1606.01933</a></p><p><a href="https://arxiv.org/pdf/1702.00887">https://arxiv.org/pdf/1702.00887</a></p><p><a href="https://arxiv.org/pdf/1409.0473">https://arxiv.org/pdf/1409.0473</a></p><div class="note note-danger">            <p>神经机器翻译(Neural Machine Translation, NMT)中遇到的信息瓶颈问题是Attention机制出现的Motivation</p>          </div><p>Attention机制出现之前，传统的编码器-解码器(Encoder-Decoder)架构在处理序列到序列(Sequence-to-Sequence, Seq2Seq)任务，尤其是神经机器翻译(Neural Machine Translation, NMT)时，面临着一个核心挑战：<strong>信息瓶颈</strong>问题。</p><p>编码器负责将整个文本(源语言句子)作为输入序列压缩成一个固定长度的上下文向量(context vector)，解码器则基于这个固定长度的上下文向量生成输出序列。<em>当输入序列较长时，固定长度的上下文向量难以承载全部重要信息，导致信息丢失，进而影响模型性能</em>，特别是在翻译长句时尤为明显。  </p><h3 id="编码-解码架构的改进"><a href="#编码-解码架构的改进" class="headerlink" title="编码-解码架构的改进"></a>编码-解码架构的改进</h3><p>对编码器的改进：</p><p>注意力机制出现之前都是只将LSTM层的最后隐藏状态传递给解码器，但是编码器的输出长度应该根据文本的长度相应地改变，而不是死板地只输出一个固定长度的向量。编码器输入序列长度为n，那么经过编码器输出的隐藏状态向量大小应该为n*k(n为序列中的单词数量，k为单个单词嵌入的维度)。也就是说，编码器会输出各个单词对应的LSTM层的隐藏状态向量。</p><p>对解码器的改进：</p><ol><li>不在使用编码器“最后”时刻的隐藏状态作为初始化，而是使用改进后的<code>n*k</code>隐藏状态大小的向量作为解码器的输入。从编码器的<code>n*k</code>隐藏状态向量中选出与各个时刻解码器输出的单词有对应关系的单词向量，完成信息对齐。</li><li>使用向量内积方法计算解码器隐藏状态向量<code>1*k</code>在多大程度上和编码器<code>n*k</code>各个单词向量“相似”。</li></ol><h3 id="Bahdanau注意力-加性注意力，2014"><a href="#Bahdanau注意力-加性注意力，2014" class="headerlink" title="Bahdanau注意力(加性注意力，2014)"></a>Bahdanau注意力(加性注意力，2014)</h3><ul><li>《Neural Machine Translation by Jointly Learning to Align and Translate》</li></ul><p>Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio在2014年发表的论文《Neural Machine Translation by Jointly Learning to Align and Translate》中首次引入了注意力机制，旨在解决上述瓶颈问题 。</p><center>  <img src="https://qnimg.lovevivian.cn/paper-nmt-attention-bahdanau-1.jpeg" alt="attention" style="zoom:60%;" /></center><p>核心思想：允许解码器生成目标词的时候，能够“有选择地”关注原序列的不同部分，而不是单纯依赖一个固定长度的上下文向量，这种机制是学习一种源序列到目标序列的“对齐”(alignment)。</p><h2 id="破晓：Transformer的革命"><a href="#破晓：Transformer的革命" class="headerlink" title="破晓：Transformer的革命"></a>破晓：Transformer的革命</h2><div class="note note-danger">            <p>Attention Is All You Need</p>          </div><p>2017年，由Google研究人员发表的论文《Attention Is All You Need》 引入了Transformer模型，这标志着深度学习领域，尤其是自然语言处理领域的一次范式转换。Transformer模型完全摒弃了以往序列处理中占主导地位的循环神经网络(RNN)和卷积神经网络(CNN)结构，<strong>而<em>完全</em>依赖于注意力机制来捕捉输入和输出之间的全局依赖关系</strong> 。</p><p>传统的RNN（包括LSTM和GRU等变体）在处理序列数据时，其固有的顺序计算特性导致了几个主要瓶颈 ：</p><ol><li><p><strong>并行计算受限</strong>：RNN按时间步顺序处理数据，使得在序列长度维度上的并行化变得困难，这限制了在现代GPU等并行计算设备上的训练效率。</p></li><li><p><strong>长距离依赖学习困难</strong>：尽管LSTM等结构通过门控机制缓解了梯度消失&#x2F;爆炸问题，但信息在长序列中逐级传递时，仍可能发生衰减或失真，导致模型难以有效捕捉远距离词语之间的依赖关系。信息从序列一端到另一端的路径长度随序列长度线性增长。</p></li></ol><p>Transformer的核心思想正是为了克服这些限制。</p><center>  <img src="https://cdn.prod.website-files.com/62528d398a42420e66390ef9/65cd0d82d6983755c129822a_Untitled.png" alt="attention" style="zoom:50%;" /></center><h3 id="自注意力机制-Self-Attention"><a href="#自注意力机制-Self-Attention" class="headerlink" title="自注意力机制 - Self Attention"></a>自注意力机制 - Self Attention</h3><p>自注意力机制是Transformer模型的核心(主要用于Encoder中)。它允许模型在处理序列中的每个元素（例如，句子中的一个词）时，同时关注序列中的所有其他元素，从而计算该元素的上下文感知表示 。</p><h3 id="掩码注意力机制-Masked-Attention"><a href="#掩码注意力机制-Masked-Attention" class="headerlink" title="掩码注意力机制 - Masked Attention"></a>掩码注意力机制 - Masked Attention</h3><p>掩码注意力机制是是Transformer模型的另一核心模块(主要用于Decoder中)，它允许模型在处理序列中的元素时，只关注序列中当前单词及之前出现过的单词，对序列中当前单词之后的信息全部mask掉。</p><h3 id="多头注意力机制-Multi-Head-Attention"><a href="#多头注意力机制-Multi-Head-Attention" class="headerlink" title="多头注意力机制 - Multi-Head Attention"></a>多头注意力机制 - Multi-Head Attention</h3><p>为了进一步增强自注意力机制的表达能力，Transformer引入了多头注意力机制 。多头注意力机制使得模型支持并行计算，极大地提高了模型的训练效率。</p><p>然而，尽管Transformer带来了革命性的进步，其核心的完全自注意力机制也引入了新的挑战：计算和内存复杂度均与序列长度成二次方关系 $(O(N^2)$ 。这在处理极长序列时会变得非常昂贵，从而催生了下一阶段对注意力机制效率优化的研究浪潮。</p><h2 id="革新：DeepSeek-V3-中的多头隐注意力（MLA）"><a href="#革新：DeepSeek-V3-中的多头隐注意力（MLA）" class="headerlink" title="革新：DeepSeek-V3 中的多头隐注意力（MLA）"></a>革新：DeepSeek-V3 中的多头隐注意力（MLA）</h2><p>随着大型语言模型（LLM）的参数规模和所需处理的上下文长度持续增长，对推理效率和训练经济性的要求也日益严苛。DeepSeek系列模型，特别是DeepSeek-V2  和DeepSeek-V3 ，引入了<strong>多头隐注意力(Multi-head Latent Attention, MLA)机制</strong>，作为其在注意力架构上的一项关键创新，旨在实现高效推理和经济的训练。</p><p>后续详细内容请跳转到文章：五一五更之五：《手撸DeepSeek-V3-R1》</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/">https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/</a></p></li><li><p><a href="https://arxiv.org/pdf/1409.0473">https://arxiv.org/pdf/1409.0473</a></p></li><li><p><a href="https://www.turingpost.com/p/attention">https://www.turingpost.com/p/attention</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五一五更之一：2025年4月总结与展望</title>
    <link href="/2025/05/01/2025%E5%B9%B44%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/05/01/2025%E5%B9%B44%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>五一放假回老家了，在家呆着也舒服，也不舒服。</p><p>舒服在于安心，回来无忧无虑，家是唯一的城堡；不舒服在于，竟然被弟弟妹妹们催婚了卧槽？！我爸我妈都没发表意见，被小一辈开始攻击了😭</p><h2 id="并不友好的人间4月天"><a href="#并不友好的人间4月天" class="headerlink" title="并不友好的人间4月天"></a>并不友好的人间4月天</h2><p>4月，大风、多雨。</p><p>天气起起伏伏，状态也是起起伏伏😑</p><p>这个月很焦虑，因为实验跑了<strong>一坨屎</strong>；深刻理解了Kaiming那句话：<strong>科研95%的时间都是沮丧的</strong>。（短短一句话，一个月的辛酸全部囊括了，无论多努力：）何况压根儿也没多努力，全部付之东流see you again；真正对当年明月那句话有了同感：历史书上，短短一页纸的内容就是一个人波澜壮阔的一生啊！）</p><p>科苑杯篮球赛也是起起伏伏，小组赛赢了两个比较弱的对手，但是遇到计算机学院这个实力稍微强一点的对手，便有点乱了阵脚。加上队友们有相继生病的，阵容并不完整，状态并不好。输给了计算机学院，真不甘心～其实是和他们有的一打的。</p><p>这个月竟然把《大明王朝1566》读完了，而且还读了《半小时漫画红楼梦》；真没想到竟然以这种漫画的形式完成了人生第一次《红楼梦》阅读。</p><p>陪熙宸看病的路上，零零碎碎读了周玲老师的另一本大作《认知驱动》，算是《认知觉醒》的姊妹篇；本质上还是教人做事。</p><h2 id="乌兰察布自驾"><a href="#乌兰察布自驾" class="headerlink" title="乌兰察布自驾"></a>乌兰察布自驾</h2><p>清明节放假和舅舅一家自驾去了乌兰察布和乌兰哈达火山群，内蒙好爽，体验感很好；牛马怎能不向往大草原啊～</p><h2 id="新知"><a href="#新知" class="headerlink" title="新知"></a>新知</h2><p>睿劼在微信读书给推荐了一本书不错：ddia《Designing Data Intensive Applications：数据密集型应用系统设计》<a href="https://ddia.qtmuniao.com/#/">https://ddia.qtmuniao.com/#/</a></p><p>冯小刚电影不错，尤其是和王朔一起搞的；现在再看，全是讽刺。</p><p>诺丁山也很好看，罗曼蒂克式的爱情片；艹，啥时候我也这么浪漫起来了哈哈哈。</p><p>————————</p><p>PS：四月很懵逼、很压抑；因为一直在经历失败、沮丧☹️想分身，想同时干很多事，但人是单线程，只能一件事一件事地干；有计划，但是又很拖延😂</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>没有饺子全是醋</title>
    <link href="/2025/04/28/%E6%B2%A1%E6%9C%89%E9%A5%BA%E5%AD%90%E5%85%A8%E6%98%AF%E9%86%8B/"/>
    <url>/2025/04/28/%E6%B2%A1%E6%9C%89%E9%A5%BA%E5%AD%90%E5%85%A8%E6%98%AF%E9%86%8B/</url>
    
    <content type="html"><![CDATA[<p>2025年4月28日，星期一，晴天☀️，第60篇博客。</p><p>这学期的第一门课结束了，王老师作为一名科研工作者和互联网一线老兵分享了他的人生感悟。没有饺子全是醋，收好这份《科研生活指南》～</p><p>鸡汤只留给需要喝它的人，宝剑只留给能够挥舞它的人，在强调一遍，没有饺子都是醋～</p><div class="note note-success">            <p>少一些书生气，多一些江湖气</p>          </div><h2 id="人"><a href="#人" class="headerlink" title="人"></a>人</h2><ul><li>主动去学，问；先动起来，干中去事上练</li><li>成绩导向过渡到利益导向</li><li>有需求一定要主动提，要经常汇报</li><li>厚脸皮、甚至不要脸，不要玻璃心</li><li>别把自己看的太重，遇到人和事要多感谢</li><li>人生从前往后看都是靠努力，从后往前看都是靠运气</li><li>设法跟对人</li></ul><h2 id="事"><a href="#事" class="headerlink" title="事"></a>事</h2><p>做事的基本逻辑</p><h3 id="事情重要-不重要，紧迫-不紧迫"><a href="#事情重要-不重要，紧迫-不紧迫" class="headerlink" title="事情重要&#x2F;不重要，紧迫&#x2F;不紧迫"></a>事情重要&#x2F;不重要，紧迫&#x2F;不紧迫</h3><p>重要又紧迫、重要但不紧迫(<strong>最值得去干的事</strong>)、不重要紧迫、不重要不紧迫</p><p>重要又紧迫的事其实都是由重要但不紧迫的事演变过来的(这得靠悟，事上练才能有所体会)，始终把注意力集中在重要又紧迫的事就像是“救火队员”；要调整自己学会规划，将更多的精力放到重要但不紧迫的事情上去。</p><h3 id="事情有千头万绪，但是一次只能揪住一头"><a href="#事情有千头万绪，但是一次只能揪住一头" class="headerlink" title="事情有千头万绪，但是一次只能揪住一头"></a>事情有千头万绪，但是一次只能揪住一头</h3><p>把自己由单线程转变成多线程，需要充分管理好自己的时间，在生活中多运用起来。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>湖仓一体技术调研</title>
    <link href="/2025/04/25/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"/>
    <url>/2025/04/25/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<center> <h1>湖仓一体技术调研</h1> </center><center>中国科学院大学计算机科学与技术学院</center><center>2025春·大数据系统与大规模数据分析</center><center>授课老师：陈世敏老师(https://www.shimin-chen.com/index-gb.html)</center><center>授课老师：孙翼老师(https://people.ucas.ac.cn/~0002034)</center><center>_DataLake小组：杨桂淼(组长)、王子曰(成员1)、王晓妍(成员2)</center><p><img src="https://learn.microsoft.com/zh-cn/azure/architecture/data-guide/scenarios/images/data-lake-use-cases.jpg" alt="引自：learn-microsoft"></p><p>前期任务及分工如下：</p><p>前期三个人分别调研一个数据湖仓系统，完成云平台账号注册以及基本的环境配置。</p><p>Apache Hudi（杨桂淼）：<a href="https://hudi.apache.org/">https://hudi.apache.org/</a></p><p>Apache lceberg（王子曰）：<a href="https://iceberg.apache.org/">https://iceberg.apache.org/</a></p><p>Delta Lake（王晓妍）：<a href="https://delta.io/">https://delta.io/</a></p><h2 id="基本概念调研"><a href="#基本概念调研" class="headerlink" title="基本概念调研"></a>基本概念调研</h2><div class="note note-success">            <p>大数据的4V特征：<strong>海量(Volume)、多样(Variety)、高速(Velocity)、价值(Value)</strong></p>          </div><p>数据管理系统的演进过程：关系型数据库 -&gt; 数据仓库  -&gt; 数据湖 -&gt; 数据湖仓</p><p><code>关系型数据库</code>：RDBMS 以关系模型为基础，将数据组织在预定义关系的二维表（即关系）中，表由列（属性）和行（记录）构成。每行数据通常拥有一个唯一标识符（主键），用于区分不同的记录。关系模型的一个重要特性是逻辑数据结构（如表、视图、索引）与物理存储结构的分离，这使得数据库管理员可以在不影响逻辑数据访问的前提下管理物理存储。结构化查询语言（SQL）是RDBMS进行数据查询和操作的标准语言。</p><p><code>数据仓库</code>：文献普遍将数据仓库定义为用于支持决策制定的、集成的、面向主题的、时变的数据集合，其核心是结构化数据、写时模式（Schema-on-Write）、ETL过程和OLAP能力，旨在为商业智能提供单一事实来源。</p><p><code>数据湖</code>：数据湖被描述为能够以原始格式存储海量、多样化数据（包括结构化、半结构化和非结构化数据）的中央存储库，采用读时模式（Schema-on-Read），支持ELT过程，并为高级分析和机器学习提供数据基础 。</p><p><code>数据湖仓</code>：湖仓一体被视为数据仓库和数据湖的融合，旨在结合两者的优点：数据湖的低成本、灵活性和开放格式，以及数据仓库的数据管理、事务能力和查询性能。其核心是通过在数据湖的开放文件格式（如Parquet）之上引入表格式（Table Format）层（如Hudi, Iceberg, Delta Lake），实现ACID事务、模式管理、数据版本控制等功能。</p><h3 id="关系型数据库"><a href="#关系型数据库" class="headerlink" title="关系型数据库"></a>关系型数据库</h3><p><strong>核心功能</strong>: 主要用于<strong>在线交易处理 (OLTP)<strong>，比如你购物下的订单、银行的转账记录等。它的核心是保证数据的</strong>一致性、准确性</strong>。</p><p>关系型数据库的特点是<strong>结构化数据</strong>: 数据以二维表格（行和列）的形式存储，结构非常规整。<strong>预定义模式 (Schema-on-Write)</strong>: 在写入数据之前，必须先定义好表的结构（比如字段名、数据类型）。</p><p>痛点是：<strong>无法处理非结构化数据</strong>: 无法存储和处理像视频、音频、图片、社交媒体帖子这类非结构化数据。<strong>分析能力弱</strong>: 为交易而生，进行大规模、复杂的查询和分析时，会严重影响正常业务的性能。</p><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><p><strong>核心功能</strong>: 为了解决关系型数据库分析能力弱的问题而诞生，专注于<strong>在线分析处理 (OLAP)</strong> 和**商业智能 (BI)**。</p><p>数据仓库的特点是：<strong>结构化和已处理</strong>: 只存储经过清洗、转换后的结构化数据，可以直接用于报表和分析。<strong>面向主题</strong>: 数据从多个业务数据库中抽取（ETL过程：抽取、转换、加载），并按照业务主题（如销售、库存）进行组织。<strong>历史数据</strong>: 存储了大量的历史数据，用于趋势分析和预测。</p><p>痛点是：<strong>数据类型单一</strong>: 仍然无法很好地应对非结构化和半结构化数据。数据在进入数仓前，不符合要求的部分就被丢弃了，导致无法进行更原始、更全面的数据探索。<strong>灵活性差、成本高</strong>: ETL过程复杂且耗时。由于采用“预定义模式”，任何需求的变更都可能需要重新设计数据模型和ETL流程，非常僵化。存储和计算的成本也很高。</p><h3 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h3><p><strong>核心功能</strong>: 为了解决数据仓库无法处理海量、多样化数据的痛点而出现。它的理念是“先存下所有东西，以后再想怎么用”。</p><p>数据湖的特点是<strong>存储一切</strong>: 可以存储任何类型的数据，包括结构化、半结构化和非结构化（如日志文件、JSON、视频、音频等）的原始数据。<strong>后定义模式 (Schema-on-Read)</strong>: 在读取和分析数据时，才根据需求去定义数据的结构。<strong>原始、未经处理</strong>: 数据以其最原始的形式被加载进来，不做任何转换。<strong>低成本存储</strong>: 通常建立在HDFS等分布式文件系统上，存储成本非常低廉。</p><p>痛点是：<strong>缺乏事务支持</strong>: 不支持ACID事务，这使得在数据湖上进行更新、删除操作变得非常复杂且不可靠，难以保证数据的一致性。<strong>数据沼泽 (Data Swamp)</strong>: 由于缺乏对数据质量的管控、元数据管理和治理，数据湖很容易变成一个无人能懂、无法使用的“数据沼泽”。</p><h3 id="湖仓一体"><a href="#湖仓一体" class="headerlink" title="湖仓一体"></a>湖仓一体</h3><p>数据湖仓（也常被称为“湖仓一体”）的出现，正是为了解决一个核心矛盾：我们既想要<strong>数据湖的灵活性和低成本</strong>，又想要<strong>数据仓库的强大分析性能和数据治理能力</strong>。</p><p>数据湖仓是一种新型的、开放的数据架构，将数据湖的低成本、灵活性与数据仓库的数据管理和分析功能相结合，旨在在同一个系统内实现对所有数据的BI和AI应用。</p><h2 id="文献资料整理"><a href="#文献资料整理" class="headerlink" title="文献资料整理"></a>文献资料整理</h2><p>论文：</p><ul><li>Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics：<a href="https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdfThe">https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdfThe</a> </li><li>Data Lakehouse: Data Warehousing and More：<a href="https://arxiv.org/abs/2310.08697">https://arxiv.org/abs/2310.08697</a></li><li>Analyzing and Comparing Lakehouse Storage Systems：<a href="https://www.cidrdb.org/cidr2023/papers/p92-jain.pdf">https://www.cidrdb.org/cidr2023/papers/p92-jain.pdf</a></li><li>Assessing the Lakehouse: Analysis, Requirements and Definition：<a href="https://www.ipvs.uni-stuttgart.de/departments/as/publications/schneijn/Assessing_the_Lakehouse_ICEIS2023.pdf">https://www.ipvs.uni-stuttgart.de/departments/as/publications/schneijn/Assessing_the_Lakehouse_ICEIS2023.pdf</a></li><li>Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores：<a href="https://www.vldb.org/pvldb/vol13/p3411-armbrust.pdf">https://www.vldb.org/pvldb/vol13/p3411-armbrust.pdf</a></li><li>Apache Hudi - The Data Lake Platform：<a href="https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/">https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/</a></li></ul><p>开源文档与技术白皮书</p><ul><li>Delta Lake：<a href="https://docs.dremio.com/25.x/sonar/query-manage/data-formats/delta-lake/">https://docs.dremio.com/25.x/sonar/query-manage/data-formats/delta-lake/</a></li><li>Hudi：<a href="https://hudi.apache.org/docs/quick-start-guide">https://hudi.apache.org/docs/quick-start-guide</a></li><li>Data Warehouse vs. Data Lake vs. Data Lakehouse: An Overview of Three Cloud Data Storage Patterns：<a href="https://go2.striim.com/hubfs/striim_tech_brief_data_storage_overview.pdf">https://go2.striim.com/hubfs/striim_tech_brief_data_storage_overview.pdf</a></li></ul><h2 id="现有系统分析"><a href="#现有系统分析" class="headerlink" title="现有系统分析"></a>现有系统分析</h2><h3 id="湖仓一体架构与功能"><a href="#湖仓一体架构与功能" class="headerlink" title="湖仓一体架构与功能"></a>湖仓一体架构与功能</h3><p><strong>目标与架构</strong>: 湖仓一体是一种全新的数据管理系统，它直接在低成本的数据湖存储之上，提供传统数据仓库的管理和性能特性，如ACID事务、数据版本控制、索引和查询优化 。其设计目标是融合数据湖的低成本、开放性和数据仓库的强大管理与优化能力 。</p><h3 id="湖仓一体的核心特征"><a href="#湖仓一体的核心特征" class="headerlink" title="湖仓一体的核心特征"></a>湖仓一体的核心特征</h3><p>单一系统：消除了数据湖和数据仓库之间的数据移动和冗余，所有工作负载（BI、数据科学、ML）都可以在同一份数据副本上进行 。</p><p>事务性元数据管理：湖仓一体的关键实现是在对象存储之上构建一个事务性的元数据层 。这一层定义了哪些数据文件构成一个表的特定版本，从而实现了ACID事务、数据版本回溯（Time Travel）等高级管理功能 。</p><h4 id="典型的分层架构"><a href="#典型的分层架构" class="headerlink" title="典型的分层架构"></a>典型的分层架构</h4><ul><li><p><strong>数据接入层 (Ingestion Layer)</strong> ：负责从各种数据源（如关系型数据库、NoSQL数据库、业务应用、流式数据源、文件等）采集数据并将其导入湖仓系统。此层支持批量导入和实时流式导入，常见工具包括Apache Kafka, Apache Flume, Spark Streaming以及各类数据库连接器和CDC工具。</p></li><li><p><strong>存储层 (Storage Layer)</strong> ：通常采用可扩展、高持久且成本效益高的云对象存储服务（如AWS S3, Azure Blob Storage, Google Cloud Storage）或分布式文件系统（如HDFS）。数据在此层以开放文件格式（如Apache Parquet, Apache ORC, Apache Avro）存储，能够容纳结构化、半结构化和非结构化数据。</p></li><li><p><strong>表格式层 (Table Format &#x2F; Metadata Layer)</strong> ：这是湖仓一体架构的核心与灵魂，是其区别于传统数据湖的关键。表格式（如Apache Hudi, Apache Iceberg, Delta Lake）在数据湖的原始文件之上提供了一个抽象层，负责管理元数据，包括表结构（Schema）、事务日志、数据版本、分区信息以及文件统计信息等。它使得上层计算引擎可以将存储在对象存储中的文件集合视为具有事务特性和结构化定义的表。</p></li><li><p><strong>查询与处理引擎层 (Query&#x2F;Processing Engines)</strong> ：各种计算引擎（如Apache Spark, Presto, Trino, Apache Flink）和云数据服务（如AWS Athena, Google BigQuery, Snowflake）通过与表格式层交互来读取、处理和分析数据。这些引擎负责执行SQL查询、数据转换、机器学习任务等。</p></li><li><p><strong>治理与目录层 (Governance &amp; Catalog Layer)</strong> ：提供数据发现、访问控制、数据血缘追踪、数据质量监控和审计等功能。通常与元数据存储服务（如Hive Metastore, AWS Glue Data Catalog, Project Nessie）集成，以实现统一的元数据视图和治理策略。</p></li></ul><h4 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h4><ul><li><p><strong>ACID事务</strong>：确保数据操作的原子性、一致性、隔离性和持久性，为并发读写提供数据一致性和可靠性保障，这是从数据仓库借鉴并应用于数据湖的关键特性。</p></li><li><p><strong>模式管理 (Schema Management)</strong> ：包括模式强制（Schema Enforcement）和模式演进（Schema Evolution）。模式强制确保写入数据符合预定义结构，防止脏数据污染；模式演进则允许在不重写整个数据集的情况下修改表结构（如增删列、修改类型）以适应业务变化。</p></li><li><p><strong>数据版本控制 (Data Versioning &#x2F; Time Travel)</strong> ：记录数据的历史版本，允许用户查询表的过去某个时间点的状态、回滚到历史版本或重现实验结果。</p></li><li><p><strong>索引与数据裁剪&#x2F;跳过 (Indexing &amp; Data Skipping&#x2F;Pruning)</strong> ：利用文件&#x2F;分区元数据统计信息（如最小&#x2F;最大值）、Bloom过滤器、Z-Order等技术，在查询时跳过不相关的数据文件或分区，从而优化查询性能。</p></li><li><p><strong>支持多样化数据与工作负载</strong> ：能够处理结构化、半结构化和非结构化数据，并支持包括BI、SQL分析、数据科学、机器学习和流处理在内的多种工作负载。</p></li><li><p><strong>计算存储分离 (Decoupled Compute and Storage)</strong> ：通常构建在云原生架构之上，允许计算资源和存储资源独立扩展，从而提供更好的灵活性和成本优化。</p></li></ul><h3 id="现有系统基本原理"><a href="#现有系统基本原理" class="headerlink" title="现有系统基本原理"></a>现有系统基本原理</h3><h4 id="Apache-Hudi"><a href="#Apache-Hudi" class="headerlink" title="Apache Hudi"></a><strong>Apache Hudi</strong></h4><p>Apache Hudi旨在为数据湖带来流式处理能力，特别是高效的记录级插入更新（Upsert）和删除操作。</p><p>Apache Hudi为数据加载和更新提供了灵活的策略，其核心机制围绕记录键（record key）、索引和事务时间线展开。</p><p><img src="https://www.biaodianfu.com/wp-content/uploads/2024/11/hudi-1.png" alt="Apache Hudi架构"></p><p>可以将 Hudi 的整体架构理解为由三大核心支柱支撑：</p><ol><li>**优化的表格式 (Optimized Table Format)**：定义了数据在数据湖中如何组织和布局。这是所有功能的基础，确保了数据的可管理性和事务性。</li><li>**丰富的表服务 (Table Services)**：在后台运行的、用于管理和优化数据表的各种自动化服务。它们是保证数据湖高性能和低成本的关键，如同数据仓库中的 DBA 工具集。</li><li>**统一的分析视图 (Unified Analytics Views)**：为上层查询引擎（如 Spark, Presto, Flink, Hive 等）提供统一、高性能的数据读取视图。</li></ol><p>它通过<strong>时间轴</strong>保证了事务性，通过<strong>文件组和文件切片</strong>实现了记录级别的更新，通过<strong>两种表类型 (CoW&#x2F;MoR)</strong> 在读写性能间提供了灵活选择，通过<strong>高效索引</strong>解决了数据定位难题，并利用一系列<strong>自动化表服务</strong>保证了数据湖的长期健康和高性能。</p><h5 id="写时复制-CoW-与读时合并-MoR-策略下的加载"><a href="#写时复制-CoW-与读时合并-MoR-策略下的加载" class="headerlink" title="写时复制 (CoW) 与读时合并 (MoR) 策略下的加载"></a>写时复制 (CoW) 与读时合并 (MoR) 策略下的加载</h5><p>Hudi支持两种主要的表类型（也是数据更新策略），它们在数据加载时的行为有所不同：</p><ul><li><strong>写时复制 (Copy-On-Write, CoW):</strong> 在CoW模式下，当数据被加载或更新时，如果记录导致现有数据文件的更改，Hudi会识别包含这些记录的文件，并将更新后的数据连同未更改的数据一起重写到一个新的版本化文件中。旧版本的文件保持不变，直到被清理策略移除。这种策略的特点是写入时会产生较高的写入放大（因为即使只更新一行，也可能重写整个文件），但在读取时不需要额外的合并操作，因此读取放大较低，查询性能通常较好 。对于批量加载主要是新数据追加的场景，CoW表现为直接写入新的Parquet文件。更新操作则完全以列式Parquet文件的形式写入，创建新的数据对象或文件版本 。  </li><li><strong>读时合并 (Merge-On-Read, MoR):</strong> MoR模式旨在优化写入延迟。加载或更新数据时，Hudi不会立即重写包含旧数据的基础文件（通常是Parquet格式）。相反，它将记录级别的变更（插入、更新、删除）写入增量的、基于行的日志文件（通常是Avro格式）。实际的数据合并操作被推迟到查询时进行，或者通过后台的异步压缩（compaction）过程定期执行，将日志文件中的变更合并到新的基础文件中 。MoR模式的写入放大较低（写入速度快），但读取时需要合并基础文件和相关的日志文件，因此读取放大较高，查询延迟可能比CoW高，除非数据已被压缩。</li></ul><h5 id="记录键-Record-Key-与索引在加载中的作用"><a href="#记录键-Record-Key-与索引在加载中的作用" class="headerlink" title="记录键 (Record Key) 与索引在加载中的作用"></a>记录键 (Record Key) 与索引在加载中的作用</h5><p>Apache Hudi的核心设计之一是围绕记录键进行操作，这对于实现高效的upsert（更新或插入）至关重要。</p><ul><li><strong>记录键 (Record Key):</strong> 每个Hudi表中的记录都由一个唯一的记录键标识。这个键由用户指定，用于在数据加载和更新过程中唯一地识别一条记录。</li><li><strong>索引 (Indexing):</strong> 为了在加载（尤其是upsert操作）时快速定位记录，Hudi采用了一种可插拔的索引机制。索引负责将记录键映射到其所在的物理文件组ID 。当一批新数据写入时，Hudi利用索引来判断每条记录是全新的插入（insert）还是对现有记录的更新（update）。常见的索引类型包括基于Bloom过滤器的索引（默认）、简单索引、HBase索引等。Bloom过滤器等索引机制可以显著加速这个打标过程，比传统的通过Spark Join进行判断要快得多 。这个索引查找和记录分类步骤是Hudi实现高效增量更新的基础 。</li></ul><h5 id="事务提交过程"><a href="#事务提交过程" class="headerlink" title="事务提交过程"></a>事务提交过程</h5><p>Hudi的事务提交过程确保了数据写入的原子性和一致性。其核心是维护一个事务时间线（timeline），记录了在表上执行的所有操作（如提交、清理、压缩、回滚等）。  </p><p>数据写入（包括加载）通常遵循以下步骤 ：  </p><ol><li><strong>去重 (Deduping):</strong> （可选）对输入批次内可能存在的重复记录键进行预处理。</li><li><strong>索引查找 (Index Lookup):</strong> 使用配置的索引机制，为输入记录打上标签（插入或更新），并确定其目标文件组。</li><li><strong>文件大小调整 (File Sizing):</strong> Hudi会运行启发式算法，尝试将数据打包到大小合适的文件中，以优化存储和查询性能。</li><li><strong>分区 (Partitioning):</strong> 根据分区键将数据分配到相应的分区路径。</li><li><strong>写入I&#x2F;O (Write I&#x2F;O):</strong><ul><li>对于CoW表，更新操作会导致生成新的版本化基础文件。插入操作会写入新的基础文件。</li><li>对于MoR表，插入操作会写入新的基础文件（如果文件组不存在）或追加到现有日志文件；更新操作会追加到对应文件组的日志文件中。</li></ul></li><li><strong>更新索引 (Update Index):</strong> 写入完成后，更新索引以反映新数据的位置。</li><li><strong>提交 (Commit):</strong> 原子地将操作元数据（如写入的文件列表、记录数统计等）记录到Hudi时间线上，形成一个新的“instant”（即时点）。Hudi在写操作之间使用多版本并发控制（MVCC）来确保一致性，并在并发写操作之间使用乐观并发控制（OCC）或非阻塞并发控制（NBCC）。</li></ol><h4 id="Apache-Iceberg"><a href="#Apache-Iceberg" class="headerlink" title="Apache Iceberg"></a><strong>Apache Iceberg</strong></h4><p>Apache Iceberg专注于提供一个开放、高性能的表格式，特别强调表结构演进的灵活性、分区管理的便捷性以及跨引擎的互操作性。</p><p><img src="https://www.biaodianfu.com/wp-content/uploads/2024/11/iceberg-1.gif" alt="Apache Iceberg架构"></p><h5 id="数据文件、清单文件-Manifest-File-与清单列表-Manifest-List-的协同"><a href="#数据文件、清单文件-Manifest-File-与清单列表-Manifest-List-的协同" class="headerlink" title="数据文件、清单文件 (Manifest File) 与清单列表 (Manifest List) 的协同"></a>数据文件、清单文件 (Manifest File) 与清单列表 (Manifest List) 的协同</h5><p>Iceberg的元数据组织呈现清晰的层次结构 ：  </p><ol><li><p><strong>数据文件 (Data Files):</strong> 这是实际存储表数据的物理文件，可以是Parquet、Avro或ORC格式 。在数据加载时，写入程序首先创建这些数据文件。这些文件一旦写入即不可变。  </p></li><li><p><strong>清单文件 (Manifest Files):</strong> 每个清单文件（Avro格式）跟踪一组数据文件 。它包含了这些数据文件的路径、文件格式、分区信息（每个数据文件属于哪个分区元组）、列级统计信息（如每列的最大&#x2F;最小值、空值计数）、以及可能的删除文件信息（用于Merge-on-Read场景）。清单文件本身也是不可变的，并且可以被多个快照复用，以避免重写未发生变化的元数据。  </p></li><li><p><strong>清单列表 (Manifest List):</strong> 每个表的快照（snapshot）都由一个清单列表文件（Avro格式）定义 。清单列表文件包含了构成该快照的所有清单文件的路径、每个清单文件所覆盖的分区范围统计信息（用于查询时裁剪掉不相关的清单文件）以及每个清单文件包含的数据文件数量等元数据。每次提交尝试都会生成一个新的清单列表，因为它总是代表一个新的快照。</p></li></ol><p>数据加载时，首先写入新的数据文件。随后，会创建一个或多个新的清单文件，这些清单文件指向新写入的数据文件并记录其元数据。最后，会创建一个新的清单列表文件，该文件指向这些新的清单文件（以及可能复用的旧清单文件，如果它们包含的数据仍然是当前快照的一部分）。</p><h5 id="Iceberg加载机制的灵活性与元数据可扩展性"><a href="#Iceberg加载机制的灵活性与元数据可扩展性" class="headerlink" title="Iceberg加载机制的灵活性与元数据可扩展性"></a><strong>Iceberg加载机制的灵活性与元数据可扩展性</strong></h5><p>Apache Iceberg将数据文件的写入与元数据提交过程解耦，并采用分层元数据结构，这为其带来了显著的灵活性和元数据操作的可扩展性，尤其是在查询规划方面。数据文件可以首先被并行且独立地写入。随后，元数据的更新（创建清单文件、清单列表和表元数据文件）也可以高效进行。其层次结构（表元数据 -&gt; 清单列表 -&gt; 清单文件 -&gt; 数据文件）使得在查询规划时能够进行有效的元数据裁剪，快速定位相关数据文件，而无需扫描所有元数据或执行大量的文件列表操作 。最终的原子提交操作仅聚焦于在Catalog中更新一个单一的指针，这相较于在分布式日志系统中协调跨多个日志段的变更（如果所有元数据都直接存储在这样的日志中），可能更为迅速和简单。</p><h4 id="Delta-Lake"><a href="#Delta-Lake" class="headerlink" title="Delta Lake"></a><strong>Delta Lake</strong></h4><p>Delta Lake最初由Databricks开发，现为Linux基金会项目，它通过在Apache Parquet文件之上构建一个事务日志层，为数据湖带来了ACID事务和可靠性。</p><p><img src="https://www.purestorage.com/content/dam/purestorage/knowledge/what-is-a-delta-lake-figure-2.png.imgo.png" alt="Delta Lake架构"></p><p>与 Hudi 类似，Delta Lake也是一个构建在数据湖（如 S3, ADLS, GCS）之上的开源存储层，旨在为数据湖带来 ACID 事务、可靠性和高性能。</p><p>Delta Lake 的架构可以概括为两个层次：</p><ol><li><strong>数据文件层 (Data Files)</strong> ：存储在云存储或 HDFS 上的 Parquet 文件。这是数据的“身体”，包含了所有的实际数据。Delta Lake 默认使用 Parquet 格式，因为它具备高效的压缩和列式存储特性，非常适合分析查询。</li><li><strong>事务日志层 (Transaction Log)<strong>：这是 Delta Lake 的“大脑和灵魂”，也是其架构的精髓所在。它是一个名为 <code>_delta_log</code> 的子目录，与数据文件存放在一起。这个日志完整、有序地记录了对 Delta 表所做的</strong>每一次变更</strong>。</li></ol><h5 id="事务日志-Transaction-Log-的角色与数据写入"><a href="#事务日志-Transaction-Log-的角色与数据写入" class="headerlink" title="事务日志 (Transaction Log) 的角色与数据写入"></a>事务日志 (Transaction Log) 的角色与数据写入</h5><p>Delta Lake的核心是位于表根目录下 <code>_delta_log</code> 子目录中的事务日志 。这个日志是有序记录了对表进行的每一个事务（如数据加载、更新、删除、模式变更等）。  </p><ul><li><strong>数据写入 (Data Writing):</strong> 当加载数据到Delta表时，数据本身以Parquet文件的形式写入表目录下的相应分区（如果表是分区的）或根目录（如果未分区）。  </li><li><strong>事务日志条目 (Transaction Log Entry):</strong> 在数据文件写入完成后，Delta Lake会为该事务创建一个新的JSON文件（例如 <code>00000000000000000001.json</code>），并将其写入 <code>_delta_log</code> 目录。这个JSON文件包含了该事务所执行的操作的详细信息，例如：<ul><li><code>add</code> 操作：列出新添加的Parquet文件的路径、分区值、统计信息（如文件大小、行数、列的最小&#x2F;最大值等）。</li><li><code>remove</code> 操作：在执行覆盖写或删除操作时，列出被移除的旧文件的路径和删除时间戳。</li><li><code>metadata</code> 操作：记录表的元数据信息，如模式、分区列、表属性等。</li><li><code>protocol</code> 操作：定义表所遵循的Delta Lake协议版本，以支持不同的特性。</li></ul></li><li><strong>原子提交 (Atomic Commit):</strong> Delta Lake通过确保每个JSON日志文件的写入是原子的来实现事务的原子性。这通常依赖于底层文件系统提供的原子操作，例如HDFS的<code>append</code>（虽然Delta主要使用创建新文件的方式）或云对象存储上的“如果不存在则放置”（put-if-absent）语义 。只有一个写入者能够成功创建特定版本号的JSON文件，从而确保了事务的串行化。</li></ul><h5 id="检查点-Checkpoints-与版本控制"><a href="#检查点-Checkpoints-与版本控制" class="headerlink" title="检查点 (Checkpoints) 与版本控制"></a>检查点 (Checkpoints) 与版本控制</h5><p>随着事务的不断发生，<code>_delta_log</code> 目录中的JSON文件数量会持续增长。为了提高查询性能（特别是获取表当前状态的速度）并避免列出和读取大量小的JSON文件，Delta Lake会定期创建检查点（checkpoint）文件 。  </p><ul><li><strong>检查点文件 (Checkpoint Files):</strong> 检查点文件（通常是Parquet格式，也可能是多部分Parquet或JSON格式的组合）整合了截至某个特定版本号（例如，每10个JSON提交创建一个检查点）的所有事务日志信息 。它包含了表的当前状态的快照，如当前有效的数据文件列表、表的模式、分区信息以及文件级别的统计数据。  </li><li><strong>版本控制 (Versioning):</strong> <code>_delta_log</code> 中的每一个JSON文件（或检查点文件）都代表了表的一个新版本。这使得Delta Lake能够支持时间旅行（time travel）查询，即查询表在过去某个特定版本或时间点的状态。</li></ul><h5 id="写时复制-CoW-与读时合并-MoR-Deletion-Vectors"><a href="#写时复制-CoW-与读时合并-MoR-Deletion-Vectors" class="headerlink" title="写时复制 (CoW) 与读时合并 (MoR) - Deletion Vectors"></a>写时复制 (CoW) 与读时合并 (MoR) - Deletion Vectors</h5><p>Delta Lake传统上主要采用写时复制（CoW）策略。当数据被更新或删除时，包含受影响记录的Parquet文件会被重写。然而，为了优化更新和删除操作的性能，特别是减少写入放大，Delta Lake引入了“删除向量”（Deletion Vectors）特性 。  </p><ul><li><p><strong>删除向量 (Deletion Vectors):</strong> 当启用删除向量后，<code>DELETE</code>、<code>UPDATE</code> 和 <code>MERGE</code> 操作可以将某些行标记为已删除或已更改，而无需立即重写包含这些行的整个Parquet文件。这些标记信息存储在与数据文件分离的删除向量文件中。读取时，查询引擎会结合数据文件和删除向量来获取最新的数据视图。这种方式类似于读时合并（MoR）的行为，因为它推迟了物理数据的重写 。  </p></li><li><p><strong>物理应用:</strong> 被删除向量标记的更改会在后续的 <code>OPTIMIZE</code> 命令执行、自动压缩（auto-compaction）触发，或 <code>REORG TABLE... APPLY (PURGE)</code> 命令运行时被物理应用到数据文件中，即重写数据文件以移除标记的行或应用更改 。</p></li></ul><h4 id="数据加载流程对比"><a href="#数据加载流程对比" class="headerlink" title="数据加载流程对比"></a>数据加载流程对比</h4><p><strong>Apache Hudi 加载流程:</strong></p><ol><li><strong>记录处理与索引:</strong> 输入记录被处理。如果是upsert操作或带有键的bulk_insert，Hudi会执行索引查找，将每条记录标记为插入或更新，并确定其目标文件组。</li><li><strong>数据分区与文件大小调整:</strong> 根据分区键对数据进行分区，并应用文件大小调整的启发式算法，尝试将数据打包到大小合适的文件中。<code>bulk_insert</code>操作可能还会根据配置的排序模式（如<code>GLOBAL_SORT</code>或<code>PARTITION_SORT</code>）对数据进行排序和重分布。</li><li><strong>数据写入:</strong> 数据被写入到基础文件（Parquet格式）和&#x2F;或日志文件（Avro格式，针对MoR表）。</li><li><strong>索引更新:</strong> 更新索引以反映新数据的位置。</li><li><strong>原子提交:</strong> 将事务元数据（如写入的文件、统计信息等）原子性地提交到Hudi的时间线上。这通常涉及到乐观并发控制（OCC）下的锁机制或MVCC协调。</li></ol><p><strong>Apache Iceberg 加载流程:</strong></p><ol><li><strong>数据文件写入:</strong> 数据首先被写入到数据文件（如Parquet、ORC或Avro格式）。</li><li><strong>清单文件创建:</strong> 创建一个新的或多个新的清单文件（Manifest File），这些文件列出了新写入的数据文件及其统计信息（如列边界值、空值计数等）。</li><li><strong>清单列表创建:</strong> 创建一个新的清单列表文件（Manifest List File），该文件指向相关的清单文件（包括新创建的和可能复用的旧清单文件）。</li><li><strong>表元数据文件创建:</strong> 创建一个新的表元数据文件（JSON格式），该文件指向新的清单列表，并包含表的当前模式、快照ID、分区规范等信息。</li><li><strong>原子提交:</strong> 尝试通过原子操作（通常是CAS）更新外部元数据存储（Catalog）中指向当前表元数据文件的指针。如果发生冲突（即其他写入者已提交），则当前写入操作会进行重试。</li></ol><p><strong>Delta Lake 加载流程:</strong></p><ol><li><strong>数据文件写入:</strong> 数据被写入到Parquet格式的数据文件中，通常分布在相应的分区目录下。</li><li><strong>事务日志条目创建:</strong> 在 <code>_delta_log</code> 目录中创建一个新的事务日志条目（一个JSON文件），该文件记录了此次事务中添加的数据文件列表、删除的文件列表（如有）、事务的元数据（如操作类型、时间戳）以及可能的协议或元数据更新。</li><li><strong>原子提交:</strong> 尝试以原子方式将新创建的JSON日志文件写入 <code>_delta_log</code> 目录。这通常依赖于底层文件系统的“如果不存在则创建”的原子语义。</li></ol><h4 id="对比分析"><a href="#对比分析" class="headerlink" title="对比分析"></a>对比分析</h4><table><thead><tr><th align="center">特性</th><th align="center">Apache Hudi</th><th align="center">Delta Lake</th><th align="center">Apache Iceberg</th></tr></thead><tbody><tr><td align="center"><strong>核心优势</strong></td><td align="center"><strong>高效记录级更新&#x2F;删除 (索引), 近实时流</strong></td><td align="center"><strong>强事务 (日志), Spark 深度集成, 优化管理</strong></td><td align="center"><strong>高性能元数据, 引擎无关, 高级分区&#x2F;模式演化</strong></td></tr><tr><td align="center"><strong>事务实现</strong></td><td align="center">时间线 (Timeline)</td><td align="center">事务日志 (Transaction Log)</td><td align="center">快照隔离 + 原子元数据更新</td></tr><tr><td align="center"><strong>关键机制</strong></td><td align="center"><strong>索引 (Index)</strong></td><td align="center"><strong>事务日志 (JSON&#x2F;Parquet)</strong></td><td align="center"><strong>分层元数据 (Manifest Lists&#x2F;Manifests)</strong></td></tr><tr><td align="center"><strong>增量处理</strong></td><td align="center"><strong>非常强大 (Incremental Query)</strong></td><td align="center">支持 (Change Data Feed)</td><td align="center">支持 (Incremental Scans)</td></tr><tr><td align="center"><strong>表类型&#x2F;视图</strong></td><td align="center"><strong>Copy-on-Write &#x2F; Merge-on-Read</strong></td><td align="center">单一表格式 (类似 CoW，但优化机制丰富)</td><td align="center">单一表格式 (物理布局优化灵活)</td></tr><tr><td align="center"><strong>与引擎集成</strong></td><td align="center">支持 Spark, Flink, Presto, Hive 等</td><td align="center"><strong>深度集成 Spark (原生 API)</strong></td><td align="center"><strong>广泛支持 (Flink, Spark, Trino, Hive 等)</strong></td></tr><tr><td align="center"><strong>分区演进</strong></td><td align="center">有限支持</td><td align="center">有限支持</td><td align="center"><strong>原生支持 (Partition Evolution)</strong></td></tr><tr><td align="center"><strong>隐藏分区</strong></td><td align="center">无</td><td align="center">无</td><td align="center"><strong>支持 (Hidden Partitioning)</strong></td></tr><tr><td align="center"><strong>时间旅行</strong></td><td align="center">支持</td><td align="center">支持</td><td align="center">支持</td></tr><tr><td align="center"><strong>模式演化</strong></td><td align="center">支持</td><td align="center">支持</td><td align="center"><strong>支持更复杂的演化 (Sort Order Evolution)</strong></td></tr><tr><td align="center"><strong>文件跳过优化</strong></td><td align="center">支持</td><td align="center"><strong>支持 (Data Skipping + Z-Ordering)</strong></td><td align="center"><strong>支持 (基于 Manifest 详细统计)</strong></td></tr><tr><td align="center"><strong>典型适用场景</strong></td><td align="center">CDC, 频繁更新, 近实时摄入</td><td align="center">Spark 生态湖仓, 强事务 ETL, Databricks 用户</td><td align="center">超大规模, 多引擎访问, 灵活分区, 高性能元数据需求</td></tr><tr><td align="center"><strong>主要推动者</strong></td><td align="center">Uber</td><td align="center">Databricks</td><td align="center">Netflix, Tabular, Dremio 等</td></tr></tbody></table><h3 id="湖仓系统对-HDFS-中非结构化数据的操作方式"><a href="#湖仓系统对-HDFS-中非结构化数据的操作方式" class="headerlink" title="湖仓系统对 HDFS 中非结构化数据的操作方式"></a>湖仓系统对 HDFS 中非结构化数据的操作方式</h3><p><strong>存储方式:</strong></p><ul><li>对于<strong>半结构化数据</strong>（如JSON、XML、Avro），Lakehouse格式（Hudi, Iceberg, Delta Lake）通常将其存储在支持复杂嵌套结构的列式文件格式中，最常见的是Apache Parquet或ORC。这些格式能够高效地压缩和查询嵌套数据，使其可以直接被SQL引擎或其他分析工具处理。</li><li>对于<strong>非结构化数据</strong>（如图像文件<code>.jpg</code>, 视频文件<code>.mp4</code>, 音频文件<code>.mp3</code>, 纯文本文档<code>.txt</code>, 二进制大对象BLOBs），这些文件本身会以其原始格式直接存储在HDFS中。Lakehouse表此时扮演的角色是管理这些文件的<strong>元数据</strong>。例如，表中的一行可能代表一个图像文件，包含的列有文件路径（指向HDFS中的实际文件）、文件大小、创建日期、相关的标签或描述、甚至是提取出的特征（如图像的宽度、高度、主要颜色等）。</li></ul><p><strong>操作方式:</strong></p><ul><li><strong>元数据管理:</strong> Lakehouse格式的核心功能（ACID事务、版本控制、时间旅行、模式演进）主要应用于它们所管理的结构化或半结构化数据，以及非结构化文件的元数据。例如，当向表中添加一个新的图像文件引用时，这个添加操作是事务性的，并且可以被版本控制。如果更新了某个图像文件的标签，这个元数据的更新也是一个事务。</li><li><strong>内容处理:</strong> 对非结构化数据内容的实际处理（例如，使用计算机视觉模型分析图像、使用自然语言处理技术分析文本、转码视频等）通常由专门的计算引擎完成，如Apache Spark结合其MLlib库、TensorFlow、PyTorch等 。这些引擎可以直接从HDFS读取由Lakehouse表元数据指向的原始文件路径。Lakehouse系统本身不直接提供对这些二进制文件内容的内部操作（例如，事务性地修改视频中的某一帧）。  </li><li><strong>与高级分析集成:</strong> 湖仓架构通过提供统一的数据访问接口，使得机器学习（ML）和数据科学工作负载能够更便捷地利用存储在数据湖中的各类数据。例如，ML模型训练可能需要读取大量图像文件（路径存储在Lakehouse表中）和对应的标注信息（也存储在Lakehouse表中）。最近的一个趋势是将从非结构化数据中提取的<strong>向量嵌入 (vector embeddings)</strong> 存储在Parquet文件中，并由Lakehouse格式管理，这使得非结构化内容可以通过其结构化的向量表示被索引和查询 。</li></ul><h3 id="Hudi加载数据慢的原因"><a href="#Hudi加载数据慢的原因" class="headerlink" title="Hudi加载数据慢的原因"></a>Hudi加载数据慢的原因</h3><p>基准测试研究（如LHBench ）指出，Hudi在批量加载大型数据集（如3TB TPC-DS）时，其加载时间远超Delta Lake和Iceberg。报告将此归因于Hudi为有键更新所做的优化，导致在批量加载时执行了“昂贵的预处理步骤”，包括“键唯一性检查和键重分布” 。这些预处理步骤具体可以分解为：</p><ul><li><p><strong>索引构建与查找 (Index construction and lookup):</strong></p><ul><li>Hudi的核心是记录键（record key）。为了有效地执行upsert操作（即如果记录键已存在则更新，否则插入），Hudi需要一个索引来快速将传入的记录键映射到其在存储中的物理位置（即文件组ID）。  </li><li>在批量加载数据时，即便是使用<code>bulk_insert</code>操作（如果提供了记录键并期望去重或后续能进行键更新），或者默认的<code>upsert</code>操作，Hudi都需要与这个索引机制交互。对于一个全新的表进行首次加载，这可能意味着需要为所有输入数据构建初始索引结构。如果使用<code>upsert</code>，则每条输入记录都需要通过索引查找来判断是插入还是更新。</li><li>虽然Hudi提供了多种索引实现（如Bloom Filter索引、Simple Index、HBase Index等），并且其内置索引（如基于文件范围和Bloom Filter的索引）声称比使用Spark Join进行相同判断快数倍 ，但对于TB级别的海量数据，这个查找和标记过程本身仍然会消耗大量的计算资源和时间。</li></ul></li><li><p><strong>主键唯一性检查与预结合 (Primary key uniqueness checks and Precombine):</strong></p><ul><li>为了保证数据的一致性，Hudi在upsert操作中会确保主键的唯一性。这包括检查输入批次内是否有重复键，以及与表中现有数据比较。</li><li>Hudi还支持“预结合”（Precombine）机制。如果在写入配置中指定了预结合字段（例如，通过<code>hoodie.datasource.write.precombine.field</code>配置一个时间戳或版本号列 ），那么在实际写入之前，Hudi会根据此字段对输入数据中具有相同记录键的多条记录进行合并，只保留“最新”或“最优先”的一条。这个过程虽然能确保数据质量和处理晚到数据，但在批量加载大量原始数据时，无疑增加了额外的计算开销。</li></ul></li><li><p><strong>数据重分布与文件大小调整 (Data redistribution and file sizing heuristics):</strong></p><ul><li><p>Hudi非常注重对底层存储文件大小的管理，以避免产生大量小文件，从而优化后续的查询性能 。在数据加载过程中，Hudi会运行启发式算法，尝试将数据打包到大小适宜（例如，接近配置的目标文件大小）的基础文件中。  </p></li><li><p>对于<code>bulk_insert</code>操作，Hudi提供了多种排序模式（<code>hoodie.bulkinsert.sort.mode</code>），如<code>NONE</code>（不排序，最快但文件大小控制较差）、<code>GLOBAL_SORT</code>（全局排序，文件大小控制最好但成本最高）、<code>PARTITION_SORT</code>（分区内排序，平衡性能和文件大小）等 。选择  </p><p><code>GLOBAL_SORT</code>或<code>PARTITION_SORT</code>模式意味着在写入数据之前需要进行大规模的Shuffle和Sort操作，这在批量加载TB级数据时是非常耗时的。即使是默认的<code>upsert</code>操作，其内部的文件组分配和数据写入逻辑也会考虑文件大小的维护。</p></li><li><p>这种主动的文件组织和大小控制策略，与某些系统可能先快速写入数据（可能产生小文件），然后依赖后续独立的、显式的压缩（compaction）步骤来整理文件的做法不同，Hudi在写入时就承担了部分数据布局的成本。</p></li></ul></li></ul><p>这些预处理步骤，虽然对于Hudi核心的增量更新和CDC场景至关重要，但在纯粹的、一次性的大批量数据追加（append-only）场景下，它们就构成了显著的性能开销。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
      <tag>数据仓库</tag>
      
      <tag>数据湖</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>沐神论文阅读速览</title>
    <link href="/2025/04/15/%E6%B2%90%E7%A5%9E%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%80%9F%E8%A7%88/"/>
    <url>/2025/04/15/%E6%B2%90%E7%A5%9E%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%80%9F%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p>2025年4月15日，星期二，晴天☀️，第58篇博客。</p><p>这篇Blog耗时长，花费了很大精力和心血，但是感觉一切都值得、有价值！</p><h1 id="跟着李沐读经典论文"><a href="#跟着李沐读经典论文" class="headerlink" title="跟着李沐读经典论文"></a>跟着李沐读经典论文</h1><p>今天听了大老师讲文献阅读课后很受启发，主要讲了文献阅读的方法以及如何找idea。</p><p>WWH → IDEA</p><ul><li>why：为什么要做这个研究？</li><li>what：研究发现了什么？</li><li>how：研究时如何实施的，用了什么方法？</li></ul><p>明确科研目的 - 结果导向 以终为始</p><ul><li><p>看文献，找方向</p></li><li><p>定方向，找创新</p></li><li><p>遇难题，找方法</p></li><li><p>始动笔，找支撑</p></li><li><p>跟进展，保状态</p></li></ul><p>对于motivation的描述一定要多下功夫，核心就是要让论文在逻辑上要形成闭环。</p><p>六月中下旬就要离开雁栖湖了，离湖之前，跟着李沐老师把人工智能领域的经典文章再读一遍。</p><h1 id="经典论文阅读"><a href="#经典论文阅读" class="headerlink" title="经典论文阅读"></a>经典论文阅读</h1><div class="note note-success">            <p>用WWH方法去分析每篇文章的核心思想，总结好模型结构图，每篇经典文章使用一句话总结说明</p>          </div><ul><li><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong>(NIPS 2012)；<a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a> ； 作者：{Alex Krizhevsky、 Ilya Sutskever、<strong>Geoffrey E. Hinton</strong>}@University of Toronto；引用次数：142220；</li></ul><p>首次提出使用卷积神经网络处理图像特征，完成图像分类任务，极大提升了图像识别的准确性。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*uxbN4FRXzv53PSlN.jpeg" alt="imgenet"></p><p>卷积操作一般都是使用卷积核在图片上扫描提取特征，使用ReLU函数作为激活函数，引入DropOut技术减轻过拟合，并在GPU上进行并行计算。</p><ul><li><strong>Deep Residual Learning for Image Recognition</strong>(CVPR 2016)； <a href="https://arxiv.org/pdf/1512.03385%EF%BC%9B">https://arxiv.org/pdf/1512.03385；</a> 作者：{<strong>Kaiming He</strong>、 Xiangyu Zhang、Shaoqing Ren、 Jian Sun}@Microsoft Research；引用次数：264384；</li></ul><p>论文首次引入了残差学习块(Residual Block)，拟合目标变为输出y与输入x之间的“差值”：H(x) &#x3D; ReLU(F(x) + x)。设计的初衷是让网络能够轻松的选择<strong>“跳过”</strong>某些层，从而使得训练深层网络成为可能。</p><center><img src="https://www.researchgate.net/publication/329954455/figure/fig1/AS:725290594623488@1549934161033/The-structure-of-ResNet-12.png" alt="Resnet" style="zoom:65%;" />  </center><p>残差块的设计实现是为了避免梯度反向传播的过程中发生消失的致命隐患。<strong>通过让快捷连接成为网络的基本组成部分，我们从结构上保证了梯度始终有一条“高速公路”可以回传到浅层，从而系统性地、主动地避免了梯度消失问题</strong>，使得设计者不必担心网络因深度增加而无法训练。</p><ul><li><strong>Attention Is All You Need</strong>(NIPS 2017)；<a href="https://arxiv.org/pdf/1706.03762%EF%BC%9B">https://arxiv.org/pdf/1706.03762；</a> 作者：{Ashish Vaswanir、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Łukasz Kaiser、Illia Polosukhin}@Google Brain&amp;Google Research&amp;University of Toronto；引用次数：175945；</li></ul><p>文章首次提出了Transformer架构，完全使用注意力机制代替之前基于CNN或RNN的编码解码解码模式，在机器翻译任务上性能得到了提升。</p><center><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" alt="Transformer-model" style="zoom:35%;" />  </center><p>Transformer架构的核心组件：位置编码、前馈神经网络层、LayerNorm归一化层、self-Attn层(MultiHead Attn)、Cross-Attn层(Masked MultiHead Attn)。</p><p>参考博客：Attention Is All You Need</p><ul><li><strong>Generative Adversarial Networks</strong>(NIPS 2014)；<a href="https://arxiv.org/abs/1406.2661%EF%BC%9B">https://arxiv.org/abs/1406.2661；</a> 作者：{Ian J. Goodfellow、Jean Pouget-Abadie、, Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil Ozair、Aaron Courville、<strong>Yoshua Bengio</strong>}@Departement d’informatique et de recherche op ´ erationnelle&amp;Universite de Montr ´eal&amp;Montreal, QC H3C 3J7；引用次数：80508；</li></ul><p>GAN网络设计的核心思想是设计俩个角色进行博弈：生成器 (Generator, G) VS 判别器 (Discriminator, D)。放弃了传统生成模型中复杂的概率密度计算，而是巧妙地将“生成”问题转化为了一个“判别”问题。通过设置一个动态的、不断变强的“对手”（判别器），来引导“自己”（生成器）不断进步。</p><p><img src="https://www.presidio.com/wp-content/uploads/2024/05/1_gPaBNqy4YPpiQx_liRJ9Mw.webp" alt="GAN"></p><ul><li><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>(ACL 2019)；<a href="https://arxiv.org/pdf/1810.04805%EF%BC%9B">https://arxiv.org/pdf/1810.04805；</a> 作者：{jacobdevlin,mingweichang,kentonl,kristout}@google.com；引用次数：128516；</li></ul><p>BERT的核心架构是Transformer的Encoder，使用Masked方式(完形填空)训练模型以具备序列生成的能力。</p><p><img src="https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png" alt="BERT"></p><p>适用的下游任务：文本分类、情感分析、命名实体识别、回答</p><p>参考博客：动手学BERT</p><ul><li><strong>Improving Language Understanding by Generative Pre-Training</strong>(OPENAI GPT)；<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf%EF%BC%9B">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf；</a> 作者：{Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever}@openai.com；引用次数：12611；</li></ul><p>参考博客：动手学GPT</p><ul><li><strong>Language Models are Unsupervised Multitask Learners</strong>(OPENAI GPT-2)；<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf%EF%BC%9B">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf；</a> 作者：{Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever}@openai.com；引用次数：15577；</li></ul><p>参考博客：动手学GPT</p><ul><li><strong>Language Models are Few-Shot Learners</strong>(NIPS 2020&#x2F;OPENAI GPT-3)；<a href="https://arxiv.org/pdf/2005.14165%EF%BC%9B">https://arxiv.org/pdf/2005.14165；</a> 作者：{Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei}@openai.com；引用次数：43857；</li></ul><p>参考博客：动手学GPT</p><ul><li><p><strong>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</strong>(ICLR 2021)；<a href="https://openreview.net/pdf?id=YicbFdNTTy">https://openreview.net/pdf?id=YicbFdNTTy</a> 作者：{Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk Weissenborn、Xiaohua Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain Gelly、Jakob Uszkoreit、Neil Houlsby}@Google Research&amp;equal advising；引用次数：59885；</p></li><li><p><strong>Masked Autoencoders Are Scalable Vision Learners</strong>(CVPR 2022)；<a href="https://arxiv.org/pdf/2111.06377%EF%BC%9B">https://arxiv.org/pdf/2111.06377；</a> 作者：{<strong>Kaiming He</strong>、Xinlei Chen、Saining Xie、Yanghao Li、Piotr Dollar、Ross Girshick}@Facebook AI Research (FAIR)；引用次数：9490；</p></li><li><p><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong>(CVPR 2021)；<a href="https://arxiv.org/pdf/2103.14030%EF%BC%9B">https://arxiv.org/pdf/2103.14030；</a> 作者：{Ze Liu、Yutong Lin、Yue Cao、Han Hu、Yixuan Wei、 Zheng Zhang 、Stephen Lin、Baining Guo}@microsoft.com；引用次数：29432；</p></li></ul><p><img src="https://user-images.githubusercontent.com/26739999/142576715-14668c6b-5cb8-4de8-ac51-419fae773c90.png" alt="swim-transformer"></p><ul><li><strong>Learning Transferable Visual Models From Natural Language Supervision</strong>(PMLR 2021)；<a href="https://arxiv.org/pdf/2103.00020%EF%BC%9B">https://arxiv.org/pdf/2103.00020；</a> 作者：{Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、 Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger、<strong>Ilya Sutskever</strong>}@OpenAI；引用次数：29432；</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ask in Any Modality</title>
    <link href="/2025/04/05/Ask-in-Any-Modality/"/>
    <url>/2025/04/05/Ask-in-Any-Modality/</url>
    
    <content type="html"><![CDATA[<p>2025年4月1日，星期二，晴天☀️，第57篇博客。</p><h1 id="Ask-in-Any-Modality-多模态检索增强生成的研究综述"><a href="#Ask-in-Any-Modality-多模态检索增强生成的研究综述" class="headerlink" title="Ask in Any Modality:多模态检索增强生成的研究综述"></a>Ask in Any Modality:多模态检索增强生成的研究综述</h1><p>本研究综述提供了对Multimodal-RAG系统的结构化和全面分析，涵盖了数据集、度量、基准、评估、方法以及检索、融合、增强和生成方面的创新。</p><p>研究对多模态检索增强生成（多模态 RAG）系统进行了深入分析。这些系统通过整合来自文本、图像、音频和视频等各种来源的外部动态信息来增强大型语言模型 (LLM)，从而解决了幻觉和依赖静态、过时训练数据等问题。</p><h2 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h2><ol><li><p>尽管LLM取得了长足进步，但仍难以生成事实准确且最新的信息。</p></li><li><p>检索增强生成 (RAG) 旨在通过允许 LLM 检索和整合外部知识来缓解这一问题。</p></li><li><p>多模态 RAG 扩展了这一能力，允许使用多种数据类型，从而产生更丰富、更符合语境的输出。然而，有效地跨这些不同模态进行对齐和推理仍面临重大挑战。</p></li></ol><p>典型的多模态 RAG 系统包含一个检索器，它使用嵌入模型从多模态知识库中查找相关信息。检索到的上下文随后被传递给生成器模型（通常是多模态 LLM），以生成最终输出。多模态学习领域的进展，例如对比语言-图像预训练 (CLIP)，对于实现不同数据类型之间的连接至关重要。</p><h2 id="架构方法"><a href="#架构方法" class="headerlink" title="架构方法"></a>架构方法</h2><img src="https://cdn-uploads.huggingface.co/production/uploads/64ba58d377dd483716aba098/N0fZ0I60EfZjITEnf6gPc.png" style="zoom:67%;" /><p><strong>Retrieval Strategies:</strong> This includes efficient search techniques like Maximum Inner Product Search (MIPS) and the use of various multimodal encoders (e.g., CLIP, BLIP). It also covers modality-specific retrieval for text, vision, video, and documents, alongside sophisticated re-ranking and filtering mechanisms to improve the quality of retrieved information.</p><p>包括高效的搜索技术，例如最大内积搜索 (MIPS) 和各种多模态编码器（例如 CLIP、BLIP）的使用。它还涵盖针对文本、视觉、视频和文档的特定模态检索，以及用于提升检索信息质量的复杂重排序和过滤机制。</p><p><strong>Fusion Mechanisms:</strong> Techniques to combine and align information from different modalities, such as score fusion, attention-based methods, and unified frameworks that project data into shared representation spaces.</p><p>用于组合和对齐不同模态信息的技术，例如分数融合、基于注意力机制的方法以及将数据投影到共享表征空间的统一框架。</p><p><strong>Augmentation Techniques:</strong> Methods to refine and enrich the retrieved data before generation, including context enrichment and adaptive or iterative retrieval processes that can adjust based on  query complexity or feedback.</p><p>用于在生成之前细化和丰富检索数据的方法，包括上下文丰富以及可根据查询复杂性或反馈进行调整的自适应或迭代检索过程。</p><p><strong>Generation Techniques:</strong> Focuses on improving the coherence, robustness, and adaptability of the generated output. This involves In-Context Learning (ICL), structured reasoning approaches like Chain-of-Thought (CoT), instruction tuning for specific tasks, and ensuring source attribution for transparency.</p><p>专注于提升生成输出的连贯性、鲁棒性和适应性。这涉及情境学习 (ICL)、结构化推理方法（例如思路链 (CoT)）、针对特定任务的指令调整，以及确保来源归属的透明度。</p><p><strong>Training Strategies:</strong> Encompasses alignment techniques (like contrastive learning using InfoNCE loss), specific loss functions for generation (e.g., Cross-Entropy for text, GAN&#x2F;Diffusion model losses for images), and methods to improve robustness against noise and biases.</p><p>包括对齐技术（如使用 InfoNCE 损失的对比学习）、生成的特定损失函数（例如，文本的交叉熵、图像的 GAN&#x2F;扩散模型损失）以及提高对噪声和偏差的鲁棒性的方法。</p><p>各个阶段技术汇总如下所示：</p><img src="https://cdn-uploads.huggingface.co/production/uploads/64ba58d377dd483716aba098/CtLxMqUEhWr6d9ztU1YZq.jpeg" style="zoom:67%;" /><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><h3 id="泛化性、可解释性和鲁棒性"><a href="#泛化性、可解释性和鲁棒性" class="headerlink" title="泛化性、可解释性和鲁棒性"></a>泛化性、可解释性和鲁棒性</h3><ul><li>领域适应和模态偏见：当前系统在适应不同领域知识以及处理不同模态（如文本、图像）的偏见方面仍有不足，常过度依赖文本信息进行检索和生成。</li><li>可解释性与溯源：提升系统的可解释性，确保能够将答案精确溯源至具体的原始信息片段（例如图像的特定部分或语音的某段），而不仅仅是整个文档或大的视觉区域。</li><li>对抗性鲁棒性：增强系统抵抗对抗性干扰（如误导性图像影响文本输出）的能力，并提高在依赖低质量或过时信息源时的性能。</li></ul><h3 id="推理、对齐和检索增强"><a href="#推理、对齐和检索增强" class="headerlink" title="推理、对齐和检索增强"></a>推理、对齐和检索增强</h3><ul><li>组合推理：提升系统进行复杂组合推理的能力，即需要逻辑整合来自不同模态的信息以生成连贯且上下文丰富的输出。虽然已有多模态思维链（Multimodal-CoT）等技术，但仍需进一步创新。</li><li>模态对齐和实体感知检索：进一步加强不同模态间的对齐，并发展实体感知的检索策略 。</li><li>知识图谱的利用：知识图谱在增强跨模态推理方面具有潜力，但在多模态RAG中的应用仍未得到充分探索 </li><li>克服检索偏见：解决检索过程中可能出现的偏见，如位置敏感性（优先选择文档中特定位置的内容）、冗余检索以及由训练数据或检索内容引入的偏见 。</li><li>统一嵌入空间：开发一个能涵盖所有模态的统一嵌入空间，以实现直接的多模态搜索，无需中间转换模型（如ASR语音识别模型），这是一个重要的开放性挑战。</li></ul><h3 id="基于智能体和自指导的系统"><a href="#基于智能体和自指导的系统" class="headerlink" title="基于智能体和自指导的系统"></a>基于智能体和自指导的系统</h3><ul><li>交互式反馈和自指导决策：未来的多模态RAG系统应能整合交互式反馈，并具备自指导决策能力，从而迭代地优化输出，而不是像静态RAG系统那样单向运行。</li><li>错误归因和反馈机制改进反馈机制，使其能准确判断错误来源于检索、生成还是其他阶段。</li><li>强化学习和人类对齐反馈：探索将强化学习和端到端的人类对齐反馈整合到多模态RAG中，以增强系统判断是否需要检索、评估检索内容相关性以及动态选择最适合生成回应的模态等能力。</li><li>任意到任意的模态支持：实现对任意模态输入到任意模态输出的鲁棒支持，以适应开放式任务的需求。</li><li>融合真实世界数据与具身智能：系统应能整合来自真实世界传感器等多样化来源的数据，并与文本、图像等传统模态结合，以增强情境感知能力。这与具身AI的趋势一致，旨在将知识与物理交互相结合，应用于机器人、导航和物理信息推理等领域。</li></ul><h3 id="长上下文处理、效率、可扩展性和个性化"><a href="#长上下文处理、效率、可扩展性和个性化" class="headerlink" title="长上下文处理、效率、可扩展性和个性化"></a>长上下文处理、效率、可扩展性和个性化</h3><ul><li>长上下文处理的计算成本：解决视频帧采样的高计算成本和处理包含图像的多页文档时的内存瓶颈问题 。需要根据内容复杂性和动态性自适应地选择相关帧。</li><li>效率和可扩展性：优化边缘部署时的检索速度与准确性的权衡，减少跨模态融合层中的冗余计算，需要更高效和可扩展的架构。</li><li>个性化 探索个性化机制，例如根据用户特定上下文（如病史）调整检索策略。</li><li>隐私保护：随着个性化机制的发展，确保隐私和减轻多模态输出中敏感数据泄露的风险至关重要。</li><li><strong>复杂推理和对抗性样本数据集的缺乏：</strong> 目前缺乏用于复杂推理任务和多模态对抗性样本的数据集，这限制了对系统鲁棒性的全面评估。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>多模态</tag>
      
      <tag>MLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王坚院士最新访谈录</title>
    <link href="/2025/04/01/%E7%8E%8B%E5%9D%9A%E9%99%A2%E5%A3%AB%E6%9C%80%E6%96%B0%E8%AE%BF%E8%B0%88%E5%BD%95/"/>
    <url>/2025/04/01/%E7%8E%8B%E5%9D%9A%E9%99%A2%E5%A3%AB%E6%9C%80%E6%96%B0%E8%AE%BF%E8%B0%88%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>2025年3月3日，星期一，阴天，第52篇博客。</p><h2 id="谈谈阿里-我眼中的"><a href="#谈谈阿里-我眼中的" class="headerlink" title="谈谈阿里(我眼中的)"></a>谈谈阿里(我眼中的)</h2><div class="note note-danger">            <p>王坚：中国工程院院士，云计算技术专家，之江实验室主任，阿里巴巴集团技术委员会主席</p>          </div><p>我是一直看好阿里的，从本科接触到大数据技术以来，我就在一直慢慢探索大数据技术领域的领先技术。随着短视频爆火、电商领域重大变革，马老师退居二线“靠边站”，阿里系内部也有人事重大调整，很多人唱衰阿里。</p><p>但是我是一直看好阿里的，因为我知道阿里是懂技术、懂变革的，有真正懂技术、做技术的人，王坚院士便是首当其冲的一位。而不是像某公司，一开始说大模型开源毫无价值，但是当DeepSeek火起来了，它又拥抱开源，早干嘛去了！</p><p>阿里在国内<strong>云计算</strong>、<strong>AI</strong>领域处于绝对领先地位，毋庸置疑、不服不行。特别是阿里云，在阿里转型期起到决定性作用。马老师也因为阿里的转型成功，重新满血而归、出山。</p><p><img src="https://camo.githubusercontent.com/f96f826593b1fd25740304dbad8a27b45c03bb67a9af28b022768657b2460c20/68747470733a2f2f616c6979756e73646b2d70616765732e616c6963646e2e636f6d2f69636f6e732f416c6962616261436c6f75642e737667" alt="aliyun"></p><h2 id="算力革命"><a href="#算力革命" class="headerlink" title="算力革命"></a>算力革命</h2><p>央视面对面栏目最新专访：<a href="https://www.bilibili.com/video/BV1xc98YAEwk/?spm_id_from=333.1007.tianma.1-1-1.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1xc98YAEwk/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><div class="note note-success">            <p>Get新观点</p><ol><li><p>一定条件下，<strong>车慢了</strong>才堵车，并不是<strong>车多了</strong>才堵车。大家都觉得堵车，所以车慢了；事实上是车慢了才堵车的。</p></li><li><p>黑土地这个词，很少人问为什么这样叫，大家只知道黑土地很肥沃。其实大家没搞明白，黑土地就是死下来的东西烂在这里才叫黑土地。创新，其实是是成千上万个公司、团队死在这里，才会涌现出的创新。就是死在这里，不能叫失败；叫失败就有价值判断了，没有失败；变成肥料也叫失败？！</p></li></ol>          </div><p><strong>云计算就是帮助小公司去做大公司做的事情，创新就是一堆看似不起眼的人却做出来了很多变革的东西。</strong></p><p>马力革命时代，人类驯服马匹作为提高生产力工具，伦敦享有“马粪城”的绰号。马力革命留下的基础设施是道路。马力革命时代不管怎么进步，本质上还是供给不足。</p><p>电力革命时代，城市文明就是电力革命，以纽约为代表的现代都市群发展，电力革命留下的基础设施是电网，新能源再怎么改进，电网没有改。电力时代不管怎么讲科技进步，最后落到一个关键点就是：人类消耗自然资源的能力大大增加，从而带来了一系列环境问题。</p><p>下一次：算力革命，互联网作为算力革命时代的基础设施——算力网。算力时代要做到的目标就是达到生活生平的提升而不增加自然资源的消耗。</p><img src="/img/bg/wangjian.jpg"/><h2 id="AI4S"><a href="#AI4S" class="headerlink" title="AI4S"></a>AI4S</h2><p>科学新范式与开放科学 - Computing,Al and The Third Paradigm</p><div class="note note-success">            <p>ChatGPT是革命的工具，而不是工具的革命。</p><p>人工智能不仅是一次工具的革命，还是一次科学革命的工具——它已成为一种可以能够打破学科壁垒的通用语言，而不仅是对科学研究的简单“赋能”。</p>          </div><p>科学研究四范式的提出者：图灵奖得主 Jim Gray</p><p>第一范式：实验（经验）科学，其关键词是<strong>观察</strong>。基于实验或经验的观察来描述自然现象。从最原始的钻木取火到哈维的血液循环学说、伽利略的动力学、达尔文的进化论等，都是实验科学的典范。</p><p>第二范式：理论科学，其关键词是“<strong>归纳</strong>”，第一范式受到实验条件的限制，难以完成对自然现象更精确的理解，科学家在自然现象基础上进行了抽象，尽量简化实验模型，去掉一些复杂的干扰因素，只留下关键因素，然后通过建构数学模型进行归纳总结为科学理论。牛顿三大定律为基础的经典力学体系、麦克斯韦理论成功解释电磁学都是其中的典范。</p><p>第三范式：计算科学，其关键词是<strong>计算</strong>，随着验证理论的难度逐渐增加和经济投入越来越大，科学家靠一个人单打独斗获得巨大科研成就的可能性基本断绝。1946年，现代意义上的电子计算机发明以来，通过计算机对科学实验进行模拟仿真逐渐普及，从而衍生出了“人脑+电脑”的第三范式。如模拟核试验、天气预报、地质演变等都是其中的典范。时至今日，离开计算机程序、数据库，很多学科的科研已是寸步难行。</p><p>第四范式：数据科学，其关键词是<strong>挖掘</strong>。随着科研数据的爆炸式增长以及大数据、人工智能的兴起，数据科学的新范式正在兴起。从某种程度上说，这种范式与计算科学最大的区别是“人脑”和“电脑”谁占主导地位。它将大量的数据进行深度神经网络训练，利用人工智能技术进行分析归纳整理，借助机器对于数据关联的敏感性从而得出更为精确的变量分析，甚至出现了“机器科学家”这种新型科研辅助工具。</p><p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTFJg05B-axONZT06GpTxUlIfDIFhnXJe75qA&s"></p><h3 id="发言稿"><a href="#发言稿" class="headerlink" title="发言稿"></a>发言稿</h3><p>科学范式这一理念已被接受的，但对于经历了哪些范式以及当前所处阶段仍存争议。以 个人见解，在第三范式以后便是仁者见仁智者见智的情况了，简单讲述来看，第三范式引入 计算机模拟，第四范式为数据驱动，而第五范式则与人工智能密切相关。</p><p>然而，真正与第 一、第二范式对比来看，第三范式尚未完全成熟，对学科的影响不够深入，数据驱动范式也 未真正形成。</p><p>人工智能如果最简单地讲，就是数据碰上了模型，模型碰上了计算，也就是说数据+模 型+计算。从这个角度来讲，事实上互联网是所有这些东西的基础，当然现在人工智能包括 数据、模型和计算都跟互联网非常相同一致，甚至说是一模一样的特征，所以当时互联网之 所以不是网络或者是简单的网络，就是因为它的规模。</p><p>在人工智能阶段，知识与财富已不再局限于源代码，因此我提出 “Open Resource Innovation”这一概念，即科技资源的全面开放。这里的资源不仅包括 代码，还涵盖数据、方法论等多方面内容。我非常提倡开源的精神，是一个人、一个机构、 一个国家对社会的贡献，是对全世界的贡献，现在事实上到了人工智能阶段，到了数据时 代，已经不能用简单的Open Source Code来表达，因为你最重要的财富、知识已经不是简 单在Source Code里体显出来。所以我觉得今天创新有一个非常大的特点，把它叫做Open Resource Innovation，也就是说要把科技资源开放出来。</p><p>当年美国对工程教育的担心，使他们提出了一个概念叫STEM（即科学、技术、工程、 数学），当时我们在讨论，到底数学是不是科学？为什么要把数学单独拿出来？后来理解了 一些事情，是因为他们觉得它是Science、Technology、Engineering的共同语言，这不是 并列，是数学把这些东西都连在一起了，所以把它单独拿出来。</p><p>而到了人工智能时代， STEM发生了变化，这个变化是什么？就是数学来统一这些东西就没有那么简单了，变成了 一个东西叫MAP，什么叫MAP？M还是数学，但今天有一个A出来了，就是AI，也就是AI会贯穿到科学、技术、工程。</p><p>但是为什么会有P？P字也很有意思，这也是跟着科技部做“大科学计划”时提出来的， 总书记在很多地方讲过“中国要为世界提供公共产品”。所以大家可以认真想一下，过去的公共产品是什么？教科书是我们的公共产品，但事实上这个世界可能做科学研究也要一个公共产品，所以这个P是中国要有机会为大家提供一个科学研究的公共产品，如果今天真的要把所有的能力都发挥出来，包括AI的能力都发挥出来，那是需要有一个公共产品来支撑的。</p><p>所以我相信如果我们能够把这样一个技术用好，我自己觉得这就是Open Resource Innovation，再也没有人可以把真正的科技创新资源放在一个非常少的圈子里能做到这些事 情，这可能也是这件事情的意义以及它的价值所在，无论是我个人，还是之江实验室去推 动，真的可以在中国乃至世界上做好这件事。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年3月总结与展望</title>
    <link href="/2025/03/31/2025%E5%B9%B43%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/03/31/2025%E5%B9%B43%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>2025年3月31日，星期一，晴天☀️，第56篇博客。</p><h2 id="3月的最后一周，难受"><a href="#3月的最后一周，难受" class="headerlink" title="3月的最后一周，难受"></a>3月的最后一周，难受</h2><p>3月最后一周非常难受，因为这周一整个周都没好天气，天气忽冷和热的给我整的很不适应，一周也没打球训练。。。</p><p>花粉过敏一直流鼻涕、打喷嚏，难受死我了。在北京的第一个春天，没想到会花粉过敏😢</p><p>上周一周为了跑实验、设计实验代码，熬夜&#x2F;不按时吃饭、暴饮暴食又把胃口搞坏了</p><p>总之，3月的最后一周过的很衰。。。</p><p>但是，总体来看3月还是很充实的，这个月小论文在按部就班进行思路描述、代码设计、进行实验，过程很艰辛，bug很多，但是边干边学还是收获了很多很多。争取在清明节前把实验的测试评估结果跑出来，后续就剩下讲故事了，加油搞！</p><div class="note note-success">            <p>先做成一坨屎，然后再优化！</p>          </div><h2 id="这个月干了啥"><a href="#这个月干了啥" class="headerlink" title="这个月干了啥"></a>这个月干了啥</h2><h3 id="🏀干球干球！"><a href="#🏀干球干球！" class="headerlink" title="🏀干球干球！"></a>🏀干球干球！</h3><p>这个月篮球技术有长进，每天打球的时间比跑步还要多，感觉把大学四年落下的比赛都补回来了。科苑杯要开始了，顺利入选学院大名单，好好训练，随时准备上场比赛。</p><p>这个月增重了，主要还是最后一周难受没怎么活动；不过确实上肢比以前结实了。4月我要减脂！！</p><h3 id="整理了Lisbon-Copenhagen团队系列文章"><a href="#整理了Lisbon-Copenhagen团队系列文章" class="headerlink" title="整理了Lisbon&amp;Copenhagen团队系列文章"></a>整理了Lisbon&amp;Copenhagen团队系列文章</h3><div class="note note-success">            <p>Image Caption Generation base on Retrieval-Agumented</p>          </div><ul><li><a href="https://arxiv.org/pdf/2302.08268">Retrieval-augmented Image Captioning</a>  ACL 2023</li></ul><p>这篇文章是该团队首次提出使用给定input image和从<strong>数据库检索</strong>得到的captions融合去生成input image的caption，而不是仅仅单独依靠图像本身去生成其caption。</p><p>使用预训练的Vision&amp;Language编码器联合编码输入图像和检索得到的相似图片的captions，之后再通过交叉注意力将编码器输出作为输入进而输入到解码器中，实现image caption的生成。</p><ul><li>SmallCap-<a href="https://arxiv.org/pdf/2209.15323"> Lightweight Image Captioning Prompted with Retrieval Augmentation</a>  CVPR 2023</li></ul><p>SMALLCAP通过在训练和推理过程中使用检索到的相关描述来增强生成能力。具体来说，对于输入图像，模型会从数据存储库中检索k个相关的描述，并将这些描述作为提示输入到解码器中。解码器根据图像特征和检索到的描述生成最终的图像描述。</p><ul><li><a href="https://aclanthology.org/2023.findings-acl.104.pdf">LMCAP: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting</a>    ACL 2023</li></ul><p>LMCAP提出了一种image-blind few-shot多语言图像描述生成模型，通过结合多语言CLIP检索的相似图像描述与XGLM语言模型的提示生成。</p><h3 id="跨模态检索实践"><a href="#跨模态检索实践" class="headerlink" title="跨模态检索实践"></a>跨模态检索实践</h3><ol><li><p>首先将COCO2017(30G)数据集中的113287张图像与Caption作为训练集，使用CLIP-ResNet50×4模型进行对比学习训练和体征提取。CLIP vision对图像进行嵌入编码，CLIP text对图片对应的caption进行嵌入编码。</p></li><li><p>之后将训练完成后的Caption向量通过FAISS构建合适的索引存储在向量数据库中(qdrant&#x2F;chromedb你自己选择，方便即可)或者以hdf5文件保存。</p></li><li><p>接着用户会输入一张新的图片(query，不在COCO训练集中)，将图片使用CLIP进行编码embedding后，与向量数据库中的caption嵌入进行相似度比较，得到与input image embedding相似度最高的前五个caption。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1.处理元数据</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> pycocotools.coco <span class="hljs-keyword">import</span> COCO<br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_metadata</span>():<br>    <span class="hljs-comment"># 路径配置</span><br>    data_root = Path(<span class="hljs-string">&quot;/auto-tmp/coco2017&quot;</span>)<br>    ann_path = data_root / <span class="hljs-string">&quot;annotations/captions_train2017.json&quot;</span><br>    output_path = data_root / <span class="hljs-string">&quot;processed/metadata.json&quot;</span><br>    <br>    <span class="hljs-comment"># 创建输出目录</span><br>    output_path.parent.mkdir(parents=<span class="hljs-literal">True</span>, exist_ok=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-comment"># 加载COCO标注</span><br>    coco = COCO(ann_path)<br>    <br>    <span class="hljs-comment"># 构建元数据</span><br>    metadata = []<br>    <span class="hljs-keyword">for</span> img_id <span class="hljs-keyword">in</span> coco.getImgIds():<br>        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))<br>        img_path = data_root / <span class="hljs-string">f&quot;train2017/<span class="hljs-subst">&#123;img_id:012d&#125;</span>.jpg&quot;</span><br>        <br>        <span class="hljs-keyword">for</span> ann <span class="hljs-keyword">in</span> anns:<br>            metadata.append(&#123;<br>                <span class="hljs-string">&quot;id&quot;</span>: ann[<span class="hljs-string">&quot;id&quot;</span>],<br>                <span class="hljs-string">&quot;image_path&quot;</span>: <span class="hljs-built_in">str</span>(img_path),<br>                <span class="hljs-string">&quot;caption&quot;</span>: ann[<span class="hljs-string">&quot;caption&quot;</span>]<br>            &#125;)<br>    <br>    <span class="hljs-comment"># 保存元数据</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(output_path, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        json.dump(metadata, f, indent=<span class="hljs-number">2</span>)<br><br>process_metadata()<br><br><span class="hljs-comment"># 2.特征提取</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> clip<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CLIPFeatureExtractor</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br>        <span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.preprocess = clip.load(<span class="hljs-string">&quot;RN50x4&quot;</span>, device=<span class="hljs-variable language_">self</span>.device)<br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_batch_processor</span>(<span class="hljs-params">self, data, batch_size, process_fn</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;通用批处理工具&quot;&quot;&quot;</span><br>        results = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(data), batch_size)):<br>            batch = data[i:i+batch_size]<br>            results.append(process_fn(batch))<br>        <span class="hljs-keyword">return</span> np.concatenate(results)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_image_features</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;提取图像特征&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 加载元数据</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/metadata.json&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            metadata = json.load(f)<br>        <br>        <span class="hljs-comment"># 获取唯一图像路径</span><br>        unique_images = <span class="hljs-built_in">list</span>(&#123;item[<span class="hljs-string">&quot;image_path&quot;</span>]: item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> metadata&#125;.values())<br>        <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_batch</span>(<span class="hljs-params">batch</span>):<br>            images = []<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>                img = Image.<span class="hljs-built_in">open</span>(item[<span class="hljs-string">&quot;image_path&quot;</span>]).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>                images.append(<span class="hljs-variable language_">self</span>.preprocess(img))<br>            <br>            <span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>                tensor = torch.stack(images).to(<span class="hljs-variable language_">self</span>.device)<br>                features = <span class="hljs-variable language_">self</span>.model.encode_image(tensor)<br>                features /= features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">return</span> features.cpu().numpy().astype(<span class="hljs-string">&quot;float32&quot;</span>)<br>        <br>        <span class="hljs-comment"># 批处理参数（根据4090显存优化）</span><br>        batch_size = <span class="hljs-number">512</span><br>        features = <span class="hljs-variable language_">self</span>._batch_processor(unique_images, batch_size, process_batch)<br>        np.save(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/image_features.npy&quot;</span>, features)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_text_features</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;提取文本特征&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/metadata.json&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            metadata = json.load(f)<br>        <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_batch</span>(<span class="hljs-params">batch</span>):<br>            texts = [item[<span class="hljs-string">&quot;caption&quot;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch]<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                inputs = clip.tokenize(texts, truncate=<span class="hljs-literal">True</span>).to(<span class="hljs-variable language_">self</span>.device)<br>                features = <span class="hljs-variable language_">self</span>.model.encode_text(inputs)<br>                features /= features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">return</span> features.cpu().numpy().astype(<span class="hljs-string">&quot;float32&quot;</span>)<br>        <br>        <span class="hljs-comment"># 文本处理批次可以更大</span><br>        batch_size = <span class="hljs-number">2048</span><br>        features = <span class="hljs-variable language_">self</span>._batch_processor(metadata, batch_size, process_batch)<br>        np.save(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/text_features.npy&quot;</span>, features)<br><br><span class="hljs-comment"># 执行特征提取</span><br>extractor = CLIPFeatureExtractor()<br>extractor.extract_image_features()<br>extractor.extract_text_features()<br><br><span class="hljs-comment"># 3.索引构建</span><br><span class="hljs-keyword">import</span> faiss<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorIndexer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.res = faiss.StandardGpuResources()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_index</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 加载特征数据</span><br>        text_features = np.load(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/text_features.npy&quot;</span>)<br>        <br>        <span class="hljs-comment"># 归一化处理</span><br>        faiss.normalize_L2(text_features)<br>        <br>        <span class="hljs-comment"># 创建GPU索引</span><br>        dim = text_features.shape[<span class="hljs-number">1</span>]<br>        index = faiss.IndexFlatIP(dim)<br>        gpu_index = faiss.index_cpu_to_gpu(<span class="hljs-variable language_">self</span>.res, <span class="hljs-number">0</span>, index)<br>        <br>        <span class="hljs-comment"># 分块添加数据（优化显存使用）</span><br>        chunk_size = <span class="hljs-number">50000</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_features), chunk_size):<br>            gpu_index.add(text_features[i:i+chunk_size])<br>        <br>        <span class="hljs-comment"># 保存索引</span><br>        cpu_index = faiss.index_gpu_to_cpu(gpu_index)<br>        faiss.write_index(cpu_index, <span class="hljs-string">&quot;/auto-tmp/coco2017/processed/faiss_index.index&quot;</span>)<br><br><span class="hljs-comment"># 构建索引</span><br>indexer = VectorIndexer()<br>indexer.build_index()<br><br><span class="hljs-comment"># 4.检索系统实现</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ImageCaptionRetriever</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 硬件配置</span><br>        <span class="hljs-variable language_">self</span>.device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br>        <br>        <span class="hljs-comment"># 加载模型</span><br>        <span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.preprocess = clip.load(<span class="hljs-string">&quot;RN50x4&quot;</span>, device=<span class="hljs-variable language_">self</span>.device)<br>        <br>        <span class="hljs-comment"># 加载索引</span><br>        <span class="hljs-variable language_">self</span>.index = faiss.read_index(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/faiss_index.index&quot;</span>)<br>        <br>        <span class="hljs-comment"># 加载元数据</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;/auto-tmp/coco2017/processed/metadata.json&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-variable language_">self</span>.metadata = json.load(f)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_format_output</span>(<span class="hljs-params">self, captions</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;按照模板格式化输出&quot;&quot;&quot;</span><br>        base_template = <span class="hljs-string">&quot;&quot;&quot;I am an intelligent image captioning bot. </span><br><span class="hljs-string">Similar images have the following captions: </span><br><span class="hljs-string">&#123;captions&#125;&quot;&quot;&quot;</span><br>        <br>        caption_list = <span class="hljs-string">&quot;\n&quot;</span>.join([<span class="hljs-string">f&quot;&lt;<span class="hljs-subst">&#123;caption&#125;</span>&gt;&quot;</span> <span class="hljs-keyword">for</span> caption <span class="hljs-keyword">in</span> captions])<br>        <span class="hljs-keyword">return</span> base_template.<span class="hljs-built_in">format</span>(captions=caption_list)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">retrieve</span>(<span class="hljs-params">self, image_path, top_k=<span class="hljs-number">5</span></span>):<br>        <span class="hljs-comment"># 图像预处理</span><br>        image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>        tensor = <span class="hljs-variable language_">self</span>.preprocess(image).unsqueeze(<span class="hljs-number">0</span>).to(<span class="hljs-variable language_">self</span>.device)<br>        <br>        <span class="hljs-comment"># 特征提取</span><br>        <span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>            features = <span class="hljs-variable language_">self</span>.model.encode_image(tensor)<br>            features /= features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-comment"># GPU加速检索</span><br>        gpu_index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), <span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.index)<br>        <br>        <span class="hljs-comment"># 执行搜索</span><br>        _, indices = gpu_index.search(features.cpu().numpy().astype(<span class="hljs-string">&quot;float32&quot;</span>), top_k)<br>        <br>        <span class="hljs-comment"># 获取原始caption</span><br>        results = [<span class="hljs-variable language_">self</span>.metadata[idx][<span class="hljs-string">&quot;caption&quot;</span>] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices[<span class="hljs-number">0</span>]]<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._format_output(results)<br><br><span class="hljs-comment"># 使用示例</span><br>retriever = ImageCaptionRetriever()<br>output = retriever.retrieve(<span class="hljs-string">&quot;/path/to/query_image.jpg&quot;</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure><h3 id="认知觉醒与认知驱动"><a href="#认知觉醒与认知驱动" class="headerlink" title="认知觉醒与认知驱动"></a>认知觉醒与认知驱动</h3><p>周岭老师的两本书：认知觉醒、认知驱动，常读常新～</p><p>读的越多，发现才能越来越把多本书的精华关联到一起。直到现在，才后知后觉地能把《穷查理宝典》、《纳瓦尔宝典》、《干法》通过《认知觉醒》关联起来。</p><p>想要得到一样东西，最好的方法就是让自己配得上它。财富、爱情、事业亦如此。</p><h2 id="人间4月天，我要🌸"><a href="#人间4月天，我要🌸" class="headerlink" title="人间4月天，我要🌸"></a>人间4月天，我要🌸</h2><p>4月要TMD早点睡了，虽然现在越来越能熬，但是熬夜是真不如早睡早起(早会早起更适合我)，争取利用清明节调整作息规律。</p><p>4月要好好养胃，绝对不能饮食不规律了，按时吃饭。</p><p>4月天气好了，比赛也会变多，注意别冒傻气，比赛千万<strong>注意别受伤</strong>！</p><p>4月继续拼，好好工作，继续读书，读论文、调整实验代码，着手小论文的工作。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年3月阅读书目与摘录反思</title>
    <link href="/2025/03/24/2025%E5%B9%B43%E6%9C%88%E9%98%85%E8%AF%BB%E4%B9%A6%E7%9B%AE%E4%B8%8E%E6%91%98%E5%BD%95%E5%8F%8D%E6%80%9D/"/>
    <url>/2025/03/24/2025%E5%B9%B43%E6%9C%88%E9%98%85%E8%AF%BB%E4%B9%A6%E7%9B%AE%E4%B8%8E%E6%91%98%E5%BD%95%E5%8F%8D%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<p>2025年3月24日，星期一，晴天☀️，第55篇博客。</p><p>自从买了汉王的Clear6便对墨水屏爱不释手了，开通了微信读书会员，感觉以后会在买纸质书上省下一大笔开销。2025年争取把买汉王阅读器的钱读回来，先读699块钱的书再说，加油，坚持！✊</p><h2 id="3月读完"><a href="#3月读完" class="headerlink" title="3月读完"></a>3月读完</h2><ul><li>《沧浪之水》</li><li>《深度学习入门：基于Python的理论与实践》</li><li>《征服市场的人》</li><li>《高质量读研：教你如何写论文、做科研》</li></ul><h2 id="3月在读"><a href="#3月在读" class="headerlink" title="3月在读"></a>3月在读</h2><ul><li><p>《大明王朝1566》</p></li><li><p>《富兰克林自传》</p></li><li><p>《云计算(典藏版)》</p></li><li><p>《在线》(重读二刷)</p></li><li><p>《深度学习进阶：自然语言处理》</p></li></ul><h2 id="书摘总结"><a href="#书摘总结" class="headerlink" title="书摘总结"></a>书摘总结</h2><h3 id="《大明王朝1566》"><a href="#《大明王朝1566》" class="headerlink" title="《大明王朝1566》"></a>《大明王朝1566》</h3><p>寒假在家里先把电视剧版《大明王朝1566》刷了一遍，回学校后又在墨水屏上读了刘和平老师《大明王朝1566》的电纸书。</p><p>[吕芳]  什么叫‘三思’?‘三思’就是‘思危、思退、思变’！知道了危险就能躲开危险，这就叫‘思危’；躲到人家都不再注意你的地方这就叫‘思退’；退了下来就有机会，再慢慢看，慢慢想，自己以前哪儿错了，往后该怎么做，这就叫‘思变’！”</p><p>[胡宗宪] 平时叫你读《左传》《通鉴》，你不以为然，叫你读一读王阳明的书，你更不以为然。还说什么‘半部《论语》可治天下！’现在我问你，孔子说的‘知不可为而为之’是什么本意！孔子是告诉世人，<strong>做事时不问可不可能，但问应不应该！</strong>毁堤淹田，伤天害理，上误国家，下害百姓，也叫‘知不可为而为之’吗！</p><p>[杨金水] 有些事不上秤没有四两，上了秤一千斤也打不住。</p><h3 id="沧浪之水——鄙视迟大为、理解池大为、追随池大为"><a href="#沧浪之水——鄙视迟大为、理解池大为、追随池大为" class="headerlink" title="沧浪之水——鄙视迟大为、理解池大为、追随池大为"></a>沧浪之水——鄙视迟大为、理解池大为、追随池大为</h3><ul><li>沧浪之水清兮，可以濯吾缨；沧浪之水浊兮，可以濯吾足。</li><li>人生在世，就是跟世界打交道，口说无凭，都是泡沫，有东西才是真的。</li></ul><p>[原文] 在极度的痛苦中，经过了许多辗转反侧之夜，我意识到许小曼并不是属于我的，也许她现在也从浪漫而伟大的牺牲激情中省悟过来。毕竟，我们的血管里流着的是不同的血。事情就这么过去了，汪贵发等人模糊而明确地说着刻毒的话，我都装着听不懂，忍了，忍了。父亲当年不也是这么忍过来的？我还是感到了一点轻松，一点安慰，平民也可以坚守自己心灵的高贵。</p><p>[自评] 穷人最后的一丝精神上的高傲。作为读者我一直在反思：贫富差距所带来的认知上的差别，真的不能让人进行正常的沟通交流吗?固执的置气挽回的一丝尊严所带来的轻松、快感与抛开身份彻底的沟通到底哪个方式更划算。或许真当自己设身处地，那一丝面子和尊严也能就已经把你压死，你说不出口，她也听不进去。</p><p>[原文] 一个人是怎样的人，别人的眼都是雪亮的。有这点雪亮，这点理解，做个好人就并不吃亏，人间自有公道。</p><p>[自评] 立场和利益很多时候要大于为人和感情。徐在大为面前没有利益相关性，而且现阶段的立场和价值观相近，池本身也是善良实在的人。反观丁，只会紧跟能帮助自己的人，这么做没有对错之分，对于丁也要学着去理解，因为丁代表生活中的大多数，没准就是未来的你和我，谁也不用装孙子。</p><p>[原文]  庄子曾说到过两只龟，一只钻在污泥里，一身腥臭，可它是活的，一只死了被供在庙堂上，供帝王占卜之用，你说你愿钻在污泥里还是供在庙堂上？污泥里就不要说脸不脸了，一身腥臭还谈脸。</p><p>[锐评] 乌龟何来腥臭？腥臭只是人的感觉，管乌龟什么事？！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SmallCap Source Code</title>
    <link href="/2025/03/15/SmallCap-Source-Code/"/>
    <url>/2025/03/15/SmallCap-Source-Code/</url>
    
    <content type="html"><![CDATA[<p>2025年3月15日，星期六，雨夹雪，第54篇博客。三一五打假日～</p><h1 id="SmallCap复现与代码设计细节分析——基于Cursor-Claude3-7-sonnet"><a href="#SmallCap复现与代码设计细节分析——基于Cursor-Claude3-7-sonnet" class="headerlink" title="SmallCap复现与代码设计细节分析——基于Cursor Claude3.7-sonnet"></a>SmallCap复现与代码设计细节分析——基于Cursor Claude3.7-sonnet</h1><p>项目源代码：<a href="https://github.com/RitaRamo/smallcap">https://github.com/RitaRamo/smallcap</a></p><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs markdown">smallcap/<br>├── .DS<span class="hljs-emphasis">_Store</span><br><span class="hljs-emphasis">├── .gitignore</span><br><span class="hljs-emphasis">├── README.md</span><br><span class="hljs-emphasis">├── SmallCap_</span>demo.ipynb        # 模型演示笔记本<br>├── coco-caption/              # 模型评估工具目录<br>├── data/                      # 数据目录<br>├── infer.py                   # 模型推理脚本<br>├── requirements.txt           # 项目依赖包<br>├── src/                       # 核心源代码目录<br>└── train.py                   # 模型训练脚本<br><br>核心代码：<br>src/<br>├── <span class="hljs-strong">__init__</span>.py<br>├── extract<span class="hljs-emphasis">_features.py        # 特征提取工具</span><br><span class="hljs-emphasis">├── get_</span>indexed<span class="hljs-emphasis">_caps.py        # 描述索引处理工具</span><br><span class="hljs-emphasis">├── gpt2.py                    # GPT-2模型实现</span><br><span class="hljs-emphasis">├── opt.py                     # OPT模型实现</span><br><span class="hljs-emphasis">├── retrieve_</span>caps.py           # 检索增强功能实现<br>├── template.txt               # 检索提示模板<br>├── utils.py                   # 通用工具函数<br>├── vision<span class="hljs-emphasis">_encoder_</span>decoder.py  # 视觉-文本模型架构<br>└── xglm.py                    # XGLM模型实现<br><br>模型评估工具目录：<br>coco-caption/<br>├── README.md<br>├── annotations/               # 评估标准描述<br>├── cocoEvalCapDemo.ipynb      # 评估演示笔记本<br>├── get<span class="hljs-emphasis">_stanford_</span>models.sh     # Stanford模型下载脚本<br>├── license.txt<br>├── pycocoevalcap/             # 评估指标实现<br>├── pycocotools/               # COCO工具集<br>└── run<span class="hljs-emphasis">_eval.py                # 评估主脚本</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">数据目录：</span><br><span class="hljs-emphasis">data/</span><br><span class="hljs-emphasis">└── image_</span>truck.jpg            # 示例图像<br></code></pre></td></tr></table></figure><h2 id="依赖流"><a href="#依赖流" class="headerlink" title="依赖流"></a>依赖流</h2><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>train.py → src&#x2F;utils.py → src&#x2F;vision_encoder_decoder.py → 语言模型(gpt2.py&#x2F;opt.py&#x2F;xglm.py)</p><p><img src="/img/fig/train2.png"></p><h3 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h3><p>infer.py → src&#x2F;utils.py → src&#x2F;vision_encoder_decoder.py → 语言模型</p><p><img src="/img/fig/train3.png"></p><h3 id="检索增强流"><a href="#检索增强流" class="headerlink" title="检索增强流"></a>检索增强流</h3><p>src&#x2F;retrieve_caps.py → src&#x2F;get_indexed_caps.py → 训练&#x2F;推理流程</p><p><img src="/img/fig/train1.png"></p><h3 id="评估流"><a href="#评估流" class="headerlink" title="评估流"></a>评估流</h3><p>infer.py(生成描述) → coco-caption&#x2F;run_eval.py(计算指标)</p><h2 id="关键功能"><a href="#关键功能" class="headerlink" title="关键功能"></a>关键功能</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| 文件 | 功能 |<br><br>|------|------|<br><br>| train.py | 训练主脚本，包含参数配置和训练逻辑 |<br><br>| infer.py | 推理主脚本，载入模型并生成描述 |<br><br>| src/vision<span class="hljs-emphasis">_encoder_</span>decoder.py | 核心模型架构，结合视觉编码器和文本解码器 |<br><br>| src/retrieve<span class="hljs-emphasis">_caps.py | 实现检索相似图像描述功能 |</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">| src/extract_</span>features.py | 提取和缓存图像特征 |<br><br>| src/utils.py | 数据处理和辅助函数 |<br><br>| SmallCap<span class="hljs-emphasis">_demo.ipynb | 演示模型使用方法和效果 |</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">| coco-caption/run_</span>eval.py | 评估生成描述质量 |<br></code></pre></td></tr></table></figure><h2 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h2><ol><li><p>numpy版本与pandas版本不匹配导致错误，适配与4090的Pytorch版本： </p><ul><li>numpy              1.21.6</li><li>pandas             1.3.5</li></ul></li><li><p>执行<code>python src/retrieve_caps.py</code>时要现在<code>src/</code>下面创建好<code>datastore/</code>子文件夹，以方便faiss即使向其中写入索引文件。否则执行到最后会报错<strong>can’t write</strong>的BUG！</p></li><li><p>&#x2F;hy-tmp&#x2F;smallcap&#x2F;datastore&#x2F;coco_index：coco_index折了两次，索引会自动生成coco_index不需要自己创建。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从SmallCap出发切入Image Caption</title>
    <link href="/2025/03/08/%E4%BB%8ESmallCap%E5%87%BA%E5%8F%91%E5%88%87%E5%85%A5Image-Caption/"/>
    <url>/2025/03/08/%E4%BB%8ESmallCap%E5%87%BA%E5%8F%91%E5%88%87%E5%85%A5Image-Caption/</url>
    
    <content type="html"><![CDATA[<p>2025年3月8日，星期六，晴天☀️，第53篇博客。今天是3·8妇女节，恭祝所有女神节日快乐！！</p><h1>SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation</h1><center>CVPR 2023</center><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ol><li><strong>研究问题:</strong> 这篇文章要解决的问题是<strong>如何在减少计算资源消耗的情况下</strong>，<strong>生成高质量的Image Caption</strong>。当前的图像描述模型通常需要大规模的数据和模型，导致预训练和微调的成本很高。</li><li><strong>研究难点:</strong> 该问题的研究难点包括：1)如何在减少模型参数的同时保持模型的性能；2)如何在不进行微调的情况下，使模型能够适应新的领域；3)如何有效地利用大规模的外部数据进行训练。</li><li><strong>相关工作:</strong> 相关工作包括基于大规模数据训练的图像描述模型，如LEMONHuge、SimVLMHuge和BLIPCapFilt-L等。此外，还有一些工作尝试通过冻结部分模型参数来减少训练成本，如ClipCap和I-Tuning。检索增强生成（Retrieval-Augmented Generation）是另一个相关的研究方向，但在图像描述任务中尚未得到充分探索。</li></ol><h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><p>这篇论文提出了<code>SMALLCAP</code>模型，用于解决图像描述任务中的<em>计算资源消耗</em>问题。具体来说，SMALLCAP通过<strong>检索增强</strong>和<strong>轻量化设计</strong>来实现高效的图像描述生成。</p><ul><li><p><strong>模型架构:</strong> SMALLCAP结合了预训练的<strong>CLIP视觉编码器和GPT-2语言解码器</strong>。CLIP编码器用于<strong>处理输入图像</strong>，生成一系列图像块嵌入。GPT-2解码器<strong>用于生成描述文本</strong>。为了减少训练参数，SMALLCAP仅在CLIP编码器和GPT-2解码器之间引入了新的跨注意力层，这些层的参数是可以训练的。</p></li><li><p><strong>检索增强:</strong> SMALLCAP通过在训练和推理过程中<strong>使用检索到的相关描述</strong>来<strong>增强生成能力</strong>。具体来说，对于输入图像，模型会从数据存储库中检索k个相关的描述，并将这些描述作为提示输入到解码器中。解码器根据图像特征和检索到的描述生成最终的图像描述。</p></li><li><p><strong>轻量化设计:</strong> 为了进一步减少训练参数，SMALLCAP通过调整跨注意力层的投影矩阵维度来控制可训练参数的数量。默认情况下，投影矩阵的维度设置为64，但可以根据需要进行调整。</p></li></ul><p><img src="/./img/fig/smallcap-model.png" alt="smallcap-model"></p><h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><ol><li><strong>数据收集:</strong> 实验使用了COCO数据集进行训练和评估。此外，还在nocaps数据集上进行了跨领域评估。</li><li><strong>实验设置:</strong> SMALLCAP的编码器和解码器分别初始化自CLIP-ViT-B&#x2F;32和GPT-2Base。模型仅训练跨注意力层的参数，其他参数保持冻结。训练使用AdamW优化器，初始学习率为1e-4，批量大小为64，训练10个epoch。</li><li><strong>检索机制:</strong> 在训练过程中，模型使用CLIP-ResNet-50x64编码器对输入图像和数据存储库中的描述进行编码，并通过余弦相似度进行最近邻搜索，检索k个相关描述。推理过程中，模型使用beam search解码生成描述。</li></ol><h2 id="结果与分析"><a href="#结果与分析" class="headerlink" title="结果与分析"></a>结果与分析</h2><ol><li><strong>COCO数据集结果:</strong> 在COCO数据集上的实验结果表明，SMALLCAP在仅有7M可训练参数的情况下，性能与其他轻量级模型相当，甚至在某些指标上超过了更大的模型。</li><li><strong>nocaps数据集结果:</strong> 在nocaps数据集上的跨领域评估中，SMALLCAP表现出色，显著优于其他轻量级模型，并且在某些情况下接近甚至超过了经过微调的大规模模型。</li><li><strong>检索增强的影响:</strong> 实验结果表明，检索增强对SMALLCAP的性能有显著提升。通过检索相关描述，模型能够更好地适应不同的领域和任务。</li></ol><h2 id="总体结论"><a href="#总体结论" class="headerlink" title="总体结论"></a>总体结论</h2><p>这篇论文提出了SMALLCAP模型，通过检索增强和轻量化设计，实现了高效的图像描述生成。实验结果表明，SMALLCAP在减少计算资源消耗的同时，保持了较高的性能，并且能够在不同领域之间进行有效的迁移。未来的研究可以进一步探索检索增强在其他多模态任务中的应用，并扩展检索数据的使用规模。</p><h2 id="论纲梳理"><a href="#论纲梳理" class="headerlink" title="论纲梳理"></a>论纲梳理</h2><p>五步法：</p><ol><li>分析标题</li><li>目录概览</li><li>先读结论</li><li>再读导论</li><li>重点章节阅读</li></ol><p>实用技巧：将所有读过的内容用自己的话复述出来，最后是写下来！</p><h3 id="标题分析"><a href="#标题分析" class="headerlink" title="标题分析"></a>标题分析</h3><blockquote><p>SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation</p></blockquote><p>主标题：SmallCap</p><p>副标题：Lightweight Image Captioning Prompted with Retrieval Augmentation</p><p>作者起这样一个标题意味着什么呢？Cap大概率要和Image Caption有关，加上Small形容词是想说明更小巧的模型吗？再看副标题，Lightweight Image Captioning意味着要进行<strong>轻量化</strong>；Lightweight Image Captioning Prompted with Retrieval Augmentation-利用<strong>检索增强</strong>功能提示轻量级图像标题。也就是说作者想通过使用<strong>检索增强</strong>技术来轻量化Image Caption的过程。</p><div class="note note-success">            <p>根据题目展开的可能联想：</p><p>思考1：为什么要轻量化？轻量化是不是由于之前的研究造成了计算资源消耗大、计算成本大、数据规模大造成的？(当然这是通过阅读Abstract了解到的)</p><p>思考2：计算资源消耗在哪里了？检索增强技术是如何减少计算资源消耗的？</p><p>思考2：探究如何在减少计算资源消耗的情况下，生成高质量的Image Caption。</p>          </div><h3 id="先读Conclusion，自己复述"><a href="#先读Conclusion，自己复述" class="headerlink" title="先读Conclusion，自己复述"></a>先读Conclusion，自己复述</h3><p>In this paper, we propose SMALLCAP, an image captioning model augmented with retrieval, which is light to train and can be transferred across domains without retraining. Results on the COCO dataset show that SMALLCAP is competitive to other lightweight-training models despite having substantially less trainable parameters, instead leveraging non-parametric information from a datastore of text. Out-of-domain evaluations show that SMALLCAP can also perform training-free domain transfer when given access to a datastore with target-domain data. Our model further benefits from diverse web and human-labeled data in addition to or in place of target-domain data. We find that SMALLCAP benefits not just from access to image captions, but also to video and audio captions (resources neglected in image captioning work in the past).</p><p>本文提出了一种新型轻量级Image Caption模型-<code>SmallCap</code>，训练方式简单且方便跨领域使用。由于利用了<strong>文本数据存储的非参数信息</strong>，使得可训练参数大大减少，但是并没有影响SmallCap在COCO数据集上的良好表现。如果可以接入包含目标域的数据库，SmallCap可以实现免训练跨领域转换。Web数据和人类标签数据会使SmallCap更加多元化。SmallCap模型还可以从video and audio captions中获益。</p><p>SMALLCAP’s small size and impressive performance in out-of-domain settings attest to the potential of retrieval augmentation as an alternative to the expensive training found in large pre-trained vision-and-language models and the costly finetuning that even previous lightweight-training models require in order to adapt to different image captioning datasets. Future work can apply our retrieval augmentation approach to a wider range of multimodal tasks, and further explore the scalability of the data used for retrieval.</p><p>SmallCap<strong>小巧的体积</strong>和<strong>良好的表现</strong>证明了检索增强技术可以代替昂贵的VLM预训练和先前轻量级训练模型的高开销微调过程。未来可以将检索增强的方法用到更广泛的多模态任务中去，探索用于检索数据的可扩展性。</p><div class="note note-success">            <p>一句话总结：本文提出了一种轻量级Image Caption模型——SmallCap，相比于使用高成本的VLM预训练和高开销的lightweight-training模型微调(相比于预训练和微调的成本持续增加)，SmallCap结合检索增强技术大大降低了训练成本并在COCO数据集上取得良好的表现。</p>          </div><h3 id="再读导论"><a href="#再读导论" class="headerlink" title="再读导论"></a>再读导论</h3><p>先进的Image Caption是通过在大规模数据集上的大模型训练完成的。规模的扩大导致了更高计算量的预训练和微调成本。不同模型版本所需的视觉域和终端用户的实际应用也不同。</p><p>ClipCap 和 I-Tuning模型都对降低成本做出了努力，它们主要是使用了<strong>现成的</strong>预训练视觉编码、解码器。模型参数被预先冻结，只<strong>训练两者之间的映射</strong>来完成Image Caption任务。参数规模确实减少了、速度也有所提高了。虽然模型易于管理起来，但是仍然<strong>不适合前文提到的实际应用</strong>，因为模型对于每个用例都需要单独训练。</p><p>SmallCap使得lightweight training, training-free domain transfer,and exploitation of large data in a training-free fashion(轻量级训练，无需训练的域转移，以及以无需训练的方式利用大数据)成为可能。</p><p>结合<strong>检索增强提示</strong>的SmallCap克服了一个关键限制：<em>需要明确的微调以使用到新的领域</em>，这种方式证明了在多模态任务中的潜力。</p><div class="note note-success">            <p>导论首先介绍了当前Image Caption领域发展的主要趋势：数据规模变大，计算成本持续增加。而先前的模型如ClipCap 和 I-Tuning确实对降本有明显的提高，但是增效方面并没有取得良好的性能，并不能领过适配多种落地任务，也就是说可以降本但无法增效。而SmallCap的提出克服了需要多次明确微调以应用到新领域的限制，实现了降本增效的效果。</p>          </div><h3 id="最后读重要章节"><a href="#最后读重要章节" class="headerlink" title="最后读重要章节"></a>最后读重要章节</h3><h4 id="model"><a href="#model" class="headerlink" title="model"></a>model</h4><p>编码器：CLIP-ViT-B&#x2F;32，解码器：$GPT-2_{Base}$</p><p>难点理解 - multi-head cross-attention layer：</p><p>编码器与解码器模型的运算在两个向量空间维度，所以要通过<strong>multi-head cross-attention layer</strong>将它们连接起来，使得解码器的每一层都关注编码器的输出。</p><p>Small降本增效的方法：</p><ol><li>通过冻结编码-解码器，只训练他们之间的随机初始化交叉住意层来降低计算开销并提高泛化。</li><li>通过交叉注意力层的的投影矩阵维度(d)控制可训练参数的数量。</li></ol><h4 id="Prompting-with-Retrieved-Captions"><a href="#Prompting-with-Retrieved-Captions" class="headerlink" title="Prompting with Retrieved Captions"></a>Prompting with Retrieved Captions</h4><p>相比于先前的image2image检索方法，本文创新地运用了image2text方法。将数据集中的image captions作为外挂知识库。检索阶段使用完整的<strong>CLIP模型</strong>与视觉-文本编码器，将两种模态数据映射到共享的向量空间。</p><p>编码器会编码input image和数据存储的内容，通过cos相似度检索得到与input image最相似的k个文本项。</p><p>解码器会将检索阶段使用的prompt作为输入Tokens，然后依照特征图像和task demonstration生成captioin。</p><h2 id="实验复现"><a href="#实验复现" class="headerlink" title="实验复现"></a>实验复现</h2><p>论文源代码地址：<a href="https://github.com/RitaRamo/smallcap">https://github.com/RitaRamo/smallcap</a></p><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>代码环境基于python3.9</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n smallcap python=3.9<br>conda activate smallcap<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p><a href="https://www.kaggle.com/datasets/shtvkumar/karpathy-splits">https://www.kaggle.com/datasets/shtvkumar/karpathy-splits</a></p><p>从上面的链接中下载COCO Karpathy splits的dataset_coco.json文件并放置在data&#x2F;目录下</p><p><a href="https://cocodataset.org/#download">https://cocodataset.org/#download</a></p><p>从上面的链接中下载2017版本的COCO images(train、val、test)，并将文件放置在data&#x2F;images&#x2F;下</p><p><a href="http://images.cocodataset.org/zips/train2017.zip">http://images.cocodataset.org/zips/train2017.zip</a></p><p><a href="http://images.cocodataset.org/zips/val2017.zip">http://images.cocodataset.org/zips/val2017.zip</a></p><p><a href="http://images.cocodataset.org/zips/test2017.zip">http://images.cocodataset.org/zips/test2017.zip</a></p><h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>目前基于ResNet的CLIP模型仍然无法从Hugging Face下载，所以需要通过pip安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install git+https://github.com/openai/CLIP.git<br></code></pre></td></tr></table></figure><p>提取特征：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir features<br>python src/extract_features.py<br></code></pre></td></tr></table></figure><p>检索captions：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python src/retrieve_captions.py<br></code></pre></td></tr></table></figure><h4 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h4><p>完成前期所有准备后进行模型训练：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python train.py<br></code></pre></td></tr></table></figure><p>Training takes up to <strong>8 hours</strong> on a single <code>NVIDIA A100 GPU</code>, using <code>16 GB</code> of the available memory.</p><p>训练完成后，模型以名称<code>&lt;rag/norag&gt; _m</code>保存&#x2F;例如 RAG_7M，用于训练以检索增强和7m可训练参数训练的型号。</p><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>使用训练好的模型进行推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python infer.py --model_path &lt;MODEL_PATH&gt;<br></code></pre></td></tr></table></figure><p>如果还指定了 <code>--checkpoint_path</code> 则推理将仅使用该检查点运行。否则，将使用 –model_path 中的所有检查点。</p><p>如果指定<code>-infer_test</code>推理使用测试数据，则使用否则VAL数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python infer.py --model_path experiments/rag_7M --checkpoint_path checkpoint-<span class="hljs-number">17712</span> --infer_test<br></code></pre></td></tr></table></figure><p>在每个相应的checkpoint subdirectory中，将模型预测存储为<code>&lt;val/test&gt; _preds.json</code>。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年2月总结与展望</title>
    <link href="/2025/03/01/2025%E5%B9%B42%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/03/01/2025%E5%B9%B42%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>2025年3月1日，星期六，雾霾，第51篇博客。</p><h2 id="假期-开学"><a href="#假期-开学" class="headerlink" title="假期 -&gt; 开学"></a>假期 -&gt; 开学</h2><p>2月是过渡的一个月，从假期的状态慢慢恢复到了开学后的学习、工作状态。</p><p>2.24号开始，开学第一周。</p><p>结束完第一周的课程，感觉这半年又将是收获满满的一个学期，因为这学期选了几门感兴趣的硬核好课。</p><p>计算所陈云霁老师团队的《智能计算系统》当之无愧的好课，这门课很创新、很硬核。从软硬件、编程框架多角度剖析大模型时代下的计算系统；而且每周四下午还第一次开设了这门课程对应的实验《智能计算系统开发与实践》，对于我这个不务正业的二流选手，岂有不蹭之理。</p><p>计算所徐俊刚老师团队的《深度学习》，国科大每年爆火的一门课程。希望这学期能认认真真跟下来，把每个实验都扎扎实实做完，再把DP的整个框架梳理梳理，建立起知识体系。徐俊刚老师这学期还开了一门《高级数据管理》，但是和我的论文讨论课冲突了，想着如果后面大老师不来的话，就翘了去听一听《高级数据管理》这门课。</p><p>计算所陈世敏老师的《大数据系统与大规模数据分析》是一门很硬的课，感觉比上学期《大数据技术》还要硬核。陈世敏老师是清华本硕，CMU博士。感觉听陈老师的课，总能听到一名互联网老兵对于技术的读到见解。</p><p>自动化所张家俊老师的《大模型技术与应用》也很不错，蹭课选手计划前两节课一定去认真听完。感觉张老师讲课很干练，基本功很扎实，没有废话、直击要害，NLP领域宗成庆老师团队的人果然都是大佬啊。</p><p>软件所彭启民老师的《Java程序设计》也值得一听，打算这学期跟着老师，把Java基础再夯实一遍。</p><p>非常幸运抢到了朱天宇老师的篮球课，以后每周二都能去打篮球咯，这学期要把篮球的基本技术在提高提高～</p><p>珍惜雁栖湖的教育资源，别和上学期一样总是逃课了😂</p><h2 id="阅读收获"><a href="#阅读收获" class="headerlink" title="阅读收获"></a>阅读收获</h2><p>开学前购入了一款汉王clear6，墨水屏的小显示器像口袋书一样，完美地解决了我的阅读的需求。开通了微信读书会员，今年开始，要努力培养阅读习惯，多读点书。</p><p>《认知觉醒》</p><p>重新认识大脑：我们人类(这一物种)经过经过漫长的进化，大脑也是逐步进化的。从进化路径来看，大脑分为三种三范式演进过程：<strong>本能脑 → 情绪脑 → 理性脑</strong>。</p><ul><li><strong>本能脑</strong>源自于爬行时代，主管本能</li><li><strong>情绪脑</strong>源于哺乳动物时代，主管情绪</li><li><strong>理性脑</strong>源于灵长类动物时代，主管认知</li></ul><p>本能脑和情绪脑伴随人类发展进化的时间最长，从远古人类开始，就是本能脑和情绪脑在支配着我们的行为，<strong>避难趋易、急于求成</strong>、躲避危险都是人性最基本的行为模式。</p><p>焦虑的根源在于：<strong>想同时做很多事，又想立即见到效果</strong>。焦虑时人的天性，是人类的默认出厂设置。千百年来所有人都一样，所以，允许自己适当的焦虑，不要较劲。</p><ul><li>想同时做很多事：<strong>避难趋易</strong></li><li>又想立即见到效果：<strong>急于求成</strong></li></ul><p>快速进步的本质就是永远让自己始终处于<strong>学习拉伸区</strong>(舒适区与难度区的交界)。</p><p>刻意练习是在处于拉伸区的同时，要<strong>定义明确清晰的目标(学会任务拆解、拆分目标) - 极度专注(最触动你的地方) - 有效的反馈 - 在拉伸区刻意练习</strong>。</p><h2 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h2><ul><li><p>每个人的进度都是不同的，过分的关注别人的进度成果只会加剧自身的焦虑与迷茫，不要既想又想，还要必须有好结果。</p></li><li><p>做好科研的方法是什么？应该如何做有价值的科学研究？为了发论文而发，为了评奖评优而发？？底层逻辑必须想清楚。</p></li><li><p>这半年要一直去想：如何做好科研？那么多学术大佬，他们的方法论是什么？怎么建立起来的？技术与研究要两手抓、两手都要硬。</p></li></ul><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>2月份完成的任务：</p><ol><li>结合王鹏、谷清水、卞龙鹏新书《大模型RAG实战》梳理了RAG基础概念与范式变迁，完成对langchain框架学习。</li><li>动手实践了<strong>RAG-from-scratch</strong>的项目代码。</li><li>读了三篇关于Graph RAG的paper&#x2F;&#x2F; 但是仍然找不到什么好的idea。</li></ol><p>3月份目前计划要完成的任务：</p><ul><li>实践<strong>RAGflow</strong>项目，复盘下多模态RAG的实践项目</li><li>总结梳理langchain框架的核心组件与常用方法，熟悉LCEL编程范式</li><li>总结一下常用的Tokenizer(中文&#x2F;英文)，熟练掌握文本预处理的技术(以拿到文本快速适配任何E-D架构为目标)</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM开发必备</title>
    <link href="/2025/02/26/LLM%E5%BC%80%E5%8F%91%E5%BF%85%E5%A4%87/"/>
    <url>/2025/02/26/LLM%E5%BC%80%E5%8F%91%E5%BF%85%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>2025年2月26日，星期三，晴，第50篇博客。</p><div class="note note-danger">            <p>困扰的问题：在进行RAG、多模态检索的实践中难免要调用大模型，但是由于“网络”和注册等诸多因素，使得我们很难方便调用的Open API。</p>          </div><p>推荐一个API网站：<a href="https://www.dmxapi.com/">https://www.dmxapi.com/</a> ，网站集成了当前主流的大模型的调用方法。</p><h2 id="openai接口"><a href="#openai接口" class="headerlink" title="openai接口"></a>openai接口</h2><p>使用openai库调用大模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-comment"># 1. 创建客户端 -&gt; 将API密钥和base_url传给OpenAI对象</span><br>client = OpenAI(<br>    api_key=<span class="hljs-string">&quot;sk-........................&quot;</span>,<br>    base_url=<span class="hljs-string">&quot;https://www.dmxapi.com/v1&quot;</span><br>)<br><br><span class="hljs-comment"># 2. chat_completion -&gt; 实现chat </span><br>chat_completion = client.chat.completions.create(<br>    <span class="hljs-comment"># message的形式是以列表存储的json串</span><br>    messages=[<br>        &#123;<br>            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br>            <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;9.11和9.12哪个更大？&quot;</span>,<br>        &#125;<br>    ],<br>    model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span><br>)<br><br><span class="hljs-built_in">print</span>(chat_completion)<br></code></pre></td></tr></table></figure><p>Embedding模型调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> openai<br><br><span class="hljs-comment"># 设置openai密钥和基础URL</span><br>openai.api_key = <span class="hljs-string">&quot;sk-*********************&quot;</span><br>openai.base_url = <span class="hljs-string">&quot;https://www.dmxapi.com/v1/&quot;</span> <span class="hljs-comment"># 做嵌入时base_url后面要添加 /</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embedding</span>(<span class="hljs-params">text</span>):<br>    response = openai.embeddings.create(<br>        model=<span class="hljs-string">&quot;text-embedding-3-small&quot;</span>,<br>        <span class="hljs-built_in">input</span>=text<br>    )<br>    <span class="hljs-keyword">return</span> response.data[<span class="hljs-number">0</span>].embedding<br><br>text = <span class="hljs-string">&quot;你好世界，我爱你！&quot;</span><br><br><span class="hljs-comment"># 获取文本嵌入</span><br>embedding = get_embedding(text)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;文本：<span class="hljs-subst">&#123;text&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;嵌入的向量维度：<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(embedding)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;嵌入的向量前5个元素：<span class="hljs-subst">&#123;embedding[:<span class="hljs-number">6</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>文本：你好世界，我爱你！<br>嵌入的向量维度：1536<br>嵌入的向量前5个元素：[0.02944854088127613, -0.059522904455661774, -0.03096095100045204, 0.06727617979049683, 0.013377008028328419]</p><p>langchain - Ollama调用嵌入模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain_community.embeddings <span class="hljs-keyword">import</span> OllamaEmbeddings<br><br><span class="hljs-comment"># 使用本地Ollama运行嵌入模型</span><br>embed = OllamaEmbeddings(<br>    model=<span class="hljs-string">&#x27;milkey/dmeta-embedding-zh&#x27;</span>,<br>)<br></code></pre></td></tr></table></figure><h2 id="Deepseek-API"><a href="#Deepseek-API" class="headerlink" title="Deepseek API"></a>Deepseek API</h2><p>opanai库也提供了Deepseek API的调用方法。</p><p>通过使用Deepseek API调用Deepseek - R1模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">client = OpenAI(api_key=<span class="hljs-string">&quot;sk-********&quot;</span>,base_url=<span class="hljs-string">&quot;https://api.deepseek.com&quot;</span>)<br>messages = [&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;9.11 and 9.8,which is greater?&quot;</span>&#125;]<br>response = client.chat.completions.create(<br>    model=<span class="hljs-string">&quot;deepseek-chat&quot;</span>,<br>    messages=messages,<br>    stream=<span class="hljs-literal">False</span><br>)<br><span class="hljs-comment"># reasoning_content = response.choices[0].message.reasoning_content</span><br>content = response.choices[<span class="hljs-number">0</span>].message.content<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开工大吉</title>
    <link href="/2025/02/05/%E5%BC%80%E5%B7%A5%E5%A4%A7%E5%90%89/"/>
    <url>/2025/02/05/%E5%BC%80%E5%B7%A5%E5%A4%A7%E5%90%89/</url>
    
    <content type="html"><![CDATA[<p>2025年2月5日，乙巳年正月初八，星期三，晴，第49篇博客&#x2F;2025年第8篇博客，正式开工！！！</p><h1 id="关于DeepSeek以及未来1年有关LLM的一些观点和思考"><a href="#关于DeepSeek以及未来1年有关LLM的一些观点和思考" class="headerlink" title="关于DeepSeek以及未来1年有关LLM的一些观点和思考"></a>关于DeepSeek以及未来1年有关LLM的一些观点和思考</h1><p><img src="https://p4.itc.cn/images01/20230410/ae3dde350de54c24a22196a05fc7dd64.jpeg"></p><div class="note note-success">            <p>观点整理自刘焕勇老师，转发切记注明出处！</p>          </div><p>明天就开工了，针对deepseek，我们静下心来看看，这波造神热度迟早要消散或者翻车。LLM帮大家解决落地问题，让大家有饭吃，有活干，才能行。我倒觉得反而利好的是qwen跟豆包，因为后两者一直在做，deepseek那套grop，会被他们借鉴。然后，原有的基础跟基本盘会进一步牢固。等大家恢复理性之后，还是会乖乖用qwen阿里云这些。即便国家造神有单子，那也是deepseek自己，但独乐乐不如众乐乐，众乐乐才是真的乐。deepseek蒸馏版都70B，现在大家落地， qwen14B都部署费劲。所以，<strong>底层公司的刚需才是真需</strong>。</p><p>Guonei 流行造神跟毁神，比比皆是。deepseek也逃不出这个魔咒。</p><p>我觉得deepseek 只要不掉进舆论的坑里，就无所谓。他们开源的细节足以复现，因此会整体抬高中国大模型的实力。这点功不可没。至于后续，那就看谁能走的更靠前。而且，DS 打破了OpenAI 他们的神话，因此会给更多中国大模型厂商以启发，不一定非得复制OpenAI 才能成功，有更多的尝试，会有更多的成果。</p><p>所以，作为社区成员的大家，保持冷静，<strong>从解决落地问题的角度出发，从吃饭的角度出发</strong>。就会明朗平静很多。</p><p>从去年DS 的积累看，今年多模态应该没问题，所以今年肯定还会有新的模型出来，而且 R1 的开源启发了无数人的火花，今年看到 R2 甚至R3 都有可能。推理过程开放后，太有启发性了，等于给大模型开了一扇窗，可以用人能够理解的方式，让人们看到大模型在怎么运作，黑箱透明度的提高，意味着我们在大模型上的研究上可能会更有方向性。今年是个可以期待的一年。</p><p><strong>今年是落地年，技术上可以期待，但也仅仅是技术本身，还是要解决吃饭问题</strong>。</p><div class="note note-danger">            <p>自己的思考：今年务必多关注多模态、强化学习与RAG的结合，多做落地实践、多动手。</p>          </div><h1 id="关于LLM的一些通俗解释和理解"><a href="#关于LLM的一些通俗解释和理解" class="headerlink" title="关于LLM的一些通俗解释和理解"></a>关于LLM的一些通俗解释和理解</h1><div class="note note-success">            <p>读AI大佬Andrej Karpathy推文所记：<a href="https://x.com/karpathy/status/1885026028428681698">https://x.com/karpathy/status/1885026028428681698</a></p>          </div><p>费曼学习法：用自己话把问题和过程描述清楚，并建立关联。</p><p>我们可以将训练大型语言模型 (LLM) 的过程巧妙地比作教育学生，并以教科书的结构为框架，阐述了当前 LLM 训练的现状和未来方向。</p><p>当我们打开任何一本教科书，都会看到三种主要类型的信息：</p><p><strong>1. 背景信息 &#x2F; 阐述 (Background information &#x2F; exposition):</strong> 这是教科书的核心内容，用于解释各种概念和知识。</p><p>学生通过阅读和学习这些内容来构建知识体系，这就像是 <strong>LLM 的预训练 (pretraining)</strong> 阶段。在预训练阶段，模型通过阅读海量的互联网文本，学习语言的规律、世界的知识，积累广泛的背景知识，为后续的学习打下基础。</p><p><strong>2. 例题及解答 (Worked problems with solutions):</strong> 教科书会提供具体的例题，并详细展示专家如何解决这些问题。</p><p>这些例题是示范，引导学生模仿学习。这与 <strong>LLM 的 监督式微调 (supervised finetuning)</strong> 阶段相对应。在微调阶段，模型学习人类专家提供的“理想答案”，学习如何生成高质量、符合人类期望的回复，例如助手类应用的“理想回答”。</p><p><strong>3. 练习题 (Practice problems):</strong> 教科书每章节末尾通常会设置大量的练习题，这些题目往往只提供最终答案，而不给出详细的解题步骤。</p><p>练习题旨在引导学生通过 试错 (trial &amp; error) 的方式进行学习。学生需要尝试各种方法，才能找到正确的答案。Karpathy认为，这与 <strong>强化学习 (reinforcement learning)</strong> 的概念高度相似。</p><p>Karpathy强调，目前我们已经让 LLM 经历了大量的“阅读”和“示例学习”，也就是预训练和监督式微调，但对于“练习题”这一环节，也就是强化学习，我们还处于一个新兴的、尚待开发的阶段。</p><p>当我们为 LLM 创建数据集时，本质上与为它们编写教科书并无二致。为了让 LLM 真正“学会”，我们需要像编写教科书一样，提供这三种类型的数据：</p><p><strong>大量的背景知识 (Background information)：</strong> 对应预训练，让模型积累广泛的知识。</p><p><strong>示范性的例题 (Worked problems)：</strong> 对应监督式微调，让模型学习高质量的输出。</p><p><strong>大量的练习题 (Practice problems)：</strong> 对应强化学习，让模型在实践中学习，通过试错和反馈不断改进。</p><p><strong>LLM 需要阅读，更需要实践。</strong> 只有通过大量的实践练习，才能真正提升 LLM 的能力，让它们更好地理解世界、解决问题。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年1月总结与展望</title>
    <link href="/2025/01/30/2025%E5%B9%B41%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/01/30/2025%E5%B9%B41%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>2025年1月30日，星期四，阴天，这是写的第48篇博客文章。</p><h2 id="这个月很忙，主旋律是考试与春节·过年。"><a href="#这个月很忙，主旋律是考试与春节·过年。" class="headerlink" title="这个月很忙，主旋律是考试与春节·过年。"></a>这个月很忙，主旋律是<strong>考试</strong>与<strong>春节·过年</strong>。</h2><p>咱中华文化就是博大精深，按理说，每年的1月、2月都应该是一年的开端，事情的开始阶段所有人的状态都应该鼓足干劲的。但是我们中国人不然，每年的1月、2月，中国人都要过自己的节日——<strong>春节</strong>。对于全体中国人来说，春节正式一年当中为数不多的放下手里工作回家团聚、休息调整的时光～</p><p>只有过了春节，才真正意味着中国人的一年正式拉开序幕了。所以，按照工作惯性与生活节奏，中国人的一年应该是从每年的2月中下旬开始才算真正的崭新一年。</p><p>1月份有一半的时间都是在复习备考中度过的，从在果壳跨年结束以后，一直持续到1月10号最后一门考试结束，然后16号离校。1月份也有意思，本应该是2025的开端，却和我的期末装了满怀，开始和结束“会面”。</p><p>期末考试整体还算说得过去，<strong>投入与回报成正比</strong>。分数高的课程确实花了很多精力在上面，分数低的课程(过了)要么是复习不到位、要么是知识体系没有建立(NLP和机器学习平时翘课太多了)。</p><p>NLP得分很低，是复习策略失误导致的；机器学习中规中矩，很多点其实也是眼高手低了。不过研究生的开始还好，不想本科那样去卷绩点、卷分数，更重要的还是对于自己能力的培养和提升。</p><p>腊月20回家，到家后和老于一起吃了个饭、喝喝酒叙叙旧，说好的过年后xdm一起聚聚，最终也没能聚齐。年前还感染了诺如病毒，上吐下泻折腾了不到四天，每年回家感冒发烧一次，很规律。</p><p>过年基本上还和往年一样，最有年味er的活动莫过于：<strong>上坟祭祖、年夜饭、拜年</strong>。</p><h2 id="这个月很爱刷剧"><a href="#这个月很爱刷剧" class="headerlink" title="这个月很爱刷剧"></a>这个月很爱刷剧</h2><p>可能也是考完试要放松了，这个月刷完了《大明王朝1566》、二刷了姜文和刘晓庆主演的《芙蓉镇》，正在刷《康熙王朝》和《雍正王朝》。假期计划把这几部历史巨著都刷一遍，再把王志文的几部经典也顺带看看。</p><h2 id="这个月很懒散"><a href="#这个月很懒散" class="headerlink" title="这个月很懒散"></a>这个月很懒散</h2><p>考完试伴随着放假，整个人就是非常的松散与懈怠，经常性的刷抖音一刷就是半天，然后刷完抖音就转战小红书继续刷，其他的事情提不起兴趣来，浪费了很多时间。</p><p>不要间歇性努力，持续性颓废哈哈哈，不过适当放松放松还是可以的。</p><h2 id="2月份，冲！"><a href="#2月份，冲！" class="headerlink" title="2月份，冲！"></a>2月份，冲！</h2><p>今年寒假考虑到天气、景区人数就不计划出游了，踏踏实实在家里沉淀吧，多学点技术、多读些书。</p><p>一直想在B站自己录点课，但是始终找不到动机。。。</p><p>抽空把李沐老师《动手学深度学习》的最后几节课程刷完，最后只剩下了BERT。论文课也要开始刷了。</p><p>2月份首先要从上个月的松散劲中走出来，尽最大努力坚持三大小改变：</p><ul><li><input disabled="" type="checkbox"> 早睡会(23:00-23:20),早起会(6:40-6:50)</li><li><input disabled="" type="checkbox"> 少刷会抖音、小红书</li><li><input disabled="" type="checkbox"> 坚持学英语、练口语和听力</li></ul><p>开学前(02.22)需要完成的的任务：</p><ul><li><input disabled="" type="checkbox"> Agentic RAG和Graph RAG技术栈梳理学习</li><li><input disabled="" type="checkbox"> RAG系统实践</li><li><input disabled="" type="checkbox"> Multi-Agent的多数据源RAG系统设计</li><li><input disabled="" type="checkbox"> Transformer架构完善和开源</li><li><input disabled="" type="checkbox"> 向量数据库代码实现与开源</li><li><input disabled="" type="checkbox"> 五子棋程序精进完善与开源</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梁文锋专访</title>
    <link href="/2025/01/29/%E6%A2%81%E6%96%87%E9%94%8B%E4%B8%93%E8%AE%BF/"/>
    <url>/2025/01/29/%E6%A2%81%E6%96%87%E9%94%8B%E4%B8%93%E8%AE%BF/</url>
    
    <content type="html"><![CDATA[<h1 id="暗涌Waves专访DeepSeek创始人梁文锋"><a href="#暗涌Waves专访DeepSeek创始人梁文锋" class="headerlink" title="暗涌Waves专访DeepSeek创始人梁文锋"></a>暗涌Waves专访DeepSeek创始人梁文锋</h1><p>暗涌：互联网和移动互联网时代留给大部分人的惯性认知是，美国擅长搞技术创新，中国更擅长做应用。</p><p>梁文锋：我们认为随着经济发展，<strong>中国也要逐步成为贡献者，而不是一直搭便车</strong>。过去三十多年 IT 浪潮里，我们基本没有参与到真正的技术创新里。我们已经习惯摩尔定律从天而降，躺在家里 18 个月就会出来更好的硬件和软件。<strong>Scaling Law 也在被如此对待</strong>。</p><p>但其实，这是西方主导的技术社区一代代孜孜不倦创造出来的，只因为之前我们没有参与这个过程，以至于忽视了它的存在。</p><div class="note note-danger">            <p>真正的差距是原创和模仿之差</p>          </div><p>暗涌：为什么 DeepSeek V2 会让硅谷的很多人惊讶？</p><p>梁文锋：在美国每天发生的大量创新里，这是非常普通的一个。他们之所以惊讶，是因为<strong>这是一个中国公司，在以创新贡献者的身份，加入到他们游戏里去</strong>。毕竟大部分中国公司习惯 follow，而不是创新。</p><p>暗涌：为什么中国公司——包括不缺钱的大厂，这么容易把快速商业化当第一要义？</p><p>梁文锋：<strong>过去三十年，我们都只强调赚钱，对创新是忽视的</strong>。创新不完全是商业驱动的，还需要好奇心和创造欲。我们只是被过去那种惯性束缚了，但它也是阶段性的。</p><p>暗涌：为什么你的定义是‘做研究、做探索’？</p><p>梁文锋：一种好奇心驱动。从远处说，我们想去验证一些猜想。比如我们理解人类智能本质可能就是语言，人的思维可能就是一个语言的过程。你以为你在思考，其实可能是你在脑子里编织语言。这意味着，在语言大模型上可能诞生出类人的人工智能（AGI）。从近处说，GPT4 还有很多待解之谜。我们去复刻的同时，也会做研究揭秘。</p><div class="note note-danger">            <p>万卡储备其实是好奇心驱动</p>          </div><p>暗涌：GPU是这次 ChatGPT 创业潮的稀缺品，你们在 2021 年就可以有先见之明，储备了 1 万枚。为什么？</p><p>梁文锋：其实从最早的 1 张卡，到 2015 年的 100 张卡、2019 年的 1000 张卡，再到一万张，这个过程是逐步发生的。几百张卡之前，我们托管在 IDC，规模再变大时，托管就没法满足要求了，就开始自建机房。很多人会以为这里边有一个不为人知的商业逻辑，但其实，主要是好奇心驱动。</p><p>暗涌：什么样的好奇心？</p><p>梁文锋：对 AI 能力边界的好奇。对很多行外人来说，ChatGPT 这波浪潮冲击特别大；但对行内人来说，2012 年 AlexNet 带来的冲击已经引领一个新的时代。AlexNet 的错误率远低于当时其他模型，复苏了沉睡几十年的神经网络研究。虽然具体技术方向一直在变，但模型、数据和算力这三者的组合是不变的，特别是当 2020 年 OpenAI 发布 GPT3 后，方向很清楚，需要大量算力；但即便 2021 年，我们投入建设萤火二号时，大部分人还是无法理解。</p><p>暗涌：所以 2012 年起，你们就开始关注到算力的储备？</p><p>梁文锋：<strong>对研究员来说，对算力的渴求是永无止境的</strong>。做了小规模实验后，总想做更大规模的实验。那之后，我们也会有意识地去部署尽可能多的算力。</p><div class="note note-danger">            <p>套路都是上一代的产物，未来不一定成立</p>          </div><p>暗涌：回到关于原创式创新的话题。现在经济开始进入下行，资本也进入冷周期，所以它对原创式创新是否会带来更多抑制？</p><p>梁文锋：我倒觉得未必。<strong>中国产业结构的调整，会更依赖硬核技术的创新</strong>。当很多人发现过去赚快钱很可能来自时代运气，就会更愿意俯身去做真正的创新。</p><p>暗涌：所以你对这件事也是乐观的？</p><p>梁文锋：我是八十年代在广东一个五线城市长大的。我的父亲是小学老师，九十年代，广东赚钱机会很多，当时有不少家长到我家里来，基本就是家长觉得读书没用。但现在回去看，观念都变了。因为钱不好赚了，连开出租车的机会可能都没了。一代人的时间就变了。</p><p><strong>以后硬核创新会越来越多</strong>。现在可能还不容易被理解，是因为整个社会群体需要被事实教育。<strong>当这个社会让硬核创新的人功成名就，群体性想法就会改变。我们只是还需要一堆事实和一个过程</strong>。</p><div class="note note-success">            <p>创新与赚快钱之间的抉择，过去赚快钱很可能是时代的馈赠，那么在经济低速发展期，没有赶上时代的红利的时候需要自己作出怎样的调整，这是至关重要的，需要不断实践不断思考，不断更新认知。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>LLM</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepSeek YYDS</title>
    <link href="/2025/01/27/DeepSeek-YYDS/"/>
    <url>/2025/01/27/DeepSeek-YYDS/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"></a>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><center>DeepSeek-AI</center><center>https://arxiv.org/pdf/2501.12948</center><center>多看论文原文，少看自媒体公众号文章的鼓吹宣传！</center><p><img src="https://i.bstr.es/drivingeco/2025/01/Deepseek-r1-logo.webp"></p><p>与普通LLM相比，推理LLM倾向于在回答之前讲问题分解为更小的步骤(推理步骤和思维链)，这类模型是在让模型学习如如何回答。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>文章介绍了DeepSeek-AI团队开发的第一代<strong>推理模型</strong>DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1是一种基于大规模强化学习训练的模型，无需监督微调作为初步步骤，展现出强大的推理能力。</p><ul><li><p>优势：推理能力强</p></li><li><p>挑战：可读性差、语言混合</p></li></ul><p><code>模型提出的标志性意义</code>：DeepSeek-R1<strong>迈出了使用纯强化学习 (RL) 提高语言模型推理能力的第一步</strong>，目标是<strong>探索 LLM 在没有任何监督数据的情况下开发推理能力的潜力</strong>，重点关注LLM<strong>通过纯 RL 过程进行模型自身进化</strong>。</p><p><img src="/./img/fig/deepseek.jpg" alt="deepseek-performance"></p><p>自身进化的同时提高了推理性能，同时，因为无需任何监督数据，极大地降低了成本。LLM拼多多称号当之无愧！国产骄傲！</p><p>DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks.</p><p>DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning.</p><p>DeepSeek-R1-Zero 是使用 DeepSeek-V3-Base 作为基础模型，结合GRPO作为RL框架经过数千步推理训练得到。然而DeepSeek-R1-Zero 遇到了可读性差、语言混合的困难，为了进一步提高推理性能，DeepSeek-AI团队由DeepSeek-R1-Zero模型改进到DeepSeek-R1模型，其结合了少量 cold-start data 和a multi-stage training pipeline。</p><p>在RL接近收敛的时候，通过对 RL 检查点进行rejection sampling来创建新的 SFT 数据，并结合 DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，然后重新训练 DeepSeek-V3-Base 模型。</p><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li><strong>Post - training</strong>: 在现有基础模型上进行大规模强化学习，该团队直接将RL应用于基础模型，而不依赖于监督微调（SFT）作为一个初步步骤。这种方法允许模型探索思想链（CoT）来解决复杂的问题，从而开发出DeepSeek-R1-Zero。团队也介绍了开发DeepSeek-R1的pipeline。该管道包含了两个RL阶段，以及两个SFT阶段，作为模型的推理和非推理能力的种子。</li><li><strong>Distillation</strong>: 蒸馏后使得小模型也具有大能量，该团队证明了更大模型的推理模式可以被提炼为更小的模型，与通过RL在小模型上发现的推理模式相比，从而获得更好的性能。</li></ol><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><ul><li><strong>General Capability:</strong> 目前，DeepSeek-R1在function calling, multi-turn, complex role-playing, and JSON output等任务上还不如DeepSeek-V3。接下来，我们计划探索可以利用多长时间的CoT来增强这些领域的任务。</li><li><strong>Language Mixing:</strong> DeepSeek-R1目前针对中文和英语进行了优化，这可能导致在处理其他语言的查询时出现语言混合问题。例如，DeepSeek-R1可能会使用英语进行推理和响应，即使查询使用的是英语或中文以外的语言。我们的目标是在未来的更新中解决这一限制。</li><li><strong>Prompting Engineering:</strong> 在评估DeepSeek-R1时，我们观察到它对提示很敏感。Few-shot的激励不断地降低了它的表现。因此，我们建议用户直接描述问题，并使用zero-shot指定输出格式，以获得最佳结果。</li><li><strong>Software Engineering Tasks:</strong> 由于评估时间长，影响了RL过程的效率，因此大规模的RL在软件工程任务中尚未得到广泛的应用。因此，DeepSeek-R1在软件工程基准测试上并没有显示出比DeepSeek-V3相比的巨大改进。未来的版本将通过对软件工程数据实施拒绝抽样或在RL过程中合并异步评估来提高效率来解决这个问题。</li></ul><h2 id="deepseek多模态LLM-Janus-Pro"><a href="#deepseek多模态LLM-Janus-Pro" class="headerlink" title="deepseek多模态LLM - Janus-Pro"></a>deepseek多模态LLM - Janus-Pro</h2><p>Unified Multimodal Understanding and Generation with Data and Model Scaling: 通过数据和模型扩展实现统一的多模态理解和生成。</p><p>论文解决的当前研究过程中的痛点是：使用相同的视觉编码器处理不同的任务会影响多模态理解的性能。Janus有成果，但是因为训练数据和模型容量有限存在不足。</p><p>Janus - Pro的创新在于改进了上述问题，提升了模型的性能，特别是在多模态理解和文本到图像的生成中表现出色。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2025/01/DeepSeek-Janus-Pro.webp" alt="model pk"></p><p>Janus - Pro作出的改进：</p><ol><li><p>优化训练策略：调整各个阶段的数据比例</p></li><li><p>扩展训练数据：涵盖多模态理解和生成领域</p></li><li><p>更大的模型扩展：扩展模型至7B</p><p><img src="https://aipapersacademy.com/wp-content/uploads/2025/01/Janus_figure3.png" alt="Method"></p></li></ol><p><strong>增强了文本到图像的稳定性</strong></p><h2 id="DeepSeek的习得过程"><a href="#DeepSeek的习得过程" class="headerlink" title="DeepSeek的习得过程"></a>DeepSeek的习得过程</h2><p>复现R1之前务必要把<strong>deepseek-v3</strong>搞懂！</p><p>DeepSeek是站在巨人的肩膀上，摸着OpenAI过河，不要去过分diss OpenAI。deepseek - R1是去“猜”o1的实现过程。DeepSeek是强在推理阶段，在SFT阶段未必是最优的解决方案。</p><p>目前市面上大部分指导集成基本上都是部署的蒸馏版的R1，本质上base还是Qwen。</p><h2 id="对DeepSeek的认知误区"><a href="#对DeepSeek的认知误区" class="headerlink" title="对DeepSeek的认知误区"></a>对DeepSeek的认知误区</h2><div class="note note-danger">            <p>误区：DeepSeek是全新物种，think是万能钥匙</p>          </div><p>R1本质上还是一个大模型，只不过是目前推理模型开源最强</p><div class="note note-danger">            <p>误区：DeepSeek-R1不需要提示词</p>          </div><p>只要是大模型，就一定需要提示词，官方已经提供了34种提示词。<strong>之前大模型解决不了的场景，DeepSeek-R1依旧不好解决。</strong></p><div class="note note-danger">            <p>误区：DeepSeek强化学习模型已经是最强模型，干死了OpenAI</p>          </div><p>DeepSeek的多模态能力不具备、处理一些极端长上下文表现不佳、较03模型基准还有不小差距、base模型好造就了推理能力好(SFT负责记忆，RL负责泛化)</p><h2 id="DeepSeek的场景机会"><a href="#DeepSeek的场景机会" class="headerlink" title="DeepSeek的场景机会"></a>DeepSeek的场景机会</h2><p>与RAG？与Agent(规划不靠谱)结合？</p><p>think是显示过程可以作为微调数据使用，蒸馏得到的qwen学到了一定的推理和数值计算能力</p><p>只会做纯量替换，使用其推理能力，目前并不会带来巨大个信。</p><h2 id="Ollama本地部署deepseek-R1"><a href="#Ollama本地部署deepseek-R1" class="headerlink" title="Ollama本地部署deepseek - R1"></a>Ollama本地部署deepseek - R1</h2><h3 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h3><p>电脑 - MacBook Pro </p><p>芯片 - Apple M3 Pro</p><p>运行内存 - 18GB</p><h3 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="安装Ollama"></a>安装Ollama</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -sSL https://ollama.com/download | bash<br></code></pre></td></tr></table></figure><p>通过curl直接安装即可</p><h3 id="ollama常用指令"><a href="#ollama常用指令" class="headerlink" title="ollama常用指令"></a>ollama常用指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动服务</span><br>ollama serve<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">显示模型信息</span><br>ollama show<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行模型</span><br>ollama run model<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">从注册表中拉取模型</span><br>ollama pull modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">列出模型</span><br>ollama list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除模型</span><br>ollama rm modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">模型使用提示符</span><br>/set | set session variables<br>/show | show model information<br>/load &lt;model&gt; | load a session or model<br>/save &lt;model&gt; | save your current session<br>/clear | clear session context<br>/bye | exit<br>/? /help | help for a command<br></code></pre></td></tr></table></figure><h3 id="下载deepseek-R1模型"><a href="#下载deepseek-R1模型" class="headerlink" title="下载deepseek-R1模型"></a>下载deepseek-R1模型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs shell">ollama pull deepseek-r1:7b<br> ~  ollama run deepseek-r1:7b                                     ok  20:40:25<br>pulling manifest<br>pulling 96c415656d37... 100% ▕████████████████▏ 4.7 GB<br>pulling 369ca498f347... 100% ▕████████████████▏  387 B<br>pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB<br>pulling f4d24e9138dd... 100% ▕████████████████▏  148 B<br>pulling 40fb844194b2... 100% ▕████████████████▏  487 B<br>verifying sha256 digest<br>writing manifest<br>success<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; 什么是信息差？</span><br>&lt;think&gt;<br>嗯，用户问的是“什么是信息差”，我之前已经给了一个比较详细的回答了。现在我要<br>模拟一个assistant对这个问题的思考过程。首先，我需要理解用户的问题到底是什么<br>。<br><br>信息差通常指的是信息不均衡的情况，可能是因为信息不对称或者获取渠道不同导致某<br>些人掌握更多的信息，而另一些人则没有。这在商业、经济和社会中都很常见，比如在<br>交易中卖家和买家的信息不对称。<br><br>然后，我要考虑用户为什么会问这个问题。可能是他们正在处理一个实际的问题，比如<br>在谈判中如何利用信息差，或者在投资时如何应对信息不对称带来的风险。也有可能他<br>们在学习相关知识，需要理解这个概念的基本含义。<br><br>接下来，我应该分析信息差的几个方面：定义、原因、影响和应用。这样可以让思考过<br>程更全面，覆盖不同的角度。此外，还可以举一些例子来说明信息差在不同场景中的表<br>现，比如股票交易中的庄家信息优势，或者在线销售中买家对产品的不了解带来的风险<br>。<br><br>另外，我还需要考虑用户可能的深层需求。他们可能不仅仅想知道定义，还想知道如何<br>利用或避免信息差带来的问题。因此，在思考过程中，应该包括一些关于应对策略的内<br>容，比如透明化的措施、对称信息的重要性以及信息获取成本的影响。<br><br>最后，总结一下信息差的重要性和其在现代经济中的角色，这样用户不仅了解了表面的<br>定义，还能理解其背后的现实意义和应用价值。<br>&lt;/think&gt;<br><br>信息差是指在信息不对称的情况下，某些人或组织掌握的信息比其他人更多、更全面或<br>更有优势，从而在决策或行动中占据一定的优势。这种差异可能来自于信息获取渠道的<br>不同、信息来源的不可靠性，或者信息本身的敏感性和隐私性。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的具体表现：</span></span><br>1. **信息不对称**：指一方掌握的信息多于另一方，导致后者处于不利地位。<br>2. **信息不对等**：一方获得大量优质信息，而另一方获取较少或低质量的信息。<br>3. **信息不透明**：某些信息被隐藏或限制，如商业机密、股票价格等。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的影响：</span></span><br>1. **经济影响**：在金融市场中，信息差可能导致市场操纵、价格误导和投资风险增<br>加。例如，庄家可能利用内部信息操控股价。<br>2. **社会影响**：信息差可能导致资源分配不公，甚至引发社会矛盾。<br>3. **个人决策**：在个人生活中，信息差可能导致不公平竞争或权力滥用。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的应对策略：</span></span><br>1. **透明化**：通过法律或制度限制信息不对称，确保信息公开和对称。<br>2. **教育与培训**：提高公众的信息素养，减少信息不对称的可能性。<br>3. **成本控制**：在获取和使用信息时，尽量降低信息不对称带来的风险。<br><br>理解信息差有助于我们更好地识别和管理信息不对称带来的挑战，在经济、社会和日常<br>生活中做出更明智的决策。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph RAG</title>
    <link href="/2025/01/24/Graph-RAG-A-Survey/"/>
    <url>/2025/01/24/Graph-RAG-A-Survey/</url>
    
    <content type="html"><![CDATA[<center>  <h1>Graph Retrieval-Augmented Generation: A Survey</h1>  https://www.arxiv.org/pdf/2408.08921</center>论文更多侧重理论概念层面，实践落地过程中需要自行体会各个阶段的实现过程。<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>通过引用外部知识库，RAG改进了LLM输出，有效地减轻了诸如“幻觉”、缺乏特定领域的知识和过时的信息等问题。但是，<strong>数据库中不同实体之间复杂的关系结构给RAG系统带来了挑战</strong>。</p><p><em>GraphRAG</em>利用<strong>跨实体</strong>的结构信息，以实现更精确和全面的检索，捕获关系知识，并促进更准确的、上下文感知的响应。</p><p>这篇文章是首次对于GraphRAG方法提供了全面的总结，规范了GraphRAG workflow(包括Graph-Based</p><p>Indexing、Graph-Guided Retrieval和Graph-Enhanced Generation)，强调了每一个阶段的核心技术和训练方法。此外，还对下有任务、应用领域、评估方法、工业案例进行阐述。</p><h2 id="工作流与关键技术"><a href="#工作流与关键技术" class="headerlink" title="工作流与关键技术"></a>工作流与关键技术</h2><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><div class="note note-success">            <p><strong>Graph-Based Indexing (G-Indexing) -&gt; Graph-Guided Retrieval (G-Retrieval) -&gt; Graph-Enhanced Generation (G-Generation)</strong> </p>          </div><p>图索引建立：包括识别或构建与下游任务对齐的图数据库，并在其上建立索引。图数据可以来自开放知识图谱或自建图数据。</p><p>图数据检索：从图数据库中提取相关信息以响应用户查询。涉及选择检索器、检索范式、检索粒度和增强技术。</p><p>图增强生成：基于检索到的图数据生成有意义的输出。涉及选择生成器、转换图数据格式和生成增强技术。</p><h3 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h3><ul><li><strong>图数据:</strong> 包括开放知识图谱和自建图数据。开放知识图谱如Wikidata、Freebase等，自建图数据则根据特定任务定制。</li></ul><p><img src="/img/fig/g-data.jpg"></p><ul><li><p><strong>索引方法:</strong> 包括图索引、文本索引和向量索引。混合索引结合了这些方法的优点。</p></li><li><p><strong>检索器:</strong> 分为非参数检索器、基于LM的检索器和基于GNN的检索器。检索范式包括一次检索、迭代检索和多阶段检索。</p></li></ul><p><img src="/img/fig/g-retrieval.jpg"></p><ul><li><strong>生成器:</strong> 包括GNNs、LMs和混合模型。生成增强技术包括预生成增强、中生成增强和后生成增强。</li></ul><p><img src="/img/fig/g-generation.jpg"></p><p><strong>GraphRAG在图索引（G-Indexing）阶段是如何构建和优化图数据库的？</strong></p><ol><li><strong>数据源选择</strong>：图数据库可以来自公共知识图谱（如Wikidata、Freebase、DBpedia等）或自构建的数据源（如文本、其他形式的数据等）。</li><li><strong>节点和边属性映射</strong>：将节点和边的属性进行映射，以便于后续的检索和生成操作。</li><li><strong>指针建立</strong>：在节点之间建立指针，以便于快速遍历和检索。</li><li><strong>数据组织</strong>：组织数据以支持快速遍历和检索操作，确保索引过程的高效性。</li></ol><p><strong>GraphRAG在图增强生成（G-Generation）阶段如何处理和转换检索到的图数据？</strong></p><p>在图增强生成（G-Generation）阶段，GraphRAG需要将检索到的图数据转换为生成器可以处理的格式。具体步骤如下：</p><ol><li><strong>数据转换</strong>：将检索到的图数据（如节点、边、路径、子图等）转换为生成器可以理解的格式。常见的转换方法包括邻接表、自然语言描述、代码形式、语法树和节点序列等。</li><li><strong>生成器输入</strong>：将转换后的图数据与查询一起作为输入提供给生成器。生成器可以是基于图神经网络（GNNs）的模型、判别式语言模型或生成式语言模型。</li><li><strong>生成响应</strong>：生成器根据输入的查询和图数据生成最终的响应。对于生成式语言模型，这通常涉及解码过程，以生成自然语言文本作为响应。</li></ol><h2 id="Future-Prospects"><a href="#Future-Prospects" class="headerlink" title="Future Prospects"></a>Future Prospects</h2><h3 id="多模态数据整合"><a href="#多模态数据整合" class="headerlink" title="多模态数据整合"></a>多模态数据整合</h3><p>知识图谱中目前存储的仍然是大量文本数据，对于多模态数据的整合仍然是挑战，知识图谱的规模升级是发展GraphRAG的底层刚需。</p><h3 id="检索机制"><a href="#检索机制" class="headerlink" title="检索机制"></a>检索机制</h3><p>当前的检索机制仍然是对于少量文本-实体的知识图谱作检索，而大量实体的复杂检索机制需要被探索。</p><h3 id="检索上下文的无损失压缩"><a href="#检索上下文的无损失压缩" class="headerlink" title="检索上下文的无损失压缩"></a>检索上下文的无损失压缩</h3><p>应用GraphRAG，检索得到信息包含大量的实体和关系，这些信息会被转换为长文本输入序列。但是长文本输入序列包含两个问题：LLMs很难解决长文本序列、对于个人用户而言推理阶段的大量计算是障碍。</p><p>因此，无损压缩对于GraphRAG是关键的。</p><h3 id="落地应用"><a href="#落地应用" class="headerlink" title="落地应用"></a>落地应用</h3><p>应用场景：客服服务系统、推荐系统、KBQA(知识问答系统)</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇调研文章全面的回顾了GraphRAG技术，系统地分类、组织了它的基本技术，训练方法和应用场景。GraphRAG通过使用从图数据集中派生出的关键知识，解决了传统RAG的关键性限制。</p><center>  <h1>Retrieval-Augmented Generation with Graph</h1>  https://arxiv.org/pdf/2501.00309</center>论文更多侧重理论概念层面，实践落地过程中需要自行体会各个阶段的实现过程。<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>与传统RAG相比，检索器、生成器和外部数据源可以做统一嵌入处理；Graph数据结构的独特性，在进行GraphRAG设计时面临着大量的挑战。</p><p>论文的主要工作：定义了包含关键组件在内的GraphRAG的总体架构(包括查询处理器、检索器、组织器、生成器和数据源)、总结了<strong>特定领域</strong>的GraphRAG技术、讨论了GraphRAG未来发展方向。</p><h2 id="图数据结构"><a href="#图数据结构" class="headerlink" title="图数据结构"></a>图数据结构</h2><p>内在的节点和边编码组成了大量异构和关系信息，是RAG中的黄金资源。</p><p><img src="/img/fig/rag-with-graph.png"></p><p>RAG处理文本和图像数据，这些数据可以统一格式化为1-D序列数据或2-D网格数据。相比之下，<strong>GraphRAG</strong>处理图结构数据，涵盖了多种格式并包含特定领域的关系信息。</p><h2 id="全面的GraphRAG架构"><a href="#全面的GraphRAG架构" class="headerlink" title="全面的GraphRAG架构"></a>全面的GraphRAG架构</h2><p>GraphRAG通过检索和生成技术来增强下游任务的执行，框架的核心在于利用图结构数据的特性，进行信息检索、数据挖掘。</p><p><img src="/img/fig/framework-of-GraphRAG.png"></p><p>GraphRAG框架的关键组件：</p><ol><li><strong>查询处理器（Query Processor）</strong>：负责预处理用户定义的查询，使其能够与图数据源进行交互。</li><li><strong>检索器（Retriever）</strong>：根据预处理后的查询从图数据源中检索相关内容。</li><li><strong>组织器（Organizer）</strong>：对检索到的内容进行整理和优化，以提高生成器的性能。</li><li><strong>生成器（Generator）</strong>：根据组织后的信息生成最终答案。</li></ol><h3 id="Query-Processor"><a href="#Query-Processor" class="headerlink" title="Query Processor"></a>Query Processor</h3><p>Query Processor是一个关键组件，它负责对用户输入的查询进行预处理，以便与图数据源进行交互。</p><p>查询处理器主要功能：</p><ul><li><strong>实体识别</strong>：从查询中识别出实体，并将其与图数据源中的节点相匹配。</li><li><strong>关系抽取</strong>：识别查询中的关系，并将其与图数据源中的边相匹配。</li><li><strong>查询结构调整</strong>：将自然语言查询转换为结构化查询，如GQL（Graph Query Language）。</li><li><strong>查询分解</strong>：将复杂的查询分解为多个子查询，以便进行多步推理。</li><li><strong>查询扩展</strong>：通过添加相关术语来丰富查询，以提高检索的准确性和相关性。</li></ul><h3 id="Retriever"><a href="#Retriever" class="headerlink" title="Retriever"></a>Retriever</h3><p>检索器（Retriever）是负责从图数据源中检索相关信息的关键组件。</p><p>检索器主要功能：</p><p><strong>检索内容</strong>：根据预处理后的查询从图数据源中检索相关内容。</p><p><strong>适应图结构数据</strong>：能够处理图结构数据的多样性和复杂性，包括不同格式和来源的信息。</p><p><strong>多跳遍历</strong>：支持多跳遍历，以捕获逻辑上相关的知识。</p><p><strong>领域特定设计</strong>：根据不同领域的特定需求进行设计，以提高检索的准确性和效率。</p><h3 id="Organizer"><a href="#Organizer" class="headerlink" title="Organizer"></a>Organizer</h3><p>负责处理检索器检索到的内容，将其与预处理后的查询结合，以生成更适应生成器（Generator）的格式。</p><p>主要功能：</p><p><strong>内容优化</strong>：对检索到的内容进行后处理和优化，以提高其质量。</p><p><strong>图结构处理</strong>：处理检索到的图结构数据，包括图剪枝、重排序和图增强。</p><p><strong>文本化</strong>：将检索到的图结构数据转换为文本格式，以便生成器可以处理。</p><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p><strong>生成器（Generator）</strong>是负责根据查询和检索到的信息生成最终答案的关键组件。生成器的任务是将组织者处理后的信息转化为具体的输出，这些输出可以是文本、图像、数值或其他形式的数据，取决于具体的应用场景。</p><p>生成器主要功能：</p><p><strong>生成最终答案</strong>：根据查询和检索到的信息生成具体的输出。</p><p><strong>适应不同任务</strong>：能够处理多种任务，如分类、生成、预测等。</p><p><strong>利用图结构信息</strong>：能够理解和利用图结构数据中的关系和模式。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
      <tag>Agentic RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agentic RAG:A Survey</title>
    <link href="/2025/01/20/Agentic-RAG-A-Survey/"/>
    <url>/2025/01/20/Agentic-RAG-A-Survey/</url>
    
    <content type="html"><![CDATA[<center>  <h1>AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON AGENTIC RAG</h1>  <h3>Aditi Singh | Abul Ehtesham | Saket Kumar | Tala Talaei Khoei</h3>  https://arxiv.org/abs/2501.09136</center><p>论文更多侧重于理论概念层面，实践中还会遇到很多问题和bug，需要自行体会。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>文章首先以LLM在<strong>文本生成</strong>和<strong>自然语言理解</strong>方面的革新引入。然后笔锋一转(however)，揭示了对依赖静态数据“革新”的局限性，静态训练数据使得LLM不能够及时地动态响应、处理实时需求。针对这一局限性，引出文章重点介绍的技术： <strong>Retrieval-Augmented Generation (RAG)</strong> 。RAG的出现使得LLM可以及时更新训练数据，利用最新数据作出实时响应。当然，传统的RAG技术在多步推理和复杂任务管理上同样存在缺陷。</p><p>Agentic RAG通过将AI agents嵌入到RAG pipeline中超越了传统RAG在多步推理和复杂任务管理上存在的缺陷。Agents利用代理式设计组件动态管理检索策略，迭代改善上下文理解，适应工作流对于复杂任务的需要。</p><p>本文从RAG范式演化与基础原则开始，提供了一个对于Agentic RAG的全面探索。对于Agentic RAG架构细节、下游任务应用，以及强调了在系统扩充、确保道德决策和优化现实应用程序的性能方面的挑战。</p><h2 id="RAG基础"><a href="#RAG基础" class="headerlink" title="RAG基础"></a>RAG基础</h2><p>RAG面临的限制：</p><p>缺乏上下文理解、无法进行多步推理、难以处理复杂任务</p><ol><li>outdated information</li><li>hallucinated responses</li><li>an inability to adapt to dynamic</li><li>real-world scenarios</li></ol><h3 id="Foundations-of-Retrieval-Augmented-Generation"><a href="#Foundations-of-Retrieval-Augmented-Generation" class="headerlink" title="Foundations of Retrieval-Augmented Generation"></a>Foundations of Retrieval-Augmented Generation</h3><div class="note note-danger">            <p>RAG &#x3D; LLM + Information retrieval</p>          </div><p>RAG就是融合LLM的生成能力和实时数据的信息检索能力</p><h3 id="Core-Components-of-RAG"><a href="#Core-Components-of-RAG" class="headerlink" title="Core Components of RAG"></a>Core Components of RAG</h3><ul><li><strong>Retrieveal</strong></li><li><strong>Augmentation</strong></li><li><strong>Generation</strong></li></ul><h2 id="AI-Agent"><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h2><h3 id="Components-of-AI-Agent"><a href="#Components-of-AI-Agent" class="headerlink" title="Components of  AI Agent"></a>Components of  AI Agent</h3><ol><li><strong>LLM (with defined Role and Task)</strong>: LLM用作核心推理引擎</li><li><strong>Memory (Short-Term and Long-Term)</strong>: 维护上下文信息</li><li><strong>Planning (Reflection &amp; Self-Critique)</strong>: 引导推理、拆分复杂任务</li><li><strong>Tools Vector Search, Web Search, APIs, etc.</strong>: 调用外部资源</li></ol><p><img src="/img/fig/Arag01.png"></p><p>Agentic工作流的四个核心模式：</p><ol><li>Reflection - 反省模式</li></ol><p><img src="/img/fig/Arag-reflection.png" alt="Reflection"></p><p>Agent根据输出相应作出自我评估，不断优化输出质量。</p><ol start="2"><li>Planning - 规划模式</li></ol><p><img src="/img/fig/Arag-planning.png" alt="Planning"></p><p>Agent制定任务执行计划，对复杂任务拆分、分配。</p><ol start="3"><li>Tool Use - 工具使用模式</li></ol><p><img src="/img/fig/Arag-tool.png" alt="Tool Use"></p><p>Agent会灵活调用APIs，扩展边界能力，同时调用数据库、数据统计分析工具、可视化工具等。</p><ol start="4"><li>Multi-Agent - 多代理协作模式</li></ol><p><img src="/img/fig/Arag-multiagent.png" alt="Multi-Agent"></p><p>多Agent协作配合，每个Agent负责特定任务，共同完成同一个目标任务。</p><h2 id="Agentic-RAG架构类别"><a href="#Agentic-RAG架构类别" class="headerlink" title="Agentic RAG架构类别"></a>Agentic RAG架构类别</h2><h3 id="Single-Agent-Agentic-RAG-Router"><a href="#Single-Agent-Agentic-RAG-Router" class="headerlink" title="Single-Agent Agentic RAG: Router"></a>Single-Agent Agentic RAG: Router</h3><p>Single-Agent架构是最基础的形式，Single-Agent可落地的功能：</p><ul><li>智能分析用户问题</li><li>选择最合适的信息源</li><li>整合多个数据库的内容</li><li>语义搜索和web搜索</li><li>推荐系统</li></ul><p><img src="/img/fig/single-agent.png"></p><p>Single-Agent架构的数据整合能力强</p><h3 id="Multi-Agent-Agentic-RAG"><a href="#Multi-Agent-Agentic-RAG" class="headerlink" title="Multi-Agent Agentic RAG"></a>Multi-Agent Agentic RAG</h3><p>多个Agent有明确分工：</p><ul><li>Agent 1：负责结构化数据查询</li><li>Agent 2：处理语义搜索</li><li>Agent 3：获取实时信息</li><li>Agent 4：负责给结构化数据查询</li><li>Agent 5：负责个性化推荐</li></ul><p><img src="/img/fig/multi-agent-rag.png"></p><h3 id="Hierarchical-Agentic-RAG-Systems"><a href="#Hierarchical-Agentic-RAG-Systems" class="headerlink" title="Hierarchical Agentic RAG Systems"></a>Hierarchical Agentic RAG Systems</h3><p>层级式架构类似于企业的组织结构，这种架构特别适合处理复杂的研究任务。</p><p><img src="/img/fig/Hierarchical-rag.png"></p><h3 id="Agentic-Corrective-RAG"><a href="#Agentic-Corrective-RAG" class="headerlink" title="Agentic Corrective RAG"></a>Agentic Corrective RAG</h3><p>自纠错架构，该架构引入了智能的质量控制机制。</p><p>相关性评估：确保检索内容的准确性</p><p>查询优化：动态调整搜索策略</p><p>外部知识补充：及时补充缺失信息</p><p>响应合成：生成高质量答案</p><p><img src="/img/fig/Agentic-Corrective-RAG.png"></p><h3 id="Adaptive-Agentic-RAG"><a href="#Adaptive-Agentic-RAG" class="headerlink" title="Adaptive Agentic RAG"></a>Adaptive Agentic RAG</h3><p>自适应架构，该架构能根据问题的复杂度动态调整处理策略</p><p><img src="/img/fig/Adaptive-Agentic-RAG.png"></p><h3 id="Graph-Based-Agentic-RAG"><a href="#Graph-Based-Agentic-RAG" class="headerlink" title="Graph-Based Agentic RAG"></a><strong>Graph-Based Agentic RAG</strong></h3><p>将检索增强生成技术与知识图谱结合起来，提升系统的推理能力</p><p>理解实体间的复杂关联、支持跨领域知识关联、优化知识组织方式</p><p><img src="/img/fig/Agent-G.png"></p><h3 id="Agentic-Document-Workflows-in-Agentic-RAG"><a href="#Agentic-Document-Workflows-in-Agentic-RAG" class="headerlink" title="Agentic Document Workflows in Agentic RAG"></a><strong>Agentic Document Workflows in Agentic RAG</strong></h3><p>文档工作流架构是企业级应用的选择</p><p><img src="/img/fig/agentic-document.png"></p><h2 id="Tool-and-FrameWork"><a href="#Tool-and-FrameWork" class="headerlink" title="Tool and FrameWork"></a>Tool and FrameWork</h2><ul><li>Langchain and LangGraph</li><li>LlamaIndex</li><li>Hugging Face Transformers and Qdrant</li><li>Neo4j and Vector databases</li></ul><h2 id="落地场景"><a href="#落地场景" class="headerlink" title="落地场景"></a>落地场景</h2><ol><li><strong>在教育与个性化学习领域中为不同学习者提供个性化和适应性强的学习体验</strong>。例如设计能够实时检索知识和提供个性化反馈的智能辅导器，根据学生进度和偏好生成定制的教育内容，多智能体系统用于协作学习模拟。</li><li><strong>在客户支持与虚拟助手领域提供与客户查询上下文相关且动态的响应。</strong> 例如构建基于人工智能的虚拟助手，用于实时客户支持；自适应系统，通过学习用户反馈改进响应；多智能体编排处理复杂的多查询互动。</li><li><strong>在多模态工作流程中增强图的应用程序领域处理需要关系理解和多模态数据集成的任务</strong>。例如，基于图的检索系统，用于连接结构化和非结构化数据；在科学研究、知识管理等领域中增强推理工作流程；将文本、图像和结构化数据的洞察综合起来，以生成可操作的输出。</li><li><strong>在以文档为中心的工作流领域自动化涉及文档解析、数据提取和多步骤推理的复杂工作流</strong>。</li></ol>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
      <tag>Agentic RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>寒假阅读书单</title>
    <link href="/2025/01/19/%E5%AF%92%E5%81%87%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95/"/>
    <url>/2025/01/19/%E5%AF%92%E5%81%87%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95/</url>
    
    <content type="html"><![CDATA[<h2 id="2025年第一个月的大部分时间都干了个啥"><a href="#2025年第一个月的大部分时间都干了个啥" class="headerlink" title="2025年第一个月的大部分时间都干了个啥"></a>2025年第一个月的大部分时间都干了个啥</h2><p>2024年12月31号在湖里度过了难忘的跨年晚会，进入2025年1月就完全进入了期末复习周，1月10号完成了期末考试，10-16号留校、回所和导师们汇报完这学期的进度以及后续安排，14号完成了晓飞老师《机器学习》课程的实验报告，16号离校去房山舅舅家住了两天，18号回家。忙忙碌碌地度过了2025年的1月份多半时光，伴随而来的寒假(大概率也是学生时代的最后一个完整寒假)正式开始了。</p><h2 id="趁着最后一个完整的寒假，多读点东西"><a href="#趁着最后一个完整的寒假，多读点东西" class="headerlink" title="趁着最后一个完整的寒假，多读点东西"></a>趁着最后一个完整的寒假，多读点东西</h2><p>论文必须要接着读哈哈哈，按着大老师的部署，继续向下推进任务，2025伴随着Agent的东风，争取做出些东西出来！！</p><p>技术相关：</p><ul><li><p>刘焕勇老师相关文章及推荐论文</p></li><li><p>从零构建向量数据库</p></li></ul><p>多方面：</p><ul><li>影响力</li><li>穷查理宝典</li><li>毛选</li><li>巴拉吉预言</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Attention is all you need</title>
    <link href="/2025/01/15/Attention-is-all-you-need/"/>
    <url>/2025/01/15/Attention-is-all-you-need/</url>
    
    <content type="html"><![CDATA[<div class="note note-danger">            <p>攻坚克难，务必拿下Transformer，彻底搞懂原理</p>          </div><p>参考教程：</p><p>[1] 动手学深度学习</p><p>[2] <a href="https://github.com/datawhalechina/fun-transformer/blob/main/docs/chapter1/introduction.md">Data Whale训练营-手撕Transformer</a></p><h2 id="Transformer名字的由来"><a href="#Transformer名字的由来" class="headerlink" title="Transformer名字的由来"></a>Transformer名字的由来</h2><p>论文的第一作者最喜欢的电影是《变形金刚》</p><div class="note note-success">            <p>论文的创新点：提出了一个新的简单的网络结构——Transformer，完全依赖于注意力机制而省去了卷积和循环</p>          </div><h2 id="注意力"><a href="#注意力" class="headerlink" title="注意力"></a>注意力</h2><p>就我个人而言，注意力对于我的作用就是提高效率，特别是提高做事情的效率。注意力提升了，在学习的时候就非常认真、专注，从而学习新的东西就会非常快。我发现一旦开始“使用”注意力，就能够快速对与外界刺激作出相应的反馈，无论是背诵、记忆、识别、计算，以上种种行为，在我注意力buff加持下都会比平时懒懒散散的状态下作出的反应要高效。</p><p>类比到机器身上，目前借鉴人类的行为模式，机器利用神经网络所从事的两大类主要任务是：运用<strong>视觉思维</strong>的识别与分类、运用<strong>语言思维</strong>的序列理解。对于这两种思维模式，在注意力buff加持下，执行任务的效率会显著提高。</p><p>而设计Transformer架构的主要任务，换言之，设计Transformer架构要解决的核心问题就是利用神经网络去提高机器翻译的准确率，设计的初衷是去解决<strong>机器翻译</strong>任务。</p><p>注意力有自主性和非自主性之分：<strong>非自主性</strong>提示是基于环境中物体的突出性和易见性(比如走在大街上在茫茫人海中一眼就发现了一个漂亮小姐姐…)、<strong>自主性</strong>是受到了认知和意识的控制。</p><h2 id="提问：如何进行机器翻译？—从Seq2Seq到Transformer"><a href="#提问：如何进行机器翻译？—从Seq2Seq到Transformer" class="headerlink" title="提问：如何进行机器翻译？—从Seq2Seq到Transformer"></a>提问：如何进行机器翻译？—从Seq2Seq到Transformer</h2><p>机器翻译是<strong>序列转换模型</strong>的一个核心问题， 其输入和输出都是长度可变的序列。</p><p>论文的摘要中提到了，当下主要的序列转换模型是基于包含一个编码器和一个解码器的复杂卷积神经网络和循环网络。表现最好的模型是使用注意力机制链接编码-解码架构。</p><div class="note note-danger">            <p>那么，编码-解码架构如何进行机器翻译呢？</p>          </div><p>这就不得不学习一下编码-解码架构：</p><p><img src="/./img/fig/encoder-d.jpg" alt="encoder-decoder"></p><p>编码器作用：接受一个长度可变的序列作为输入，将其转换为<strong>固定形式</strong>的编码状态。</p><p>解码器作用：将固定形式的编码状态<strong>映射</strong>到长度可变的变长序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Encoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Decoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br><span class="hljs-comment">#@ EncoderDecoder基类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderDecoder</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,encoder,decoder</span>):<br>    <span class="hljs-built_in">super</span>(EncoderDecoder,<span class="hljs-variable language_">self</span>).__init__()<br>    <span class="hljs-variable language_">self</span>.encoder = encoder<br>    <span class="hljs-variable language_">self</span>.decoder = decoder<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forword</span>(<span class="hljs-params">self,enc_x,dec_x</span>):<br>    enc_output = <span class="hljs-variable language_">self</span>.encoder(enc_x)<br>    dec_state = <span class="hljs-variable language_">self</span>.decoder.init_state(enc_output)<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.decoder(dec_x,dec_state)<br></code></pre></td></tr></table></figure><p>给一个英文的输入序列：“我有一只笔”，首先，编码-解码架构会将这个变长的英语序列编码为一种“状态”，然后对这种状态进行解码，一个词元接着一个词元就生成被翻译的可变长度序列输出：“I have a pen”。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%881%EF%BC%89.PNG" alt="翻译过程"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;bos&gt;：Begin Of Sequence（BOS）<br>&lt;eos&gt;：End Of Sequence（EOS）<br>“BOS I have a pen EOS”，在这里，BOS 和 EOS 分别标记句子的开始和结束。<br></code></pre></td></tr></table></figure><p>在处理可变长度的序列时，使用 BOS 和 EOS 可以减少对 <strong>填充</strong>（<strong>padding</strong>）的依赖，因为模型可以根据这些标记来识别序列的边界。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%882%EF%BC%89.png" alt="seq2seq"></p><p>编码器所做的工作就是把一大段文字浓缩成一个简短的摘要，它把整个输入序列压缩成一个精华的上下文向量。然后，解码器就像一个作家，根据这个摘要重新创作出一篇完整的文章。</p><p>但是，上下文向量是固定维度的，这就导致会造成信息损失，特别是细粒度的丢失。</p><p>我们试图把一本厚厚的大百科全书的内容全部塞进一个张张小的记忆卡片里。Seq2Seq模型在做类似的事情，它需要把整个输入序列的信息压缩成一个固定大小的上下文向量。这就好比你只能记住百科全书的概要，而丢失了很多细致入微的细节。</p><p>同时，Seq2Seq模型有时候就像一个有短期记忆障碍的人，它很难回忆起很久以前发生的事情。序列虽然是变长的，但是长度过大模型往往难以捕捉到序列开始和结束之间的长期依赖关系，就像试图回忆一个长故事的每一个细节一样困难。</p><p>Teacher forcing：就像是考试时老师不断给你提示答案的一种训练方法</p><p>Exposure Bias：模型可能会过分依赖那些训练时的“提示”，而忽略了如何独立解决问题。</p><p><img src="/./img/fig/seq2seq.jpg" alt="简易的翻译流程图"></p><p>在Encoder-Decoder架构中，编码器和解码器之间使用一个<strong>固定长度</strong>的“上下文向量”来传递信息，这就好比通信机制中的“压缩-解压缩”的过程。</p><p>将一张将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 5000X5000 像素的图片也压缩到 100KB，看上去就模糊了。Encoder-Decoder也同样面临着这种困境：当输入的信息过长时(很长的上下文)，就会丢失掉一些信息。</p><p>基于这种缺陷，于是后面便创造出来Attention注意力机制。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><div class="note note-danger">            <p>编码-解码架构的痛点</p>          </div><p><code>长过程依赖</code>：输入句子很长的时候，编码器的理解后会创建一个糟糕的总结“上下文向量”。</p><p><code>梯度消失/爆炸</code>：RNN无法记住较长的句子和序列，LSTM被认为比RNN更能捕捉长依赖关系但是特定情况下容易遗忘。</p><p>那么，经过编码器创建上下文向量时，如何更小程度地保证句子中的信息损失较小呢？</p><p>于是，注意力机制应运而生。Bahdanau等人(2015)提出一个简单但优雅的想法：<strong>不仅在上下文向量中考虑所有的输入词，还可以赋予每个输入词相对重要性。</strong></p><p>注意力就是在编码器隐藏状态中搜索一组可获得最相关信息的位置。</p><p>Attention模型最大的特点就是不再将输入序列Encoder阶段生成<strong>固定长度</strong>的“content vector”，而是编码成一个向量的序列，解决固定长度而产生的“<strong>信息过长，信息丢失</strong>”问题。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%8810%EF%BC%89.png" alt="attention"></p><p>这样以来，输入是序列，输出是序列，而中间的过渡变量仍然是序列。<strong>变化一小步，进步一大步</strong>。</p><p>Attention机制就是<strong>将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。</strong></p><div class="note note-success">            <p>核心</p>          </div><p>Attention的核心工作是：<strong>保持专注，关注重点</strong>，抛给你一张图片，关注有效信息、有效特征；同时，尽可能地减少无效信息、无效特征的干扰。</p><p>Attention的核心思想是：<strong>加权求和</strong></p><blockquote><p>从上面的建模，我们可以大致感受到 Attention 的思路简单，四个字“<strong>带权求和</strong>”就可以高度概括，大道至简。做个不太恰当的类比，人类学习一门新语言基本经历四个阶段：死记硬背（通过阅读背诵学习语法练习语感）-&gt; 提纲挈领（简单对话靠听懂句子中的关键词汇准确理解核心意思）-&gt; 融会贯通（复杂对话懂得上下文指代、语言背后的联系，具备了举一反三的学习能力）-&gt; 登峰造极（沉浸地大量练习）。 这也如同attention的发展脉络，RNN 时代是死记硬背的时期，attention 的模型学会了提纲挈领，进化到 transformer，融汇贯通，具备优秀的表达学习能力，再到 GPT、BERT，通过多任务大规模学习积累实战经验，战斗力爆棚。 要回答为什么 attention 这么优秀？是因为它让模型开窍了，懂得了提纲挈领，学会了融会贯通。 — — 阿里技术</p></blockquote><div class="note note-success">            <p>全局注意力与局部注意力</p>          </div><p>受到计算机视觉的影响，我们将滤波器和感受野的思路引入注意力机制。</p><h2 id="Transformer模型结构"><a href="#Transformer模型结构" class="headerlink" title="Transformer模型结构"></a>Transformer模型结构</h2><p><img src="/./img/fig/transformer.jpg" alt="transformer"></p><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Encoder</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;init函数中实现对于各个层的定义&#x27;&#x27;&#x27;</span><br>        <span class="hljs-built_in">super</span>(EncoderLayer,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 定义注意力层</span><br>        <span class="hljs-variable language_">self</span>.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)<br>        <span class="hljs-comment"># 定义归一化层</span><br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(p=drop_prob)<br>        <br>        <span class="hljs-comment"># 定义前向传播层</span><br>        <span class="hljs-variable language_">self</span>.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)<br>        <span class="hljs-comment"># 定义归一化层</span><br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(p=drop_prob)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        residual_x = x <span class="hljs-comment"># 30 x 200 x 512 </span><br>        x = <span class="hljs-variable language_">self</span>.attention(x,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.dropout1(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.norm1(x + residual_x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        residual_x = x <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.ffn(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.dropout2(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.norm2(x + residual_x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,d_model,ffn_hidden,num_heads,drop_prob,num_layers</span>):<br>        <span class="hljs-built_in">super</span>(Encoder,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)<br>                                      <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layers(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><div class="note note-success">            <p>全局注意力与局部注意力</p>          </div><p>attention计算为什么要除以$\sqrt{d_k}$ ？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob</span>):<br>        <span class="hljs-built_in">super</span>(DecoderLayer,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-string">&#x27;&#x27;&#x27;init函数中实现对于各个层的定义&#x27;&#x27;&#x27;</span><br>        <span class="hljs-variable language_">self</span>.self_attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)<br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(p=drop_prob)<br>        <span class="hljs-variable language_">self</span>.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)<br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(p=drop_prob)<br>        <span class="hljs-variable language_">self</span>.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)<br>        <span class="hljs-variable language_">self</span>.norm3 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout3 = nn.Dropout(p=drop_prob)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y, decoder_mask</span>):<br>        _y = y <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.self_attention(y,mask=decoder_mask) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout1(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 1&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm1(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 1&quot;</span>)<br>        <br>        _y = y <span class="hljs-comment"># 30 x 200 x 512 </span><br>        y = <span class="hljs-variable language_">self</span>.encoder_decoder_attention(x,y,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout2(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 2&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm2(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 2&quot;</span>)<br>        <br>        _y = y <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.ffn(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout3(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 3&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm3(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 3&quot;</span>)<br>        <span class="hljs-keyword">return</span> y <span class="hljs-comment"># 30 x 200 x 512</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SequentialDecoder</span>(nn.Sequential):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *inputs</span>):<br>        x,y,mask = inputs<br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._modules.values():<br>            y = module(x,y,mask) <span class="hljs-comment">## 30 x 200 x 512</span><br>        <span class="hljs-keyword">return</span> y<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob,num_layers=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(Decoder,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)<br>                                          <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y, mask</span>):<br>        <span class="hljs-comment">#x : 30 x 200 x 512</span><br>        <span class="hljs-comment">#y : 30 x 200 x 512</span><br>        <span class="hljs-comment">#mask : 200 x 200</span><br>        y = <span class="hljs-variable language_">self</span>.layers(x,y,mask)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><h4 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入：需要经过softmax计算的序列</span><br><span class="hljs-comment"># 输出：经过softmax计算得到的序列</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (np.exp(x).T / np.<span class="hljs-built_in">sum</span>(np.exp(x), axis=-<span class="hljs-number">1</span>)).T<br><br><span class="hljs-comment"># 点积缩放注意力</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">scaled_dot_production_attention</span>(<span class="hljs-params">q,k,v,mask=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        实现点积缩放注意力机制</span><br><span class="hljs-string">            输入：</span><br><span class="hljs-string">                q: query</span><br><span class="hljs-string">                k: key</span><br><span class="hljs-string">                v: value</span><br><span class="hljs-string">                mask = None</span><br><span class="hljs-string">            输出：</span><br><span class="hljs-string">                out: attention * value</span><br><span class="hljs-string">                attention: attention score</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># q,k,v大小: 30 x 8 x 200 x 64</span><br>    <span class="hljs-comment"># mask大小: 200 x 200</span><br>    d_k = q.size()[-<span class="hljs-number">1</span>] <span class="hljs-comment"># 64</span><br>    scaled = torch.matmul(q, k.transpose(-<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)) / math.sqrt(d_k) <span class="hljs-comment"># 30 x 8 x 200 x 200</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;scaled.size() : <span class="hljs-subst">&#123;scaled.size()&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---ADDing MASK---&quot;</span>)<br>        scaled += mask <span class="hljs-comment"># 200 x 200 Broading add</span><br>    attention = F.softmax(scaled,dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># 30 x 8 x 200 x 200</span><br>    values = torch.matmul(attention,v) <span class="hljs-comment"># 30 x 8 x 200 x 64</span><br>    <span class="hljs-keyword">return</span> values, attention<br></code></pre></td></tr></table></figure><h4 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多头注意力机制   </span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;多头注意力机制的实现&#x27;&#x27;&#x27;</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        MultiHeadAttention input:</span><br><span class="hljs-string">            input_dim: 输入维度</span><br><span class="hljs-string">            d_model: 模型维度</span><br><span class="hljs-string">            num_heads: 注意力头数量</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        MultiHeadAttention output:</span><br><span class="hljs-string">            attention: 注意力分数</span><br><span class="hljs-string">            output: attention * v </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,d_model,num_heads</span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadAttention,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.d_model = d_model <span class="hljs-comment"># 512</span><br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads <span class="hljs-comment"># 8</span><br>        <span class="hljs-variable language_">self</span>.head_dim = d_model // num_heads <span class="hljs-comment"># 64</span><br>        <span class="hljs-comment"># 定义qkv层</span><br>        <span class="hljs-variable language_">self</span>.qkv_layer = nn.Linear(d_model, <span class="hljs-number">3</span> * d_model) <span class="hljs-comment"># 512 x 1536</span><br>        <span class="hljs-comment"># 定义线性层</span><br>        <span class="hljs-variable language_">self</span>.linear_layer = nn.Linear(d_model,d_model) <span class="hljs-comment"># 512 x 512</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,inputs,mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===MultiHeadAttention===&quot;</span>)<br>        batch_size,sequence_length,d_model = inputs.size() <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x.size() = :<span class="hljs-subst">&#123;inputs.size()&#125;</span>&quot;</span>)<br>        qkv = <span class="hljs-variable language_">self</span>.qkv_layer(inputs) <span class="hljs-comment"># 30 x 200 x 1536</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        qkv = qkv.reshape(batch_size,sequence_length,<span class="hljs-variable language_">self</span>.num_heads,<span class="hljs-number">3</span> * <span class="hljs-variable language_">self</span>.head_dim) <span class="hljs-comment"># 30 x 200 x 8 x 192</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv reshape size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        qkv = qkv = qkv.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>) <span class="hljs-comment"># 30 x 8 x 200 x 192</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv permute size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        q,k,v = qkv.chunk(<span class="hljs-number">3</span>,dim = -<span class="hljs-number">1</span>) <span class="hljs-comment"># each chunk: 30 x 8 x 200 x 64</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;q,k,v size() = : <span class="hljs-subst">&#123;q.size()&#125;</span>&quot;</span>)<br>        values,attention = scaled_dot_production_attention(q,k,v,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># attention 30 x 8 x 200 x 200; value 30 x 8 x 200 x 64</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;attention size() = : <span class="hljs-subst">&#123;attention.size()&#125;</span>&quot;</span>)<br>        values = values.reshape(batch_size,sequence_length,<span class="hljs-variable language_">self</span>.num_heads * <span class="hljs-variable language_">self</span>.head_dim) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;values size() = : <span class="hljs-subst">&#123;values.size()&#125;</span>&quot;</span>)<br>        out = <span class="hljs-variable language_">self</span>.linear_layer(values) <span class="hljs-comment"># </span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 位置编码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositonalEncoding</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        PositionalEncoding input:</span><br><span class="hljs-string">            d_model:</span><br><span class="hljs-string">            max_sequence_length:</span><br><span class="hljs-string">        PositionalEncoding output:</span><br><span class="hljs-string">            PE: A matrix with positional score</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,max_sequence_length</span>):<br>        <span class="hljs-built_in">super</span>(PositonalEncoding,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.max_sequence_length = max_sequence_length<br>        <span class="hljs-variable language_">self</span>.d_model = d_model<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        even_i = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-variable language_">self</span>.d_model,<span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>()<br>        denominator = torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">10000</span>,even_i/<span class="hljs-variable language_">self</span>.d_model)<br>        position = torch.arange(<span class="hljs-variable language_">self</span>.max_sequence_length).reshape(<span class="hljs-variable language_">self</span>.max_sequence_length,<span class="hljs-number">1</span>)<br>        even_PE = torch.sin(position / denominator)<br>        odd_PE = torch.cos(position / denominator)<br>        stacked = torch.stack([even_PE,odd_PE],dim = <span class="hljs-number">2</span>)<br>        PE = torch.flatten(stacked,start_dim = <span class="hljs-number">1</span>,end_dim =<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> PE<br></code></pre></td></tr></table></figure><h3 id="前向传播网络"><a href="#前向传播网络" class="headerlink" title="前向传播网络"></a>前向传播网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 前向传播网络   </span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionwiseFeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, hidden, drop_prob=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(PositionwiseFeedForward, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(d_model, hidden) <span class="hljs-comment"># 512 x 2048</span><br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(hidden, d_model) <span class="hljs-comment"># 2048 x 512</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=drop_prob)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===FeedForward===&quot;</span>)<br>        x = <span class="hljs-variable language_">self</span>.linear1(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x linear1 size() = : <span class="hljs-subst">&#123;x.size()&#125;</span>&quot;</span>)<br>        x = <span class="hljs-variable language_">self</span>.relu(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        x = <span class="hljs-variable language_">self</span>.dropout(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        x = <span class="hljs-variable language_">self</span>.linear2(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x linear2 size() = : <span class="hljs-subst">&#123;x.size()&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><h2 id="机器翻译任务实践"><a href="#机器翻译任务实践" class="headerlink" title="机器翻译任务实践"></a>机器翻译任务实践</h2>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GPT</tag>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年度总结</title>
    <link href="/2024/12/31/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    <url>/2024/12/31/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<center><h1>永远相信美好的事情即将发生</h1></center><div class="note note-danger">            <p>2025年 务必谦虚谨慎 艰苦奋斗 戒骄戒躁</p><p>写每一篇文章前都要牢牢把握好一个目的：<strong>写这个东西的动机是什么，要解决什么问题？</strong></p><p>毛主席讲过，做文章是要解决问题的。</p>          </div><center><img src="https://cdnv2.ruguoapp.com/FtwhP5bICelEIUZWWcJbhEnlIXLwv3.heic?imageMogr2/auto-orient/heic-exif/1/format/jpeg/thumbnail/1500x2000%3E/interlace/1" alt="永远相信美好的事情即将发生" style="zoom:37%;" /></center><h2 id="终点和起点，分别和相聚-2024大事记"><a href="#终点和起点，分别和相聚-2024大事记" class="headerlink" title="终点和起点，分别和相聚 - 2024大事记"></a>终点和起点，分别和相聚 - 2024大事记</h2><p>2024年主要经历了两件大事：本科毕业、研究生生涯开启，分别和相聚是这一年的主旋律。</p><p>2024年开始认真经营这个网站，发布了<strong>39篇</strong>博客文章，当然有好几篇文章的内容仍然需要完善修改。</p><p>2024年去的地方是这20多年里走过地方最多的一年：杭州、珠海、广州、澳门、佛山、正定、太原、五台、大同、济南、泰安、天津、上海、雄安新区；从小到大来了这么多次北京，市区里面新解锁的地方有慕田峪长城、雁栖湖、红螺寺、香山、大会堂、国博(这几个地方还真是第一次身临其境去参观)。</p><p>2024年的下半年来到果壳后，结实了很多更加优秀的朋友和老师，对于做事的动机和思维认知又有了一点新的提高。</p><p>2024年带着老妈和老弟去了趟上海，第一次做导游带着家人出去玩。</p><p>2024年奶奶住院，第一次在医院陪床，真实经历了晚上的医院到底是什么样子。</p><h2 id="技术路线更新"><a href="#技术路线更新" class="headerlink" title="技术路线更新"></a>技术路线更新</h2><p>2024年的上半年基本上属于<strong>half - GAP - year</strong>，本科这四年跑的快了，没来得及好好喘口气，今年上半年打算停下来歇一歇，走一走，深呼吸一下。</p><p>2024年下半年来到国科大后慢慢开始确定了研究方向，下半年其实主要就是学了深度学习的原理与使用，大模型领域的相关知识。</p><p>技术路线由原来的<strong>大数据技术路线</strong>逐渐转移、更新转化到<strong>人工智能、大模型开发</strong>上来了。大模型、数据存储、信息检索、多模态大数据分析是需要精进的技术。</p><h2 id="每一天都是崭新的，做行动的巨人"><a href="#每一天都是崭新的，做行动的巨人" class="headerlink" title="每一天都是崭新的，做行动的巨人"></a>每一天都是崭新的，做行动的巨人</h2><p>2024年懒了很多，可能也是和GAP year有关，勤奋和懒惰其实是相对的，不是每天11点睡7点起你就给出努力和勤奋，比起早睡早起，对于时间的利用效率更是关键，当然早睡早起的好习惯绝对是好的。</p><p>长远来看。<strong>系统性思维</strong>要优于<strong>目标性思维</strong>。长期坚持每天规律饮食、规律作息，长远来看要比短期减肥、减脂更有价值。</p><p>2025年希望自己继续保持前本科时代的生活习惯和作风，永远积极向上，做行动的巨人，不拖延。</p><h2 id="想探索的"><a href="#想探索的" class="headerlink" title="想探索的"></a>想探索的</h2><p>想买个相机，好好学一学摄影</p><p>想回所后再买辆公路车，把环线和北京的边边角角都走一遍</p><center><h1>2025年，继续做时间的朋友、做实践的朋友</h1></center><p>觉得罗振宇老师真的厉害，不知不觉的，跨年演讲坚持做了10年了。</p><p>其实我一直关注跨年演讲，吴晓波、罗振宇，包括何帆老师的《变量》系列书籍、吴军老师系列书籍，这些有价值的东西对我的认知和心态都有了很大的改进和提升。</p><p>摘抄里面的一些好的观点，希望用它们贯穿整个2025吧！</p><p>2025，请继续加油！</p><div class="note note-success">            <p>所有来自未来的好消息，都是我们今天努力的模样。</p><p>命运不在人的身上，而在人的周围。</p><p>悲观只是一种看法，乐观却是一种行动。</p><p>“买三送一”：凭借自己的努力多活三年，科技进步再帮助你多活一年</p><p>幸福总量新法则：挣得多不如活得长</p><p>所有的暴躁都是因为能力不够，所有的劳苦都是因为方法不对。</p>          </div><div class="note note-danger">            <p><strong>“发大愿，迈小步，走远路，磕长头，不停顿，不着急”。</strong></p>          </div>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>年度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年12月总结与展望</title>
    <link href="/2024/12/31/12%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/12/31/12%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<center><img src="https://hips.hearstchina.com/hmg-prod/images/%E5%9B%BE%E7%89%871-65af6a5382828.png?resize=640:*" alt="告别2024" style="zoom:90%;" />  </center><h2 id="11月任务回顾"><a href="#11月任务回顾" class="headerlink" title="11月任务回顾"></a>11月任务回顾</h2><p>2024年最后一个月，主要任务就一个：把之前<strong>深度学习</strong>所有相关知识整合、查漏补缺、复现Coding</p><p>今天5号了，这个月的任务除了复习最后的三门课之外，更多的精力要分配给如下任务：</p><ul><li>卷积神经网络学习(还剩VGG、ResNet没有实现)</li><li>语言神经网络学习</li><li>多读经典论文、多复现代码(循环神经网络、Attention、Transformer、BERT掌握)</li><li>精进python、C++(必须精通使用)</li></ul><h2 id="12月完成度"><a href="#12月完成度" class="headerlink" title="12月完成度"></a>12月完成度</h2><p>12与集中在RNN、注意力机制和Transformer复现上</p><ul><li><p>经典的卷积神经网络复现✅</p></li><li><p>语言神经网络学习 - <strong>仍在学习中</strong>，结合NLP期末复习</p></li><li><p>循环神经网络✅</p></li><li><p>Attention✅</p></li><li><p>Transformer✅</p></li><li><p>BERT(集中精力都放在了Transformer上) ❌</p></li></ul><h2 id="1月假期计划安排"><a href="#1月假期计划安排" class="headerlink" title="1月假期计划安排"></a>1月假期计划安排</h2><div class="note note-success">            <p>真正的高手都是把一个程序反复写，每一遍都和前一遍不一样，有提高有改进；做设计要优先于细节完善 —— 杨力祥</p><p>细节实现前，多画图多做设计！</p>          </div><p>假期需要完成的主要任务 - List：</p><ol><li><strong>继续完善</strong> 秋季学期所有的学科笔记与知识梳理&#x2F;真题复现，可选择性的放到CSDN或git上，想着做几个高质量有价值的git仓库或者CSDN文章</li><li><strong>继续完善</strong> C++课程的五子棋程序，争取学会如何在C++编程中使用pytorch，使用神经网络训练五子棋的AI</li><li><strong>继续完善</strong> 大数据作业实验，参考珊瑚书完成对分布式向量数据库的实现(大工程)</li><li><strong>继续完善</strong> 深度学习知识体系，对经典的代码反复研读复现，做一些小应用</li><li><strong>继续完善</strong> Transformer架构复现与应用，认真学一下数据如何在torch上进统一计算与模型训练代码的编写</li><li><strong>继续完善</strong> 博客网站上的全部文章，争取把所有内容做实</li><li><strong>继续学习</strong> 大模型领域的知识与实践：模型的学习(BERT、CLIP、BLIP)和Agentic RAG应用的技能修炼。RAG应用是关乎饭碗型的任务，务必持续学习深入，扎实提升技能！</li><li><strong>继续研读</strong> 领域内的经典论文</li></ol>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习与大数据管理总结复习</title>
    <link href="/2024/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>有题库就不再总结零散的知识点，重点看自己不熟悉的地方</p>          </div><p><strong>过拟合</strong>：一味追求提高对训练数据的预测能力，所选模型复杂度往往会比真实模型高。学习时选择的模型包含参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测的很差的现象。</p><p><strong>K折交叉验证</strong>：随机地将已给数据切分为K个互不相交的大小相同的子集；然后利用K-1个子集的数据训练模型，利用剩下的子集进行测试模拟。将这个过程对可能的K种选择重复进行，最后选出K次测评中平均测试误差最小的模型。</p><p>分类与回归本质上都是一种<strong>映射关系</strong>的建立，本质区别就是分类时离散数据、回归时连续数据。区别：分类强调依据类别标签y对样本x空间的划分，回归强调x与回归值的拟合</p><p>集成学习</p><p>数据维度</p><p>大数据的特征</p><p>数据科学的定义</p><h2 id="数据科学的生命周期"><a href="#数据科学的生命周期" class="headerlink" title="数据科学的生命周期"></a>数据科学的生命周期</h2><p>数据科学的生命周期、特点</p><p>数据科学项目的基本流程和特点</p><p>样本选择</p><p>批判性思维</p><p>判断逻辑谬误</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>数据采集的主要技术</p><p>缺失、冗余、噪声的数据处理</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>LDA(线性鉴别分析)：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到这条直线上，再根据投影点来判断新样本的类别。</p><p>决策树 - ID3</p><p>朴素贝叶斯</p><p>Logistics回归</p><p>k-近邻</p><p>SVM：支持向量机是一个分类算法，它的目标就是确定一个超平面，从而将不同类别的数据分隔开以达到分类的目标。当训练数据线性可分的时候，通过硬间隔最大化，学习一个线性分类器，即线性可分的支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，即软间隔支持向量机；当训练数据线性不可分时，通过使用核函数技巧或者软间隔最大化学习非线性支持向量机。</p><p>集成学习</p><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>划分聚类</p><p>层次聚类</p><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>一元</p><p>多元</p><p>非线性</p><p>逐步</p><h2 id="关联规则分析"><a href="#关联规则分析" class="headerlink" title="关联规则分析"></a>关联规则分析</h2><p>支持度、置信度</p><p>数据降维</p><p>PCA</p><p>时间序列分析</p><p>图数据挖掘</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 机器学习.周志华</p><p>[2] 统计学习方法. 李航</p><p>[3] 中国科学院大学机器学习课程资料(秋).  周晓飞</p><p>[4] 中国科学院大学机器学习课程资料(秋). 沙瀛、周川</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>期末总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自然语言处理基础总结复习</title>
    <link href="/2024/12/25/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/12/25/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="理解自然语言处理领域的基础技术与核心任务"><a href="#理解自然语言处理领域的基础技术与核心任务" class="headerlink" title="理解自然语言处理领域的基础技术与核心任务"></a>理解自然语言处理领域的基础技术与核心任务</h2><blockquote><p>什么是实体？怎样理解序列？</p><p>Entity vs Object；实体是“某种概念”的实例化，比如一个具体的地名、人名，我觉着实体其实和面向对象中对象的概念相似，就是一个<strong>东西</strong>，一个<strong>具体的东西</strong>。</p><p>广义的序列我认为可以理解为一串文字序列，一串经过嵌入后的文本向量。</p><p>词性标注：基于机器学习的方法，往往需要对分词后的词进行词性标注，词性一般包括动词、形容词、名词。标注的目的就是表征词的隐含状态，隐含状态构成了状态转移序列。</p><p>命名实体识别：一般指从文本中识别具有特定类别的实体，比如人名、地名、机构名、专有名词等。</p></blockquote><h3 id="核心任务"><a href="#核心任务" class="headerlink" title="核心任务"></a>核心任务</h3><div class="note note-success">            <p>六大核心任务与对应的11个子任务</p><p>文章顺序按照<strong>自顶向下</strong>进行总结梳理与复习</p>          </div><h4 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h4><ul><li>实体识别与抽取</li><li>实体消岐</li><li>关系抽取</li><li>事件抽取</li></ul><h4 id="归纳类别"><a href="#归纳类别" class="headerlink" title="归纳类别"></a>归纳类别</h4><p>文本分类</p><h4 id="情感识别"><a href="#情感识别" class="headerlink" title="情感识别"></a>情感识别</h4><p>情感分类</p><h4 id="回答问题"><a href="#回答问题" class="headerlink" title="回答问题"></a>回答问题</h4><ul><li>问答系统</li><li>机器阅读理解</li><li>智能对话</li></ul><h4 id="凝练内容"><a href="#凝练内容" class="headerlink" title="凝练内容"></a>凝练内容</h4><p>自动文摘</p><h4 id="语言翻译"><a href="#语言翻译" class="headerlink" title="语言翻译"></a>语言翻译</h4><p>机器翻译</p><h3 id="自然语言处理基础技术"><a href="#自然语言处理基础技术" class="headerlink" title="自然语言处理基础技术"></a>自然语言处理基础技术</h3><ul><li>分本分类</li></ul><p>利用计算机对大量文本的文档按照分类标准实现<strong>自动归档</strong>，目标就是将一段文字或者一个文本自动分配给一个或多个类别。</p><ul><li>分本匹配</li></ul><p>判断两个文本之间的“<strong>相似度</strong>”问题，并进一步地可以将两个文本进行对齐。</p><ul><li>序列标注</li></ul><p>对于给定的一个输入序列，使用模型对这个序列的每一个位置标注一个<strong>相应的标签</strong>。序列标注问题主要包括词性标注、语义角色标注、信息抽取(命名实体识别、关系抽取、事件抽取、信息集成)。</p><ul><li>序列生成</li></ul><p>根据输入内容序列和历史序列信息来<strong>预测序列的生成</strong>，序列标注可以看作是文本分类的推广。</p><h3 id="基于NLP领域内基础技术延伸出的基本模型方法与技术"><a href="#基于NLP领域内基础技术延伸出的基本模型方法与技术" class="headerlink" title="基于NLP领域内基础技术延伸出的基本模型方法与技术"></a>基于NLP领域内基础技术延伸出的基本模型方法与技术</h3><p>文本处理技术：分词技术、嵌入技术</p><p>文本分类：TF - IDF、CNN、RNN、LSTM</p><p>文本匹配：CBOW、Skip-gram</p><p>序列标注：隐马尔可夫模型HMM、维特比算法、RNN+CRF</p><p>序列生成：Seq2Seq、Attention、Transformer、BERT</p><h2 id="自然语言处理范式变迁"><a href="#自然语言处理范式变迁" class="headerlink" title="自然语言处理范式变迁"></a>自然语言处理范式变迁</h2><p>第一范式：特征工程+算法(概率统计时代)</p><p>第二范式：自动获取特征(表示学习)端到端分类</p><p>第三范式：预训练+精调范式</p><p>第四范式：预训练+提示+预测范式</p><p>第五范式：大模型</p><h2 id="基础概念复习"><a href="#基础概念复习" class="headerlink" title="基础概念复习"></a>基础概念复习</h2><h3 id="机器学习与深度学习基础相关"><a href="#机器学习与深度学习基础相关" class="headerlink" title="机器学习与深度学习基础相关"></a>机器学习与深度学习基础相关</h3><p>梯度消失的原因：在误差反向传播时，每一层都要乘以激活函数的导数，若导数值小于1，将导致误差越来越小，直至消失；如果导数过大，则会发生梯度爆炸。</p><p>激活函数的作用：增强网络表达能力，加入非线性因素</p><p>随机梯度下降与梯度下降区别：每次参数更新的数据量不同</p><h3 id="神经网络相关"><a href="#神经网络相关" class="headerlink" title="神经网络相关"></a>神经网络相关</h3><p>卷积层的作用：通过卷积操作减少参数</p><p>池化层的作用：通过下采样减少网络规模</p><p>全连接层：将池化层单元扁平化</p><p>RNN的参数训练方法：BPTT</p><h3 id="语言模型相关"><a href="#语言模型相关" class="headerlink" title="语言模型相关"></a>语言模型相关</h3><p>什么是词向量：将词转换为稠密向量的分布式表示</p><p>词向量的特征：语义相似的词，其词向量在空间距离上更相近</p><p>CBOW是训什么：用上下文预测中心词</p><p>Skip-Gram是训什么：用中心词预测上下文</p><p>Attention中普通模式和键值对模式异同：普通模式将输入信息直接作为键和值，键值对模式下“键”用于计算注意力分布，“值”用于生成最终的输出。</p><p>Prompt的核心思想是：将输入文本转换为模型可以理解的形式</p><p>什么是软注意力：选择的信息是所有输入信息在注意力分布下的期望</p><p>什么是硬注意力：只关注到<strong>某一个位置上</strong>的信息</p><p>什么是BERT：BERT通过使用双向Transformer编码器来捕捉文本中单词的上下文信息。</p><p>什么是BART：BART采用了一种编码-解码架构，编码器部分是双向的，类似于BERT，可以捕捉输入文本的上下文信息；解码器部分是自回归的，类似于传统的序列生成模型（如GPT），可以逐步生成输出序列。</p><p>GPT使用的是Transformer的什么：解码器部分</p><p>CRF的作用：CRF能够对整个序列进行全局优化，而不是单独地对每个元素进行标注，会考虑整个序列的上下文信息，使得标注结果更加一致和合理。</p><p>生成任务中常用的评价指标：BLUE、ROUGE</p><h2 id="2024年秋学期-期末考试真题回忆"><a href="#2024年秋学期-期末考试真题回忆" class="headerlink" title="2024年秋学期-期末考试真题回忆"></a>2024年秋学期-期末考试真题回忆</h2><p>家人们，能记住多少算多少哈哈哈哈~</p><p>考试时间：2025年1月6日</p><h3 id="选择题-10-1"><a href="#选择题-10-1" class="headerlink" title="选择题 10 * 1"></a>选择题 10 * 1</h3><ol><li><p>CBOW训练的是什么</p></li><li><p>skip_gram训练的是什么</p></li><li><p>激活函数有什么作用：增强网络表达能力</p></li><li><p>指针网络的作用：不仅可以用于机器翻译任务，还可以用于其他任务</p></li><li><p>词向量之间相似的原因解释</p></li><li><p>Attention的输入形式</p></li><li><p>top-k采样、贪心解码相关</p></li><li><p>Chatgpt相关的问题不正确的是：每次使用都是先微调</p></li></ol><p>还有两个记不起来了~</p><h3 id="填空题-10-2"><a href="#填空题-10-2" class="headerlink" title="填空题 10*2"></a>填空题 10*2</h3><p>   1.机器学习三要素</p><ol start="2"><li><p>池化层作用是什么</p></li><li><p>什么是硬注意力</p></li><li><p>BART中为什么要使用位置嵌入</p></li><li><p>Seq2Seq序列生成模型的三种类型</p></li><li><p>CRF的作用是什么</p></li><li><p>生成任务中常用的评价指标是什么</p></li><li><p>RNN训练方法是什么</p></li><li><p>文本匹配常用的建模方法</p></li><li><p>什么是曝光偏差</p></li></ol><h3 id="简答题-5-6"><a href="#简答题-5-6" class="headerlink" title="简答题 5*6"></a>简答题 5*6</h3><ol><li><p>什么是BPE算法？简要概述其工作原理</p></li><li><p>Transformer实现并行处理所用到的技术有哪些？</p></li><li><p>神经网络语言语言模型存在的问题？使用RNN+词向量的方式是如何解决这些问题的</p></li><li><p>简述第二、三、四、五范式特点和应用场景</p></li><li><p>简述传统的事件抽取任务，并给出传统的事件抽取的建模任务有哪些？具体实现步骤</p></li></ol><h3 id="计算题-10-1"><a href="#计算题-10-1" class="headerlink" title="计算题 10*1"></a>计算题 10*1</h3><p>维特比算法</p><h3 id="设计题-12-1"><a href="#设计题-12-1" class="headerlink" title="设计题 12*1"></a>设计题 12*1</h3><p>题干记得是：“这是一家正宗的俄式西餐，味道浓厚，环境很好”</p><p>(1) 写出属性情感输出</p><p>(2) 设计一个情感属性关联模型，解释其原理，画出模型图</p><h3 id="设计题-18-1"><a href="#设计题-18-1" class="headerlink" title="设计题 18*1"></a>设计题 18*1</h3><p>(1) 检索式问答系统包含哪几个部分？作用是什么？</p><p>(2) 设计一个检索式问答系统，并解释其原理</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>期末总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG综述一览</title>
    <link href="/2024/12/18/RAG%E7%BB%BC%E8%BF%B0%E4%B8%80%E8%A7%88/"/>
    <url>/2024/12/18/RAG%E7%BB%BC%E8%BF%B0%E4%B8%80%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p>参考文章：<a href="https://mp.weixin.qq.com/s/lbeeblGYvb1JeC0gW4AE9g">https://mp.weixin.qq.com/s/lbeeblGYvb1JeC0gW4AE9g</a></p><p>2025.02.23的自我批评：其实这篇综述文章早就应该写出来，但是就是一直拖着，结果一直到现在才想着去完善。最主要的原因是因为DataWhale发布了一篇推文，全面的梳理了RAG范式发展，与当初自己的思路不谋而合。所以，这学期一定不要拖延，想的事情就赶紧去做！！！</p><h2 id="三篇关键的综述"><a href="#三篇关键的综述" class="headerlink" title="三篇关键的综述"></a>三篇关键的综述</h2><p>[1] ZHAO P, ZHANG H, YU Q, 等. Retrieval-Augmented Generation for AI-Generated Content: A Survey[A&#x2F;OL]. arXiv, 2024[2024-06-21]. <a href="http://arxiv.org/abs/2402.19473">http://arxiv.org/abs/2402.19473</a>.　</p><p>[2] GAO Y, XIONG Y, GAO X, 等. Retrieval-Augmented Generation for Large Language Models: A Survey[A&#x2F;OL]. arXiv, 2024[2024-03-27]. <a href="http://arxiv.org/abs/2312.10997">http://arxiv.org/abs/2312.10997</a>.</p><p>[3] FAN W, DING Y, NING L, 等. A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models[A&#x2F;OL]. arXiv, 2024[2024-06-17]. <a href="http://arxiv.org/abs/2405.06211">http://arxiv.org/abs/2405.06211</a>. </p><p>三篇综述把RAG的三个基本范式，<strong>朴素RAG</strong>、<strong>高级RAG</strong>、<strong>模块化RAG</strong>介绍的非常清楚明了。</p><h2 id="范式发展"><a href="#范式发展" class="headerlink" title="范式发展"></a>范式发展</h2><div class="note note-success">            <p>Naive RAG -》Advanced RAG -》 Modular RAG -》Graph RAG -》 Agentic RAG</p>          </div><p>RAG技术起源于论文：LEWIS P, PEREZ E, PIKTUS A, 等. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks[A&#x2F;OL]. arXiv, 2021[2025-01-27]. <a href="http://arxiv.org/abs/2005.11401%EF%BC%9BRAG%E9%A6%96%E5%85%88%E8%A2%AB%E7%94%A8%E4%BA%8ELLMs%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E6%9D%A5%E5%A2%9E%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%9A%8F%E5%90%8E%E8%A2%AB%E7%94%A8%E4%BA%8E%E5%BE%AE%E8%B0%83%E4%B8%8E%E6%8E%A8%E7%90%86%E4%BB%BB%E5%8A%A1%E4%B8%AD%E3%80%82">http://arxiv.org/abs/2005.11401；RAG首先被用于LLMs的预训练阶段来增强语言模型，随后被用于微调与推理任务中。</a></p><h3 id="三范式阶段"><a href="#三范式阶段" class="headerlink" title="三范式阶段"></a>三范式阶段</h3><p>三种基本范式的演化历程是：NaiveRAG -》AdvancedRAG -〉 ModularRAG，2024年微软开源的GraphRAG开启了RAG的第四种范式，融合了知识图谱；在2024年下半年AgenticRAG出现，是前四种范式的集大成者，且具有自适应性。</p><p>NaiveRAG主要由三部分组成:索引、检索和生成。AdvancedRAG围绕检索前和检索后提出了多种优化策略，其过程与朴素RAG相似，仍然遵循链状结构。ModularRAG继承和发展了以前的范式，整体上展示了更大的灵活性。</p><ul><li><p>Advanced RAG代表作：<a href="http://arxiv.org/abs/2405.13576%EF%BC%8C%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E3%80%81%E6%A8%A1%E5%9D%97%E5%8C%96%E7%9A%84%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%8C%85FlashRAG%EF%BC%8C%E5%B7%A5%E5%85%B7%E5%8C%85%E5%AE%9E%E7%8E%B0%E4%BA%86">http://arxiv.org/abs/2405.13576，提出了一个高效、模块化的开源工具包FlashRAG，工具包实现了</a> 12 种先进的 RAG 方法，并收集和整理了 32 个基准数据集。</p></li><li><p>Modular RAG代表作：<a href="http://arxiv.org/abs/2407.21059%EF%BC%8C%E5%B0%86%E5%A4%8D%E6%9D%82%E7%9A%84RAG%E7%B3%BB%E7%BB%9F%E5%88%86%E8%A7%A3%E4%B8%BA%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%A8%A1%E5%9D%97%E5%92%8C%E4%B8%93%E9%97%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E7%AC%A6%EF%BC%8C%E5%BD%A2%E6%88%90%E4%B8%80%E4%B8%AA%E9%AB%98%E5%BA%A6%E5%8F%AF%E9%87%8D%E9%85%8D%E7%BD%AE%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82">http://arxiv.org/abs/2407.21059，将复杂的RAG系统分解为独立的模块和专门的操作符，形成一个高度可重配置的框架。</a></p></li></ul><p><img src="/img/fig/RAG-fanshi.png" alt="三种RAG范式之间的比较"></p><h3 id="Graph-RAG"><a href="#Graph-RAG" class="headerlink" title="Graph RAG"></a>Graph RAG</h3><p>图本身具有 “<strong>节点由边连接</strong> “的特性，可以编码大量的异构和关系信息，使其成为 RAG 在大量实际应用中的丰富资源。</p><p>针对LLMs在缺乏特定领域知识、实时更新信息和专有知识时遇到的问题，提出了GraphRAG作为一种解决上述问题的框架，通过结合<strong>图数据库</strong>中的结构化信息来增强LLMs的输出。</p><ul><li>LIANG L, SUN M, GUI Z, 等. <strong>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</strong>. arXiv, 2024[2024-11-12]. <a href="https://arxiv.org/abs/2409.13731">https://arxiv.org/abs/2409.13731</a>.</li></ul><p>论文引入了一个专业领域知识服务框架，称为知识增强生成（KAG）。为了充分发挥**知识图谱（KG）**和**向量检索**的优势，通过五个关键方面双向增强大型语言模型（LLM）和知识图谱（KG），从而提高生成和推理性能：（1）LLM友好的知识表示；（2）知识图谱和原始块之间的相互索引；（3）逻辑形式引导的混合推理引擎；（4）知识与语义推理的对齐；（5）KAG的模型能力增强。</p><p>一个专为大型语言模型（LLM）设计的友好型知识表示框架。LLMFriSPG将实例与概念区分开来，通过概念实现与 LLMs 的对接。SPG 的属性被划分为知识区和信息区，也就是静态区和动态区，分别与具有严格模式约束的决策专业知识以及具有开放信息表示的文档检索索引知识相兼容。</p><h3 id="Agentic-RAG"><a href="#Agentic-RAG" class="headerlink" title="Agentic RAG"></a>Agentic RAG</h3><p>Agentic RAG 将 ReACT 的推理能力与 Agent 的任务执行能力相结合，创建一个动态和自适应的系统。与遵循固定管道的传统 RAG 不同，Agentic RAG 通过使用 ReACT 根据用户查询的上下文动态协调 Agent，引入了灵活性。</p><ul><li>SINGH A, EHTESHAM A, KUMAR S, 等. Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG[A&#x2F;OL]. arXiv, 2025[2025-01-26]. <a href="http://arxiv.org/abs/2501.09136">http://arxiv.org/abs/2501.09136</a>.</li></ul><p>论文从RAG范式演化与基础原则开始，提供了一个对于Agentic RAG的全面探索。文章总结出了Agentic工作流的四个核心模式与七大Agentic-RAG架构。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-agent RAG Systems调研报告</title>
    <link href="/2024/12/18/Multi-agent-RAG-Systems%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/"/>
    <url>/2024/12/18/Multi-agent-RAG-Systems%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="Agentic-RAG调研报告—探索搭建适配多数据源的Multi-agent-RAG-System"><a href="#Agentic-RAG调研报告—探索搭建适配多数据源的Multi-agent-RAG-System" class="headerlink" title="Agentic RAG调研报告—探索搭建适配多数据源的Multi-agent RAG System"></a>Agentic RAG调研报告—探索搭建适配多数据源的Multi-agent RAG System</h1><h2 id="Multi-agent-RAG架构设计"><a href="#Multi-agent-RAG架构设计" class="headerlink" title="Multi-agent RAG架构设计"></a>Multi-agent RAG架构设计</h2><h3 id="应用场景与需求"><a href="#应用场景与需求" class="headerlink" title="应用场景与需求"></a>应用场景与需求</h3><p>根据不同的知识库与应用特点构造了不同的<strong>查询引擎</strong>，它们面向不同的领域知识，采用了不同的索引(比如VectorIndex与GraphIndex)。我们需要给使用者(可能是人或应用)提供一致性体验，他们无须关心后端使用的真实查询引擎是哪一个，只需要输入问题即可获得正确的答案。$^{[1]}$</p><p><img src="/./img/fig/agentrag.jpg" alt="Multi-agent RAG"></p><h3 id="适配多数据源"><a href="#适配多数据源" class="headerlink" title="适配多数据源"></a><strong>适配多数据源</strong></h3><p>计算机中的文档：<code>TXT文档</code>、<code>PDF文档</code>、<code>Office文档</code></p><p>存储在多种类型数据库中的结构化数据、半结构化数据：<code>MySQL</code>、<code>PostageSQL</code>、<code>Redis</code>、<code>MongDB</code></p><p>存储在图数据库中图数据：<code>Neo4j</code></p><p>存储在向量数据库中的向量数据：<code>Milvus</code>、<code>Chroma</code>、<code>Elastic</code></p><h2 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h2><ol><li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook/">https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook/</a> - OpenAI Agent + Query Engine Experimental Cookbook: 融合了vector_tool和sql_tool</li><li><a href="https://github.com/karthik-codex/Autogen_GraphRAG_Ollama">https://github.com/karthik-codex/Autogen_GraphRAG_Ollama</a> - GraphRAG + AutoGen + Ollama + Chainlit UI &#x3D; Local Multi-Agent RAG Superbot：这个应用程序将 <strong>GraphRAG</strong> 与 <strong>AutoGen</strong> 代理集成在一起，由 Ollama 的本地LLM提供支持，可实现免费的离线嵌入和推理。</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 基于大模型的RAG应用开发与优化—构建企业级LLM应用. 严灿平. 中国工信出版集团</p><p>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407123&idx=2&sn=80ddf495e6c08af3355d8bf4a5109328&chksm=83836f06b4f4e610a99aa86e9882b7416c8f3ff42519c7aaff7d99a8ce936493e43a623f6e83&scene=178&cur_album_id=3276284267911856128#rd">RAG行业问答博客观点开源整理：从RAG评估、应用参考、开源框架到操作实践总结</a></p><p>[3] <a href="https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file">https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file</a></p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agentic RAG with LlamaIndex</title>
    <link href="/2024/12/10/Agentic-RAG-with-LlamaIndex/"/>
    <url>/2024/12/10/Agentic-RAG-with-LlamaIndex/</url>
    
    <content type="html"><![CDATA[<h2 id="传统RAG"><a href="#传统RAG" class="headerlink" title="传统RAG"></a>传统RAG</h2><p>作为一个首次接触RAG的开发者，简单理解RAG就是给LLM外挂一个知识库。</p><p>可笑的是记得第一次开组会给两位导师将MMLMs RAG System的综述博客，讲完以后导师问我如何通俗理解RAG，能不能举一个例子。我停顿了3秒，尴尬地说讲不出来。</p><p>讲不出实际的应用案例，说明你对这个领域内的知识、方法还不够理解。</p><p>老师随后说：LLM知道杨桂淼是谁吗？我说：不知道。老师接着说：如何事先已经把杨桂淼的基本信息都告诉了LLM，然后你在针对杨桂淼对LLM进行提问，那它可以回答出你的问题吗？我说：应该可以了。确实，外挂一个数据库，里面存放着所有有关杨桂淼信息的网页，当我query杨桂淼的相关问题时，LLM根据知识库返回的信息进行生产响应，这就是RAG。</p><div class="note note-success">            <p>RAG最核心的思想：给LLM补充外部知识以提高生成质量</p>          </div><p><img src="/./img/fig/rag2.jpg" alt="RAG-经典"></p><p>RAG应用整体上划分为两大阶段：</p><ol><li>数据索引：知识-加载-分割-嵌入-索引</li><li>数据查询：根据索引进行检索前处理-检索-检索后处理-生成</li></ol><p>RAG pipeline主要包括三大组件：</p><ol><li>外部知识源</li><li>提示模版</li><li>生成模型</li></ol><h2 id="微调与RAG的区别"><a href="#微调与RAG的区别" class="headerlink" title="微调与RAG的区别"></a>微调与RAG的区别</h2><p>还是以一个例子说明</p><p>如果把LLM比做一个优秀的学生正在参加一门考试，微调与RAG的区别如下：</p><p><strong>RAG</strong>：考试过程中给他提供这门课的全部参考资料，要求他现做现用，并争取写出答案</p><p><strong>微调</strong>：在考试前一天对他进行辅导，让他成为某个领域&#x2F;学科的专家，然后再去参加考试</p><h2 id="RAG与具有理解超长上下文能力的LLM"><a href="#RAG与具有理解超长上下文能力的LLM" class="headerlink" title="RAG与具有理解超长上下文能力的LLM"></a>RAG与具有理解超长上下文能力的LLM</h2><p>大模型的上下文窗口(content window)正在以不可思议的速度增大，在超长上下文中精确检索出特定位置的某个事实性知识(<code>大海捞针的能力</code>)也带来了一个争议性话题：<strong>如果未来能够把几百个文档一股脑式地全部丢进大模型的上下文窗口，并且大模型能够在其中检索出事实性知识，那么我们还有必要做外部索引与检索给大模型进行知识外挂吗？</strong></p><p>RAG的核心功能仍然是检索。事实上，大量实验测试表明，受限于主流LLM所依赖的底层Transformer架构的基本原理，当前理解超长上下文的能力并不像宣传的那样出色。</p><p>RAG与具有理解超长上下文能力LLM之间的trade-off是相辅相成、取长补短式发展的，不应该是谁取代谁。</p><h2 id="RAG演化三范式"><a href="#RAG演化三范式" class="headerlink" title="RAG演化三范式"></a>RAG演化三范式</h2><p><img src="/./img/fig/rag1.png" alt="RAG演化三范式"></p><h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Naive RAG中包含最基本的三个阶段：索引、检索、生成，是基本的链式结构。</p><h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>Advanced RAG在Naive RAG的基础之上，增加了Pre-Retrieval和Post-Retrivel两个过程。</p><p><em>Pre-retrieval process</em>：这个阶段主要关注的任务是<strong>优化索引结构</strong>和<strong>初始查询</strong>。优化索引的目标是提高被索引内容的质量，查询优化是使用户初始问题更加清晰并适用于检索任务。</p><p><em>Post-Retrieval Process</em>：这个阶段的主要任务包括chunk重排和内容压缩。重排检索信息取重定位最相关的内容对于边缘提示是关键策略。将所有相关文档全部喂给LLM会造成信息过载，聚焦最有用的信息并突出关键信息段压缩文本。</p><h3 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h3><p>改变了前两种模式的链式结构，转而模块化结构。增加了相似度检索的<strong>search module</strong>，通过微调重定义了<strong>retriever</strong>。同时支持序列化处理和end-to-end端到端的处理</p><h2 id="Agentic-RAG"><a href="#Agentic-RAG" class="headerlink" title="Agentic RAG"></a>Agentic RAG</h2><blockquote><p>Agentic RAG describes an AI agent-based implementation of RAG，<strong>the core of Agentic RAG is Agent.</strong></p><p>Agentic RAG—your super-smart digital library assistant!</p></blockquote><p>主动式RAG是基于AI agent的RAG实现，将AI agent 整合到RAG pipeline中，编排组件以执行简单的信息检索任务和生成之外的操作，以克服non-agentic pipeline的限制。</p><p>Agentic RAG更典型的是在检索组件的pipeline中使用agents。特别的，检索组件通过使用可访问不同检索器工具(tools)的检索agetns而成为agentic。</p><p>Agentic RAG应用推理的检索场景：</p><ol><li>Decide whether to retrieve information or not</li><li>Decide which tool to use to retrieve relevant information</li><li>Formulate the query itself</li><li>Evaluate the retrieved context and decide whether it needs to re-retrieve.</li></ol><p>Agentic RAG的主要工作流程：</p><div class="note note-success">            <p>Document Processing—Creating Embedings—Indexing—Retrieval—Agent based Reasoning—Generation</p>          </div><p><img src="/./img/fig/rag3.jpg" alt="Agentic RAG"></p><h3 id="Router-Engine"><a href="#Router-Engine" class="headerlink" title="Router Engine"></a>Router Engine</h3><p>在基础的RAG pipeline中，LLM仅仅用于整合生成答案。而在Agentic RAG中，LLM会被作为Agent使用在检索、增强、生成的各个阶段。</p><blockquote><p>Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific <strong>database searches</strong>, or <strong>merging different information streams</strong>. —RAG:A survey</p></blockquote><h3 id="Tool"><a href="#Tool" class="headerlink" title="Tool"></a>Tool</h3><p>检索组件的工具选择：</p><ul><li>Vector search Engine</li><li>Web search</li><li>Calculator</li><li>software(Email、chat programs)</li></ul><p>Agentic RAG属于RAG范式中的第三阶段：Modular RAG。</p><p>从文档的读取开始，处理流程基本上都是模块化层层封装好各个组件，已实现查询引擎的功能。</p><p>无论是引擎模块还是预测模块，最终都是将底层方法层层封装为<strong>工具方法</strong>，在将所有的工具方法整合为最终的引擎模块。</p><div class="note note-success">            <p>文档读取-文档分割-索引建立<strong>VectorIndex(nodes)</strong>-使用索引作为查询引擎<strong>index.as_query_engine</strong>-调用<strong>QueryEngineTool</strong>方法将查询引擎进行工具封装-将工具整合为LLM的<strong>predict_and_call</strong>模块进行生产应答</p>          </div><h2 id="Implementing-a-Basic-Multi-Document-Agentic-RAG"><a href="#Implementing-a-Basic-Multi-Document-Agentic-RAG" class="headerlink" title="Implementing a Basic Multi-Document Agentic RAG"></a>Implementing a Basic Multi-Document Agentic RAG</h2><p>课程源：<a href="https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex">deeplearning.ai短期课</a></p><p>博客链接：<a href="https://www.analyticsvidhya.com/blog/2024/09/multi-document-agentic-rag-using-llamaindex/">https://www.analyticsvidhya.com/blog/2024/09/multi-document-agentic-rag-using-llamaindex/</a></p><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>必要的库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> SimpleDirectoryReader<br><span class="hljs-keyword">from</span> llama_index.core.node_parser <span class="hljs-keyword">import</span> SentenceSplitter<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> Settings<br><span class="hljs-keyword">from</span> llama_index.llms.openai <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> llama_index.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbedding<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> SummaryIndex, VectorStoreIndex<br><span class="hljs-keyword">from</span> llama_index.core.tools <span class="hljs-keyword">import</span> QueryEngineTool<br><span class="hljs-keyword">from</span> llama_index.core.query_engine.router_query_engine <span class="hljs-keyword">import</span> RouterQueryEngine<br><span class="hljs-keyword">from</span> llama_index.core.selectors <span class="hljs-keyword">import</span> LLMSingleSelector<br></code></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>首先将文档转换为nodes插入到DocumentStore数据库中。</p><h3 id="对同一数据建立SummaryIndex和VectorIndex"><a href="#对同一数据建立SummaryIndex和VectorIndex" class="headerlink" title="对同一数据建立SummaryIndex和VectorIndex"></a>对同一数据建立SummaryIndex和VectorIndex</h3><h3 id="对索引建立查询引擎"><a href="#对索引建立查询引擎" class="headerlink" title="对索引建立查询引擎"></a>对索引建立查询引擎</h3><h3 id="定义检索增强的路由查询引擎"><a href="#定义检索增强的路由查询引擎" class="headerlink" title="定义检索增强的路由查询引擎"></a>定义检索增强的路由查询引擎</h3>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>GPT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal RAG and Prompt Compression</title>
    <link href="/2024/12/08/Multimodal-RAG-and-Prompt-Compression/"/>
    <url>/2024/12/08/Multimodal-RAG-and-Prompt-Compression/</url>
    
    <content type="html"><![CDATA[<h1 id="Multimodal-RAG与Prompt-compression再梳理"><a href="#Multimodal-RAG与Prompt-compression再梳理" class="headerlink" title="Multimodal RAG与Prompt compression再梳理"></a>Multimodal RAG与Prompt compression再梳理</h1><h2 id="Multimodal-RAG-System-Work-Flow"><a href="#Multimodal-RAG-System-Work-Flow" class="headerlink" title="Multimodal RAG System Work Flow"></a>Multimodal RAG System Work Flow</h2><p>多模态RAG的完整工作流如下图所示：</p><p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-woXqKviW7JlYQlmaecD0QpvsiyX4PNJoAB6d4wEm_y4Ks6qXuJNTZ61KTOyJ1v8xi1k4CujEisnRNAWM4LIpgQs_HREGihbG3bquz1y-YXsNt84k7Y9jWy_Vip6yLPiIfNmnTAyI86xvEzbYFSpu69g?key=jCBGNvykPV6Ye85DC-Uxrg" alt="Multimodal RAG System"></p><p>再次梳理一遍多模态RAG系统的工作流程：</p><div class="note note-danger">            <p>1.多模态信息输入—2.特征提取—3.特征融合—4.信息检索—5.上下文构建—6.生成答案</p>          </div><p>1.加载所有文档，并使用类似unstructure.io的文档加载器提取文本块、图像和表格。</p><p>2.如有必要，将HTML表格转换为markdown；他们通常对LLM非常有效</p><p>3.将每个文本块、图像和表格传递到GPT-4o等多模式LLM中，并获得详细的摘要。</p><p>4.将摘要存储在向量数据库中，将原始文档片段存储在Redis等文档数据库中</p><p>5.使用多向量检索器使用公共document_id连接两个数据库，以识别哪个摘要映射到哪个原始文档块。</p><p>6.将这个多向量检索系统与GPT-4o等多模态LLM连接起来。</p><p>7.查询系统，并根据与查询类似的摘要，获取原始文档片段，包括表格和图像，作为上下文。</p><p>8.使用上述上下文，使用多模态LLM生成问题的答案。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><h4 id="多模态输入"><a href="#多模态输入" class="headerlink" title="多模态输入"></a>多模态输入</h4><p>在VQA系统中，<strong>文本+图像</strong>是常见的输入形式。</p><p>多模态RAG系统处理的是多种类型的数据，主要包括：</p><ul><li><strong>图像（Image）</strong>：可以是用户上传的图片、视频帧等视觉内容。</li><li><strong>文本（Text）</strong>：用户的问题、描述、上下文信息等语言内容。</li></ul><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p><strong>图像特征提取</strong>：</p><ul><li>使用预训练的卷积神经网络（如ResNet、EfficientNet）或专门的多模态模型（如CLIP、VisualBERT）提取图像特征。</li><li>图像特征通常表示为高维向量或特征图。</li></ul><p><strong>文本特征提取</strong>：</p><ul><li>使用基于Transformer的模型（如BERT、RoBERTa）或其他词嵌入技术（如Word2Vec、GloVe）将文本转换为嵌入向量。</li><li>文本特征表示为上下文相关的向量表示。</li></ul><h3 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h3><h4 id="联合表示（Joint-Representation）"><a href="#联合表示（Joint-Representation）" class="headerlink" title="联合表示（Joint Representation）"></a><strong>联合表示（Joint Representation）</strong></h4><p>将图像特征和文本特征结合成一个统一的表示，以便模型能够同时考虑两种模态的信息。</p><ul><li>拼接式融合（Concatenation）：<ul><li>直接将图像特征和文本特征向量拼接在一起。</li></ul></li><li>交互式融合（Interactive Fusion）：<ul><li>使用自注意力机制（Self-Attention）或跨注意力机制（Cross-Attention）在Transformer编码器中融合多模态特征。</li><li>例如，VisualBERT将图像区域特征和文本嵌入在同一序列中，通过自注意力层实现交互。</li></ul></li></ul><h4 id="联合嵌入（Joint-Embedding）"><a href="#联合嵌入（Joint-Embedding）" class="headerlink" title="联合嵌入（Joint Embedding）"></a><strong>联合嵌入（Joint Embedding）</strong></h4><p>通过多模态对齐技术（如对比学习）将不同模态的特征投射到同一语义空间中，使得跨模态检索和匹配更为有效。</p><ul><li>CLIP（Contrastive Language-Image Pretraining）：<ul><li>将图像和文本通过独立的编码器映射到共同的嵌入空间，通过对比损失优化使相关的图像和文本特征靠近，不相关的则远离。</li></ul></li><li>BLIP&#x2F;BLIP2</li></ul><h3 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h3><h4 id="检索模块"><a href="#检索模块" class="headerlink" title="检索模块"></a><strong>检索模块</strong></h4><p>基于融合后的多模态特征，从<strong>知识库</strong>或<strong>文档库</strong>中检索与输入相关的信息。</p><ul><li>相似度计算：<ul><li>计算输入特征与知识库中每个文档或片段的相似度（如余弦相似度）。</li></ul></li><li>索引与搜索：<ul><li>使用高效的向量索引技术（如FAISS、Annoy）加速大规模相似度搜索。</li></ul></li></ul><h4 id="检索结果过滤与排序"><a href="#检索结果过滤与排序" class="headerlink" title="检索结果过滤与排序"></a><strong>检索结果过滤与排序</strong></h4><ul><li>过滤：<ul><li>排除与查询不相关或低质量的检索结果。</li></ul></li><li>排序：<ul><li>根据相似度得分对检索结果进行排序，选取最相关的若干条信息作为生成答案的依据。</li></ul></li></ul><h3 id="上下文构建"><a href="#上下文构建" class="headerlink" title="上下文构建"></a>上下文构建</h3><h4 id="上下文整合"><a href="#上下文整合" class="headerlink" title="上下文整合"></a><strong>上下文整合</strong></h4><div class="note note-success">            <p>上下文的内容：检索到的相关信息与原始输入（图像和问题文本）进行整合后的内容</p>          </div><ul><li>上下文增强：<ul><li>将检索到的文本信息与输入问题结合，提供更丰富的背景知识。</li></ul></li><li>图文结合：<ul><li>将检索到的文本信息与图像特征共同输入生成模型，辅助生成更准确的答案。</li></ul></li></ul><h4 id="Prompt-Compression（提示压缩）"><a href="#Prompt-Compression（提示压缩）" class="headerlink" title="Prompt Compression（提示压缩）"></a><strong>Prompt Compression（提示压缩）</strong></h4><p>为了提高模型处理效率和响应速度，对输入提示进行压缩，将多模态信息以更紧凑的形式传递给生成模型。</p><ul><li>联合特征表示：<ul><li>使用预训练模型生成的联合嵌入向量作为压缩后的提示。</li></ul></li><li>模板化设计：<ul><li>设计高效的prompt模板，将图像和文本信息结构化地嵌入其中。</li></ul></li></ul><h3 id="答案生成"><a href="#答案生成" class="headerlink" title="答案生成"></a>答案生成</h3><h4 id="生成模块"><a href="#生成模块" class="headerlink" title="生成模块"></a><strong>生成模块</strong></h4><p>利用生成模型（如GPT、Mistral）基于整合后的上下文信息生成自然语言答案。</p><ul><li>自回归生成（Autoregressive Generation）：<ul><li>模型逐步生成答案，每一步依赖于之前生成的内容和上下文。</li></ul></li><li>条件生成（Conditional Generation）：<ul><li>生成过程受到输入条件（如问题文本和图像特征）的约束，确保生成内容与输入相关。</li></ul></li></ul><h4 id="生成优化"><a href="#生成优化" class="headerlink" title="生成优化"></a><strong>生成优化</strong></h4><ul><li>多样性与一致性：<ul><li>通过调整生成参数（如温度、顶层采样）控制答案的多样性和一致性。</li></ul></li><li>格式控制：<ul><li>使用特定的prompt模板引导生成内容的格式和风格。</li></ul></li></ul><p><img src="https://go2heart.github.io/echosight/static/images/teaser.png" alt="多模态RAG"></p><h2 id="Prompt的具体形式"><a href="#Prompt的具体形式" class="headerlink" title="Prompt的具体形式"></a>Prompt的具体形式</h2><h3 id="Multimodal-RAG-System博客里面的Prompt格式"><a href="#Multimodal-RAG-System博客里面的Prompt格式" class="headerlink" title="Multimodal RAG System博客里面的Prompt格式"></a><a href="https://www.analyticsvidhya.com/blog/2024/09/guide-to-building-multimodal-rag-systems/">Multimodal RAG System</a>博客里面的Prompt格式</h3><p>博客中prompt构建函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">multimodal_prompt_function</span>(<span class="hljs-params">data_dict</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Create a multimodal prompt with both text and image context.</span><br><span class="hljs-string">    This function formats the provided context from `data_dict`, which contains</span><br><span class="hljs-string">    text, tables, and base64-encoded images. It joins the text (with table) portions</span><br><span class="hljs-string">    and prepares the image(s) in a base64-encoded format to be included in a </span><br><span class="hljs-string">    message.</span><br><span class="hljs-string">    The formatted text and images (context) along with the user question are used to</span><br><span class="hljs-string">    construct a prompt for GPT-4o</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    formatted_texts = <span class="hljs-string">&quot;\n&quot;</span>.join(data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;texts&quot;</span>])<br>    messages = []<br>    <br>    <span class="hljs-comment"># Adding image(s) to the messages if present</span><br>    <span class="hljs-keyword">if</span> data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;images&quot;</span>]:<br>        <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;images&quot;</span>]:<br>            image_message = &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image_url&quot;</span>,<br>                <span class="hljs-string">&quot;image_url&quot;</span>: &#123;<span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">f&quot;data:image/jpeg;base64,<span class="hljs-subst">&#123;image&#125;</span>&quot;</span>&#125;,<br>            &#125;<br>            messages.append(image_message)<br>    <br>    <span class="hljs-comment"># Adding the text for analysis</span><br>    text_message = &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>,<br>        <span class="hljs-string">&quot;text&quot;</span>: (<br>            <span class="hljs-string">f&quot;&quot;&quot;You are an analyst tasked with understanding detailed information </span><br><span class="hljs-string">                and trends from text documents,</span><br><span class="hljs-string">                data tables, and charts and graphs in images.</span><br><span class="hljs-string">                You will be given context information below which will be a mix of </span><br><span class="hljs-string">                text, tables, and images usually of charts or graphs.</span><br><span class="hljs-string">                Use this information to provide answers related to the user </span><br><span class="hljs-string">                question.</span><br><span class="hljs-string">                Do not make up answers, use the provided context documents below and </span><br><span class="hljs-string">                answer the question to the best of your ability.</span><br><span class="hljs-string">                </span><br><span class="hljs-string">                User question:</span><br><span class="hljs-string">                <span class="hljs-subst">&#123;data_dict[<span class="hljs-string">&#x27;question&#x27;</span>]&#125;</span></span><br><span class="hljs-string">                </span><br><span class="hljs-string">                Context documents:</span><br><span class="hljs-string">                <span class="hljs-subst">&#123;formatted_texts&#125;</span></span><br><span class="hljs-string">                </span><br><span class="hljs-string">                Answer:</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>        ),<br>    &#125;<br>    messages.append(text_message)<br>    <span class="hljs-keyword">return</span> [HumanMessage(content=messages)]<br><br></code></pre></td></tr></table></figure><p>经过<code>multimodal_prompt_function</code>处理后的信息被整合为一个messages列表：messages &#x3D; [image_message,text_message]。</p><p>构建出的Prompt主要包含两部分：图像信息+文本信息</p><p>假设检索到的上下文包含两个文本段落和一张图像，用户问题为“Tell me detailed statistics of the top 5 years with largest wildfire acres burned”</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;image_url&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;image_url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...&quot;</span><span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;image_url&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;image_url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...&quot;</span><span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;text&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;You are an analyst tasked with understanding detailed information and trends from text documents, data tables, and charts and graphs in images. You will be given context information below which will be a mix of text, tables, and images usually of charts or graphs. Use this information to provide answers related to the user question. Do not make up answers, use the provided context documents below and answer the question to the best of your ability.\n\nUser question:\nTell me detailed statistics of the top 5 years with largest wildfire acres burned\n\nContext documents:\n[Text from first document]\n[Text from second document]\n\nAnswer:&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><h3 id="EchoSight论文里的Prompt格式"><a href="#EchoSight论文里的Prompt格式" class="headerlink" title="EchoSight论文里的Prompt格式"></a>EchoSight论文里的Prompt格式</h3><h4 id="最终的Prompt格式"><a href="#最终的Prompt格式" class="headerlink" title="最终的Prompt格式"></a>最终的Prompt格式</h4><p><strong>Context + Question</strong>：<code>prompt</code> 格式通常包括上下文和问题：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">&lt;维基百科文章或section&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>具体例子：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">Paris</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">capital</span> <span class="hljs-string">of</span> <span class="hljs-string">France,</span> <span class="hljs-string">located</span> <span class="hljs-string">in</span> <span class="hljs-string">the</span> <span class="hljs-string">northern</span> <span class="hljs-string">part</span> <span class="hljs-string">of</span> <span class="hljs-string">the</span> <span class="hljs-string">country.</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">What</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">capital</span> <span class="hljs-string">of</span> <span class="hljs-string">France?</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><h4 id="带图像的prompt"><a href="#带图像的prompt" class="headerlink" title="带图像的prompt"></a>带图像的prompt</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image features:</span> <span class="hljs-string">&lt;图像的特征向量或描述&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>可以将图像特征转化为文本描述：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">This</span> <span class="hljs-string">is</span> <span class="hljs-string">an</span> <span class="hljs-string">image</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">cat</span> <span class="hljs-string">sitting</span> <span class="hljs-string">on</span> <span class="hljs-string">a</span> <span class="hljs-string">couch</span> <span class="hljs-string">next</span> <span class="hljs-string">to</span> <span class="hljs-string">a</span> <span class="hljs-string">window</span> <span class="hljs-string">with</span> <span class="hljs-string">sunlight</span> <span class="hljs-string">streaming</span> <span class="hljs-string">in.</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">What</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">cat</span> <span class="hljs-string">doing?</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>也可以将图像通过特征提取模型转换为特征向量，以特定格式嵌入到 <strong>prompt</strong> 中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image features:</span> [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-string">...</span>]  <span class="hljs-comment"># 图像特征向量</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><h4 id="图像-文本-Prompt-格式-联合表示形式"><a href="#图像-文本-Prompt-格式-联合表示形式" class="headerlink" title="图像+文本 Prompt 格式-联合表示形式"></a>图像+文本 Prompt 格式-联合表示形式</h4><p>将图像特征和文本信息结合成一个联合输入，可以使用 CLIP 这样的多模态模型将图像和文本映射到相同的嵌入空间</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image and Text features:</span> <span class="hljs-string">&lt;联合特征表示&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>CLIP、VisualBERT、FLAVA等模型是基于 <strong>多模态数据联合训练</strong> 的。这些模型已经通过海量的图像-文本配对数据进行训练，能够在 <strong>文本</strong> 和 <strong>图像</strong> 之间建立语义连接。</p><p>Transformer模型的 <strong>自注意力机制</strong>（Self-attention）能够有效地对输入的不同部分进行加权，理解其中的关系。当我们输入 <strong>图像特征</strong> 和 <strong>问题文本</strong> 时，模型通过注意力机制结合这两部分信息，从而产生推理。</p><p>GPT、Mistral等非常擅长处理长文本和复杂的上下文关系。通过 <strong>自回归生成</strong> 或 <strong>条件生成</strong>，模型会基于已知的上下文信息生成答案。</p><h2 id="RAG链构建"><a href="#RAG链构建" class="headerlink" title="RAG链构建"></a>RAG链构建</h2><p>RAG链构建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">multimodal_rag = (<br>        &#123;<br>            <span class="hljs-string">&quot;context&quot;</span>: itemgetter(<span class="hljs-string">&#x27;context&#x27;</span>),<br>            <span class="hljs-string">&quot;question&quot;</span>: itemgetter(<span class="hljs-string">&#x27;input&#x27;</span>),<br>        &#125;<br>            |<br>        RunnableLambda(multimodal_prompt_function)<br>            |<br>        chatgpt<br>            |<br>        StrOutputParser()<br>)<br></code></pre></td></tr></table></figure><ol><li>使用<code>itemgetter</code>从输入数据中提取<code>&#39;input&#39;</code>（用户的问题）。</li><li>通过<code>RunnableLambda</code>运行<code>multimodal_prompt_function</code>，将提取的<code>context</code>和<code>question</code>传入，生成多模态prompt。</li><li>生成的prompt传递给<code>chatgpt</code>模型（假设为GPT-4模型的实例）进行回答生成。</li><li>使用<code>StrOutputParser</code>将模型生成的回答解析为字符串格式。</li></ol><p>信息检索模块构建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">retrieve_docs = (itemgetter(<span class="hljs-string">&#x27;input&#x27;</span>)<br>                    |<br>                retriever_multi_vector<br>                    |<br>                RunnableLambda(split_image_text_types))<br></code></pre></td></tr></table></figure><ol><li>使用<code>itemgetter</code>从输入数据中提取<code>&#39;input&#39;</code>（用户的问题）。</li><li><code>retriever_multi_vector</code>使用多向量检索技术（如基于CLIP的多模态向量检索）从知识库中检索与查询相关的文档和图像。</li><li>通过<code>RunnableLambda(split_image_text_types)</code>将检索到的内容分为文本和图像两类，便于后续处理。</li></ol><p>整合RAG链与检索模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)<br>                                               .assign(answer=multimodal_rag)<br>)<br></code></pre></td></tr></table></figure><ol><li>使用<code>RunnablePassthrough.assign</code>将<code>retrieve_docs</code>的输出赋值给<code>context</code>键。</li><li>使用<code>.assign</code>将先前定义的<code>multimodal_rag</code>链赋值给<code>answer</code>键。</li><li>通过这种方式，最终的<code>multimodal_rag_w_sources</code>链能够同时处理检索到的上下文和生成的答案。</li></ol>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学循环神经网络</title>
    <link href="/2024/12/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/12/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>由于pytorch已经将三个循环神经网络、LSTM进行了高度的封装，所以本节不再去关注网络结构的设计和复现，而是重点关注如何在自己设计的网络结构中将RNN和LSTM融入进去以实现特等的任务。</p><p>卷积神经网络是借鉴人类视觉的思想，教会计算机识别东西；从循环神经网络开始，我们的核心任务就是教会计算机理解序列数据。</p><p>人类并不是每时每刻都从头开始思考。 当我们阅读这篇文章时，会根据对前面单词的理解来理解每个单词。</p><h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络-RNN"></a>循环神经网络-RNN</h2><p>循环神经网络是神经网络的一种，为什么取名字要叫做循环呢？</p><p>就是因为它的神经元输出会在下一个时间步作为反馈进行输入，使整个网络具有处理<strong>序列数据</strong>的能力。</p><p>一个复杂的系统包含若干的最小单元组件，每个最小单元组件都会重复自身动作从而构成整个系统的运转，“循环”变应运而生。</p><p>普通的神经网络处理的是独立同分布数据，层与层之间就是简单的前馈链接关系，输入和输出长度是固定的。</p><p>而循环神经网络具有记忆能力，这种记忆能力体现在上层神经元的输出会作为下层神经元的输入，可以处理时序数据，而且输入输出长度不固定。<br>$$<br>h_{t}&#x3D;tanh(h_{t-1}W_{h}+x_{t}W_{x}+b)<br>$$<br>$$<br>&#x3D; tanh(h_{t-1}W_{hh}+b_{hh}+x_{t}W_{ih}+b_{ih})<br>$$</p><p>模型输入：input_size,hidden_size</p><p>模型输出：output_size,ht</p><p><img src="/img/fig/rnn.jpg" alt="RNN"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,input_size, hidden_size, num_layers=<span class="hljs-number">1</span>, batch_first=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br>        <span class="hljs-variable language_">self</span>.input_size = input_size<br>        <span class="hljs-variable language_">self</span>.bidirectional = <span class="hljs-literal">False</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.W_ih = nn.Parameter(torch.rand(<span class="hljs-variable language_">self</span>.input_size, <span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.W_hh = nn.Parameter(torch.rand(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.b_ih = nn.Parameter(torch.zeros(<span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.b_hh = nn.Parameter(torch.zeros(<span class="hljs-variable language_">self</span>.hidden_size))<br>        <br>    <span class="hljs-comment">#前向传播的两个参数分别为x_t(输入变量)和h_prev(隐藏变量)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x_t,h_prev=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># part 1: torch.matmul(x_t, self.W_ih)</span><br>        <span class="hljs-comment"># x_t包含多个时间步，形状为[batch_size, time_steps_num, input_dim]</span><br>        <span class="hljs-comment"># W_ih形状为[input_dim, hidden_size]</span><br>        <span class="hljs-comment"># torch.matmul(x_t, self.W_ih) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-comment"># part 2: torch.matmul(h_prev, self.W_hh)</span><br>        <span class="hljs-comment"># h_prev 形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-comment"># W_hh形状为[hidden_size, hidden_size]</span><br>        <span class="hljs-comment"># torch.matmul(h_prev, self.W_hh) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-keyword">if</span> h_prev == <span class="hljs-literal">None</span>:<br>             h_prev = torch.zeros( x_t.size(<span class="hljs-number">0</span>), <span class="hljs-variable language_">self</span>.hidden_size)<br>        output = torch.tanh(torch.matmul(x_t, <span class="hljs-variable language_">self</span>.W_ih) + <span class="hljs-variable language_">self</span>.b_ih + torch.matmul(h_prev, <span class="hljs-variable language_">self</span>.W_hh) + <span class="hljs-variable language_">self</span>.b_hh)<br>        <span class="hljs-keyword">return</span> output,output[:,-<span class="hljs-number">1</span>,:].unsqueeze(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><div class="note note-success">            <p>RNN的梯度消失&#x2F;爆炸</p>          </div><p>RNN的梯度消失或爆炸问题主要体现在“循环”过程中隐藏变量的迭代计算过程中。</p><ul><li>隐藏变量的参数&gt;1: 每迭代一次都会呈现幂律型增长，经过ReLU激活后会趋向于正无穷</li><li>隐藏变量的参数&lt;1: 每迭代一次都会呈现幂律型衰退，经过ReLU激活后始终等于0</li></ul><h2 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络-LSTM"></a>长短期记忆网络-LSTM</h2><p>网络的输入：隐藏变量H、记忆C、输入序列X</p><p>网络的输出：输出序列Y、隐藏变量H、新的记忆C</p><p><img src="/img/fig/lstm.jpg" alt="LSTM"></p><p>参数初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_paramaters</span>(<span class="hljs-params">vocab_size,hidden_units</span>):<br>    std = <span class="hljs-number">0.01</span><br>    input_units = output_units = vocab_size<br>    <br>    <span class="hljs-comment">#正态分布</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape) * std<br>    <br>    <span class="hljs-comment">#LSTM_cell_weights</span><br>    forget_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    input_gate_weights = normal((input_units+hidden_units,hidden_units))<br>    output_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    c_tilda_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    <br>    <span class="hljs-comment"># 偏置项</span><br>    forget_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    input_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    output_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    c_tilda_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    <br>    <span class="hljs-comment">#输出层参数</span><br>    hidden_output_weights = normal((hidden_units, output_units))<br>    output_bias = torch.zeros((<span class="hljs-number">1</span>, output_units))<br>    <br>    <span class="hljs-comment"># 将所有参数添加到字典</span><br>    paramaters = &#123;<br>        <span class="hljs-string">&#x27;fgw&#x27;</span>: forget_gate_weights,<br>        <span class="hljs-string">&#x27;igw&#x27;</span>: input_gate_weights,<br>        <span class="hljs-string">&#x27;ogw&#x27;</span>: output_gate_weights,<br>        <span class="hljs-string">&#x27;cgw&#x27;</span>: c_tilda_gate_weights,<br>        <span class="hljs-string">&#x27;fgb&#x27;</span>: forget_gate_bias,<br>        <span class="hljs-string">&#x27;igb&#x27;</span>: input_gate_bias,<br>        <span class="hljs-string">&#x27;ogb&#x27;</span>: output_gate_bias,<br>        <span class="hljs-string">&#x27;cgb&#x27;</span>: c_tilda_gate_bias,<br>        <span class="hljs-string">&#x27;how&#x27;</span>: hidden_output_weights,<br>        <span class="hljs-string">&#x27;ob&#x27;</span>: output_bias<br>    &#125;<br>    <br>    <span class="hljs-comment"># 设置 requires_grad=True 以启用梯度计算</span><br>    <span class="hljs-comment"># 确保所有参数在反向传播中能够计算梯度</span><br>    <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> paramaters.values():<br>        para.requires_grad_(<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-keyword">return</span> paramaters<br></code></pre></td></tr></table></figure><p>网络结构定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LSTM</span>(<span class="hljs-params">inputs,state,params</span>):<br>    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,<br>     W_hq, b_q] = params <span class="hljs-comment">#各个门的参数</span><br>    (H,C) = state <span class="hljs-comment">#隐藏参数和记忆参数</span><br>    outputs = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs:<br>        I = torch.sigmoid((x @ W_xi) + (H @ W_hi) + b_i) <span class="hljs-comment">#输入门</span><br>        F = torch.sigmoid((x @ W_xf) + (H @ W_hf) + b_f) <span class="hljs-comment">#遗忘门</span><br>        O = torch.sigmoid((x @ W_xo) + (H @ W_ho) + b_o) <span class="hljs-comment">#输出门</span><br>        C_tilda = torch.tanh((x @ W_xc) + (H @ W_hc) + b_c) <span class="hljs-comment">#候选记忆</span><br>        C = F * C + I * C_tilda<br>        H = O * torch.tanh(C)<br>        Y = (H @ W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs,dim=<span class="hljs-number">0</span>),(H,C)<br></code></pre></td></tr></table></figure><p>对于每个LSTM门，它的输出是通过如下方式计算的：</p><ul><li>假设当前的输入向量为 xt&#x3D;[x1,x2,x3,x4]（维度是 <code>input_units = 4</code>），上一时刻的隐藏状态为 $h_{t-1} &#x3D; [h_1, h_2, h_3]$（维度是 <code>hidden_units = 3</code>）。</li><li>我们将这两个向量拼接起来，得到一个向量 [x1,x2,x3,x4,h1,h2,h3]，维度是7。</li><li>然后，我们将这个7维的向量与权重矩阵相乘，得到一个维度为3的向量（即隐藏状态的维度）。这个3维的向量就是该门的输出（例如，遗忘门的输出），它会被用来更新LSTM的状态。</li></ul><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>传统的循环神经网络（RNN）在处理长序列数据时常常遇到梯度消失或梯度爆炸的问题，这限制了它们捕捉长期依赖关系的能力。为了解决这些问题，长短期记忆网络（LSTM）被提出，它通过引入门控机制来控制信息的流动，从而有效缓解了梯度消失问题。GRU模型则是在LSTM的基础上进一步简化和发展的，它由Cho等人在2014年提出，旨在简化LSTM的结构，同时保持类似的性能。</p><p>GRU通过合并LSTM中的遗忘门和输入门为更新门，并引入重置门，同时合并单元状态和隐藏状态，使得模型更为简洁，训练速度更快。这种结构上的简化并没有牺牲性能，GRU在很多任务中的表现与LSTM相当，甚至在某些情况下更优。因此，GRU因其高效和简洁的特性，在自然语言处理、语音识别、时间序列预测等多个领域得到了广泛的应用。</p><p><img src="https://zh.d2l.ai/_images/gru-3.svg" alt="GRU"></p><p>参数初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device)*<span class="hljs-number">0.01</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">three</span>():<br>        <span class="hljs-keyword">return</span> (normal((num_inputs, num_hiddens)),<br>                normal((num_hiddens, num_hiddens)),<br>                torch.zeros(num_hiddens, device=device))<br><br>    W_xz, W_hz, b_z = three()  <span class="hljs-comment"># 更新门参数</span><br>    W_xr, W_hr, b_r = three()  <span class="hljs-comment"># 重置门参数</span><br>    W_xh, W_hh, b_h = three()  <span class="hljs-comment"># 候选隐状态参数</span><br>    <span class="hljs-comment"># 输出层参数</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></table></figure><p>模型定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gru</span>(<span class="hljs-params">inputs,state,params</span>):<br>    W_xr,W_xz,W_xh,b_z,b_h,b_r,W_hr,W_hz,W_hh,W_hq,b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)<br>        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)<br>        H_tilda = torch.tanh((X @ W_xh) + ((R * H)@W_hh) + b_h)<br>        H = Z * H + (<span class="hljs-number">1</span> - Z) * H_tilda<br>        Y = H @ W_hq + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs,dim=<span class="hljs-number">0</span>),(H,)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年11月总结与展望</title>
    <link href="/2024/12/05/11%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/12/05/11%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="消失的半个月，难得喘口气"><a href="#消失的半个月，难得喘口气" class="headerlink" title="消失的半个月，难得喘口气"></a>消失的半个月，难得喘口气</h2><p>折腾了半个月的分布式文件系统期末结课结课实验，终于汇报结束了。</p><p>第15周了，很多课程都是实验节课，最近半个月实验压力也别大。但是好在昨天把大数据的实验和C++的实验都完成了，周六还有一场英语考试，紧张刺激的15周就终于要迎来胜利的曙光了！</p><p>这半个月主要在忙这些事，科研搁置了很多。今天开始，我要把失去的，都补回来哈哈哈哈！！</p><p><code>最后一个月的工作重心转移到MLLMs RAG prompt与技术学习上来。</code></p><h2 id="做难事，收获颇丰"><a href="#做难事，收获颇丰" class="headerlink" title="做难事，收获颇丰"></a>做难事，收获颇丰</h2><p>本科之前从来没有接触过分布式，Java编程也是菜的一批…</p><p>其实这个大实验很早就着手准备了，但是一直理解不了架构，导致一拖再拖。最后的一周冲刺，每天晚上干到一点多，这种感觉前所未有。</p><p>马老师说过：梦想还是要有的，万一实现了呢</p><p><img src="/img/life/file.jpg" alt="难忘的下午"></p><p>上个月的主线任务完成度不错：务必11月要执行的-<strong>带着把程序写成东西的思想，疯狂Coding</strong>！！！！</p><p>11月完成的任务：</p><ol><li>循环神经网路、attention机制、Transformer架构基础知识理解(但是又快忘了😂)</li><li>三个卷积神经网络结构的编写</li><li>Java分布式文件系统项目(大部头费时费力收获颇丰)</li><li>C++五子棋程序(大部头费时费力收获颇丰)</li></ol><h2 id="切入"><a href="#切入" class="headerlink" title="切入"></a>切入</h2><p>要快速上手一个方向或者一个小领域，一般来说需要掌握一些基础知识（最基本的概念和原理）、前沿进展（近几年的研究方向与热点）和特定的工具（例如编程语言、框架）。</p><div class="note note-success">            <p>思考：深度学习领域基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p><p>思考：自然语言处理领域的基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p><p>思考：大模型领域基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p>          </div><h2 id="客观分析一下自己目前的水平"><a href="#客观分析一下自己目前的水平" class="headerlink" title="客观分析一下自己目前的水平"></a>客观分析一下自己目前的水平</h2><div class="note note-success">            <p>革命尚未成功，同志还需努力</p>          </div><p>机器学习基础：吴恩达机器学习课程刷完，理解基本概念</p><p>深度学习：小土堆pytorch教程刷完，李沐的实战教程一直在断断续续看不系统。经典的网络架构梳理出来了，但是对于架构细节与实现还是<strong>眼高手低</strong>的状态。</p><p>NLP：这学期听了自然语言处理这门课，但是吸收效果一般，尚未开始系统学习。</p><h2 id="12月任务"><a href="#12月任务" class="headerlink" title="12月任务"></a>12月任务</h2><div class="note note-danger">            <p>不要吝惜代码，实现，虽然不是必要的，会很大程度上加深你的理解。</p>          </div><p>2024年最后一个月，主要任务就一个：把之前<strong>深度学习</strong>所有相关知识整合、查漏补缺、复现Coding</p><p>今天5号了，这个月的任务除了复习最后的三门课之外，更多的精力要分配给如下任务：</p><ul><li>卷积神经网络学习(还剩VGG、ResNet没有实现)</li><li>语言神经网络学习</li><li>多读经典论文、多复现代码(循环神经网络、Attention、Transformer、BERT掌握)</li><li>精进python、C++(必须精通使用)</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM训练常用操作-从训练到崩溃</title>
    <link href="/2024/11/22/LLM%E8%AE%AD%E7%BB%83%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB/"/>
    <url>/2024/11/22/LLM%E8%AE%AD%E7%BB%83%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="Conda相关"><a href="#Conda相关" class="headerlink" title="Conda相关"></a>Conda相关</h2><h3 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">创建虚拟环境(指定python版本)</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda create --name echosight python=3.10<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">激活虚拟环境</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda activate echosight<br>(echosight) root@I1dc83206c700201ce5:~/Echo/EchoSight#<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">退出虚拟环境</span><br>(echosight) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda deactivate echosight<br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight#<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看虚拟环境列表</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda env list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除虚拟环境</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda remove --name echosight --all<br><br><br></code></pre></td></tr></table></figure><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p>引用自ACwing笔记：</p><h3 id="git基本概念"><a href="#git基本概念" class="headerlink" title="git基本概念"></a>git基本概念</h3><p>工作区：仓库的目录。工作区是独立于各个分支的。</p><p>暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。</p><p>版本库：存放所有已经提交到本地仓库的代码版本。</p><p>版本结构：树结构，树中每个节点代表一个代码版本。</p><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">git config --global user.name xxx：设置全局用户名，信息记录在~/.gitconfig文件中<br>git config --global user.email xxx@xxx.com：设置全局邮箱地址，信息记录在~/.gitconfig文件中<br>git init：将当前目录配置成git仓库，信息记录在隐藏的.git文件夹中<br>git add XX：将XX文件添加到暂存区<br>git add .：将所有待加入暂存区的文件加入暂存区<br>git rm --cached XX：将文件从仓库索引目录中删掉<br>git commit -m &quot;给自己看的备注信息&quot;：将暂存区的内容提交到当前分支<br>git status：查看仓库状态<br>git diff XX：查看XX文件相对于暂存区修改了哪些内容<br>git log：查看当前分支的所有版本<br>git reflog：查看HEAD指针的移动历史（包括被回滚的版本）<br>git reset --hard HEAD^ 或 git reset --hard HEAD~：将代码库回滚到上一个版本<br>git reset --hard HEAD^^：往上回滚两次，以此类推<br>git reset --hard HEAD~100：往上回滚100个版本<br>git reset --hard 版本号：回滚到某一特定版本<br>git checkout — XX或git restore XX：将XX文件尚未加入暂存区的修改全部撤销<br>git remote add origin git@git.acwing.com:xxx/XXX.git：将本地仓库关联到远程仓库<br>git push -u (第一次需要-u以后不需要)：将当前分支推送到远程仓库<br>git push origin branch_name：将本地的某个分支推送到远程仓库<br>git clone git@git.acwing.com:xxx/XXX.git：将远程仓库XXX下载到当前目录下<br>git checkout -b branch_name：创建并切换到branch_name这个分支<br>git branch：查看所有分支和当前所处分支<br>git checkout branch_name：切换到branch_name这个分支<br>git merge branch_name：将分支branch_name合并到当前分支上<br>git branch -d branch_name：删除本地仓库的branch_name分支<br>git branch branch_name：创建新分支<br>git push --set-upstream origin branch_name：设置本地的branch_name分支对应远程仓库的branch_name分支<br> --set-upstream-to=origin/branch_name1 branch_name2：将远程的branch_name1分支与本地的branch_name2分支对应<br>git checkout -t origin/branch_name 将远程的branch_name分支拉取到本地<br>git stash：将工作区和暂存区中尚未提交的修改存入栈中<br>git stash apply：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素<br>git stash drop：删除栈顶存储的修改<br>git stash pop：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素<br>git stash list：查看栈中所有元素<br></code></pre></td></tr></table></figure><h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><h3 id="Access-authority"><a href="#Access-authority" class="headerlink" title="Access authority"></a>Access authority</h3><p>Hugging  Face拉取模型-恒源云服务器为例</p><p>服务器上要开启AI学术加速，对于GitHub、hugging face等网站可以实现上传、下载加速</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">(base) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/LLM-model#export https_proxy=http://turbo.gpushare.http_proxy=http://turbo.gpushare.com:30000<br>(base) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/LLM-model#export https_proxy=http://turbo2.gpushare.http_proxy=http://turbo2.gpushare.com:30000<br></code></pre></td></tr></table></figure><p>由于Hugging  Face属于国外网站，正常访问需要科学上网，所以使用git clone时同样也需要<strong>ssh认证</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">ssh秘钥创建</span><br>(base) root@I1dc83206c700201ce5:~/.ssh# ssh-keygen -t ed25519 -C &quot;youremail@gmail.com&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">复制公钥给Hugging Face设置</span><br>(base) root@I1dc83206c700201ce5:~/.ssh# ls<br>authorized_keys  id_ed25519  id_ed25519.pub<br>(base) root@I1dc83206c700201ce5:~/.ssh# vim id_ed25519.pub <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行报错</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">OSError: We couldn<span class="hljs-string">&#x27;t connect to &#x27;</span>https://huggingface.co<span class="hljs-string">&#x27; to load this file, couldn&#x27;</span>t find it <span class="hljs-keyword">in</span> the cached files and it looks like BAAI/EVA-CLIP-8B is not the path to a directory containing a file named config.json.</span><br>(echosight) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/hf-llm/Mistral-7B-Instruct-v0.2# cd /usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/<br>(echosight)root@I1dc83206c700201ce5:/usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/# ls<br>(echosight)root@I1dc83206c700201ce5:/usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/# vim constants.py<br><span class="hljs-meta prompt_"># </span><span class="language-bash">修改constants.py里面的hugging face参数为镜像网站</span><br>HUGGINGFACE_CO_URL_HOME = &quot;https://hf-mirror.com/&quot;<br>_HF_DEFAULT_ENDPOINT = &quot;https://hf-mirror.com/&quot;<br></code></pre></td></tr></table></figure><h3 id="拉取仓库模型"><a href="#拉取仓库模型" class="headerlink" title="拉取仓库模型"></a>拉取仓库模型</h3><p>首先在hf官网创建model的token，之后提供两种方式下载模型到服务器或者本地</p><p>第一种方法：使用huggingface_hub直接下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download<br><span class="hljs-comment"># snapshot_download(repo_id=&quot;decapoda-research/llama-7b-hf&quot;)</span><br>snapshot_download(repo_id=<span class="hljs-string">&quot;THUDM/chatglm3-6b&quot;</span>)<br></code></pre></td></tr></table></figure><p>第一种方法：使用huggingface-cli下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">(echosight)root@I1dc83206c700201ce5:/hy-tmp/# huggingface-cli download --resume-download &lt;repo/name&gt; --local-dir &lt;path/to/local/dir&gt;<br></code></pre></td></tr></table></figure><ul><li>&lt;repo&#x2F;name&gt;：替换为你要下载的Hugging Face Hub上的模型或数据集的仓库名称</li><li>&lt;path&#x2F;to&#x2F;local&#x2F;dir&gt;：替换为你想要保存模型或数据集的本地目录路径</li><li><code>–resume-download</code>：可以尝试恢复一个之前中断的下载，这个选项特别有用，当你的网络连接不稳定，或者下载过程中因为某种原因被中断</li></ul><h2 id="Ollama常用操作"><a href="#Ollama常用操作" class="headerlink" title="Ollama常用操作"></a>Ollama常用操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动服务</span><br>ollama serve<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">显示模型信息</span><br>ollama show<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行模型</span><br>ollama run model<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">从注册表中拉取模型</span><br>ollama pull modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">列出模型</span><br>ollama list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除模型</span><br>ollama rm modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">模型使用提示符</span><br>/set | set session variables<br>/show | show model information<br>/load &lt;model&gt; | load a session or model<br>/save &lt;model&gt; | save your current session<br>/clear | clear session context<br>/bye | exit<br>/? /help | help for a command<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EchoSight论文研读复现</title>
    <link href="/2024/11/16/EchoSight%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E5%A4%8D%E7%8E%B0/"/>
    <url>/2024/11/16/EchoSight%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="EchoSight-Advancing-Visual-Language-Models-with-Wiki-Knowledge"><a href="#EchoSight-Advancing-Visual-Language-Models-with-Wiki-Knowledge" class="headerlink" title="EchoSight: Advancing Visual-Language Models with Wiki Knowledge"></a><em>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</em></h1><center> Yibin Yan && Weidi Xie </center><center> School of Artificial Intelligence, Shanghai Jiao Tong University </center> <center> 原文连接：https://arxiv.org/pdf/2407.12735 </center> <center> 论文主页：https://go2heart.github.io/echosight/ </center> <h2 id="Advancing-Visual-Language-Models"><a href="#Advancing-Visual-Language-Models" class="headerlink" title="Advancing Visual-Language Models"></a>Advancing Visual-Language Models</h2><div class="note note-success">            <p>推进VLM，推进点在哪里？</p>          </div><p>现有的视觉语言模型VLM(GPT-4V、Gemini、LLaVA、Phi-3-Vision)已经可以很好的解决Standard VQA任务，表现出来较强的图像分析和较准确的答案生成。</p><p>但是现有的VLM在KB-VQA(知识型视觉问答)任务上仍然存存在的问题：<strong>幻觉</strong>、<strong>不存在的文本与内部偏见</strong>和<strong>良好的检索机制</strong>。</p><p>本文的推进点就是设计并实现了一种新型的多模态RAG框架：<strong>EchoSight</strong>，这种方法显著增强了检索到的文本知识和视觉内容之间的对齐，从而提高了E-VQA和InfoSeek等基准测试的性能。</p><blockquote><p>Information Retrieval+LLM &#x3D; RAG<br>Information Retrieval+MLLM &#x3D; M-RAG</p></blockquote><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ul><li><p>EchoSight首先通过使用<em>visual-only</em> information的方法搜索wiki文章</p></li><li><p>根据相关性结合text-image query对候选文章进行重排序</p></li><li><p>整合了一个RAG的检索-重排过程</p></li></ul><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ul><li><p>提出了MLLM RAG框架：<strong>EchoSight</strong>，可以回答需要<strong>细粒度</strong>的百科全书知识的视觉问题</p></li><li><p>采取了一种<strong>retrieval-and-reranking scheme</strong>方法提升了检索效果</p></li><li><p>在Encyclopedic VQA和InfoSeek数据集上进行实验均取得的结果均优于现有的VLMs和其他RAG架构</p></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><div class="note note-success">            <p>检索增强VQA的问题定义-&gt;retrieval-and-reranking-&gt;答案生成</p>          </div><p>EchoSight的整体架构图如下所示：</p><p><img src="https://go2heart.github.io/echosight/static/images/overall.png" alt="EchoSight总体视图"></p><p><strong>4个主要组成</strong></p><ul><li><p>an external knowledge base(KB): Wiki</p></li><li><p>a retriever: $S_\Omega$</p></li><li><p>a reranker: $S_r^\text{sec}$ + $sec_{vl}$</p></li><li><p>an answer generator: LLM</p></li></ul><h3 id="Problem-Formation"><a href="#Problem-Formation" class="headerlink" title="Problem Formation"></a>Problem Formation</h3><p>输入：一张参考图片和一个自由形式的文本型问题</p><p>输出：答案</p><p>目标：建立一个视觉问答系统</p><p>辅助：接入外部知识源</p><p>知识源是一百万规模的实体文章及与它们相关的来自维基百科网页的图像</p><p>$$<br>B &#x3D; {(a_{1},I_{1}),(a_{2},I_{2})…(a_{n},I_{n})}<br>$$</p><p>整个过程从检索器开始，利用参考图像过滤并提取与相似图像有关的KB文档实体，之后重排器会<br>获取候选实体并使用它们的文本内容来重新排序，基于参考图片和文本问题之间的相关性；最后<br>将重排KB实体“喂”给答案生成器产生最终答案。</p><h3 id="Retrieval-and-Reranking"><a href="#Retrieval-and-Reranking" class="headerlink" title="Retrieval and Reranking"></a>Retrieval and Reranking</h3><p>这一阶段的主要目标就是根据给定的参考图像和文本问题从大规模外部数据集中识别相关实体。</p><h4 id="Visual-only-Research"><a href="#Visual-only-Research" class="headerlink" title="Visual-only Research"></a>Visual-only Research</h4><p>纯视觉搜索识别视觉上与查询图像相似的候选对象。Knowledge Base中存储格式：<strong>“image-article” pairs</strong>，优化图片检索是至关重要的。</p><p><strong>方法</strong>：将所有图片转换成向量，利用<em>cosine similarity</em>评估度量与参考图片的相似性。</p><p>$$<br>S_\Omega&#x3D;{s_i&#x3D;\left\langle\frac{v_r}{||v_r||}\cdot\frac{v_i}{||v_i||}\right\rangle,i&#x3D;1,\dots,n}<br>$$</p><p>$v_{r}$表示参考图片的嵌入；$v_{i}$表示数据库图像的嵌入。</p><p>FALSS库进行向量搜索，并保留Top-k最佳匹配图像和对应的文章实体。$\mathcal{E}_v&#x3D;{(a_1,I_1),\ldots,(a_k,I_k)},k\ll n$</p><h4 id="Multimodel-Reranking"><a href="#Multimodel-Reranking" class="headerlink" title="Multimodel Reranking"></a>Multimodel Reranking</h4><p>评估视觉和文本信息以重新排序检索实体。集成整合包含文本和视觉输入，来自多模态查询和top-k个检索得到的wiki文章实体，<br>确保相关性最高的文章排在顶部。</p><p><strong>方法</strong>：运用<em>Q-Former</em>架构从文本问题和参考图像中提取多模态信息，生成32query tokens。</p><p>$$<br>z_m^i&#x3D;\text{Q-Former}\left(I_\text{ref},Q\right)^i<br>$$</p><p>${z_m^i}$记为参考图片$I_{ref}$和文本问题$Q$的第$i_{th}$个query token嵌入。</p><p>在候选文章中，本论文将候选的wiki文章切分成以文章标题为前缀的sectioins：${sec_1^i,sec_2^i,\dots,sec_p^i}$。使用Q-Former文本编码器对sections进行编码，使用BLIP-2的权重初始化Q-Former，并微调除视觉编码器之外的所有参数。</p><p>重排得分：</p><p>$$<br>S_r^\text{sec}&#x3D;\max_{1\leq i\leq N_q}\left(\sin(z_m^i,z_s^\text{sec})\right)<br>$$</p><p>这种方法计算了在<strong>多模态query token嵌入</strong>和<strong>wiki文章section的token嵌入</strong>之间最高的成对相似性，每一个多模态query token嵌入${z_m^i}$都来自“img-question”对。</p><p>计算完重排得分后，最后一步是进行多模态重排，reranker将前一阶段视觉相似度得分和重排得分合成一个加权和：</p><p>$$<br>\begin{aligned}\sec_{vl}&amp;&#x3D;\arg\max_{\text{sec}\in a}(\alpha\cdot S_v^\text{sec}+(1-\alpha)\cdot S_r^\text{sec})\end{aligned}<br>$$</p><p>$sec_{vl}$表示的是经过reranker得到的最高排序实体<strong>section</strong>，注意这里是section而不是article。${\alpha}$是权重参数用来平衡视觉相似度得分$S_v^\text{sec}$和重排得分$S_r^\text{sec}$。</p><p>Note:$S_v^\text{sec}$有别于前文中的$S_\Omega$，它是在visual-only搜索阶段使用<strong>sec所属的wiki条目</strong>中的最佳匹配图像来计算的。</p><h4 id="Reranking-Training"><a href="#Reranking-Training" class="headerlink" title="Reranking Training"></a>Reranking Training</h4><p>前文中使用对比学习框架完成了硬负采样，负样本的选取是从视觉相似但是内容有别的例子中选取的，以至于最初的visual-only检索并不成功。训练重排器的目的就是为多模态queries选取最相关的文章以提高系统的整体的准确率和有效性。</p><p>训练的损失函数定义为：</p><p>$$<br>\mathcal{L}&#x3D;-\log\frac{\exp(\max_{1\leq i\leq N_q}\sin(z_m^i,z_s)&#x2F;\mathcal{T})}{\sum_{j&#x3D;1}^N\exp(\max_{1\leq i\leq N_q}\sin(z_m^i,z_s^j)&#x2F;\mathcal{T})}<br>$$</p><p>${z_s}$是positive section嵌入，$N$是全部的正负样本数量，$\mathcal{T}$是一个控制softmax平滑分布的超参数。</p><h3 id="Answer-Generation-with-LLMs"><a href="#Answer-Generation-with-LLMs" class="headerlink" title="Answer Generation with LLMs"></a>Answer Generation with LLMs</h3><p>一旦从KB中识别出相关条目，LLMs会整合所有的信息来生成answer，$A &#x3D; LLM(sec_{vl},Q)$。LLM是答案的genertor，$sec_{vl}$是检索得到的wiki文章section，$Q$是目标问题。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="两个数据集"><a href="#两个数据集" class="headerlink" title="两个数据集"></a>两个数据集</h3><p>Encyclopedic VQA 和 InfoSeek</p><p><strong>Encyclopedic VQA</strong>：contains 221k unique question and answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples</p><p><strong>InfoSeek</strong>：comprises 1.3M visual information-seeking questions, covering more than 11K visual entities from OVEN</p><h3 id="度量指标"><a href="#度量指标" class="headerlink" title="度量指标"></a>度量指标</h3><p>关注两个度量指标：检索和问题回答。</p><p>检索衡量的是在大规模多模态知识库中检索相关文章的准确性，问题回答衡量的是对于视觉问题提供准确回答的有效性。</p><h4 id="检索度量"><a href="#检索度量" class="headerlink" title="检索度量"></a>检索度量</h4><p>本文使用标准<strong>Recall@K</strong>，Recall@K评估是否正确的文章实体出现在top-k的检索结果中。</p><p>只有当一篇文章的URL与目标URL完全匹配时，它才被认为是正确的，这使得我们的检索评估与只匹配所检索到的文章的答案内容的方法相比更加严格和精确。</p><h4 id="问题回答的度量"><a href="#问题回答的度量" class="headerlink" title="问题回答的度量"></a>问题回答的度量</h4><p>E-VQA：BEM score</p><p>InfoSeek：VQA Accuracy and Relaxed Accuracy</p><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><h4 id="检索器"><a href="#检索器" class="headerlink" title="检索器"></a>检索器</h4><p>Eva-CLIP作为检索器，将参考图片和数据库图片进行视觉嵌入</p><h4 id="重排器"><a href="#重排器" class="headerlink" title="重排器"></a>重排器</h4><p>重排器使用带权重LAVIS的BLIP-2</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p>E-VQA：Mistral-7B-Instruct-v0.2</p><p>InfoSeek：LLaMA-8B-Instruct</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>很明显，我们提出的EchoSight框架实验结果远远超过了之前的工作，甚至接近了原始E-VQA(Mensink等人，2023)在基准报告的上行结果，其中采用了两个巨大的模型，即“Google Lens”用于知识检索，PaLM用于答案生成。</p><p><img src="https://go2heart.github.io/echosight/static/images/VQA.png" alt="实验结果"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>使用E-VQA数据集进行消融实验。</p><p>在检索方面，我们进行了以下分析：<br>(i)比较检索中不同的vision backbones<br>(ii)研究重新排序范围的影响<br>(iii)调查硬负抽样的重要性。</p><p>在最终答案生成时，我们进行了消融研究：<br>(i)各种语言模型的影响<br>(ii)在oracle设置下使用答案生成器进行实验。</p><h2 id="Conclusion、Limitations-and-Future-work"><a href="#Conclusion、Limitations-and-Future-work" class="headerlink" title="Conclusion、Limitations and Future work"></a>Conclusion、Limitations and Future work</h2><p>Conclusion:</p><p>在本文中，介绍了一种新的检索增强视觉语言系统EchoSight，用于解决基于知识的视觉问题回答(VQA)的挑战。通过改进的两阶段方法，EchoSight显示出显著的性能改善，E-VQA的准确率为41.8%，InfoSeek的准确率为31.3%。</p><p>EchoSight的成功强调了高效的检索过程和多模态信息的集成在基于知识的VQA任务中提高大型语言模型(LLMs)性能方面的重要性。</p><p>Limitations:</p><ul><li><p>严重依赖检索知识源的高质量和完整性；在特定领域上表现欠佳</p></li><li><p>多模态重排阶段需要消耗大量计算资源，耗时长，不利于实时应用</p></li></ul><p>Future work:</p><div class="note note-danger">            <p>优化计算开销能不能和prompt压缩和检索得分结合起来？</p>          </div><ul><li><p>提高知识源质量</p></li><li><p>优化多模态计算开销</p></li></ul><h2 id="复现代码的实验环境"><a href="#复现代码的实验环境" class="headerlink" title="复现代码的实验环境"></a>复现代码的实验环境</h2><p>GPU：Nvidia RTX 4090</p><p>显存：16GB</p><p>内存：50GB </p><h2 id="Git拉取源码仓库"><a href="#Git拉取源码仓库" class="headerlink" title="Git拉取源码仓库"></a>Git拉取源码仓库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp# git clone https://github.com/Go2Heart/EchoSight.git<br>Cloning into &#x27;EchoSight&#x27;...<br>remote: Enumerating objects: 532, done.<br>remote: Counting objects: 100% (130/130), done.<br>remote: Compressing objects: 100% (108/108), done.<br>remote: Total 532 (delta 66), reused 62 (delta 22), pack-reused 402 (from 1)<br>Receiving objects: 100% (532/532), 3.03 MiB | 3.11 MiB/s, done.<br>Resolving deltas: 100% (245/245), done.<br><br>root@I1dbca09acd00201b2e:/hy-tmp# cd EchoSight/<br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# ls<br>README.md  data_utils.py  demo                   lavis  requirements.txt  test<br>app.py     dataset        fine_tune_reranker.py  model  scripts           utils<br></code></pre></td></tr></table></figure><ul><li><p>model&#x2F;文件夹下存放着模型源代码</p></li><li><p>dataset&#x2F;文件夹下存放着wiki知识源</p></li><li><p>scripts&#x2F;文件夹下存放着训练脚本shell文件</p></li><li><p>requirements.txt文件存放着项目所需要的第三方库</p></li><li><p>app.py是前端检索的demo程序</p></li></ul><h2 id="Conda-创建虚拟环境"><a href="#Conda-创建虚拟环境" class="headerlink" title="Conda 创建虚拟环境"></a>Conda 创建虚拟环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">1.Create conda environment</span><br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# conda create -n echosight python=3.10<br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# conda activate echosight<br><span class="hljs-meta prompt_">#</span><span class="language-bash">2.Install the required packages</span><br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# pip install -r requirements.txt<br><br>Successfully built antlr4-python3-runtime fairscale ffmpy iopath<br></code></pre></td></tr></table></figure><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><h3 id="Enclyclopedic-VQA"><a href="#Enclyclopedic-VQA" class="headerlink" title="Enclyclopedic-VQA"></a>Enclyclopedic-VQA</h3><ul><li><a href="https://storage.googleapis.com/encyclopedic-vqa/encyclopedic_kb_wiki.zip">Encylopedic-VQA’s 2M Knowledge Base</a>：4.9GB</li><li><a href="https://drive.google.com/file/d/1cQYul-my2FtqfCND2FeqgF9TCMU3u5xz/view?usp=drive_link">Enclopedic-VQA KB Images Faiss Index</a>：15GB</li></ul><h3 id="Infoseek"><a href="#Infoseek" class="headerlink" title="Infoseek"></a>Infoseek</h3><ul><li><a href="https://drive.google.com/file/d/1cIbKtYryD7XBAw0tjrrCvMCJC2rIzLM5/view?usp=drive_link">Our InfoSeek’s 100K Knowledge Base</a>：280M</li><li><a href="https://drive.google.com/file/d/1cDuL45c1iYwB0_BSlTmrMzbEE8ik2cVJ/view?usp=drive_link">InfoSeek KB Images Faiss Index</a>：1.2GB</li></ul><h3 id="将数据集上传到云服务器"><a href="#将数据集上传到云服务器" class="headerlink" title="将数据集上传到云服务器"></a>将数据集上传到云服务器</h3><p>使用<em>xftp</em>传输</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>EchoSight 的多模态重排序器使用 Encyclopedic-VQA 数据集和相应的 2M 知识库进行训练。要训练多模式重新排序器，请在更改必要的配置后运行 bash 脚本：</p><p>修改配置文件参数路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#vim train_reranker.sh<br><br>export CUDA_VISIBLE_DEVICES=0<br>python fine_tune_reranker.py \<br>   --blip-model-name &#x27;blip2_reranker&#x27; \<br>   --num-epochs 20 \<br>   --num-workers 4 \<br>   --learning-rate 1e-5 \<br>   --batch-size 6 \<br>   --transform targetpad \<br>   --target-ratio 1.25  \<br>   --save_frequency 50000 \<br>   --train_file /PATH/TO/TRAIN_FILE \<br>   --knowledge_base_file /PATH/TO/KNOWLEDGE_BASE_JSON_FILE \<br>   --negative_db_file /PATH/TO/NEGATIVE_DB_FILE \<br>   --inat_id2name /PATH/TO/INATURALISTS_ID2NAME \<br>   --save-training \<br><br></code></pre></td></tr></table></figure><p>–train_file：训练数据文件的路径。 训练文件应与 Encyclopedic-VQA 提供的格式相同。</p><p>–knowledge_base_file：JSON 格式的知识库文件的路径。 格式应与Encyclopedic-VQA 的格式相同。</p><p>–negative_db_file：用于训练的硬负样本数据库文件的路径。</p><p>–inat_id2name：iNaturalist ID 到名称映射文件的路径。</p><p>执行脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#bash scripts/test_reranker.sh<br></code></pre></td></tr></table></figure><p>test_reranker:</p><p>我们的重新排序模块权重可以在[Checkpoint]下载，要使用经过训练的模型进行推理，调整必要的参数后运行提供的 test_reranker.sh脚本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#vim test_reranker.sh<br><br>export CUDA_VISIBLE_DEVICES=0<br>python -m test.test_reranker \<br>    --test_file /PATH/TO/TESTFILE\<br>    --knowledge_base /PATH/TO/KNOWLEDGE_BASE_JSON_FILE\<br>    --faiss_index /PATH/TO/KNOWLEDGE_BASE_FAISS_INDEX\<br>    --retriever_vit eva-clip \<br>    --top_ks 1,5,10,20 \<br>    --retrieval_top_k 20\<br>    --perform_qformer_reranker \<br>    --qformer_ckpt_path /PATH/TO/QFORMER-CKPT\<br></code></pre></td></tr></table></figure><p>–test_file：测试文件的路径。</p><p>–knowledge_base：知识库 JSON 文件的路径。</p><p>–faiss_index：用于高效相似性搜索的 FAISS 索引文件的路径。</p><p>–save_result_path：保存结果 json 文件的路径。</p><h2 id="重点关注的问题"><a href="#重点关注的问题" class="headerlink" title="重点关注的问题"></a>重点关注的问题</h2><div class="note note-success">            <ol><li>prompt的具体格式是啥样的？-》看代码，做这一方面一定要对格式有一个清楚的了解</li><li>MLLMs RAG的prompt(最后生成答案前的一步)</li><li>重排的方法有哪些？可以调研一下</li></ol>          </div>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典卷积神经网络复现</title>
    <link href="/2024/11/15/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    <url>/2024/11/15/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>基于<code>pytorch</code>将经典的卷积神经网络架构复现一下</p>          </div><h2 id="卷积神经网络基础知识"><a href="#卷积神经网络基础知识" class="headerlink" title="卷积神经网络基础知识"></a>卷积神经网络基础知识</h2><p>数据又表格数据过渡到图像的像素数据，网络结构由全连接的多层感知机过渡到卷积结构。</p><p>卷积神经网络就是将空间不变性这一概念系统化，基于这个模型使用较少的参数学习有用的表示。</p><p><img src="https://i.kinja-img.com/gawker-media/image/upload/w6gfkxdf7lyoygaog9d7.jpg" alt="where is Weldo?"></p><p>卷积神经网络的输入：n * n * 3的图片向量</p><p>卷积神经网络的输出：类别标签&#x2F;类别向量(经过softmax归一化后)</p><p>通道：图像一般包含3个通道&#x2F;3种原色(RGB)，一个颜色就是一个色彩通道。</p><p>感受野：卷积神经网络在图片上设定的区域大小</p><p>步幅：感受野在图片上移动的距离</p><p>填充：感受野在移动过程中可能会超出图片的像素范围，超出范围的地方用数字补值的方式就是填充</p><p>滤波器：让每个感受野都只有一组参数从而达到简化的方法。</p><p>卷积层：感受野+参数共享</p><p>下采样：把图像<strong>偶数列</strong>都拿掉，<strong>奇数行</strong>都拿掉，图像变成为原来的 1&#x2F;4，但是不会影响里面是什么</p><p>东西。</p><p>汇聚：汇聚没有参数只是一个操作，将滤波器产生的结果进行分组后按要求汇聚(取最大&#x2F;取平均)</p><p>池化层：取窗口中的最大值最为输出结果，然后滑动窗口减少其空间尺寸</p><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>卷积神经网络的开山之作，1998年由LeCun Yang提出。</p><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1TI1aGBZ4dybR6__DI9dzA.png" alt="LeNet-5"></p><h3 id="复现代码"><a href="#复现代码" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment">#LeNet-5一共有7层，输入图像大小为32*32*1，输出对应10个类别的条件概率</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet5</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet5,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layer1 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：1；输出channels：6，卷积核：5*5</span><br>          <span class="hljs-comment">#Conv2d(1,6,5,1,0)的参数依次是：输入通道数，输出通道数，卷积核大小、步长、填充</span><br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#2*2卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer2 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：6；输出channels：16，卷积核：5*5</span><br>            nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#2*2卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Flatten(),<span class="hljs-comment">#将卷积后的16@5*5图像输出展平</span><br>            nn.Linear(<span class="hljs-number">400</span>,<span class="hljs-number">120</span>),<span class="hljs-comment">#线性层</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),<span class="hljs-comment">#线性层</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>),<span class="hljs-comment">#线性层</span><br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x = <span class="hljs-variable language_">self</span>.layer2(x)<br>        <span class="hljs-built_in">print</span>(x.shape)<br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>2014年ImageNet比赛的冠军</p><h3 id="网络结构图-1"><a href="#网络结构图-1" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://i0.wp.com/bainsa.xyz/wp-content/uploads/2023/11/a1.png?fit=940,469&ssl=1" alt="AlexNet"></p><h3 id="结构细节"><a href="#结构细节" class="headerlink" title="结构细节"></a>结构细节</h3><div class="note note-danger">            <p>卷积核大小如何确定？</p>          </div><p>虽然说目前有比较火的研究方向针对这种自动神经网络结构搜索（NAS），这些自动搜索出来的网络在常规数据集上的建模结果显示（当然是达到一定的准确度）：自动搜索出来的网络中的卷积核的类别有包括各种常见的型号（3 * 3、5 * 5、7*7），且在网络中的前后排布没有规律。</p><div class="note note-danger">            <p>nn.BatchNorm2d()的作用？</p>          </div><p>在深度神经网络中，梯度消失是一个常见的问题。BatchNorm2d通过对激活函数前添加归一化层，抑制了梯度消失的问题，从而加速了优化过程。</p><p>BatchNorm2d通过对数据的归一化处理，使得权重初始化的影响减小，无论权重的初始值如何，都可以通过归一化和仿射变换得到较好的效果。</p><div class="note note-danger">            <p>nn.Dropout(0.5,inplace&#x3D;True)的作用？</p>          </div><p>nn.Dropout模块的作用是在训练过程中随机关闭一部分神经元，以增加模型的泛化能力。通常，我们在全连接层之后应用dropout，以避免破坏卷积层中重要的空间信息。</p><p>一般建议在10%-50%之间设置dropout比例</p><h3 id="复现代码-1"><a href="#复现代码-1" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment">#AlexNet的输入为224*224*3图像，输出为1000个类别的条件概率</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Neural network model consisting of layers proposed by AlexNet paper</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AlexNet,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layer1 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：3；输出channels：96，卷积核：11*11</span><br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">96</span>,<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">96</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer2 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：96；输出channels：256，卷积核：5*5</span><br>            nn.Conv2d(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer3 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：256；输出channels：384，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">394</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer4 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：384；输出channels：384，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">394</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer5 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：384；输出channels：256，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>    <br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Dropout(<span class="hljs-number">0.5</span>,inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">6</span>*<span class="hljs-number">6</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Dropout(<span class="hljs-number">0.5</span>,inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">1000</span>),<br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x = <span class="hljs-variable language_">self</span>.layer2(x)<br>        x = <span class="hljs-variable language_">self</span>.layer4(x)<br>        x = <span class="hljs-variable language_">self</span>.layer5(x)<br>        output = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>2014年ImageNet比赛的亚军</p><h3 id="网络结构图-2"><a href="#网络结构图-2" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://production-media.paperswithcode.com/methods/vgg_7mT4DML.png" alt="VGG"></p><p>每一个长方体“板子”就是一张图片数据的一个tensor，搭建网络的过程中关注每一块“板子”之间的变化即可。</p><h3 id="复现代码-2"><a href="#复现代码-2" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">基于pytorch搭建VGG卷积神经网络模型-16层网络结构</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGGmodel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(VGGmodel,<span class="hljs-variable language_">self</span>).__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.features = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">1000</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>._initialize_weights()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.features(x)<br>        x = nn.Flatten(x)<br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>VGG网络只是对网络的层书进行堆叠，并没有进行结构性的创新，不过加深网络深度确实可以提高模型效果。</p><h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>GoogleNet也被叫做InceptionNet，它解决了一个核心问题是选择多大的卷积核是最合适的问题。在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。</p><p><img src="https://zh.d2l.ai/_images/inception.svg" alt="Inception块的架构"></p><p>动手学深度学习：Inception块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。 </p><ol><li>难点1是如何并行设计出网络结构？</li><li>难点2是并行输入输出的通道数如何计算？</li></ol><h3 id="网络结构图-3"><a href="#网络结构图-3" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://zh.d2l.ai/_images/inception-full.svg" alt="GoogleNet"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment">#channel1-channel4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels,channel1, channel2,channel3,channel4,**kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception,<span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-comment">#路径1:单层1*1卷积层</span><br>        <span class="hljs-variable language_">self</span>.path1 = nn.Conv2d(in_channels,channel1,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#路径2:1x1卷积层后接3x3卷积层</span><br>        <span class="hljs-variable language_">self</span>.path2_1 = nn.Conv2d(in_channels,channel2[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path2_2 = nn.Conv2d(channel2[<span class="hljs-number">0</span>],channel2[<span class="hljs-number">1</span>],<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#路径3:1x1卷积层后接5x5卷积层</span><br>        <span class="hljs-variable language_">self</span>.path3_1 = nn.Conv2d(in_channels,channel3[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path3_2 = nn.Conv2d(channel3[<span class="hljs-number">0</span>],channel3[<span class="hljs-number">1</span>],<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>        <span class="hljs-comment">#路径4:3x3最大汇聚层后接1x1卷积层</span><br>        <span class="hljs-variable language_">self</span>.path4_1 = nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path4_2 = nn.Conv2d(in_channels,channel4,<span class="hljs-number">1</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        p1 = nn.ReLU(<span class="hljs-variable language_">self</span>.path1(x))<br>        p2 = nn.ReLU(<span class="hljs-variable language_">self</span>.path2_2(nn.ReLU(<span class="hljs-variable language_">self</span>.path2_1(x))))<br>        p3 = nn.ReLU(<span class="hljs-variable language_">self</span>.path3_2(nn.ReLU(<span class="hljs-variable language_">self</span>.path3_1(x))))<br>        p4 = nn.ReLU(<span class="hljs-variable language_">self</span>.path4_2(<span class="hljs-variable language_">self</span>.path4_2(x)))<br>        <span class="hljs-keyword">return</span> torch.cat((p1,p2,p3,p4),dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-comment">#第一个模块使用64个通道、7*7卷积层</span><br>        <span class="hljs-variable language_">self</span>.b1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第二个模块使用两个卷积层：第一个卷积层是64个通道、1*1卷积层；第二个卷积层使用将通道数量增加三倍的卷积层</span><br>        <span class="hljs-variable language_">self</span>.b2 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">192</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第三个模块串联两个完整的Inception块</span><br>        <span class="hljs-comment">#难点是理解各个路径通道数的计算</span><br>        <span class="hljs-variable language_">self</span>.b3 = nn.Sequential(<br>            Inception(<span class="hljs-number">192</span>,<span class="hljs-number">64</span>,(<span class="hljs-number">96</span>,<span class="hljs-number">128</span>),(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>)),<br>            Inception(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>,(<span class="hljs-number">128</span>,<span class="hljs-number">92</span>),(<span class="hljs-number">32</span>,<span class="hljs-number">94</span>)),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第四模块串联了5个Inception块</span><br>        <span class="hljs-variable language_">self</span>.b4 = nn.Sequential(<br>            Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第五模块包含两个Inception块</span><br>        <span class="hljs-variable language_">self</span>.b5 = nn.Sequential(<br>            Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            nn.Flatten()<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">10</span>)<br>        <br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = nn.Sequential(<span class="hljs-variable language_">self</span>.b1,<span class="hljs-variable language_">self</span>.b2,<span class="hljs-variable language_">self</span>.b3,<span class="hljs-variable language_">self</span>.b4,<span class="hljs-variable language_">self</span>.classifier)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>2015年ImageNet比赛的冠军，通过残差模块能够成功训练出152层深度的残差网络。</p><p>设计的灵感来源于：加深神经网络的时候会出现一个Degradation，准确率上升到达饱和，在持续增加深度会导致模型的准确率下降。</p><p>假设一个比较浅的网络达到了饱和准确率，那么在它后面加上几层恒等映射层误差不会增加，也就是说更深的模型不会是模型的效果下降。恒等映射的思想就是ResNet的灵感来源。</p><h3 id="网络结构图-4"><a href="#网络结构图-4" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://miro.medium.com/v2/resize:fit:2000/1*6hF97Upuqg_LdsqWY6n_wg.png" alt="ResNet"></p><h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">ResNet网络结构复现</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#残差块的定义</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResiDual</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels,num_channels,use_1X1conv = <span class="hljs-literal">False</span>,stride = <span class="hljs-number">1</span>,*kwargs</span>):<br>        <span class="hljs-built_in">super</span>(ResiDual,<span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-comment">#定义第一个3*3的卷积层</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels,num_channels,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#定义第二个3*3的卷积层</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(num_channels,num_channels,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> use_1X1conv:<br>            <span class="hljs-variable language_">self</span>.conv3 = nn.Conv2d(in_channels,num_channels,<span class="hljs-number">1</span>,stride=stride)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.conv3 = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">#定义批量规范化层1</span><br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(num_channels)<br>        <span class="hljs-comment">#定义批量规范化层2</span><br>        <span class="hljs-variable language_">self</span>.bn2 = nn.BatchNorm2d(num_channels)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        y = nn.ReLU(<span class="hljs-variable language_">self</span>.bn1(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        y = <span class="hljs-variable language_">self</span>.bn2(<span class="hljs-variable language_">self</span>.conv2(y))<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.conv3:<br>            x = <span class="hljs-variable language_">self</span>.conv3(x)<br>        y += x<br>        <span class="hljs-keyword">return</span> F.relu(y)<br><br><span class="hljs-comment">#定义ResNet架构</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(ResNet,<span class="hljs-variable language_">self</span>).__init__(*args, **kwargs)<br>        <span class="hljs-comment"># 初始化与GoogleNet相似</span><br>        <span class="hljs-variable language_">self</span>.b1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment"># 定义残差块</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">ResBlock</span>(<span class="hljs-params">in_channels,num_channels,num_residuals,first_block=<span class="hljs-literal">False</span></span>):<br>            block = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>                <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>                    block.append(ResiDual(in_channels,num_channels,use_1X1conv=<span class="hljs-literal">True</span>,stride=<span class="hljs-number">2</span>))<br>                <span class="hljs-keyword">else</span>:<br>                    block.append(ResiDual(num_channels,num_channels))<br>            <span class="hljs-keyword">return</span> block<br>        <br>        <span class="hljs-comment"># 在ResNet加入所有残差块</span><br>        <span class="hljs-variable language_">self</span>.b2 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,first_block=<span class="hljs-literal">True</span>),<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b3 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b4 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b5 = nn.Sequential(<br>            *ResNet(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>    <br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forword</span>(<span class="hljs-params">self,x</span>):<br>        x = nn.Sequential(<br>            <span class="hljs-variable language_">self</span>.b1,<span class="hljs-variable language_">self</span>.b2,<span class="hljs-variable language_">self</span>.b3,<span class="hljs-variable language_">self</span>.b4,<span class="hljs-variable language_">self</span>.b5,<br>            nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>        )<br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="note note-danger">            <p>1.卷积神经网络的基本概念一定要熟练掌握</p><p>2.对于使用pytorch搭建神经网络模型要熟练掌握代码编写的过程与逻辑</p><p>3.对于步长和填充值的计算要理解，特别是GoogleNet和ResNet的padding和stride值的计算</p><p>4.如何根据网络架构图取复现代码是理解网络结构和pytorch工具箱的结合</p>          </div>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++五子棋</title>
    <link href="/2024/11/11/C-%E4%BA%94%E5%AD%90%E6%A3%8B/"/>
    <url>/2024/11/11/C-%E4%BA%94%E5%AD%90%E6%A3%8B/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>在Windows中一切矩形的东西都是窗口</p>          </div><p>微软MFC的CPoint类库</p><h2 id="五子棋设计"><a href="#五子棋设计" class="headerlink" title="五子棋设计"></a>五子棋设计</h2><p>需要的类和方法：</p><p>棋子：黑棋、白棋、最后一步</p><p>点类：参考微软MFC的CPoint类库，看似细节很见水平</p><p>棋盘：数组（GetAt()、SetAt()）、init、display函数、输入函数、输出</p><p>玩家：分类（人人、人机），需要创建裁判对象、AI对象（大部分AI事都是它的事）</p><p>AI：计算连子数、推荐落子、评分、随机（分值相等且对称的地方随机）</p><p>裁判：计算输赢、违规-virtual虚函数（叠子、出界、禁手）裁判要先合规再计算输赢</p><p>工头：棋盘、棋子、玩家、时钟对象的初始化，猜先、写棋谱、交换手</p><p>时钟：超时</p><p>开始游戏-选择人人对战、人机对战</p><h2 id="棋子的设计"><a href="#棋子的设计" class="headerlink" title="棋子的设计"></a>棋子的设计</h2><p>0: 空白&#x2F;没有落子</p><p>1:黑子</p><p>2:白子</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年10月总结与展望</title>
    <link href="/2024/11/03/10%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/11/03/10%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="得与失"><a href="#得与失" class="headerlink" title="得与失"></a>得与失</h2><p>得：心情上不再纠缠与患得患失了，月下旬相比于月初和月中，要平和了许多。学习上稳扎稳打吧，慢慢的在往前走。</p><p>失：李沐的课程并没有按进度跟上学习、一些经典的论文都没有读；这个月旷课不少。</p><p>有得有失：这个月罕见地勇敢了一次，不知怎的。但是方式方法我认为并不妥当，会很冒失、唐突，但是于我本人，我绝对真诚。真诚有用吗？说不好。以后还要继续真诚吗？还会的。阅读能学到东西，同样地，经历、体验也能学到东西，看自己咯，无论顺利还是失败，开心还是失落，总能收获些什么，学到些什么，知道自己还能在哪方面提高些什么。杨力祥老师上周说了句话很值得思考：优秀的本质就是要学会随时改变自己的想法。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>💡想法转变：老师不再是传统意义上的“传道授业解惑”，要解放思想。老师与学生更像是“买卖”的商业关系，老师“卖”知识、经验，学生是“客户”。顾客就是上帝，不能总想靠考试难为学生，课就是卖点，学生也不都是傻子(虽然现在大学生都很好骗包括我自己)，东西好不好上一节课就知道。</p><p>天下没有什么东西是不变的，优秀的人一定会随时改变自己的思路与想法，这是优秀的标志，不要咬死了死脑筋，那是愚蠢</p><h2 id="11月务必"><a href="#11月务必" class="headerlink" title="11月务必"></a>11月务必</h2><p>务必11月要执行的：<strong>带着把程序写成东西的思想，疯狂Coding</strong>！！！！</p><h2 id="缺点改正"><a href="#缺点改正" class="headerlink" title="缺点改正"></a>缺点改正</h2><p>⚠️效率不高</p><p>⚠️早睡早起的习惯很难保持了</p><h2 id="💡一个想法"><a href="#💡一个想法" class="headerlink" title="💡一个想法"></a>💡一个想法</h2><p>最近在系统梳理<strong>深度学习</strong>的知识体系，想着把所学的做一个主动输出，正好自己也有Mac本。目前有个想法就是自己在B站上录课，去把自己所学的讲出来，别管有没有人看，自己主动去把知识讲出来，做一个费曼学习法。</p><p>知识体系梳理详细内容可以参考本人的历史文章中拙作<a href="https://mungeryang.top/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/">《我们是怎么学习的？机器是如何学习我们的学习的？》</a>（还在补充更新中）。</p><h2 id="观影"><a href="#观影" class="headerlink" title="观影"></a>观影</h2><p>勇敢的心这个网名用了快10年了，但是都没看过勇敢的心这部电影。</p><p>前天晚上抽时间认真看完了这部电影，这今年看的难得的好片，不愧是奥斯卡金像奖。</p><p>这是一部任务纪传型电影，主人公是苏格兰民族独立英雄<strong>威廉·华莱士</strong>，印象最深的一句台词是，最后他被处死时高喊的那句：<strong>Freedom</strong>！</p><p>平民抗争、权贵压迫、国家观、民族观在这部电影里都体现的淋漓尽致。</p><p>不由得想起一句话：<strong>由此上溯到一千八百四十年，从那时为了反对内外敌人、争取民族，在历次斗争的牺牲的人民英雄们永垂不朽！</strong></p><h2 id="🌲11月学习与工作部署"><a href="#🌲11月学习与工作部署" class="headerlink" title="🌲11月学习与工作部署"></a>🌲11月学习与工作部署</h2><p>重点任务：继续梳理深度学习知识框架，按着杨力祥老师上课讲的思路去摸索、整理、关联</p><p>需要做的：这个月要<strong>多动手去Code</strong>了，全力Code</p><p>需要Code的任务(按任务优先级)：</p><ol><li>PythonMLLMs RAG训练</li><li>C++复现Deep Learning、Java实现分布式文件系统模拟</li><li>Hive大数据实验</li></ol><p>11.03-11.07：</p><ul><li>着手对LLM RAG、MLLMs RAG进行实操训练，技术流程要做到心里有数，能get到大老师的点</li><li>别忘了大老师给的任务点，以及上篇论文的盲点梳理。</li></ul><p>11.08-11.14:</p><ul><li>梳理完深度学习框架，动手实践复现框架</li><li>老师idea复现</li><li>月中总结+休息调整</li></ul><p>11.18-11.25:</p><ul><li>复现完成经典的CNN模型</li><li>EchoSight论文研读复现</li><li>要梳理好Transformer、Attention的核心架构</li><li>Java分布式文件系统着手编写</li></ul><p>11.26-11.30:</p><ul><li>Rag VL论文研读复现</li><li>C++五子棋程序要完成一半</li><li>争取完成Java分布式文件系统</li></ul><h2 id="希望这个月（非鸡汤）"><a href="#希望这个月（非鸡汤）" class="headerlink" title="希望这个月（非鸡汤）"></a>希望这个月（非鸡汤）</h2><p>知难上，戒焦躁</p><p>常自省，穷途明</p><p>事毕于今，不溺过往</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM basic knowledge</title>
    <link href="/2024/10/28/LLM-basic-knowledge/"/>
    <url>/2024/10/28/LLM-basic-knowledge/</url>
    
    <content type="html"><![CDATA[<p>【<a href="https://datawhalechina.github.io/llm-cookbook/#/">面向开发者的LLM教程</a>】</p><p>LLM是通过预测下一个词的的监督学习方式进行训练的。具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入 Context 预测下一个词的概率分布。通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异,语言模型逐步掌握了语言的规律，学会了预测下一个词。</p><p>在训练过程中,研究人员会准备大量句子或句子片段作为训练样本,要求模型一次次预测下一个词，通过反复训练促使模型参数收敛，使其预测能力不断提高。经过在海量文本数据集上的训练，语言模型可以达到十分准确地预测下一个词的效果。这种<strong>以预测下一个词为训练目标的方法使得语言模型获得强大的语言生成能力</strong>。</p><div class="note note-danger">            <p>思考：这个和当初<strong>Alpha Go</strong>生成下一步棋的方式是不是有异曲同工之妙？</p>          </div><p><a href="https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file">https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file</a></p><p>input (prompt) -&gt; fn{LLM} -&gt; output</p><h2 id="历史发展"><a href="#历史发展" class="headerlink" title="历史发展"></a>历史发展</h2><p>大语言模型发展主要经历了统计语言模型 –&gt; 神经语言模型 –&gt; 预训练语言模型 –&gt; 大语言模型四个阶段</p><p>图片引用自<a href="https://cleaner.love/">Cleaner 知识库</a></p><p><img src="/img/fig/history-LLM.png" alt="语言模型发展史"></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h3><p>Token的概念相当于文字，一个中文文字对应一个token，比如我爱你，对应三个token：我+爱+你；一个英文的字符对应一个token，比如love，而icecream对应两个token：ice+cream。</p><p>联系理解，计算机理解数据的形式是<strong>比特流</strong>，那么类比联想理解，LLM理解数据的“字节”就是一个Token。</p><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>使用大语言模型在<strong>大规模语料</strong>上进行预训练，大语言模型可以获得通用的语言理解与生成能力，掌握较为广泛的世界知识，具备解决众多下游任务的性能潜力。</p><p>预训练大模型类比机器学习的训练模式，其实就是通过算法将模型在训练集上先进行训练</p><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><p>指令微调是指使用自然语言形式的数据对预训练后的大语言模型进行<strong>参数微调</strong>，它是增强和激活大语言模型特定能力的重要方法之一。通过使用任务输入与输出的配对数据进行模型训练，可以使语言模型掌握通过问答形式进行任务求解的能力和较强的指令遵循能力，并且能够无需下游任务的训练样本或者示例就可以解决训练中未见过的任务。</p><h3 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h3><p>经过大规模的预训练和有监督指令微调，大语言模型具备了解决各种任务的通用能力和指令遵循能力，但是同时也可能生成<code>有偏见的</code>、<code>冒犯的</code>以及<code>事实错误的</code>文本内容。这些潜在的有害行为，可能在下游应用中产生严重的影响与危害，进一步被恶意使用者进行放大与利用。因此，在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个 关键研究问题，通常称这一研究问题为<strong>人类对齐</strong>。</p><h3 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h3><p>模型泛化是指一些模型可以应用（泛化）到其他场景，通常为采用迁移学习、微调等手段实现泛化。</p><h3 id="涌现"><a href="#涌现" class="headerlink" title="涌现"></a>涌现</h3><p>模型规模达到一定阈值以上后，会在多步算术、大学考试、单词释义等场景的准确性显著提升，称为涌现。</p><h2 id="RAG与微调"><a href="#RAG与微调" class="headerlink" title="RAG与微调"></a>RAG与微调</h2><p>目前的大模型都是预训练语言模型 LLaMA、GPT4o、o1-mini</p><ul><li>互联网公开的海量数据</li><li>私有化数据</li></ul><p>针对互联网数据无法及时更新与专业领域私有化数据的解决方案：</p><ul><li>RAG-检索增强生成：数据（语料）数据大部分是文献资料、文档</li><li>SFT-微调：QA-法律咨询、心理诊断等一问一答数据标签归类，微调出一个适用场景的垂直场景</li></ul><p>RAG缺点：</p><p>检索出来的文档片段不完整，重排序</p><p>对于系统延迟低场景(检索器检索)</p><p>Embeding Model-嵌入</p><p>将文本转换成一组N维的浮点数，文本向量又叫做Embedings</p><p>向量之间可以计算距离，距离的远近对应语义的相似性</p><p>以前的工程中，keywords、sql &#x3D; 精确匹配，使用关系型数据库进行存储。现在文本嵌入、图片嵌入等 &#x3D; 距离相似度匹配，使用向量数据库存储</p><p>原始文档 -》embedded</p><p>query  -》embedded</p><p>必须使用同一个Embedded model</p><h2 id="参考学习的资料："><a href="#参考学习的资料：" class="headerlink" title="参考学习的资料："></a>参考学习的资料：</h2><p>[1] 面向开发者的LLM教程:<a href="https://datawhalechina.github.io/llm-cookbook/#/">https://datawhalechina.github.io/llm-cookbook/#/</a></p><p>[2] LLMBOOK:<a href="https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file">https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file</a></p><p>[3] <a href="https://developer.aliyun.com/article/1361737">https://developer.aliyun.com/article/1361737</a></p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>抉择本身就是向前吧</title>
    <link href="/2024/10/24/%E6%8A%89%E6%8B%A9/"/>
    <url>/2024/10/24/%E6%8A%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h2 id="选择与看待结果"><a href="#选择与看待结果" class="headerlink" title="选择与看待结果"></a>选择与看待结果</h2><p>选择不分对错，抉择本身就是向前吧！</p><p>健健康康、内心阳光；事物都是在波动、曲折中发展的，无论结果如何，都要保持好心态。</p><p>尊重不同的声音，调整自己的状态。</p><p>结果当然重要，符合自己的预期，那得开心死；但是，结果如果不达预期，就不正常了吗？就不合理了吗？存在即合理啊，只是没达到你自己的预期，你会失落、会感到遗憾、会不开心。但是，没有达到预期，自己就没有一点问题吗？我想自身还是有很大问题的，而且不要过多的去抱怨周围的环境。</p><h2 id="要勇敢"><a href="#要勇敢" class="headerlink" title="要勇敢"></a>要勇敢</h2><p>要勇敢，但是勇敢的方式不要有攻击性，这还是要慢慢改、慢慢学的。</p><h2 id="爱"><a href="#爱" class="headerlink" title="爱"></a>爱</h2><p>好好爱自己、好好爱家人，爱人先爱己，勿忘心安。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal RAG System Survey</title>
    <link href="/2024/10/20/Multimodal-RAG-System-Survey/"/>
    <url>/2024/10/20/Multimodal-RAG-System-Survey/</url>
    
    <content type="html"><![CDATA[<h1 id="A-Comprehensive-Guide-to-Building-Multimodal-RAG-Systems"><a href="#A-Comprehensive-Guide-to-Building-Multimodal-RAG-Systems" class="headerlink" title="A Comprehensive Guide to Building Multimodal RAG Systems"></a>A Comprehensive Guide to Building Multimodal RAG Systems</h1><p>本篇博文的主要内容是对一篇综述性博客文章()的翻译阅读与总结。通过阅读本篇文章，理清楚多模态RAG的基本概念与应用场景、部署方式、工作流程。</p><p>RAG系统的优势是可以整合自身数据并增强LLM的智力，对于问题给出更多精确的回答。然而，关键的限制就是RAG系统仅仅是它仅适用于文本数据。 许多现实世界的数据本质上是多模态的，是文本、图像、表格等形式的混合。</p><p>该篇综述的主要目的就是着眼构建一个多模态的RAG系统，使用智能数据转换和多模态LLM解决混合数据格式。</p><h2 id="传统的RAG系统"><a href="#传统的RAG系统" class="headerlink" title="传统的RAG系统"></a>传统的RAG系统</h2><p>RAG系统架构通常包括：</p><ol><li>数据处理和索引</li><li>检索和响应生成</li></ol><h3 id="数据处理和索引"><a href="#数据处理和索引" class="headerlink" title="数据处理和索引"></a>数据处理和索引</h3><p>加载文档中的文本内容-&gt;将大型文本元素拆分为较小的块-&gt;使用嵌入器模型将它们转换为嵌入-&gt;将块和嵌入存储到矢量数据库中</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-5.webp" alt="建立索引"></p><h3 id="检索和响应生成"><a href="#检索和响应生成" class="headerlink" title="检索和响应生成"></a>检索和响应生成</h3><p>用户提问-&gt;文档检索-&gt;提示-&gt;LLM-&gt;回答</p><p>从向量数据库中检索与输入问题相似的相关文本文档块，然后将问题和上下文文档块发送到LLM以生成人类。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-1-3.webp" alt="检索生成"></p><h2 id="传统RAG系统的限制"><a href="#传统RAG系统的限制" class="headerlink" title="传统RAG系统的限制"></a>传统RAG系统的限制</h2><ul><li>无法获悉实时数据</li><li>系统仅会利用好我们自身向量数据库中的数据</li><li>大部分RAG系统仅仅致力于文本数据的检索和生成</li><li>传统的LLMs 仅仅处理文本内容去生成回答</li><li>无法利用多模态数据进行工作</li></ul><h2 id="什么是多模态？"><a href="#什么是多模态？" class="headerlink" title="什么是多模态？"></a>什么是多模态？</h2><p>Multimodal data is essentially data belonging to multiple modalities.</p><p>多模态数据本质上是属于多种模态的数据。 模态的正式定义来自人机交互 (HCI) 系统的背景，其中模态被称为计算机和人类之间的单个独立输入和输出通道的分类（更多详细信息请参阅维基百科）。 常见的计算机-人类模式包括以下内容：</p><p>文本：Input and output through written language(e.g., chat interfaces).</p><p>语音：Voice-based interaction(e.g., voice assistants).</p><p>视频：Image and video processing for visual recognition (e.g., face detection).</p><p>手势：Hand and body movement tracking(e.g., gesture controls).</p><p>触控：Haptic feedback and touchscreens.</p><p>音频：Sound-based signals (e.g., music recognition, alerts).</p><p>生物识别：Interaction through physiological data (e.g., eye-tracking, fingerprints).</p><p>本综述重点关注处理文本、图像和表格。 理解此类数据所需的关键组件之一是多模态大语言模型 (LLM)。</p><p>多模态的本质是混合了多种多种模式和格式的数据：</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-2_11zon.webp" alt="多模态数据"></p><h2 id="什么是多模态大语言模型？"><a href="#什么是多模态大语言模型？" class="headerlink" title="什么是多模态大语言模型？"></a>什么是多模态大语言模型？</h2><p>多模态大语言模型 (LLM) 本质上是基于 Transformer 的 LLM，已针对多模态数据进行预训练和微调，以分析和理解各种数据格式，包括文本、图像、表格、音频和视频。 </p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-3-2.webp" alt="MLLMs"></p><p> 理想情况下，真正的多模态模型不仅应该能够理解混合数据格式，还应该生成相同的数据格式。</p><p>文生图、图生文、文生音、文生视频</p><p>多模态编码阶段：</p><p>利用现有的成熟模型对各种模式的输入进行编码。</p><p>LLM理解和响应阶段</p><p>LLM被用作NExT GPT的核心代理。从技术上讲，他们采用Vicuna LLM，该LLM<strong>将来自不同模态的表示作为输入</strong>，并对<strong>输入进行语义理解和推理</strong>。它输出1）直接的文本响应和2）每种模态的<strong>信号标记</strong>，这些标记用作指示<strong>解码层是否生成多模态内容以及如果是则生成什么内容的指令</strong>。</p><p>多模态生成阶段</p><p>从LLM（如果有的话）<strong>接收具有特定指令的多模态信号</strong>，<strong>基于Transformer的输出投影层将信号令牌表示映射到后续多模态解码器可以理解的表示中</strong>。从技术上讲，他们采用了不同模态代的现成潜在条件<strong>扩散模型</strong>，即用于图像合成的稳定扩散（SD）、用于视频合成的Zeroscope和用于音频合成的AudioLDM。</p><p>目前，大多数可供实际使用的多模态LLM都是片面的，这意味着它们可以理解混合数据格式，但只能生成文本响应。</p><p><strong>GPT-4V &amp; GPT-4o</strong> (OpenAI):</p><p><strong>Gemini</strong> (Google):</p><p><strong>Claude</strong> (Anthropic):</p><p>想要构建完全开源的解决方案或担心数据隐私或延迟并更喜欢在内部本地托管所有内容，可以借助目前最流行的开放和开源多式联运模型构建：</p><p>LLaVA-NeXT: An open-source multimodal model with capabilities to work with text, images and also video, which an improvement on top of the popular LLaVa model</p><p>PaliGemma: A vision-language model from Google that integrates both image and text processing, designed for tasks like optical character recognition (OCR), object detection, and visual question answering (VQA). </p><p>Pixtral 12B: An advanced multimodal model from Mistral AI with 12 billion parameters that can process both images and text. Built on Mistral’s Nemo architecture, Pixtral 12B excels in tasks like image captioning and object recognition.</p><p>目前为止最为强大的多模态模型是Open-AI的GPT-4.o</p><h2 id="多模态RAG系统的工作流"><a href="#多模态RAG系统的工作流" class="headerlink" title="多模态RAG系统的工作流"></a>多模态RAG系统的工作流</h2><p>多模态RAG系统的构建流程与本篇博客的构建选择：</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-4-2.webp" alt="workflow"></p><p>不好理解的一部分？</p><p>端到端的工作流：</p><p>如何去理解Summary？为什么图像、表格、和文本中在嵌入的时候都变成了Image Summary、Table Summary、Text Summary？</p><p>Option 1：</p><p>选择1缺陷的原因是多模态嵌入模型通常无法对这些视觉图像中的数字等粒度信息进行编码，并将其压缩为有意义的嵌入。</p><p>Option 2：</p><p>选项 2 受到严重限制，因为我们最终不会在该系统中使用图像，即使它可能包含有价值的信息，并且它不是真正的多模式 RAG 系统。 </p><p>Option 3：</p><p>将继续使用选项 3 作为我们的多模态RAG 系统工作流程。 在此工作流程中，我们将从图像、表格以及可选的文本块中创建摘要，并使用多向量检索器，这可以帮助根据相应的原始图像、表格和文本元素映射和检索原始图像、表格和文本元素。 </p><h2 id="Multi-Vector-Retrieval-Workflow"><a href="#Multi-Vector-Retrieval-Workflow" class="headerlink" title="Multi-Vector Retrieval Workflow"></a>Multi-Vector Retrieval Workflow</h2><p>考虑到我们将在上一节中讨论的工作流程，对于我们的检索工作流程，我们将使用如下图所示的多向量检索器，正如 LangChain 博客中推荐和提到的那样。 多向量检索器的主要目的是充当包装器，并帮助将每个文本块、表格和图像摘要<strong>映射到</strong>实际的文本块、表格和图像元素，然后可以在检索期间获得这些内容。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-5-2.webp" alt="多向量检索器"></p><p>首先使用 Unstructed 等文档解析工具分别提取文本、表格和图像元素。 然后，我们将<strong>把每个提取的元素传递到 LLM 中，并生成详细的文本摘要</strong>，如上所示。 接下来，我们将使用任何流行的<strong>嵌入器模型</strong>（例如 OpenAI Embedders）将<strong>摘要及其嵌入存储到向量数据库</strong>中。 我们还将每个摘要的相应原始文档元素（文本、表格、图像）存储在文档存储中，文档存储可以是任何数据库平台，例如 Redis。</p><p>当用户提出问题时，首先，多向量检索器检索相关摘要，这些摘要在语义（嵌入）相似性方面与问题相似，然后使用公共的 doc_ids、原始文本、表格和图像 返回的元素将进一步传递给 RAG 系统的 LLM 作为回答用户问题的上下文。</p><h2 id="多模态RAG的架构细节"><a href="#多模态RAG的架构细节" class="headerlink" title="多模态RAG的架构细节"></a>多模态RAG的架构细节</h2><ol><li>Load all documents and use a document loader like <strong>unstructured.io</strong> to extract text chunks, image, and tables.</li><li>If necessary, convert HTML tables to markdown; they are often very effective with LLMs</li><li>Pass each text chunk, image, and table into a multimodal LLM like GPT-4o and get a detailed summary.</li><li>Store summaries in a vector DB and the raw document pieces in a document DB like Redis</li><li>Connect the two databases with a common document_id using a multi-vector retriever to identify which summary maps to which raw document piece.</li><li>Connect this multi-vector retrieval system with a multimodal LLM like GPT-4o.</li><li>Query the system, and based on similar summaries to the query, get the raw document pieces, including tables and images, as the context.</li><li>Using the above context, generate a response using the multimodal LLM for the question.</li></ol><hr><p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-woXqKviW7JlYQlmaecD0QpvsiyX4PNJoAB6d4wEm_y4Ks6qXuJNTZ61KTOyJ1v8xi1k4CujEisnRNAWM4LIpgQs_HREGihbG3bquz1y-YXsNt84k7Y9jWy_Vip6yLPiIfNmnTAyI86xvEzbYFSpu69g?key=jCBGNvykPV6Ye85DC-Uxrg" alt="架构"></p><p>1.加载所有文档，并使用类似unstructure.io的文档加载器提取文本块、图像和表格。</p><p>2.如有必要，将HTML表格转换为markdown；他们通常对LLM非常有效</p><p>3.将每个文本块、图像和表格传递到GPT-4o等多模式LLM中，并获得详细的摘要。</p><p>4.将摘要存储在向量数据库中，将原始文档片段存储在Redis等文档数据库中</p><p>5.使用多向量检索器使用公共document_id连接两个数据库，以识别哪个摘要映射到哪个原始文档块。</p><p>6.将这个多向量检索系统与GPT-4o等多模态LLM连接起来。</p><p>7.查询系统，并根据与查询类似的摘要，获取原始文档片段，包括表格和图像，作为上下文。</p><p>8.使用上述上下文，使用多模态LLM生成问题的答案。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我们是怎么学习的？机器是如何学习我们的学习的？</title>
    <link href="/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/"/>
    <url>/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>写代码和写情书是一样的，得多练 ——杨力祥</p>          </div><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>杨力祥老师上节课讲了，之所以自己职业生涯在很早的时候投入到AI领域，包括复现Alpho Go；在弄清楚机器是如何学习的同时，也想知道人自身是怎么学习的？</p><p><strong>关联、体系</strong>是学习的关键，不要被一些看似“懂得多”、“行家”某些人忽悠。学东西，任何一门东西，都要有体系的去学、有结构的去学。不要想玩沙子一样，看似懂得很多、知道的很多，但是只要一追问、深究，或者问问A和那个B之间有啥联系，他便说不上来了。</p><p>力祥哲学(把程序写成东西—面向对象思想的精髓)同样适合用于学习人工神经网络的体系架构与技术细节。</p><p>按着<strong>把程序写成东西</strong>的思路，思考如何学习CNN、RNN、GAN呢？——还是去找“像什么”？每一层的具体细节可能有所差别，但是层与层之间的结构可以抽取共性。于是可以<strong>抽象出Layer类</strong>，基于Layer类去做派生，实现不同层本身之间的差异。</p><p><img src="/img/fig/history-dl.jpg" alt="深度学习发展史"></p><p>图片引自$^{[2]}$</p><h2 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h2><p>杨老师上课一直在无形地传授给我们如何学习？如何思考？</p><p>但是自己一直想不明白，于是昨天晚上下课，去找杨老师交流了一下他的学习方法。（2024.11.04）</p><p>杨老师和我说，他小时候正赶上“文革”时期，和我爷爷奶奶姥姥姥爷差不多，小时候没有书可看，所以很多时候就靠想，想不出来也要想。</p><p>按着把程序写成东西的思想，切入到一个新领域，要从这个领域内最基础的例子出发，也就是说，往这件事的根上倒。杨老师带团队做操作系统，一开始就是找的Linux内核v0.01版本，Linux内核最古老最原始的一版本；和女儿学人工智能的时候，咨询了业内的大佬，也是从书写数字识别开始入手的。问题导向，而且是可以揭示这个领域内最本质的问题出发开始往后面派生。</p><p>最后，杨老师还问了我一个问题：<strong>人为什么会问问题？</strong>老师让我把这个问题整明白，这个问题思考清楚了，怎么学习也就差不多快弄明白了。而且老师还举了个例子，人类在教黑猩猩说话的过程中，花费大量的时间确实教会了黑猩猩哑语，但是这个过程中黑猩猩始终没有提问题，想想这是为什么？</p><p>想要快速切入进一个领域应该做到：</p><ol><li>想，人家能想出来，你也可以；只是你的大脑从来就没有经过“想”的训练过程</li><li>案例导向，往根上倒，找最基本最经典的案例做切入点</li><li>能发现并且会提出切入领域内的问题，会提问</li></ol><h2 id="认知-人与机器"><a href="#认知-人与机器" class="headerlink" title="认知-人与机器"></a>认知-人与机器</h2><p>认知科学、认知心理学</p><p>杨老师说，咱们的祖先从猿猴进化为人类，其中学会的一个最重要的技能就是学会了<strong>分类</strong>。在课堂上，讲到面向对象的核心思想的时候，杨老师也是按照分类的思想，引申出类和对象的关系。分类本质上就是一颗二叉树&#x2F;多叉树，在实际思考时，是自下而上(树的层次结构)<strong>抽取共性</strong>，抽象就是脱离形式提取共性，但写的时候从上到下派生(树的层次结构)。</p><p><img src="/img/fig/nao.jpg" alt="认识与分类"></p><p>好了，上面介绍了人类认知的起源过程。OK，那么机器实现分类靠的是什么？机器如果学会了分类是不是就可以慢慢地开始学习人类的所有的思维模式呢？</p><p>初学机器学习，我们就学了回归和分类。回归可以用于预测<em>多少</em>的问题，但是分类问题不是问你“多少”，而是问你“哪一个”。</p><p>回归方法中最基础的方法就是线性回归，分类问题中最基础的方法就是softmax。</p><p>通常，机器学习实践者用<em>分类</em>这个词来描述两个有微妙差别的问题： </p><ol><li>我们只对样本的“硬性”类别感兴趣，即属于哪个类别； </li><li>我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。$^{[1]}$</li></ol><div class="note note-success">            <p>能搞明白<strong>怎么去分类</strong>，就可以慢慢地学着和人类一样去学习了。</p>          </div><h3 id="Sigmod"><a href="#Sigmod" class="headerlink" title="Sigmod"></a>Sigmod</h3><p>Sigmoid 函数常用于<code>二分类问题</code>的激活函数。</p><p>函数的定义域为所有实数，通常输出范围为 <strong>0 到 1</strong>，但某些变体（如<a href="https://en.wikipedia.org/wiki/Hyperbolic_functions">双曲正切</a>）的输出值介于 -1 和 1 之间。</p><p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p><p><img src="/img/fig/sigmod.png"></p><h3 id="Softmax-归一化指数函数"><a href="#Softmax-归一化指数函数" class="headerlink" title="Softmax-归一化指数函数"></a>Softmax-归一化指数函数</h3><p>本质上是一个<code>多分类</code>激活函数<br>$$<br>\text{softmax}(z_i) &#x3D; \frac{e^{z_i}}{\sum_{j&#x3D;1}^{n} e^{z_j}}<br>$$</p><p><img src="/img/fig/softmax.png"></p><p> softmax 函数通常用于<strong>分类任务</strong>的<code>神经网络模型</code>的<strong>最后一层</strong>，它通过取每个输出的指数并通过除以所有指数的总和来标准化这些值，<code>将原始输出分数（也称为 logits）转换为概率</code>。此过程可确保输出值在 (0,1) 范围内且总和为 1，使其可解释为概率。</p><p>神经网络的输出格式通常为one-hot格式，one-hot是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。（神经网络的输出格式）</p><p>Softmax的作用是可以将输入转换为概率分布，确保所有输出的总和为1</p><h3 id="Softmax函数和Sigmoid函数有何区别？"><a href="#Softmax函数和Sigmoid函数有何区别？" class="headerlink" title="Softmax函数和Sigmoid函数有何区别？"></a>Softmax函数和Sigmoid函数有何区别？</h3><p>单看函数图像，貌似Softmax函数和Sigmoid函数没有任何区别呀！</p><p>Softmax函数适用于<strong>多分类</strong>问题，将多个类别的概率进行归一化；而Sigmoid函数主要应用于<strong>二分类</strong>问题，将实数映射到0到1之间的概率。</p><h4 id="二分类情况下"><a href="#二分类情况下" class="headerlink" title="二分类情况下"></a>二分类情况下</h4><p>Softmax 和 Sigmoid 实际上是等价的，它们的图像几乎完全相同。</p><h4 id="多分类状况下"><a href="#多分类状况下" class="headerlink" title="多分类状况下"></a>多分类状况下</h4><ul><li>Softmax 考虑所有类别之间的关系，每个类别的概率受到其他类别的影响。</li><li>Sigmoid 则是独立计算每个类别的概率，不保证概率和为 1。</li></ul><h2 id="脑科学"><a href="#脑科学" class="headerlink" title="脑科学"></a>脑科学</h2><p>[引自weibo博主blog：<strong>indigo</strong>]</p><p>推荐最近读过的最棒的一本书《A Brief History of Intelligence》，作者 Max Bennett 不是学者和生物学专家，而是一位创业者和 AI 专家，他联合创办了估值过十亿美金的 AI 公司 Bluecore。 这本“智能简史”我认为可以和 Yuval Harari 的《人类简史》比肩，从地球生物视角纵览了智能的演化，全书建立一个描述生物大脑进化的新框架，作者称其为“五次突破”，这对现在 AI 与具身智能的研究有巨大的参考价值。</p><p><strong>关联</strong>的学习方法很符合大脑结构，<strong>神经元之间的连接</strong>正好反映出了这种关联关系。</p><p>下面这部分，结合大脑的结构，我们从最简单的线性神经网络、感知机开始本次“元学习”之旅～</p><h3 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h3><p>通过softmax回归构建出单层线性神经网络：<br>$$<br>o_1 &#x3D;x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1 \<br>$$</p><p>$$<br>o_2 &#x3D;x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2 \<br>$$</p><p>$$<br>o_3 &#x3D;x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3 \<br>$$</p><p><img src="/img/fig/softnet.jpg" alt="单层线性网络"></p><p>$$<br>\mathbf{O}&#x3D;\mathbf{X}·\mathbf{W}+\mathbf{b} \<br>$$</p><p>$$<br>\hat{\mathbf{Y}}&#x3D;\mathrm{softmax}(\mathbf{O})&#x3D;\frac{\exp(o_j)}{\sum_k\exp(o_k)} \<br>$$</p><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的<strong>仿射</strong>变换决定。 因此，softmax回归是一个<em>线性模型</em>。</p><h3 id="计算图和感知机"><a href="#计算图和感知机" class="headerlink" title="计算图和感知机"></a>计算图和感知机</h3><div class="note note-success">            <p>感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p>          </div><h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p>感知机是神经网络的原型，顾名思义，一种具有感知能力的机器。</p><p>有感知这件事可不一般吧，有感知以后，当接受到外界刺激后，便会对刺激产生反应了。</p><p>老师上课点名，听到名字后会立即喊“到”（如果不翘课的话嘿嘿），触摸到尖锐的物体或高温热水，会下意识的缩手；都是反应和感知。</p><p>在深度学习领域，感知机的本质其实是一种运用<strong>Logistic回归</strong>的二分类模型。它只能处理二分类问题而且必须是线性可分的问题。</p><p>由简单的线性回归网络过渡延伸到多层感知机，本质上就是网络中间的隐藏层的层数增加了。</p><p><img src="/img/fig/duoceng.jpg" alt="多层多分类感知机"> </p><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>Sigmod函数的计算图过程：</p><p><img src="/img/fig/jisuantu.png" alt="计算图流程-邱锡鹏"></p><h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><blockquote><p> 建立一个体系，将知识建立起联系：<strong>softmax</strong>、<strong>sigmod</strong>、<strong>感知机</strong>、<strong>计算图</strong>、<strong>激活函数</strong>、<strong>反向传播</strong></p></blockquote><p>我们是怎么学习的？始终不要忘记这个任务主线。</p><p>那么，我是怎么认识的呢？</p><p>反正我最初是被家大人带着认识东西的。</p><p>就我个人而言，从我出生，就是大人们带着我认识各种东西。一个典型的例子，刚出生没多久，学说话。老妈就对我说：“叫妈妈”、老爸就对我说：“叫爸爸”。从小开始，就学着认识人、认识各种东西。如果将我这个“婴儿”比作一个机器人（有点抽象哈哈），那么，父母带着我识人识物的过程不就是对我的一个训练过程吗？！</p><p>随着我慢慢长大，我在最初的被人带着学（学会了一些东西、认识了一些人和东西）的基础之上，我开始自学主动去探索、认识更多的东西。这期间我也会犯错，但是，每犯错一次，都会加深我的印象，纠正的我之前的想法与认识。</p><p>慢慢地，随着我认识的越多、学到的东西越多、对事物的分类也愈来愈清晰以后，我逐渐会自己学习、自己产出、自己创造生成了。</p><p>别人问我：“1+1&#x3D;？”，我不仅可以准确的告诉她答案，我也会不由自主的<strong>联想</strong>到“1*1”、“1&#x2F;1”、“1-1”。</p><div class="note note-success">            <p>错误的认识或分类，会作为一个刺激信号在大脑皮层中进行一次<strong>反向传播</strong>，调整参数，然后让我们更新认知，纠正错误。</p><p>这是我理解的人的学习方式，抛开基于统计，对于“机器学习的过程”，我猜亦如此</p>          </div><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNet</span><br>&#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-built_in">MyNet</span>();<br>  ~<span class="hljs-built_in">MyNet</span>();<br>  <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">InintInput</span><span class="hljs-params">(<span class="hljs-type">int</span> nodes)</span></span>;<br>  <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">InitOutput</span><span class="hljs-params">(<span class="hljs-type">int</span> nodes)</span></span>;<br>  <span class="hljs-built_in">FunctionChoice</span>(string s);<br>  <span class="hljs-built_in">Back</span>();<br>  <span class="hljs-built_in">Build</span>(vector&lt;<span class="hljs-type">float</span>&gt; x,vector&lt;<span class="hljs-type">float</span>&gt; w,vector&lt;<span class="hljs-type">float</span>&gt; b);<br><span class="hljs-keyword">private</span>:<br>  vector&lt;<span class="hljs-type">float</span>&gt; w;<br>  vector&lt;<span class="hljs-type">float</span>&gt; b;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="剖析神经元结构"><a href="#剖析神经元结构" class="headerlink" title="剖析神经元结构"></a>剖析神经元结构</h3><p><img src="/img/fig/shenjing.jpg" alt="神经元结构"> </p><p>输入：自变量&#x2F;参数 x列向量</p><p>激活函数:在计算网络中， 一个节点的<strong>激活函数</strong>定义了该节点在给定的输入或输入的集合下的输出。$^{[WiKi]}$</p><p>参数：w,b向量</p><p>输出：output列向量</p><h3 id="OO思想看神经网络"><a href="#OO思想看神经网络" class="headerlink" title="OO思想看神经网络"></a>OO思想看神经网络</h3><div class="note note-success">            <p>神经元网络能看成东西吗？用面向对象思想怎么玩？—本质上还是在<strong>搭积木</strong></p><p>高级思考：设计好的网络结构内存开哪？每一层要不要自己开内存？每层开内存也是共性</p>          </div><p>用带参数的构造函数初始化一个小网络，宽度、层数</p><p>1.动手写sin函数-泰勒展开，精度多少看rn项&#x2F;误差项0.000001</p><p>生成式最先淘汰的就是程序员，因为程序员最会训练程序员。</p><p>GitHub是黑窝。</p><p>操作系统和编译器能不能变成面向对象的方法写呢？</p><h3 id="学习的动机-提出好问题"><a href="#学习的动机-提出好问题" class="headerlink" title="学习的动机-提出好问题"></a>学习的动机-提出好问题</h3><div class="note note-success">            <p>怎么才能提出高水平的问题？</p>          </div><h3 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播-Back Propagation"></a>反向传播-Back Propagation</h3><p>反向传播的思想就是网络结构一般以随机参数开始，伴随着不尽如人意的初始效果，通过“学习”（反向传播）赋予网络“穿越”的能力，使其自身不断自我更新，从而达到一个理想的结果。$^{[4]}$</p><p>以实际正确类别与预测类别的损失为基础，以终为始，反向更新权重参数。</p><div class="note note-success">            <p>做题做错了，对答案，将错题的正解记录到错题本上，下次再遇见条件反射出错题本上的正确解题步骤修正自己之前的惯性错误方法。</p>          </div><p><strong>C++复现反向传播函数：</strong></p><h2 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN-卷积神经网络"></a>CNN-卷积神经网络</h2><p>参见文章《经典卷积神经网路的复现》</p><h2 id="RNN-循环神经网络"><a href="#RNN-循环神经网络" class="headerlink" title="RNN-循环神经网络"></a>RNN-循环神经网络</h2><p>参见文章《都手学循环神经网络》</p><h2 id="Attention-is-all-you-Need"><a href="#Attention-is-all-you-Need" class="headerlink" title="Attention is all you Need"></a>Attention is all you Need</h2><p>参见文章《Transformer复现》</p><p>思考：注意力机制为什么没有和神经网络语言模型和概率统计模型没有关联关系？</p><p>注意力机制就是一个加权求和机制，应用非常灵活，应用面非常之广</p><p>参见文章《Transformer复现》</p><p>可以尝试复现Transformer，大作业之一</p><p>出题：填空、简答之类的概念性；设计题（应用）</p><p>文本匹配任务的建模方法：孪生网络和交互聚合</p><p>序列标注任务的建模方法</p><h2 id="参考文献与资料"><a href="#参考文献与资料" class="headerlink" title="参考文献与资料"></a>参考文献与资料</h2><p>[1]动手学深度学习：<a href="https://zh.d2l.ai/">https://zh.d2l.ai/</a></p><p>[2]动手实践机器学习：<a href="https://aibydoing.com/">https://aibydoing.com/</a></p><p>[3]<strong>中国科学院大学.杨力祥老师课程《C++面向对象程序设计》</strong></p><p>[4]耿直哥随笔.<a href="https://gengzhige-essay.readthedocs.io/docs/03%20%E6%84%9F%E7%9F%A5%E6%9C%BA/3-3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3.html">反向传播的深入理解</a></p><p>[5] <a href="https://xpqiu.github.io/">邱锡鹏</a>，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/">https://nndl.github.io/</a>, 2020.</p><div class="note note-danger">            <p>记住特征-学会分类-类别中抽取共性-共性中总结概念-概念指导解决问题-解决问题过程中总结方法-方法加深特征印象</p>          </div>]]></content>
    
    
    <categories>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>创新思维</tag>
      
      <tag>力祥哲学</tag>
      
      <tag>系统论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>恐惧还是贪婪</title>
    <link href="/2024/10/14/%E6%81%90%E6%83%A7%E8%BF%98%E6%98%AF%E8%B4%AA%E5%A9%AA/"/>
    <url>/2024/10/14/%E6%81%90%E6%83%A7%E8%BF%98%E6%98%AF%E8%B4%AA%E5%A9%AA/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>别人恐惧时你要贪婪，别人贪婪时你要恐惧—沃伦·巴菲特</p>          </div><h2 id="对JJ要有信心—做时间的朋友、长期主义"><a href="#对JJ要有信心—做时间的朋友、长期主义" class="headerlink" title="对JJ要有信心—做时间的朋友、长期主义"></a>对JJ要有信心—做时间的朋友、长期主义</h2><p>今年总体是赚钱的，见好就收，还有两个月过年，猥琐发育，坚持住✊！今年的机会也差不多就到这了，没那么多心思去关注了，学习快跟不上了哈哈哈～</p><p>国庆节后，心情就是过山车。。。最重要的原因就是因为<strong>ZB市场</strong>的起起伏伏。</p><p><img src="/img/life/184.jpeg"></p><p>（当时心想，我要是有100个w，一周的操作足以半生衣食无忧，人在市场面前还是好渺小。。。）</p><p>当年明月在《明朝那些事儿》里说的非常好，<strong>我们这个古老的民族，五千多年生生不息，只要自己人不折腾自己人，日子肯定会愈过愈好。</strong></p><p>都说这几年大环境不好、jj不好，但是实话实讲，今年开始，只要动机正有耐心，还是慢慢能赚到钱的。</p><p><img src="/img/life/jj1.jpeg"></p><p>直到上周，才真正体会到了老舅前一阵子的感慨：压力最大的不是下跌你陪了多少进去，而是疯涨的时候你不知道什么时候收手，人性都是贪婪的，而且是常态。</p><p><img src="/img/life/jj2.jpeg"></p><p>我是国庆节前的一个周五下午，看了zzj会议释放的信号，和微博、水木社区的某些评论，决定赌一把，想着投机赚一笔。雷总说的好：“<strong>机会来了，猪都能飞啊</strong>！”。真是第一次感到市场的力量，无形的手，指数级增长的魅力。</p><h2 id="理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是"><a href="#理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是" class="headerlink" title="理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是"></a>理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是</h2><p>三年多了，感谢国家让我体会到了什么叫<strong>指数级增长</strong>，一颗小草根伴随着时代的飓风呼啸而过。</p><p>斐波那契数列真不是吹的！</p><h3 id="创业板"><a href="#创业板" class="headerlink" title="创业板"></a>创业板</h3><p>涨的最疯狂的板块，说实话，<strong>不懂，赚钱纯靠投机</strong>。所以到年底，<strong>抽出来，别被套进去，见好就收</strong>。后期无论再怎么疯狂，都首先观望，不着急入手，维持在500以下<strong>求稳定长期回报</strong>。</p><p><img src="/img/life/jj-chuangye.jpeg"></p><h3 id="永利债券"><a href="#永利债券" class="headerlink" title="永利债券"></a>永利债券</h3><p>目前为止，持有时间最长的一支基金，也是体会到了时间的朋友。这波高潮没有赶上，在高潮结束后，市场情绪稳定后，我决定加码永利。毕竟债券的风险要低与股票型基金，只有长期持有，才有回报的可能，被总想着挣快钱。</p><p><img src="/img/life/jj-zhaiquan.jpeg"></p><h3 id="芯片半导体"><a href="#芯片半导体" class="headerlink" title="芯片半导体"></a>芯片半导体</h3><p>我个人长期<strong>看好芯片半导体</strong>，毕竟现在自己从事AI相关的行当。但是具体规则目前还没学明白，未来趋势看不清，我想买未来，但是风险难把握。所以，高潮小赚一笔过后，抽离，维持在500以下<strong>求稳定长期回报</strong>。</p><p><img src="/img/life/jj-xinpian.jpeg"></p><p>今天JF军对湾湾进行联合利剑-2024B JS演习，军工大涨，对于这种机会就属于转瞬即逝，想抓住很难。</p><h1 id="日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾"><a href="#日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾" class="headerlink" title="日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾"></a>日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾</h1><p>现在愈来愈对这句话感同身受，能总结出这句话来的人真是牛逼！</p><p>从本科开始试水理财，加上疫情两年多消费不大，而且也没谈对象，自己还是在支付宝上攒下了一笔钱。时至今天回头再看，当初的这个攒钱决策时正确的，因为能给今天带来很大的自由空间。那么下一步就是要思考，如何在一个更好的平台上去发现更多的资源，创造更多的财富。注意，这里说的是财富，不是钱。</p><p>ZB市场起起伏伏，恰如生活一样，都是<strong>波动的</strong>，事事如你所愿，很多时候是异想天开。唯一能做的也只是把握好能把握住的：真诚待人、踏实工作、精进技能、爱自己的家人和爱的人、锻炼好身体、保持好心态；能做到上述这些的，已是凤毛麟角。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop集群配置汇总</title>
    <link href="/2024/10/12/Hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B1%87%E6%80%BB/"/>
    <url>/2024/10/12/Hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="汇总目的"><a href="#汇总目的" class="headerlink" title="汇总目的"></a>汇总目的</h2><ol><li>老师给的参考教程看的头大</li><li>畅哥在用Ubuntu，之前自己都很懒用的配置好的Centos7，所以决定学一学Ubuntu系统</li></ol><h2 id="啥是伪分布式？与集群有啥区别？"><a href="#啥是伪分布式？与集群有啥区别？" class="headerlink" title="啥是伪分布式？与集群有啥区别？"></a>啥是伪分布式？与集群有啥区别？</h2><p>伪分布式模式是一种在单节点上模拟多节点运行的方式，‌Hadoop进程以分离的‌Java进程运行，节点既作为‌NameNode也作为DataNode，同时读取‌HDFS中的文件。这种模式虽然不是真正的分布式，但可以在单台机器上模拟多节点环境，方便调试和理解分布式系统的运行机制。</p><p>在分布式部署中，Hadoop集群由多台计算机组成，每台计算机都可以独立运行Hadoop的各个组件，如HDFS（Hadoop分布式文件系统）和MapReduce。数据会被分散存储在不同的节点上，任务也会并行执行在不同的节点上，从而提高了整个系统的性能和可靠性。</p><p>我是这样理解的：严格意义上来讲，一台计算机中开多台虚拟机属于伪分布式操作；但是多虚拟机恰恰模拟出了多计算机的效果，所以叫它集群配置。真正的分布式最好的例子就是阿里云买三台服务器，统一配置，最直观，可惜成本最大。。。。</p><h2 id="操作系统的选择"><a href="#操作系统的选择" class="headerlink" title="操作系统的选择"></a>操作系统的选择</h2><p>Ubuntu18.04&#x2F;22.04</p><p>Centos7</p><h2 id="虚拟机的选择"><a href="#虚拟机的选择" class="headerlink" title="虚拟机的选择"></a>虚拟机的选择</h2><p>Vmare</p><p>Virtual Box</p><p>云服务器</p><p>推荐的组合：Centos7+Vmare、Vmare+Ubuntu、云服务器(单机版，集群费用成本太高)</p><h1 id="以下都是高质量教程-我的除外"><a href="#以下都是高质量教程-我的除外" class="headerlink" title="以下都是高质量教程(我的除外)"></a>以下都是高质量教程(我的除外)</h1><h2 id="基于Ubuntu的集群搭建详细教程"><a href="#基于Ubuntu的集群搭建详细教程" class="headerlink" title="基于Ubuntu的集群搭建详细教程"></a>基于Ubuntu的集群搭建详细教程</h2><h3 id="高质量博客推荐"><a href="#高质量博客推荐" class="headerlink" title="高质量博客推荐"></a>高质量博客推荐</h3><p>厦门大学林子雨老师的大数据博客网站非常不错，里面文章内容也非常有料：</p><p><a href="https://dblab.xmu.edu.cn/blog/category/big-data/">https://dblab.xmu.edu.cn/blog/category/big-data/</a></p><p>老师秉持着开源精神，将大部分资源全部开源了，推荐给每一个我的博客的读者！！！羊毛该薅就得薅</p><h3 id="简略版本："><a href="#简略版本：" class="headerlink" title="简略版本："></a>简略版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/94/">https://dblab.xmu.edu.cn/blog/94/</a></p><h3 id="集群版本："><a href="#集群版本：" class="headerlink" title="集群版本："></a>集群版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/8/">https://dblab.xmu.edu.cn/blog/8/</a></p><h3 id="伪分布式版本："><a href="#伪分布式版本：" class="headerlink" title="伪分布式版本："></a>伪分布式版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/7/">https://dblab.xmu.edu.cn/blog/7/</a></p><h3 id="CSDN-blog教程："><a href="#CSDN-blog教程：" class="headerlink" title="CSDN-blog教程："></a>CSDN-blog教程：</h3><p>金州饿霸: <a href="https://blog.csdn.net/wangjian530/article/details/142781899">https://blog.csdn.net/wangjian530/article/details/142781899</a></p><h2 id="基于Centos7的集群搭建详细教程"><a href="#基于Centos7的集群搭建详细教程" class="headerlink" title="基于Centos7的集群搭建详细教程"></a>基于Centos7的集群搭建详细教程</h2><h3 id="伪分布式版本：-1"><a href="#伪分布式版本：-1" class="headerlink" title="伪分布式版本："></a>伪分布式版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/190/">https://dblab.xmu.edu.cn/blog/190/</a></p><h3 id="我自己的配置-不详细、慎重参考"><a href="#我自己的配置-不详细、慎重参考" class="headerlink" title="我自己的配置(不详细、慎重参考)"></a>我自己的配置(不详细、慎重参考)</h3><h4 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim core-site.xml<br>&lt;configuration&gt;<br>&lt;!-- 指定HDFS 中NameNode 的地址--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>    &lt;value&gt;hdfs://node1:8020&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- 指定Hadoop 运行时产生文件的存储目录--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>    &lt;value&gt;/export/server/hadoop-3.3.0/data&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- 在web UI访问HDFS使用的用户名 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br>    &lt;value&gt;root&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p>配置hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim hdfs-site.xml<br>&lt;!-- 指定Hadoop 辅助名称节点主机配置--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>    &lt;value&gt;node2:9868&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><h4 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim mapred-site.xml<br>&lt;configuration&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;!-- 指定MR 运行在YARN 上--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>    &lt;value&gt;yarn&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- MR App Master环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- MR MapTask环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- MR ReduceTask环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt; <br></code></pre></td></tr></table></figure><h4 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim yarn-site.xml<br>&lt;configuration&gt;<br>&lt;!-- Reducer 获取数据的方式--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 指定YARN 的ResourceManager 的地址--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>    &lt;value&gt;node1&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 服务器请求最小内存 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;<br>    &lt;value&gt;512&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 服务器请求最大内存 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;<br>    &lt;value&gt;2048&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 容器虚拟内存与物理内存之间的比率 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;<br>    &lt;value&gt;4&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h4 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim workers <br>node1<br>node2<br>node3<br></code></pre></td></tr></table></figure><h4 id="配置hadoop-env-sh"><a href="#配置hadoop-env-sh" class="headerlink" title="配置hadoop-env.sh"></a>配置hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim hadoop-env.sh<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br><br>export HDFS_NAMENODE_USER=root<br>export HDFS_DATANODE_USER=root<br>export HDFS_SECONDARYNAMENODE_USER=root<br>export YARN_RESOURCEMANAGER_USER=root<br>export YARN_NODEMANAGER_USER=root<br></code></pre></td></tr></table></figure><h4 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim /etc/profile<br>export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin<br></code></pre></td></tr></table></figure><h4 id="配置文件传输到其他节点"><a href="#配置文件传输到其他节点" class="headerlink" title="配置文件传输到其他节点"></a>配置文件传输到其他节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 server]# scp -r /etc/profile node2:$PWD  <br>[root@node1 server]# scp -r /etc/profile node3:$PWD<br>[root@node1 server]# scp /etc/profile node2:/etc <br>[root@node1 server]# scp /etc/profile node3:/etc<br></code></pre></td></tr></table></figure><p>格式化与一键启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">只能格式化一次</span><br>[root@node1 hadoop-3.3.0]# bin/hdfs namenode -format<br><br>[root@node1 sbin]# start-dfs.sh<br>[root@node1 sbin]# stop-dfs.sh<br>[root@node1 sbin]# start-yarn.sh<br>[root@node1 sbin]# stop-yarn.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">一键启动YARN、HDFS</span><br>start-all.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">一键关闭YARN、HDFS</span><br>start-all.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">单独启动或关闭集群</span><br>hdfs -daemon start namenode<br>hdfs -daemon stop namenode<br>hdfs -daemon start datanode<br>hdfs -daemon stop datanode<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年8-9月总结与展望</title>
    <link href="/2024/09/30/8-9%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/09/30/8-9%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="论文研读"><a href="#论文研读" class="headerlink" title="论文研读"></a>论文研读</h2><p>这个月只读了一篇关于RAG的综述文章，详细的笔记可以参考RAG for LLM：A survey这篇博客文章。</p><p>通过浏览该领域以及拓宽知识面，又找到了如下几个感兴趣的点：</p><ul><li><p>RAG</p></li><li><p>Text2SQL</p></li><li><p>多模态检索</p></li><li><p>Agent</p></li></ul><p>读文献的能力、论文写作的能力是需要一步步培养、积累、学些的，不能放松对写作能力的培养，<strong>写作和表达</strong>是这一年在雁栖湖要努力下功夫去打磨的本领。</p><h2 id="技术学习"><a href="#技术学习" class="headerlink" title="技术学习"></a>技术学习</h2><p>【重要】小土堆-Pytorch</p><p>【重要】李沐-动手学深度学习</p><p>ziliz-milvus数据库检索</p><p>黑马程序员-Spark、Flink、NoSQL</p><p>个人认为：这半年最主要的任务就是打好机器学习和深度学习的基础，一定要对原理有深刻的理解，对动手实现做到熟练运用。</p><p>不要着急，多下功夫。</p><h2 id="适应节奏"><a href="#适应节奏" class="headerlink" title="适应节奏"></a>适应节奏</h2><p>正式开学的一周，很不适应。时间安排上、上课的节奏上、作息规律上都不是非常适应。好在我认为我的适应能力还是很强的，从第二周开始，慢慢调整过来了，主要是中秋节去了趟雄安，和好朋友们毕业后聚了一聚，很多开学之初困扰的心结慢慢打开了。这个心结有学业方面的，生活方面的，感情方面的。聊了很多，也听他们聊了很多。虽人生南北多歧路，君向潇湘我向秦，但是最终的目标都是一样的。为了未来更好的生活而努力奋斗。</p><p>回来以后我整个人状态好了很多，最明显的变化就是食欲大增，心情好多了，更专注于自己的事情了。不要着急，给自己一个缓冲时间，慢慢发力。</p><p>现在变得越来越像一个夜猫子了，睡的越来越晚，早上也是不能早起了。我觉着要相对调整生物钟，和本科的时候不一样了，不要强迫自己，而必须保证一天最好的状态，效率第一。</p><h2 id="遇见良师"><a href="#遇见良师" class="headerlink" title="遇见良师"></a>遇见良师</h2><p>这个月和杨力祥老师课后长谈了两次，一次是在9.18晚上下课后，一次是在今天9.30号晚上下课后。两次长谈都是我们下课主动去找老师问问题，然后老师根据问题展开的多方面的讨论。</p><p>这是我在果壳遇到的难得的良师益友，他从思想上改变了我对计算机、对人工智能、对如何思考的观点与思维模式。每次和老师聊完，心中彷佛都燃烧了一团火，像雷总说的那样，要做世界上伟大的程序员。</p><p>鉴赏力的培养、创新力的训练、如何想问题、如何做关联、预见力的洞察、跨学科的素养，这些都是大学问，值得深思。<strong>不要盲目努力、勤奋，先想明白了再去做，再去执行</strong>。</p><h2 id="10月文献阅读list"><a href="#10月文献阅读list" class="headerlink" title="10月文献阅读list"></a>10月文献阅读list</h2><p>基于对LLM RAG - A Survey的论文分析研读，总结出以下可以展开的研究点： </p><p>RAG与NLP</p><p>检索</p><h3 id="Multimodel-RAG-Systems"><a href="#Multimodel-RAG-Systems" class="headerlink" title="Multimodel RAG Systems"></a>Multimodel RAG Systems</h3><p>周五(2024.10.25)完成了Multimodel RAG System的汇报，梳理了一下这个领域目前所需的技术栈和任务流程。</p><p>对于prompt compression方向要深入了解，同时，李老师让举一个基本的例子，没有回答上来，说明还是对基本概念的理解有所欠缺。大老师在方向的深度上给了帮助，需要深入推进下去。</p><p>要好好梳理一下LLM的知识框架了，所涉及到的基本知识，脑子里要有一个体系结构。</p><p>按部就班的跟着大老师的节奏走，争取11.22完成既定任务。</p><h3 id="杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha-Go"><a href="#杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha-Go" class="headerlink" title="杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha Go"></a>杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha Go</h3><p>诺奖开奖！</p><p>优质博客：<a href="http://xtf615.com/2018/02/10/AlphaGo/">http://xtf615.com/2018/02/10/AlphaGo/</a></p><p>Mastering the game of Go with deep neural networks and tree search：<a href="https://www.nature.com/articles/nature16961">https://www.nature.com/articles/nature16961</a></p><p><a href="https://arxiv.org/pdf/1706.04052">https://arxiv.org/pdf/1706.04052</a></p><h3 id="李沐的课要坚持刷完，动手学深度学习"><a href="#李沐的课要坚持刷完，动手学深度学习" class="headerlink" title="李沐的课要坚持刷完，动手学深度学习"></a>李沐的课要坚持刷完，动手学深度学习</h3><p>10.14-10.21:完成感知机到kaggle01实训的学习，一定要多动手实践！！</p><p>10.26-10.31:完成LLM基本知识框架的梳理、神经网络结构理解、注意力机制</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>希望未来一个月有所提高的方面：</p><ul><li>文献阅读能力：把我核心能力、代码复现能力、找idea</li><li>深度学习实践：未来一个月多动手学深度学习、多实操、多总结</li><li>科学论文写作与英语：英语听说读写的精进，论文、文章的写作能力怎么样提升，思考</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>月度总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI大模型时代人才需求与技能培养</title>
    <link href="/2024/09/29/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%BA%BA%E6%89%8D%E9%9C%80%E6%B1%82%E4%B8%8E%E6%8A%80%E8%83%BD%E5%9F%B9%E5%85%BB/"/>
    <url>/2024/09/29/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%BA%BA%E6%89%8D%E9%9C%80%E6%B1%82%E4%B8%8E%E6%8A%80%E8%83%BD%E5%9F%B9%E5%85%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="AI大模型需要的三类人才"><a href="#AI大模型需要的三类人才" class="headerlink" title="AI大模型需要的三类人才"></a>AI大模型需要的三类人才</h2><p>核心基础理论与架构、大模型+X、支持大模型的计算机系统</p><h2 id="技能与素质需求"><a href="#技能与素质需求" class="headerlink" title="技能与素质需求"></a>技能与素质需求</h2><h3 id="大模型核心基础理论与架构"><a href="#大模型核心基础理论与架构" class="headerlink" title="大模型核心基础理论与架构"></a>大模型核心基础理论与架构</h3><p>机器学习基础：熟悉机器学习基础算法，线性回归、决策树、随机向量机等，要理解过拟合、正则化、交叉验证等概念</p><p>深度学习理论：神经网络的基本结构和工作原理，熟练使用深度学习框架，如：tensorfllow、pytorch，理解卷积神经网络CNN、循环神经网络RNN、Transformer等结构</p><h3 id="大模型-X"><a href="#大模型-X" class="headerlink" title="大模型+X"></a>大模型+X</h3><p>交叉学科，行业赋能，落地应用。大模型在各行各业的落地使用。</p><h3 id="支持大模型的计算机系统"><a href="#支持大模型的计算机系统" class="headerlink" title="支持大模型的计算机系统"></a>支持大模型的计算机系统</h3><p>不做模型本身，但是也要了解大模型的原理、机器学习与深度学习。更多地侧重对于大模型的训练、部署与使用层面，了解如何评估模型的性能，掌握云计算与大数据技术，能够处理大规模数据集，会使用云服务对打模型进行部署服务与运用。</p><p>引用自：郑纬民院士</p><p>视频源：<a href="https://www.bilibili.com/video/BV1jUtKekEkR/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1jUtKekEkR/?spm_id_from=333.1007.tianma.2-2-5.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>人工智能</tag>
      
      <tag>职业发展</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch and Deep-learning NoteBook</title>
    <link href="/2024/09/28/Pytorch-and-Deep-learning-NoteBook/"/>
    <url>/2024/09/28/Pytorch-and-Deep-learning-NoteBook/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>PyTorch与深度学习笔记，持续更新….</p><p>不要着急，扎扎实实打好基本功！</p>          </div><h2 id="参考学习资料"><a href="#参考学习资料" class="headerlink" title="参考学习资料"></a>参考学习资料</h2><p>小土堆-pytorch学习教程：<a href="https://www.bilibili.com/video/BV1hE411t7RN/?p=10&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1hE411t7RN/?p=10&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p>李沐-动手学深度学习：<a href="https://www.bilibili.com/video/BV1wM4m117mp/?spm_id_from=333.337.search-card.all.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1wM4m117mp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p>李宏毅深度学习教程LeeDL-Tutorial（苹果书）：<a href="https://github.com/datawhalechina/leedl-tutorial?tab=readme-ov-file">https://github.com/datawhalechina/leedl-tutorial?tab=readme-ov-file</a></p><p>张贤同学pytorch学习笔记：<a href="https://pytorch.zhangxiann.com/">https://pytorch.zhangxiann.com/</a></p><p>《pytorch实用教程(第二版)》<a href="https://tingsongyu.github.io/PyTorch-Tutorial-2nd/">https://tingsongyu.github.io/PyTorch-Tutorial-2nd/</a></p><h2 id="conda常用指令"><a href="#conda常用指令" class="headerlink" title="conda常用指令"></a>conda常用指令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建环境</span><br>conda create -n your_env_name python=x.x(x.x为python版本)<br>eg: conda create -n pytorch_tutorial python=3.7<br><span class="hljs-meta prompt_"># </span><span class="language-bash">激活环境</span><br>source avtivate your_env_name<br>eg: source activate pytorch_tutorial<br><span class="hljs-meta prompt_"># </span><span class="language-bash">推出环境</span><br>source deactivate<br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除环境</span><br>conda remove -n your_env_name -all<br>eg: conda remove -n pytorch_tutorial -all<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看已有的虚拟环境</span><br>conda env list / conda info -e<br></code></pre></td></tr></table></figure><h2 id="PyTorch-CookBook"><a href="#PyTorch-CookBook" class="headerlink" title="PyTorch  CookBook"></a>PyTorch  CookBook</h2><h3 id="PyTorch安装"><a href="#PyTorch安装" class="headerlink" title="PyTorch安装"></a>PyTorch安装</h3><p>CPU版本的安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install torch torchvision torchaudio<br></code></pre></td></tr></table></figure><p>GPU版本：</p><p>安装指令可以去官网查看与之适配的CUDA版本，选择合适的操作系统、编程语言与CUDA版本安装即可</p><p><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p><h3 id="PyTorch常用子文件"><a href="#PyTorch常用子文件" class="headerlink" title="PyTorch常用子文件"></a>PyTorch常用子文件</h3><p>执行完pip install torch后，Torch的安装路径为：&#x2F;Users&#x2F;mungeryang&#x2F;anaconda3&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;torch，在程序中import torch其实是从安装路径中进行导入。</p><p>torch中重要的组件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">_pycache: 该文件夹存放python解释器生成的字节码,目的是通过牺牲一定的存储空间来提高加载速度<br>_C: 辅助C语言代码调用的一个模块,PyTorch的底层计算代码采用的是C++语言编写，并封装成库，供pytorch的python语言进行调用<br>include: 存放着上面描述底层计算所需的C++代码<br>lib: lib文件夹下包含大量的.lib .dll文件（分别是静态链接库和动态链接库）,torch文件夹中98%内容都在lib中<br>autograd: pytorch的核心模块与概念，它实现了梯度的自动求导，开发人员只需编写前向传播代码，反向传播部分由autograd自动实现<br>nn: 使用频率最高的模块，搭建网络的网络层就在nn.modules里边<br>onnx: pytorch模型转换到onnx模型表示的核心模块<br>optim: 优化使用的方法函数，都在optim文件夹中<br>utils: utils是各种软件工程中常见的文件夹，包含了各类常用工具，其中比较关键的是data文件夹，tensorboard文件夹<br>datasets: 常用的数据集写的数据读取函数，例如常见的cifar, coco, mnist,svhn,voc都是有对应的函数支持<br>torchvision:<br>models: 它里边存放了经典的、可复现的、有训练权重参数可下载的深度学习模型<br>ops: 视觉任务特殊的功能函数<br>transforms: transforms是pytorch自带的图像预处理、增强、转换工具<br></code></pre></td></tr></table></figure><p><em>菜鸟使用pytorch实现图像分类任务时可能考虑到的任务点</em>：</p><p>图像数据是哪用一行代码读取进来的？</p><p>transforms.Compose是如何工作对图像数据进行转换的？</p><p>ToTensor又有哪些操作？</p><p>如何编写Dataset？</p><p>DataLoader有什么功能？如何使用？有什么需要注意的？</p><p>模型如何按自己的数据流程搭建？</p><p>nn有哪些网络层可以调用？</p><p>损失函数有哪些？</p><p>优化器是如何更新model参数的？</p><p>学习率调整有哪些方法？如何设置它们的参数？</p><p>model.train()与model.eval()作用是什么？</p><p>optimizer.zero_grad()是做什么？为什么要梯度清零？</p><p>scheduler.step() 作用是什么？应该放在哪个for循环里？</p><h2 id="核心板块-Tensor"><a href="#核心板块-Tensor" class="headerlink" title="核心板块 - Tensor"></a>核心板块 - Tensor</h2><p>Pytorch中核心的数据结构就是张量——Tensor</p><p>张量表示的是一个多维数组，它是标量、向量、矩阵的拓展。标量(Scaler)是零维张量，向量(Vector)是一维张量，矩阵(Matrix)是二维张量，一个RGB数组是三维张量。</p><p>torch.Tensor的八大属性：</p><ol><li>data: 多维数组</li><li>dtype: 多维数组的数据类型</li><li>shape: 多维数组的形状</li><li>device: tensor所在设备，cpu或GPU</li><li>grad,  grad_fn, is_leaf, requires_grad: 梯度计算中需要的</li></ol><p>Pytorch训练的五大要素：数据、模型、损失函数、优化器、迭代训练</p><p>多维数组是主要的数据处理形式</p><h2 id="数据模块-DataSet、DataLoader、transforms"><a href="#数据模块-DataSet、DataLoader、transforms" class="headerlink" title="数据模块 - DataSet、DataLoader、transforms"></a>数据模块 - DataSet、DataLoader、transforms</h2><p>DataSet是一个抽象基类，提供给用户自定义数据读取方式，核心在于<strong>getitem</strong>方法中对数据的处理。</p><p>DataLoader是pytorch数据加载核心，包括了大乱数据、采样机制、多进程数据加载、组装Batch。</p><h3 id="DataSet类代码实战"><a href="#DataSet类代码实战" class="headerlink" title="DataSet类代码实战"></a>DataSet类代码实战</h3><p>虽然DataLoader是pytorch核心数据模块，但是改动最多与原数据最为接近的是DataSet。</p><p>DataSet类中核心功能：</p><ol><li><strong>getitem</strong>：实现读取一个样本的功能。通常是传入索引，然后实现从磁盘中读取数据，并行处理数据，返回样本数据。getitem返回的数据会在dataloader中组装成一个batch。</li><li><strong>len</strong>：返回数据集大小</li></ol><p>Dataset负责与磁盘打交道，将磁盘上的数据读取并预处理好，提供给DataLoader，而DataLoader只需要关心如何组装成批数据，以及如何采样。</p><h3 id="DataLoader类代码实战"><a href="#DataLoader类代码实战" class="headerlink" title="DataLoader类代码实战"></a>DataLoader类代码实战</h3><h2 id="模型板块"><a href="#模型板块" class="headerlink" title="模型板块"></a>模型板块</h2><p>nn.Module</p><h2 id="创建简单的神经网络"><a href="#创建简单的神经网络" class="headerlink" title="创建简单的神经网络"></a>创建简单的神经网络</h2><p>使用Pytorch创建一个神经网络意味着创建一个新的类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicNet</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(BasicNet,<span class="hljs-variable language_">self</span>).__init__()<br>    <br>    <span class="hljs-comment">#初始化所有的权重和偏差</span><br>    <span class="hljs-variable language_">self</span>.w00 = nn.Parameter(torch.tensor(<span class="hljs-number">1.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b00 = nn.Parameter(torch.tensor(-<span class="hljs-number">0.85</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w01 = nn.Parameter(torch.tensor(-<span class="hljs-number">40.8</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.w10 = nn.Parameter(torch.tensor(<span class="hljs-number">12.6</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b10 = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w11 = nn.Parameter(torch.tensor(<span class="hljs-number">2.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.final_bias = nn.Parameter(torch.tensor(-<span class="hljs-number">16.0</span>), requires_grad = <span class="hljs-literal">False</span>)<br>    <br>  <span class="hljs-comment"># 前向传播</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>    <br>    input_to_top_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w00 + <span class="hljs-variable language_">self</span>.b00<br>    top_relu_output = F.relu(input_to_top_relu)<br>    scaled_top_relu_output = top_relu_output * <span class="hljs-variable language_">self</span>.w01<br>    <br>    input_to_buttom_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w10 + <span class="hljs-variable language_">self</span>.b10<br>    buttom_relu_output = F.relu(input_to_buttom_relu)<br>    scaled_buttom_relu_output = buttom_relu_output * <span class="hljs-variable language_">self</span>.w11<br>    <br>    input_to_final_relu = scaled_top_relu_output + scaled_buttom_relu_output + <span class="hljs-variable language_">self</span>.final_bias<br>    output = F.relu(input_to_final_relu)<br>    <br>    <span class="hljs-keyword">return</span> output<br>  <br>input_doses = torch.linspace(start = <span class="hljs-number">0</span>,end = <span class="hljs-number">1</span>,step = <span class="hljs-number">11</span>)<br>model = BasicNet()<br>output = model(input_doses)<br></code></pre></td></tr></table></figure><p>从创建网络到训练的完整流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicNet</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(BasicNet,<span class="hljs-variable language_">self</span>).__init__()<br>    <br>    <span class="hljs-comment">#初始化所有的权重和偏差</span><br>    <span class="hljs-variable language_">self</span>.w00 = nn.Parameter(torch.tensor(<span class="hljs-number">1.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b00 = nn.Parameter(torch.tensor(-<span class="hljs-number">0.85</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w01 = nn.Parameter(torch.tensor(-<span class="hljs-number">40.8</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.w10 = nn.Parameter(torch.tensor(<span class="hljs-number">12.6</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b10 = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w11 = nn.Parameter(torch.tensor(<span class="hljs-number">2.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.final_bias = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_grad = <span class="hljs-literal">True</span>)<br>    <br>  <span class="hljs-comment"># 前向传播</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>    <br>    input_to_top_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w00 + <span class="hljs-variable language_">self</span>.b00<br>    top_relu_output = F.relu(input_to_top_relu)<br>    scaled_top_relu_output = top_relu_output * <span class="hljs-variable language_">self</span>.w01<br>    <br>    input_to_buttom_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w10 + <span class="hljs-variable language_">self</span>.b10<br>    buttom_relu_output = F.relu(input_to_buttom_relu)<br>    scaled_buttom_relu_output = buttom_relu_output * <span class="hljs-variable language_">self</span>.w11<br>    <br>    input_to_final_relu = scaled_top_relu_output + scaled_buttom_relu_output + <span class="hljs-variable language_">self</span>.final_bias<br>    output = F.relu(input_to_final_relu)<br>    <br>    <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = SGD(model.parameters(),lr = <span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>  <br>  total_loss = <span class="hljs-number">0.0</span><br>  <br>  <span class="hljs-keyword">for</span> iteration <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(inputs)):<br>    input_i = inputs[iteration]<br>    label_i = labels[iteration]<br>    output_i = model(input_i)<br>    <br>    loss = (output_i - label_i) ** <span class="hljs-number">2</span><br>    <br>    loss.backward()<br>    total_loss += <span class="hljs-built_in">float</span>(loss)<br>    <br><span class="hljs-keyword">if</span>(total_loss &lt; <span class="hljs-number">0.0001</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Num steps:&quot;</span> + <span class="hljs-built_in">str</span>(epoch))<br>  <span class="hljs-keyword">break</span><br>  optimizer.step()<br>  optimizer.zero_grad()<br>  <br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step: &quot;</span> + <span class="hljs-built_in">str</span>(epoch) + <span class="hljs-string">&quot;Final bias: &quot;</span> + <span class="hljs-built_in">str</span>(model.final_bias) + <span class="hljs-string">&quot;\n&quot;</span>)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李沐交大讲座-大语言模型的实践经验和未来预测</title>
    <link href="/2024/09/23/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/"/>
    <url>/2024/09/23/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-trends-Personal-Career-Choices"><a href="#LLM-trends-Personal-Career-Choices" class="headerlink" title="LLM trends &amp;&amp; Personal Career Choices"></a>LLM trends &amp;&amp; Personal Career Choices</h1><h1 id="——大语言模型的实践经验和未来预测"><a href="#——大语言模型的实践经验和未来预测" class="headerlink" title="——大语言模型的实践经验和未来预测"></a>——大语言模型的实践经验和未来预测</h1><h2 id="主讲人"><a href="#主讲人" class="headerlink" title="主讲人"></a>主讲人</h2><p>李沐：上海交大2011届计算机科学与工程系本硕系友。他曾担任亚马逊资深首席科学家，任加州大学伯克利分校和斯坦福大学的访问助理教授，是前Marianas Labs联合创始人。他的研究关注分布式系统和机器学习算法。发表了50余篇人工智能顶级会议论文，在CMU读博期间更是两年内发表了理论计算机领域的FOCS、神经网络领域的NIPS、数据挖掘领域的KDD和操作系统领域的OSDI等不同领域的顶级国际会议一作论文。他是深度学习框架Apache MXNet的创始人之一，合著了开源深度学习教材《动手学深度学习》。他目前是BosonAI的联合创始人。</p><p>时间地点：2024.08.25-上海交通大学计算机科学与技术系</p><h2 id="大语言模型与“炼丹”"><a href="#大语言模型与“炼丹”" class="headerlink" title="大语言模型与“炼丹”"></a>大语言模型与“炼丹”</h2><p>语言模型的三大核心组成：<strong>算力、算法、数据</strong></p><p>语言模型也好，整个机器学习模型也好，本质上就是把数据通过算力和算法压进中间那个模型里面，使得模型有一定的能力，在面对一个新的数据时，它能够在原数据里面找到相似的东西，然后做一定的修改，输出你要的东西。</p><p>沐神用“炼丹”的比喻将现在的语言模型就是一个炼丹的过程：数据是原材料，算力是设备(比如炼丹炉)，算法就是“丹方”。数据这个原材料不是很好搞，是个体力活；算力也很重要，要想火大一点，设备就要先进一点，设备越好练出来的东西质量也会更好。算法这个丹方，每年都在进步，就算有人指导你做，实际操作时候也会变得不一样。“像是徒手发火箭，发射前要动手调一调，没调好就会炸掉”</p><p>数据以参数的形式存储在LLM中，需要使用prompt工程进行数据的提取与使用。(LLM时代，大数据存储是以<strong>参数</strong>的形式进行的)</p><h2 id="硬件的发展规律，算力方面-LLM不是特别有性价比的东西"><a href="#硬件的发展规律，算力方面-LLM不是特别有性价比的东西" class="headerlink" title="硬件的发展规律，算力方面-LLM不是特别有性价比的东西"></a>硬件的发展规律，算力方面-LLM不是特别有性价比的东西</h2><h3 id="带宽"><a href="#带宽" class="headerlink" title="带宽"></a>带宽</h3><p>带宽是让芯片靠的更近一些</p><p>讲带宽是因为LLM训练要分布式进行，现在的带宽是一根光纤承载 400Gigabits，下一代就是 double，变成 800Gigabits。</p><p>数据中心采用水冷当时对机架降温可以提高信息传输效率，提升训练速度。</p><p>“英伟达的 GB200 这个卡就可以把 GPU 都放在一起，那么它们之间的通讯会变得更好一些。你可以理解成：之前我们做多核，把单核封装到一个芯片里面，现在是说多核不够，我要做多卡，多卡以前是分布在一个房间里面，现在是多卡也要尽量放在一起，这是一个趋势。就是一块芯片那么大，早就做不上去了，这是台积电等面临的工艺难题，现在是尽量把这些东西弄得近一些。”</p><h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><p>内存是制约模型的一大瓶颈</p><p><strong>现在的语言模型，核心是把整个世界的数据压进模型里面</strong>，但是这么做模型就会被搞的非常大，几百个G的样子。跑起来以后，参数和中间变量会更大，因此内存就需要很大。</p><p>但是内存大小这个东西快要遇到瓶颈了，因为它内存占面积，一块芯片就那么大，划一个区域给算力，划一块给内存，基本上就剩不下什么东西了。</p><p>沐神预测，一块芯片200G内存，基本上就要从工艺上就到瓶颈了。当然我不是搞硬件的，对于这些知识就是认真学习，在实践中去感受。</p><p>记得王坚院士去年在云栖大会上的演讲给出的观点是，目前的模型和算法并没有匹配上当前的算力资源。</p><h3 id="算力"><a href="#算力" class="headerlink" title="算力"></a>算力</h3><p>算力从长期来看会越来越便宜</p><p>模型做的越大，就会发现资源其实是问题。数据中心需要大量的供电；短期来看，算力翻倍，价格可能会有 1.4 倍的提升。但是长期来看，当竞争变得越来越激烈，摩尔定律会发挥作用，就是说算力翻倍，价格不一定变。所以<strong>长期来看算力会变得越来越便宜。</strong></p><p>关于算力的观点：你可以认为摩尔定律还是会发挥作用，就是训练会两倍两倍地变便宜。所以你今天训练一个模型，一年之后它的价值会减半。很多时候，大家不要去想我现在能搞多大的模型，一年之后，这个模型会贬值。我想说，大模型不是特别有性价比的东西。你要想清楚，<strong>从长期来看，你的模型能带来什么价值，让你能够保值</strong>。</p><h2 id="趋势：从语言到多模态"><a href="#趋势：从语言到多模态" class="headerlink" title="趋势：从语言到多模态"></a>趋势：从语言到多模态</h2><h3 id="语言模型：100B-500B参数会是主流"><a href="#语言模型：100B-500B参数会是主流" class="headerlink" title="语言模型：100B-500B参数会是主流"></a>语言模型：100B-500B参数会是主流</h3><p>每次预训练，无论是 OpenAI 还是别的模型，基本都是用 10T 到 50T token 做预训练。开源的话基本也在 10T token 以上。这个数据量我觉得差不多了，不会再往一个更大的尺寸去发展。原因是，人类历史上的数据比这个多是多，但是看多样性、质量的话，我觉得 10T 到 50T 这个规模就差不多了。</p><p>比较好的一线的模型就是 500B，超过 500B 不是训练不动，而是做 serving 很难。</p><h3 id="语音模型：延迟更低、信息更丰富"><a href="#语音模型：延迟更低、信息更丰富" class="headerlink" title="语音模型：延迟更低、信息更丰富"></a>语音模型：延迟更低、信息更丰富</h3><p>新的语音技术可以让语音直接进去，然后利用文本语言模型强大的能力去发掘里面的信息。在做输出的时候也是一样的，我的输出可以根据你的输入的个性化场景来变换语调、情绪。这是一点。</p><p>另一点是延迟更短。之前我要先输出一句话，再进到一个模型去把语音输出来，这个延迟可能是 1 秒。现在我们大概可以做到 300 毫秒。</p><h3 id="音乐模型：不是技术问题，而是商业问题"><a href="#音乐模型：不是技术问题，而是商业问题" class="headerlink" title="音乐模型：不是技术问题，而是商业问题"></a>音乐模型：不是技术问题，而是商业问题</h3><p>我觉得这一块的进展从来不是一个技术问题。它的技术其实比语音麻烦一点，因为音乐比人说话更复杂一点。但是实际上它还是一个版权的问题。</p><h3 id="图像模型：图像越来越有神韵"><a href="#图像模型：图像越来越有神韵" class="headerlink" title="图像模型：图像越来越有神韵"></a>图像模型：图像越来越有神韵</h3><p>图片应该是整个 AIGC 领域做得最早的，也是效果最好的。现在大家可以做到 100 万以上像素的图片的生成。大家说得最多的是图片要有灵魂。之前你去看那些文生图的工具，它的风格还是很假，但现在你会看到跟真的很接近，当然它还缺那么一点点灵魂，不过这一块说不定很快就有了。</p><h3 id="视频模型：尚属早期"><a href="#视频模型：尚属早期" class="headerlink" title="视频模型：尚属早期"></a>视频模型：尚属早期</h3><p>Sora 出来之后，大家非常关注视频模型。这个实际上还算比较早期，通用的 video 生成还是非常贵，因为 video 数据特别难弄。视频模型的训练成本很有可能低于数据处理的成本，所以你没有看到市面上有特别好的开源模型出来。问题在于生成一张图片容易，但生成一连串连贯的图片，并保持一致性是很难的。</p><h3 id="多模态模型：整合所有信息"><a href="#多模态模型：整合所有信息" class="headerlink" title="多模态模型：整合所有信息"></a>多模态模型：整合所有信息</h3><p><strong>目前存在一种趋势，即多模态</strong>。现如今，多模态技术的发展趋势在于整合不同类型的模态信息，尤其是文本信息，因为文本含有丰富的信息并且易于获取。通过利用在文本上学到的技能，可以将这些能力泛化到其他模态，如图片、视频和声音。</p><h3 id="交互方式的一点改变"><a href="#交互方式的一点改变" class="headerlink" title="交互方式的一点改变"></a>交互方式的一点改变</h3><p>比如在点菜时，在 ChatGPT 出来之前我们与手机的交互方式是刷刷刷和点点点，这是最简单的方式，对人类来说也不耗费精力，能不说就不说。但在 ChatGPT 出来之后，大家打破了这种观念，他们愿意去输入一段很长的文字去做事情，这是因为设计好的东西不一定满足我们的所有需求，可能满足了 80%，但没有满足对细节的需求，这时可以通过长文本，即输入很长的文字来解决。但输入长文字还是不如说话方便，所以在微信上很多人会说我语音留言会方便点。</p><p>未来大家可能会越来越能接受对方用一个很长的语音跟你描述一些事情，让你去完成。随着技术的发展，未来的语音控制系统将能够处理更加复杂和具体的任务，这种技术的自然和便捷性将显著提高。</p><h2 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h2><p>预训练时工程问题，后训练才是技术问题</p><p>垂直模型也需要通用知识</p><p>评估很难但是很重要</p><p>数据决定模型的上限</p><h2 id="打卡式人生"><a href="#打卡式人生" class="headerlink" title="打卡式人生"></a>打卡式人生</h2><p>从最基本的目标来说，去大公司，是为了升职加薪；读 PhD ，你要保证自己能毕业；而创业的目标是要能推出产品，要么上市，要么卖掉，这是每天都需要思考的。</p><h3 id="打工人"><a href="#打工人" class="headerlink" title="打工人"></a>打工人</h3><p>打工人的好处是，可以在一个相对简单的环境里学习各种从业知识，比如一个技术如何落地、产品怎么做出来、怎么设计、怎么运营、怎么管理。</p><p>那么打工人的坏处是什么？坏处就是停留在打工人或者职业经理人的思维。</p><h3 id="读PHD：真心热爱研究，不然难以坚持"><a href="#读PHD：真心热爱研究，不然难以坚持" class="headerlink" title="读PHD：真心热爱研究，不然难以坚持"></a>读PHD：真心热爱研究，不然难以坚持</h3><h4 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h4><p>读 PhD 的好处是，在几年的时间里可以专心探索某一个领域，反正也没钱赚，也没有升职加薪的机会。PHD期间要有大部分时间都要花在写作和演讲上，努力提高自己的表达与写作的能力。</p><p>还有一个好处，很多公司的研发职位要求就是 PhD。</p><h4 id="坏处"><a href="#坏处" class="headerlink" title="坏处"></a>坏处</h4><p>读 PhD 的坏处是什么？</p><p>首先，很少有实验室能参与大项目的研发。</p><p>其次是研究课题和导师风格都很挑人，需要你去适应，这个适应过程，要么看你的适应能力有多好，要么看你导师的适应能力有多好。你在公司里面还能够部门之间跳一跳，但读 PhD 就更难一些。</p><p>最后，<strong>要真的热爱研究，不然坚持不下去</strong>，你会觉得研究这个东西到底有什么意义，写这篇论文要干嘛。其实，你可以这样想：我写这篇文章就是为了练习写作，等到更厉害、更大的成果做出来后，写作不能给我拉后腿。你要有一个更远大的目标，是真的热爱它。</p><h3 id="动机的来源"><a href="#动机的来源" class="headerlink" title="动机的来源"></a>动机的来源</h3><p>强烈的创业动机要么来自很深沉、很底层的欲望，要么来自很深的恐惧。</p><p>欲望与恐惧是动机的主要来源。</p><p>欲望是越底层越好，名、利、权，都是底层的欲望，要直面自己的欲望，也要直面自己的恐惧，这种恐惧是可以让你抑郁的恐惧，也是让你感受到生死的恐惧。</p><p>你需要把欲望和恐惧转变成积极向上的动机，这一点很重要，你的动机一定是正确的，符合价值观的，因为逃避、放纵满足不了欲望，也缓解不了恐惧，唯一克服它的办法是，把它变成一个积极向上、符合社会价值的一个动机。</p><h2 id="持续进步的妙招"><a href="#持续进步的妙招" class="headerlink" title="持续进步的妙招"></a>持续进步的妙招</h2><p>反思性写作与阶段式总结可以保持持续进步。</p><p>你要形成一个习惯，定个闹钟，每周一晚上花 30 分钟对自己进行总结，每个季度要<strong>总结</strong>，翻看之前你的写的周记，看看这个季度的目标是否完成，下个季度要做什么。</p><p><strong>选择比努力更重要，但选择的前提是搞清楚你的目标是什么。</strong></p><div class="note note-success">            <p>最好的时代：新的技术带来了很多新的机会</p><p>最坏的时代：需要付出比上一代人更多</p>          </div><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>讲座视频：</p><p><a href="https://www.bilibili.com/video/BV1dHWkewEWz/?spm_id_from=333.337.search-card.all.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1dHWkewEWz/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p><a href="https://www.youtube.com/watch?v=ziHUcDh0DwM">https://www.youtube.com/watch?v=ziHUcDh0DwM</a></p><p>阅读文章：</p><p><a href="https://mp.weixin.qq.com/s/FctMj2SXotdn8MlL9oSfAg">机器之心推文</a></p><p><a href="https://zhuanlan.zhihu.com/p/714533901">创业一年，人间三年</a></p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>深度学习</tag>
      
      <tag>职业发展</tag>
      
      <tag>机器学习</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG for LLM - A survey</title>
    <link href="/2024/09/19/RAG-for-LLM-A-survey/"/>
    <url>/2024/09/19/RAG-for-LLM-A-survey/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG-for-LLM-A-survey"><a href="#RAG-for-LLM-A-survey" class="headerlink" title="RAG for  LLM - A survey"></a>RAG for  LLM - A survey</h1><p>论文题目：</p><p>Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>论文链接：</p><p><a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></p><p>翻译：</p><p><a href="https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey">https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在LLM得到广泛应用的同时，对于LLM 幻觉、过时知识、不透明&#x2F;不可追踪的推理过程仍然面临巨大挑战。</p><p>可行的解决方式：RAG-检索生成增强</p><p>方式：连接外部数据库- RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases.</p><p>RAG paradigms（RAG三大范式）：</p><ul><li>Naive RAG</li><li>Advanced RAG</li><li>Modular RAG</li></ul><p>文章重点强调了嵌入在RAG系统中的关键组件，提高了对RAG系统的理解。此外，文章也给出了对于RAG系统的最新评估标准和框架。</p><h2 id="Introduction-RAG在大模型阶段的发展轨迹"><a href="#Introduction-RAG在大模型阶段的发展轨迹" class="headerlink" title="Introduction-RAG在大模型阶段的发展轨迹"></a>Introduction-RAG在大模型阶段的发展轨迹</h2><p>RAG技术最初与Transformer架构相吻合，通过增加外部知识库，在早期阶段用于细化预训练技术（pre-training）。</p><p>随着ChatGPT的兴起，大语言模型在长上下文的理解中展现出了强大能力。</p><p>RAG研究转向在推理阶段为LLM提供更好的信息，以回答更复杂和知识密集型的任务，导致RAG研究的快速发展。</p><p>后续随着研究进一步深入，RAG技术不再局限于推理阶段而是开始更多的和微调技术相结合。</p><p>总结：预训练-推理-微调</p><p>RAG技术本身经历了快速发展，但是目前仍然没有对RAG系统发展的清楚阐述，这篇文章的写作目的之一就是填补这一空白，去为读者绘制RAG发展路径并评估他未来的发展路径。</p><p>文章旨在阐明检索增强技术的演变，评估各种方法在各自上下文中的优点和缺点，并推测即将到来的趋势和创新。</p><p>本文的主要贡献：</p><ul><li>系统性回顾了RAG最新的方法、技术，描述了RAG三大范式的演变，将RAG研究置于LLM前景中</li><li>识别并阐明了RAG的三大核心技术：Retrieval、Generation、Augmentation，阐明了这些组件如何协同形成有效的RAG框架</li><li>总结了RAG的当前评估方法，涵盖了26个任务，近50个数据集，概述了评估目标和指标，以及当前的评估基准和工具。预计RAG的未来方向，强调应对当前挑战的潜在增强。</li></ul><p>通过认真阅读、分析原文后，应该可以对本文的三大贡献进行连贯复述。</p><h2 id="SectionII-RAG三大范式"><a href="#SectionII-RAG三大范式" class="headerlink" title="SectionII-RAG三大范式"></a>SectionII-RAG三大范式</h2><h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Indexing</p><p>索引编排阶段会将所有类型的文档（PDF、HTML、Word、Markdown）转换为统一的文本格式。之后，为了使用语言模型的文本限制，“格式化”后的文本会被切分成chunks，使用嵌入模型，这些chunks会被转换为向量存储在向量数据库中。</p><p>Retrieval</p><p>RAG系统会将用户请求(query)转换为一个向量表示，然后通过计算查询向量和chunks向量之间的相似度来检索与query最相近的top k个chunks。</p><p>Generation</p><p>将用户提出的查询和所选文档合成为一个连贯的提示，大型语言模型的任务是制定响应。在正在进行的对话情况下，任何现有的历史都会可以集成到提示中，是模型能够有效参与多轮对话交互。</p><p>检索阶段的挑战：准确率和召回率；导致错误或不相关的chunk，以及缺少关键信息。</p><p>生成阶段的困难：出现幻觉</p><p>增强阶段的障碍：检索信息与不同任务结合可能具有挑战性，有时会导致输出不连贯。对于复杂需求，基于最初查询的单一检索并不足以获取足够的上下文信息。</p><h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>相比于Naive RAG，Advanced RAG专注于提高检索质量，运用了pre-retrieval 和 post-retrieval策略。</p><p>pre-retrieval重点关注优化索引结构和原始查询，优化索引的目标是提高被索引的内容的质量。</p><p>post-retrieval阶段主要是整合有效查询，主要的方法包括chunks重排和上下文压缩。重新排序检索到的信息以将最相关的内容重新定位到提示的边缘是关键策略。</p><h3 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h3><p>相比于前两种范式，模块化RAG的适应性和多功能性得到提高。</p><p>方法：添加一个搜索模块进行相似度搜索，并通过微调细化检索器。重组RAG模块+重排RAG管道来解决目前新的挑战，引入额外组件提高检索和处理能力。</p><p>创新：</p><p>Rewrite-Retrieve-Read Model 利用LLM的能力通过重写模块和LM反馈机制来改进检索查询，以更新重写模型。，提高任务性能。</p><p>Generate-Read 用LLM生成的内容替换传统的检索</p><p>ReciteRead 强调从模型权重中检索，增强了模型处理知识密集型任务的能力</p><p>子查询和假设文档嵌入 (HyDE)旨在通过关注生成答案和真实文档之间的嵌入相似性来提高检索相关性</p><p>调整：Demonstrate-Search-Predict(DSP)框架和迭代的Retrieve-Read-Retrieve-Read流使用增强了模块的协同复杂理解。</p><h3 id="RAG与微调"><a href="#RAG与微调" class="headerlink" title="RAG与微调"></a>RAG与微调</h3><p>RAG通过提供实时知识更新和有效利用具有高可解释性的知识源，在动态环境中表现出色。然而，在检索方面有更高的延迟和伦理考虑。</p><p>RAG 和 微调技术之间的选择取决于应用程序上下文中数据动态、定制和计算能力的具体需求。RAG 和 微调技术不是互斥的，可以相互补充，增强了模型在不同层次上的能力。在某些情况下，它们的组合使用可能会导致最佳性能。涉及RAG和微调技术的优化过程可能需要多次迭代才能获得满意的结果。</p><h2 id="检索-RETTRIEVAL"><a href="#检索-RETTRIEVAL" class="headerlink" title="检索-RETTRIEVAL"></a>检索-RETTRIEVAL</h2><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="Semi-structured-data"><a href="#Semi-structured-data" class="headerlink" title="Semi-structured data"></a>Semi-structured data</h4><p>特别关注对于<strong>半结构化数据</strong>的处理分析。典型的半结构化数据例如PDF包括文本和表信息。</p><p>有两大原因导致传统RAG系统在处理半结构化数据时面临挑战：</p><ol><li>文本的切分会不经意地拆分表格，导致检索过程中的数据损坏</li><li>表与数据的结合会使语义相似度检索变得更加复杂</li></ol><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>所有的方法目前都不是最优的，该领域还有大量的研究机会。</p><p>关注北航关于Text-2-SQL的最新成果</p><h4 id="Structured-data"><a href="#Structured-data" class="headerlink" title="Structured data"></a>Structured data</h4><p>图检索使GNN神经网络、LLM与RAG相结合，通过LLM的软提示来增强图形理解和问答能力，并使用Prize-Collecting Steiner Tree进行优化图检索的结果。</p><h4 id="LLMs-Generated-Content"><a href="#LLMs-Generated-Content" class="headerlink" title="LLMs-Generated Content"></a>LLMs-Generated Content</h4><p>LLM 生成的上下文通常包含更准确的答案，因为其与因果语言建模的预训练目标能更好地对齐。</p><h3 id="Retrieval-Granularity"><a href="#Retrieval-Granularity" class="headerlink" title="Retrieval Granularity"></a>Retrieval Granularity</h3><p>粗粒度检索单元理论上可以为问题提供更多相关信息，但它们也可能包含冗余内容，这可能会分散下游任务中的检索器和语言模型。</p><p>另一方面，细粒度检索单元粒度增加了检索的负担，不能保证语义完整性并满足所需的知识。</p><p>选择恰当的检索粒度是一个简单的策略用来提高检索和下游任务的表现。</p><p>检索粒度从粗到细包括：Token, Phrase, Sentence, Proposition, Chunks, Document。将Propositions作为检索单元可以提高检索的相关性和精准度。</p><p>知识图谱的检索粒度包括：Entity, Triplet, and sub-Graph.</p><h3 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h3><p>在索引阶段，文档将被处理、分割并转换为要存储在向量数据库中的嵌入。索引的构造质量决定了是否可以在检索阶段获取正确的上下文。</p><h4 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h4><p>分块常用的方法是将文档分为不同的标记的chunks。</p><p>分块需要在语义完整性和上下文长度之间进行trade-off。更长的块会包含更长的上下文，但是噪声会增加、处理时间也会增加；短块包含的噪声小，但是能包含的上下文信息也更少。</p><p>于是后面提出了以句子作为检索单元，前后句被提供作为LLM的上下文内容。</p><h4 id="元数据附加"><a href="#元数据附加" class="headerlink" title="元数据附加"></a>元数据附加</h4><p>在检索过程中为文档时间戳分配不同的权重可以实现时间感知的RAG，确保知识的新鲜度并避免过时的信息。</p><p>元数据也可以被人工构建(添加段落摘要以及引入假设问题)。具体来说，使用 LLM 生成文档可以回答的问题，然后计算检索过程中原始问题和假设问题之间的相似度，以减少问题和答案之间的语义差距。</p><h4 id="结构化索引"><a href="#结构化索引" class="headerlink" title="结构化索引"></a>结构化索引</h4><p>层次索引结构和知识图谱都可以提高信息检索的准确率。</p><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>问题本身的复杂性、语义模糊性都是查询过程中遇到的困难。</p><p>查询优化的方法主要有三大类：</p><p>1.查询扩展</p><ul><li>多路查询</li><li>子查询</li><li>Chain-of-Verification(CoVe)</li></ul><p>2.查询转换-查询重写</p><p>3.查询路由</p><h3 id="嵌入-Embedding"><a href="#嵌入-Embedding" class="headerlink" title="嵌入-Embedding"></a>嵌入-Embedding</h3><p>怎么理解嵌入？</p><p>Embedding 是将离散的非结构化数据(图片、视频、音频、文本)通过Embedding Model转换为连续的向量表示的技术。</p><p>Embedding 常常用于将文本数据中的单词、句子或文档映射为固定长度的实数向量，使得文本数据能够在计算机中被更好地处理和理解。通过 Embedding，每个单词或句子都可以用一个实数向量来表示，这个向量中包含了该单词或句子的语义信息。</p><p>RAG系统中，检索是通过计算嵌入问题和文档块之间的相似度实现的。</p><p>没有最好的答案去回答“要使用哪一个嵌入模型”，然而，对于不同的问题可以有不同的方法。</p><h3 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h3><p>Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.</p><p>这篇文章介绍了一种通过指令微调将知识集成到白盒模型中的创新方法。检索器模块直接替换为根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难并提高模型性能。</p><h2 id="增强-AUGMENTATION-PROCESS-IN-RAG"><a href="#增强-AUGMENTATION-PROCESS-IN-RAG" class="headerlink" title="增强-AUGMENTATION PROCESS IN RAG"></a>增强-AUGMENTATION PROCESS IN RAG</h2><p>增强的过程主要侧重于优化检索的过程，这一部分介绍了三大检索增强的过程：迭代式检索、递归式检索、自适应检索。</p><p>迭代检索涉及检索和生成之间交替，允许在每一步从知识库中更丰富、更有针对性的上下文。递归检索涉及逐步细化用户查询并将问题分解为子问题，然后通过检索和生成不断解决复杂问题。自适应检索侧重于使 RAG 系统能够自主确定外部知识检索是否必要以及何时停止检索和生成，通常使用 LLM 生成的特殊标记来控制。</p><h2 id="生成-GENERATION"><a href="#生成-GENERATION" class="headerlink" title="生成-GENERATION"></a>生成-GENERATION</h2><p>在检索后，将所有检索到的信息直接输入到LLM中回答问题并不是一个好的实践。</p><h3 id="上下文配置"><a href="#上下文配置" class="headerlink" title="上下文配置"></a>上下文配置</h3><p>过长的上下文信息会导致LLM“忽视中间段落”，与人类一样，LLM 往往只关注长文本的开头和结尾，同时忘记中间部分。因此，在 RAG 系统中，我们通常需要进一步处理检索到的内容。</p><p>重排：从根本上重新排序对文档(chunks)块进行重新排序以首先突出最相关的结果，有效地减少整体文档池，在信息检索中切断双重目的，充当增强器和过滤器，为更精确的语言模型处理提供细化的输入。</p><p>上下文选择&#x2F;压缩：对RAG过程的一个误解是相信尽可能多地检索相关文档并用长检索提示包含他们是有好处的。然而，大量的信息也会带来大量的干扰，削弱大语言模型对于关键信息的敏锐度。</p><p>压缩方法包括检测并移除不重要的tokens，将上下文转化为人类很难理解但是LLM很好理解的形式。</p><p>小语言模型SLM用作过滤器，大语言模型LLM用作重排代理。在信息抽取任务中，指导LLM去重组被SLM识别的挑战性样本会导致效果显著提升。</p><h3 id="LLM微调"><a href="#LLM微调" class="headerlink" title="LLM微调"></a>LLM微调</h3><p>当LLM缺少特定领域的数据的时候，外部知识可以被提供通过微调技术。</p><p>微调的另一个好处是可以控制模型的输入和输出。它可以让LLM适应特定的数据格式和按照指示以特定风格生成响应。</p><p>通过强化学习将LLM输出与人类或检索器偏好对齐是一种潜在的方法。除了与人类偏好对齐外，还可以与微调模型和检索器的偏好保持一致。</p><p>当环境防止访问强大的专有模型或更大的参数开源模型时，一种简单有效的方法是提取更强大的模型（例如 GPT-4）。</p><h2 id="下游任务与评估"><a href="#下游任务与评估" class="headerlink" title="下游任务与评估"></a>下游任务与评估</h2><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>RAG的核心任务仍然是<strong>问答</strong>(QA)，QA包括传统的单步&#x2F;多步问答、多项选择、特定领域问答、长文本场景问答。</p><p>RAG也不断扩展到多个下游任务，如信息提取(IE)、对话生成、代码搜索等。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>上下文相关性和噪声鲁棒性对于评估检索质量很重要，而答案忠实度、答案相关性、负拒绝、信息集成和反事实鲁棒性对于评估生成质量很重要。</p><h2 id="RAG面临主要挑战和未来前景"><a href="#RAG面临主要挑战和未来前景" class="headerlink" title="RAG面临主要挑战和未来前景"></a>RAG面临主要挑战和未来前景</h2><h3 id="RAG-VS-长上下文"><a href="#RAG-VS-长上下文" class="headerlink" title="RAG VS 长上下文"></a>RAG VS 长上下文</h3><p>目前，LLM 可以毫不费力地管理超过 200,000 个标记的上下文。这也引发了关于当LLM不受上下文限制的时候是否还需要RAG的讨论。</p><p>对于RAG过程，整个检索和推理过程是可观察的，而仅依靠长上下文生成仍然是一个黑匣子。在超长上下文的背景下开发新的 RAG 方法是未来研究趋势之一。</p><h3 id="RAG鲁棒性"><a href="#RAG鲁棒性" class="headerlink" title="RAG鲁棒性"></a>RAG鲁棒性</h3><p>检索过程中噪声或矛盾信息的存在会对RAG的输出质量产生不利影响。这种情况被比喻地称为“Misinformation 可以比根本没有信息更糟糕”。</p><p>提高RAG对这种对抗性或反事实输入的抵抗力正在获得研究势头，已成为一个关键的性能指标。</p><p>The Power of Noise: Redefining Retrieval for RAG Systems.研究结果表明，包含不相关的文档可能会意外地提高准确性超过 30%，这与质量降低的初始假设相矛盾。</p><p>结果强调了开发专门的策略将检索与语言生成模型集成的重要性，强调了进一步研究和探索对 RAG 鲁棒性的必要性。</p><h3 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h3><p>将RAG与微调技术相结合正在成为领先策略，确定RAG与微调最佳整合取决于是顺序、交替还是端到端的训练。</p><p>另一个趋势是将特殊功能的小语言模型SLM引入RAG，并通过RAG系统进行微调。</p><h3 id="scaling-laws-of-RAG"><a href="#scaling-laws-of-RAG" class="headerlink" title="scaling laws of RAG"></a>scaling laws of RAG</h3><p>基于RAG的端到端模型和预训练模型仍然是当前研究人员的重点之一。</p><p>Kaplan, Jared, et al. “Scaling Laws for Neural Language Models.” arXiv: Learning,arXiv: Learning, Jan. 2020.</p><p>缩放规律已经被建立对LLM，但是应用目前尚未确定。</p><h3 id="Production-Ready-RAG"><a href="#Production-Ready-RAG" class="headerlink" title="Production-Ready RAG"></a>Production-Ready RAG</h3><p>然而，提高检索效率，提高大型知识库的文档召回率，保证数据安全，例如防止LLM中元数据和文档来源的无意披露，是仍然有待解决的挑战。</p><p>RAG技术在特殊方向的趋势：1.特定需求的定制化。2.降低最初的学习曲线来简化RAG使用。3.优化RAG来更好的服务生产环境。</p><h3 id="多模态RAG"><a href="#多模态RAG" class="headerlink" title="多模态RAG"></a>多模态RAG</h3><p>RAG在图像、音频视频、编码领域的应用。</p><h2 id="个人阅读总结"><a href="#个人阅读总结" class="headerlink" title="个人阅读总结"></a>个人阅读总结</h2><h3 id="通原理"><a href="#通原理" class="headerlink" title="通原理"></a>通原理</h3><p>对于RAG技术栈的原理自己在脑海中应该有了一定的轮廓，接下来就是重点学习一些技术细节，比如<strong>微调技术、嵌入技术</strong>，更多去关注下游任务实践与部署应用。</p><p>读完论文后要动手去尝试去实现一下RAG技术，通过动手实践真正对这门技术有一个具体的认识，而不是仅仅漂浮于理论上。练实践带动并反作用于学习，真正从实践中体会部署技术、微调技术、训练技术。</p><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><h3 id="知趋势"><a href="#知趋势" class="headerlink" title="知趋势"></a>知趋势</h3><p>读完论文后要对RAG的未来发展趋势自己心里要有一定的认知。这项技术已经做了解决了哪些问题？还能解决哪些问题？哪些方面还需提高或面临困难？对于上面三个问题，通过后期继续学习后要有清楚的认识。起步阶段不要着急，刚刚读了这个方向的一篇文章而已。</p><p>同样更重要的就是，了解趋势与发现问题后，如何去解决？如何转换为自己Idea，这也是关键。</p><h3 id="着力点"><a href="#着力点" class="headerlink" title="着力点"></a>着力点</h3><p>多模态RAG(Multi-modal RAG)、半结构化数据检索(处理半结构化数据的方法优化)、鲁棒性(RAG的对抗性或反事实输入的抵抗力)</p><p>对于半结构化数据的最新研究成果主要参考北京航空航天大学：<a href="https://arxiv.org/pdf/2408.16991">https://arxiv.org/pdf/2408.16991</a></p><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>LLM编码能力在数据库表上执行SQL和将表格转化为文本都不是最好的处理方法，所以对于LLM处理半结构化数据时，探索最优化方法仍然是趋势。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《在线》-读书笔记</title>
    <link href="/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>王坚院士《在线》书摘与读书笔记——互联网、数据和计算</p><p>我们很幸运，生活在一个创造决定未来的时代。但是创造能否决定未来取决于我们的信念和坚持，坚持相信的，相信坚持的。</p><p>计算，为了无法计算的价值</p>          </div><h2 id="榜样-王坚"><a href="#榜样-王坚" class="headerlink" title="榜样-王坚"></a>榜样-王坚</h2><p><strong>王坚</strong>（1962年10月—），中国信息技术专家、心理学家，阿里巴巴集团首席技术官，中国工程院院士，阿里云的创始人。曾主持研发中国唯一自研的云操作系统飞天。</p><img src="/img/bg/wangjian.jpg"/><h2 id="云计算的三次浪潮"><a href="#云计算的三次浪潮" class="headerlink" title="云计算的三次浪潮"></a>云计算的三次浪潮</h2><p>第一次浪潮中，云计算改变了今天全球互联网，奈飞、米哈游为代表的企业从第一天起100%完完整整就在云上。</p><p>第二次浪潮中，“传统企业”开始使用云计算。最标志性的事件是2022年北京冬奥会上，云计算第一次承载了赛事的核心系统。</p><p>人工智能和云计算的一次结合，这是云计算的第三次浪潮。</p><p>云计算的第三次浪潮，王坚认为，2023年人工智能和云计算有了一次集中的体现和爆发，GPT模型的出现使得计算机对科技创新的革命产生了非常重要的影响。云计算和GPT的关系就是电和电动机的关系。未来事实上云计算的算力都是会被这些在智能时代的电动机，就是被模型消耗掉。</p><h2 id="数据与计算"><a href="#数据与计算" class="headerlink" title="数据与计算"></a>数据与计算</h2><p>原子比特化，比特在线化</p><p>到底什么是<strong>在线</strong>？从思维发展与技术发展的角度来说，在线就是连接的属性，必须先连接起来才有在线这回事。从改变行业与生活的角度来说，连接就是在线的结果，在线后才能连接渗透社会生活的各个方面。</p><p>就像我们每天喝水、用电一样，计算也在逐步成为了一种新的公共服务。</p><p><strong>去IOE</strong>：摆脱企业对IBM小型机、Oracle数据库、EMC存储系统的依赖，拥抱云计算，拥抱互联网。</p><p>“<strong>大数据</strong>”这个词搞错了，数据不是因为大而产生的价值，而是因为在线上而产生了价值，因为数据从此可以在更大范围流动它产生的价值，这是真正的数据带来的巨大变化。</p><h2 id="在线定律"><a href="#在线定律" class="headerlink" title="在线定律"></a>在线定律</h2><p>定律一：每一个比特都在互联网上</p><p>定律二：每个比特都可以在互联网上流动</p><p>定律三：比特所代表的每个对象在互联网上都是可以计算的</p><h2 id="谈对技术的热爱"><a href="#谈对技术的热爱" class="headerlink" title="谈对技术的热爱"></a>谈对技术的热爱</h2><p>当我们谈对技术的热爱时，大家并没有真正意识到，我们要为之付出什么。一个不太恰当的比喻就是，我们与技术的关系就是农夫与蛇的关系。</p><p>什么是对技术的热爱？你真的相信技术会改变很多东西吗？有没有足够的自信和热爱，去捂暖这条蛇，哪怕它苏醒后会反咬你一口？当你真正热爱一个东西时，你很难预料最终的结果。当你把这条蛇揣在怀里时，你面对的最大考验是对你心脏的考验。你不知道这条蛇醒来以后是否会反咬你一口，也许你的身体足够强大，可以抵抗。</p><h2 id="人工智能、城市大脑——下一个10年的登月计划"><a href="#人工智能、城市大脑——下一个10年的登月计划" class="headerlink" title="人工智能、城市大脑——下一个10年的登月计划"></a>人工智能、城市大脑——下一个10年的登月计划</h2><p>世界上有三种智能：人的智能、机器的智能、动物的智能。</p><p>多数人谈及人工智能，考虑的都是人会不会被人工智能所取代，这是一个奇怪的逻辑。人们在让狗去寻找毒品的时候从来没有说过人类的鼻子被狗的鼻子取代了。人类要对机器有足够的尊重，要尊重机器在某些方面已经超过了人类。</p><p>互联网发展到今天，我们要做的，使用机器解决人类解决不了的问题。</p><p>今天，世界各国城市的可持续发展都面临着重大的挑战，这些挑战也到来了一个难得的机遇，就是用机器智能解决城市发展过程中产生的许多问题。</p><p>城市大脑要做的就是以互联网为基础设施，利用丰富的城市数据资源，对城市进行全局的实时分析，解决今天无法靠人脑解决的问题。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力祥哲学-把程序写成东西</title>
    <link href="/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/"/>
    <url>/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>天下大事，必做于细</p><p>站在设计者的角度去思考问题</p>          </div><img src="/img/life/oo.jpg"/><h2 id="把程序写成东西"><a href="#把程序写成东西" class="headerlink" title="把程序写成东西"></a>把程序写成东西</h2><p>感谢今生遇见杨力祥老师！！！活了20多年了，第一次遇到一位将面向对象思想讲解这么透彻的老师。</p><p>老师一上来的课程就非常硬核，告诉我们，<strong>面向对象就是把程序写成东西</strong></p><p>以麦当劳炸薯条的例子，教会我们区分面向过程和面向对象。面向过程就是流程化，第一步做什么第二部做什么。。。最后一步做什么。规则、时间、火候、尺寸必须一点都不能差。</p><p>而面向对象是将工作的各个流程打包成几个“东西”，任何人只要学会使用这个东西，就可以炸出麦当劳家的高质量薯条。</p><h2 id="复用"><a href="#复用" class="headerlink" title="复用"></a>复用</h2><p>为什么要学面向对象？有什么好处？</p><p>答案就是：<strong>复用</strong>，程序写一次，在不修改的情况下大量拷贝使用。而复用的本质是<strong>共性</strong>，程序只有写成东西才容易找到共性。</p><p>杨老师还从人类思维认知起源给我们讲起，人类最初学会的工作就是<code>分类</code>，引申出类和对象的关系。将分类比做一个多叉树，在实际思考时，是自下而上抽取共性，抽象就是脱离形式提取共性，但写的时候从上到下派生。</p><p>为什么把程序写成东西就容易复用？</p><p>因为人类祖先最早学会的技能就是分类，根据东西的<strong>共性</strong>进行分类，面向对象根本目的是提高复用率。</p><p>程序写成东西的唯一目的就是复用。复用就是自己可以作为贡献者也可以作为创新者。</p><p>不要做“万能工具箱类”，万能工具箱类不可以进行程序的复用，谁都可以拿来用。</p><h2 id="共性与个性"><a href="#共性与个性" class="headerlink" title="共性与个性"></a>共性与个性</h2><p>共性+差异&#x3D;个性</p><p>面向对象提高效率的理论基础与使用依据：<strong>共性远远远远大于差异，但是差异体现价值</strong>，自然界的事物亦如此。</p><p>继承派生的过程中加入了多态的思想，于是产生了个性的差异价值；如果只是单纯的继承派生无法体现出个性的差异价值。</p><p>虚函数是技术、多态是思想。</p><p>越能加入差异反而复用率越多，越能体现差异越能找到共性。</p><p>加机制产生变异，减机制适者生存。</p><h2 id="创新思维"><a href="#创新思维" class="headerlink" title="创新思维"></a>创新思维</h2><p>创新力与创新思维不是天生的，一定是要经过后天大量培训锻炼出来的。分析问题要抓住本质，这句话很空，但是很难做到。</p><p>作为未来的计算机从业者，凡事用数据和计算的思维去思考。</p><p>开源的东西想卡脖子也是很容易的，代码很难体现出思想上的变化。</p><h2 id="鉴赏力"><a href="#鉴赏力" class="headerlink" title="鉴赏力"></a>鉴赏力</h2><p>不管是机器还是人，提高鉴赏力尤为重要。</p><h2 id="C语言拾遗"><a href="#C语言拾遗" class="headerlink" title="C语言拾遗"></a>C语言拾遗</h2><p>1.为什么要发明C++&#x2F;为什么要复用，单纯使用C语言不行吗？</p><p>面向对象就是把程序写成东西，东西可以复用。实际生产生活中，利益趋使、采取封装，采取找边界，采取把程序写成东西，可以省钱，这才是最重要的原因！</p><p>面向对象就是把程序写成东西，是东西就要有边界(水是唯一能想到的唯一没有边界的东西)。而硬件无法做到边界感，需要人为创造，编译器看着阻止越界。</p><p>边界靠编译器维护，所以不要耍小聪明越级跳过编译器。</p><p>2.C与C++区别与联系</p><p>class从c语言中struct衍生而来，只是多了成员函数与访问控制（public、private，protected）</p><p><strong>class is type</strong></p><p><strong>类作用域</strong>：function::method1()，过去函数库的玩法不行了，面向对象需要的是类库</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//a也是一个对象</span><br><span class="hljs-type">int</span>   a;<br><span class="hljs-comment">//CDate类的实例化</span><br>CDate odate;<br></code></pre></td></tr></table></figure><p>对象在内存中的存储：内存中只有对象：对象的内存结构是只存一份代码，但是存有n个数据</p><p>3.业界规范编码</p><p>每一个类写两个文件(以CDate类为例)：一个CDate.h文件放在Headers File文件夹下，另一个是CDate.cpp文件放在Source File文件夹下。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//Headers File/CDate.h</span><br><span class="hljs-comment">//头文件中只声明类和函数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CDate</span>&#123;<br><span class="hljs-keyword">private</span>:<span class="hljs-comment">//不允许从类的外部直接访问、修改成员变量</span><br>  <span class="hljs-type">int</span> m_iYear;<br>  <span class="hljs-type">int</span> m_iMonth;<br>  <span class="hljs-type">int</span> m_iDay;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">isLeapYear</span><span class="hljs-params">()</span></span>;<br>  <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Print</span><span class="hljs-params">()</span></span>;<br>  <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Set</span><span class="hljs-params">(<span class="hljs-type">int</span>,<span class="hljs-type">int</span>,<span class="hljs-type">int</span>)</span></span>;<br>&#125;;<br><br><span class="hljs-comment">//Source File/CDate.cpp</span><br><span class="hljs-comment">//头文件中只声明类和函数</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&quot;CDate.h&quot;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CDate::isLeapYear</span><span class="hljs-params">()</span></span>&#123;<br>  <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CDate::Print</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; m_iYear &lt;&lt; <span class="hljs-string">&quot;\&quot; &lt;&lt; m_iMonth &lt;&lt; &quot;</span>\<span class="hljs-string">&quot; &lt;&lt; m_iDay &lt;&lt; endl;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">void CDate::Set(int y,int m,int d)&#123;</span><br><span class="hljs-string">  //函数定义</span><br><span class="hljs-string">&#125;</span><br></code></pre></td></tr></table></figure><p>4.声明与定义</p><p>操作系统通电即有值（0010010101010….）</p><p>什么叫函数声明？-只给编译器看不占内存</p><p>函数声明的作用：函数的栈空间是靠函数的声明开辟的，被调函数的形参空间是主调函数做出来的。而空间大小有参数的类型决定，仅仅告诉编译器要撑多大空间。</p><p>函数外不能进行赋值运算，编译完是指令的（动作）必须进函数。无论是变量还是函数，只要<strong>定义就要在内存中出东西</strong>。</p><p>所有C语言编译完是指令的，一定要进函数，不能在全局位置，指令必须存在函数内部。函数调用必须也要在函数内部进行，main函数是出发点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-type">int</span> a;<span class="hljs-comment">//变量的声明，全局变量直接在内存中静态代码区赋值为0</span><br><span class="hljs-type">int</span> b;<br>b = <span class="hljs-number">5</span><span class="hljs-comment">//函数外的赋值操作，编译报错！！</span><br><span class="hljs-type">int</span> c = <span class="hljs-number">10</span>;<span class="hljs-comment">//定义了一个变量c并初始化为10，与前两行不等价</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  <span class="hljs-type">int</span> a = <span class="hljs-number">1</span>;<span class="hljs-comment">//变量的声明+定义 </span><br>  <br>&#125;<br></code></pre></td></tr></table></figure><p>main函数存在的意义：</p><p>禁止全局的语句进行运算操作，函数调用也是语句</p><p>1）编译完是指令的（动作）必须进函数</p><p>2）函数调用是指令也要进函数</p><p>3）总要有一个初始调用，于是mian函数来干这件事</p><p>5.狗屎值与野指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  <span class="hljs-type">int</span> a;<span class="hljs-comment">//生成狗屎值-只声明不赋值</span><br>  <span class="hljs-comment">//千万不要这么写</span><br><span class="hljs-type">int</span> *p;<span class="hljs-comment">//野指针-指向位置随机，危险系数极大</span><br>*p = <span class="hljs-number">5</span>;<span class="hljs-comment">//给一个随机位置赋值，会发生难以预料的危险，如果这个指针指向了内核</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>C++有两类人，一类是负责设计类机构的，另一部分是基于类架构写程序的人</p><p>C++语言的设计与演化</p><h2 id="接受工头的思想"><a href="#接受工头的思想" class="headerlink" title="接受工头的思想"></a>接受工头的思想</h2><p>工头就是抽取共性，事物都是共性大于差异。</p><p>可以从Vector类中的set、get方法出发，用public方法去修改private数据。</p><p>程序写成东西，东西就是要有一套数据和一套配套数据的操作，面向对象也是相对独立、完整的东西，就像每一个人一样。数据都要private，修改数据要用借助public方法。</p><p>五子棋的工头和围棋的工头抽取共性，然后从大共性中派生出象棋的工头、跳棋的工头。。。。</p><p>要把东西就要放在一眼看的范围</p><p>虚函数是C++面向对象程序设计中影响最显著的特性（影响的是复用）。</p><p>虚函数赋值0为纯虚函数，虚函数为抽象类，其不能创建对象。</p><p>添加差异获取复用共性的收益。</p><p>虚函数在基类和派生类中必须是一模一样！</p><p>对于一些程度词，读书的时候要重点关注，什么是好？什么是最显著？</p><h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><p>写程序前要成为行业专家，挣钱多是因为活不好干</p><p>面向对象的程序设计学会画两种设计图：</p><ul><li>类架构图</li><li>对象图</li></ul><p>会画图、会设计是理清思路的关键！</p><p>以银行系统为例：能把人民币取款和微信取款两个行为的共性抽取出来，写出来才是能力。</p><p>真正的共性是脱离了具象的，更接近本质，既体现了内在含义，又脱离了具体的对象，同时又是各个多态共有的东西。</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>雁栖一年，科研三年</title>
    <link href="/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/"/>
    <url>/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="开学"><a href="#开学" class="headerlink" title="开学"></a>开学</h2><p>开学参加了很多次的入学讲座和培训，听了很多老师的演讲和致辞。</p><p>曹亚男老师结合她自己的经历，告诉我们要<code>做有品位的研究</code>。针对科学研究，提出了两大方向：针对开放问题的开创探讨和针对封闭数据的优先探索。问题的价值作为核心，与个人兴趣、资源、能力、知识储备紧密结合。做有意义的课题，要平衡好“我喜欢”和“我的能力”。在雁栖湖的一年，学好专业课程的同时，一定要定好自己的研究点(至少3个)，学硕开题前至少要投出去一篇。</p><p>林政老师在开学典礼的致辞中也告诫我们，研究生的学习过程中，要<code>多关注why而不是how</code>。对于新技术、新方法，主要要弄懂为什么这样用？为什么是这个结构？至于说怎么用，那是学习过程中需要解决的问题。林老师也提醒到，学习课程的同时，不要忘记培养自己的科研能力与科学素养。</p><h2 id="关于选课"><a href="#关于选课" class="headerlink" title="关于选课"></a>关于选课</h2><p>新学期选课工作全部结束啦，抢到了自己心仪的课程。新学期还是立足本专业，结合李老师的建议侧重数据库和人工智能相关。</p><p>核心课选了晓飞老师的机器学习、曹亚男老师的自然语言处理、沙老师的大数据管理与分析。专业课选了黄晶老师的大数据技术、计算机苏老师的数据库新技术。</p><p>希望自己好好利用在雁栖湖的这一年时光，多学点真本领、多培养一些科学素养、多积累一些科研经验。上好课是基本的态度，此外还要在立足上好专业课的同时多读读经典论文，毕竟自己是学术型硕士，科学研究的基本素养与方法论还是要多多去积累。</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>开学一周，结实了身边很多优秀的同学。依稀记得，第一次开班会的时候，大家在做自我介绍，每个人都会说自己的本科毕业院校是哪里。我留心关注了一下，河北大学应该是“倒数第三好”。说实话，本科院校说出来其实是不自信的，在我上台前我也犹豫过、摇摆过，到底要不要说。最终，我还是大大方方、很坦然的向大家做了介绍。此时此刻，彷佛就像刚步入高中，步入沧州市第一中学那个优秀的环境中。但是，唯一不同的是，我的心态和那时有了很大的变化。</p><p>刚从乡镇走出来，步入一中的时候，那时候身边都是比自己优秀的人。说实话，那三年过的非常迷茫和压抑。初中的时候，你在学校是“呼风唤雨的”，但是到了一个更大的环境中、一个竞争更激烈的环境中，你彷佛就迷失了自己。那几年你会发现，你好像无论怎么努力、怎么使劲学都没办法赶超周围的人，你一直在盲目的进行分数的“攀比”。学业学业跟不上、擅长的体育也不再是特长、个人感情不顺利、二叔车祸离世家庭带来创伤…真就想掉入深渊一样，周围全是黑暗，看不到一丝希望与光亮。</p><p>本科的时候，你考了一个相对来说正常发挥的分数，以专业第一的身份进入大学。在前三年没有懈怠，因为不装逼的说，当时确实还是有遗憾。推免完成后，顺利完成学业。这四年你的表现不错，通过积极体育锻炼、综合素质锻炼、遇到了一群好的室友，你的性格、心态有了很大改变。唯一重要的是，这四年，我觉着你一直在<code>找自己</code>。找自己就是客观坦然的面对自己，从容地面对自己的优点和缺点，三人行必有我师焉，你也在时时刻刻向身边优秀的人学习。真诚、勇敢、向上社交。</p><p>好了，话说回来了，来到研究生阶段，又是到了一个更大的环境中、一个竞争更激烈的环境中，彷佛是轮回。我想，此时的你一定不同于高中时期的你，你的心里一定多了几分淡定与从容。就随着这份淡定与从容，继续向前吧！千磨万击还坚劲，任尔东西南北风。走好自己的路，心安即强大。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>雁栖一年，科研三年。前路漫漫，忘君珍重、珍惜。</p><p>​                                                                                                                                                    2024年8月30日于国科大雁栖湖</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DataWhale AI夏令营-LLM实训</title>
    <link href="/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/"/>
    <url>/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/</url>
    
    <content type="html"><![CDATA[<h2 id="day01-baseline搭建"><a href="#day01-baseline搭建" class="headerlink" title="day01-baseline搭建"></a>day01-baseline搭建</h2><div class="note note-success">            <p>首先注册魔塔社区帐号，免费领取魔塔GPU算力资源</p>          </div><p>新建GPU算力环境，下载相关第三方库与拉取镜像资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 拉取git镜像</span></span><br>git lfs install<br>git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 安装第三方库</span></span><br>pip install streamlit==1.24.0<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 启动demo</span></span><br>streamlit run AICamp_yuan_baseline/Task\ 1：零基础玩转源大模型/web_demo_2b.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure><h2 id="day02-RAG原理与实践"><a href="#day02-RAG原理与实践" class="headerlink" title="day02-RAG原理与实践"></a>day02-RAG原理与实践</h2><p>检索增强生成 <strong>(Retrieval Augmented Generation,RAG)</strong> 是一种使用来自私有或专用数据源的信息来辅助文本生成的技术。它将检索模型(设计用于搜索大型数据集或知识库)和生成模型(例如大型语言模型 (LLM))，此类模型会使用检 索到的信息生成可供阅读的文本回复)结合在一起。</p><h3 id="LLM局限性"><a href="#LLM局限性" class="headerlink" title="LLM局限性"></a>LLM局限性</h3><p><a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p><p>这篇文章由来自Facebook AI Research、University College London、New York University三大科研教育机构的12名作者 (Patrick Lewis等)共同完成。文章主要介绍了一种新颖的检索增强生成(RAG)模型，该模型旨在解决预训练语言模型 在知识密集型NLP任务中的局限性，RAG技术被首次提出。</p><p>文章中阐述了传统大模型的局限性:传统的大型预训练模型虽然拥有存储大量事实知识的能力，但在 (query accuracy)和更 (knowledge updates)时存在不足。</p><p>同样，在实际业务场景中，通用的基础大模型可能存在无法满足我们需求的情况，主要有以下几方面原因：</p><ul><li><p>知识局限性：大模型的知识来源于训练数据，而这些数据主要来自于互联网上已经公开的资源，对于一些实时性的或者非公开的，由于大模型没有获取到相关数据，这部分知识也就无法被掌握。</p></li><li><p>数据安全性：为了使得大模型能够具备相应的知识，就需要将数据纳入到训练集进行训练。然而，对于企业来说，数据的安全性至关重要，任何形式的数据泄露都可能对企业构成致命的威胁。</p></li><li><p>大模型幻觉：由于大模型是基于概率统计进行构建的，其输出本质上是一系列数值运算。因此，有时会出现模型“一本正经地胡说八道”的情况，尤其是在大模型不具备的知识或不擅长的场景中。</p></li></ul><h3 id="RAG基本步骤"><a href="#RAG基本步骤" class="headerlink" title="RAG基本步骤"></a>RAG基本步骤</h3><p><img src="/img/post/rag-buzhou.png" alt="基本步骤"></p><ul><li><p>索引：将文档库分割成较短的 <strong>Chunk</strong>，即文本块或文档片段，然后构建成向量索引。</p></li><li><p>检索：计算问题和 Chunks 的相似度，检索出若干个相关的 Chunk。</p></li><li><p>生成：将检索到的Chunks作为背景信息，生成问题的回答。</p></li></ul><h3 id="RAG完整链路图"><a href="#RAG完整链路图" class="headerlink" title="RAG完整链路图"></a>RAG完整链路图</h3><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><p>图片来源:(<a href="https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png">https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png</a>)</p><p>用户进行query查询后，RAG会先进行检索，之后将检索到的 <strong><code>Chunks</code></strong> 和 <strong><code>query</code></strong> 一并输入到大模型，进而回答用户的问题。</p><p>为了完成检索，需要离线将文档（ppt、word、pdf等）经过解析、切割甚至OCR转写，然后进行向量化存入数据库(vector database)中。</p><h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h3><p>知识库中包含了多种类型的文件，如pdf、word、ppt等，这些 <code>文档</code>（Documents）需要提前被解析，然后切割成若干个较短的 <code>Chunk</code>，并且进行清洗和去重。</p><p>然后，我们会将知识库中的所有 <code>Chunk</code> 都转成向量，这一步也称为 <code>向量化</code>（Vectorization）或者 <code>索引</code>（Indexing）。<code>向量化</code> 需要事先构建一个 <code>向量模型</code>（Embedding Model），它的作用就是将一段 <code>Chunk</code> 转成 <code>向量</code>（Embedding）。</p><p>随着新知识的不断存储，向量的数量也会不断增加。这就需要将这些向量存储到 <code>数据库</code> （DataBase）中进行管理。</p><h3 id="在线计算"><a href="#在线计算" class="headerlink" title="在线计算"></a>在线计算</h3><p>在实际使用RAG系统时，当给定一条用户 <code>查询</code>（Query），需要先从知识库中找到所需的知识，这一步称为 <code>检索</code>（Retrieval）。在 <code>检索</code> 过程中，用户查询首先会经过向量模型得到相应的向量，然后与 <code>数据库</code> 中所有 <code>Chunk</code> 的向量计算相似度，最简单的例如  <code>余弦相似度</code>，然后得到最相近的一系列 <code>Chunk</code> 。</p><p>由于向量相似度的计算过程需要一定的时间，尤其是 <code>数据库</code> 非常大的时候。可以在检索之前进行 <code>召回</code>（Recall），即从 <code>数据库</code> 中快速获得大量大概率相关的 <code>Chunk</code>，然后只有这些 <code>Chunk</code> 会参与计算向量相似度。这样，计算的复杂度就从整个知识库降到了非常低。</p><p>随着知识库的增大，除了检索的速度变慢外，检索的效果也会出现退化。这是由于 <code>向量模型</code> 能力有限，而随着知识库的增大，已经超出了其容量，因此准确性就会下降。在这种情况下，相似度最高的结果可能并不是最优的。</p><p>为了解决这一问题，提升RAG效果，研究者提出增加一个二阶段检索——<code>重排</code> (Rerank)，即利用 <code>重排模型</code>（Reranker），使得越相似的结果排名更靠前。这样就能实现准确率稳定增长，即数据越多，效果越好（如上图中紫线所示）。</p><p>通常，为了与 <code>重排</code> 进行区分，一阶段检索有时也被称为 <code>精排</code> 。而在一些更复杂的系统中，在 <code>召回</code> 和 <code>精排</code> 之间还会添加一个 <code>粗排</code> 步骤，这里不再展开，感兴趣的同学可以自行搜索。综上所述，在整个 <code>检索</code> 过程中，计算量的顺序是 <code>召回</code> &gt; <code>精排</code> &gt; <code>重排</code>，而检索效果的顺序则是 <code>召回</code> &lt; <code>精排</code> &lt; <code>重排</code> 。</p><p>至此，一个完整的RAG链路就构建完毕了。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks </p><p>[2] Gao, Yunfan, et al. “Retrieval-augmented generation for large language models: A survey.” <strong>arXiv preprint arXiv:2312.10997</strong> (2023).</p><p>[3] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewriting for retrieval-augmented large language models,” <strong>arXiv preprint arXiv:2305.14283</strong>, 2023.</p><p>[4] QAnything: <a href="https://github.com/netease-youdao/QAnything">https://github.com/netease-youdao/QAnything</a></p><p>[5] When Large Language Models Meet Vector Databases: A Survey <a href="https://doi.org/10.48550/arXiv.2402.01763">https://doi.org/10.48550/arXiv.2402.01763</a> </p><h2 id="RAG技术实践"><a href="#RAG技术实践" class="headerlink" title="RAG技术实践"></a>RAG技术实践</h2><p>前置条件：使用day01搭建好的baseline环境</p><p>下载环境所需的任务包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git lfs install<br>git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-built_in">cp</span> AICamp_yuan_baseline/Task\ 3：源大模型RAG实战/* .<br></code></pre></td></tr></table></figure><p>双击打开<code>Task 3：源大模型RAG实战.ipynb</code>，然后运行所有单元格。</p><p>在环境中安装<code>streamlit</code>,为了后续进行模型微调以及Demo搭建(day01已经安装完毕)。</p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>在RAG实战过程中，需要构建一个向量模型。向量模型通常是一个BERT架构，是一个Transformer Encoder。</p><p>在本次学习中，选用基于BERT架构的向量模型 <code>bge-small-zh-v1.5</code>，它是一个4层的BERT模型，最大输入长度512，输出的向量维度也为512。</p><p>向量模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&quot;AI-ModelScope/bge-small-zh-v1.5&quot;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Yuan大模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>构造向量索引，分装一个向量模型类EmbeddingModel：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EmbeddingModel</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for EmbeddingModel</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(path)<br><br>        <span class="hljs-variable language_">self</span>.model = AutoModel.from_pretrained(path).cuda()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading EmbeddingModel from <span class="hljs-subst">&#123;path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">self, texts: <span class="hljs-type">List</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate embedding for text list</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        encoded_input = <span class="hljs-variable language_">self</span>.tokenizer(texts, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>        encoded_input = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()&#125;<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            model_output = <span class="hljs-variable language_">self</span>.model(**encoded_input)<br>            sentence_embeddings = model_output[<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>]<br>        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> sentence_embeddings.tolist()<br></code></pre></td></tr></table></figure><p>通过传入模型路径，新建一个 <code>EmbeddingModel</code> 对象 <code>embed_model</code>。初始化时自动加载向量模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create embedding model...&quot;</span>)<br>embed_model_path = <span class="hljs-string">&#x27;./AI-ModelScope/bge-small-zh-v1___5&#x27;</span><br>embed_model = EmbeddingModel(embed_model_path)<br></code></pre></td></tr></table></figure><p><code>EmbeddingModel</code> 类还有一个 <code>get_embeddings()</code> 函数，它可以获得输入文本的向量表示。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>为了实现向量检索，定义一个向量库索引类 <code>VectorStoreIndex</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量库索引类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorStoreIndex</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for VectorStoreIndex</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, doecment_path: <span class="hljs-built_in">str</span>, embed_model: EmbeddingModel</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.documents = []<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(doecment_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>):<br>            line = line.strip()<br>            <span class="hljs-variable language_">self</span>.documents.append(line)<br><br>        <span class="hljs-variable language_">self</span>.embed_model = embed_model<br>        <span class="hljs-variable language_">self</span>.vectors = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings(<span class="hljs-variable language_">self</span>.documents)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.documents)&#125;</span> documents for <span class="hljs-subst">&#123;doecment_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarity</span>(<span class="hljs-params">self, vector1: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], vector2: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate cosine similarity between two vectors</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dot_product = np.dot(vector1, vector2)<br>        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> magnitude:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> dot_product / magnitude<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, k: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        question_vector = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings([question])[<span class="hljs-number">0</span>]<br>        result = np.array([<span class="hljs-variable language_">self</span>.get_similarity(question_vector, vector) <span class="hljs-keyword">for</span> vector <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vectors])<br>        <span class="hljs-keyword">return</span> np.array(<span class="hljs-variable language_">self</span>.documents)[result.argsort()[-k:][::-<span class="hljs-number">1</span>]].tolist() <br></code></pre></td></tr></table></figure><p>类似地，通过传入知识库文件路径，新建一个 <code>VectorStoreIndex</code> 对象 <code>index</code>。初始化时会自动读取知识库的内容，然后传入向量模型，获得向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create index...&quot;</span>)<br>doecment_path = <span class="hljs-string">&#x27;./knowledge.txt&#x27;</span><br>index = VectorStoreIndex(doecment_path, embed_model)<br></code></pre></td></tr></table></figure><p>上文提到 <code>get_embeddings()</code> 函数支持一次性传入多条文本，但由于GPU的显存有限，输入的文本不宜太多。</p><p>所以，如果知识库很大，需要将知识库切分成多个batch，然后分批次送入向量模型。</p><p><code>VectorStoreIndex</code> 类还有一个 <code>get_similarity()</code> 函数，它用于计算两个向量之间的相似度，这里采用了余弦相似度。<code>VectorStoreIndex</code> 类的入口，即查询函数 <code>query()</code>。传入用户的提问后，首先会送入向量模型获得其向量表示，然后与知识库中的所有向量计算相似度，最后将 <code>k</code> 个最相似的文档按顺序返回，<code>k</code>默认为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">question = <span class="hljs-string">&#x27;介绍一下广州&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Question:&#x27;</span>, question)<br><br>context = index.query(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Context:&#x27;</span>, context)<br></code></pre></td></tr></table></figure><h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3><p>为了实现基于RAG的生成，我们还需要定义一个大语言模型类 <code>LLM</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义大语言模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLM</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for Yuan2.0 LLM</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat tokenizer...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat model...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading Yuan2.0 model from <span class="hljs-subst">&#123;model_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, context: <span class="hljs-type">List</span></span>):<br>        <span class="hljs-keyword">if</span> context:<br>            prompt = <span class="hljs-string">f&#x27;背景：<span class="hljs-subst">&#123;context&#125;</span>\n问题：<span class="hljs-subst">&#123;question&#125;</span>\n请基于背景，回答问题。&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            prompt = question<br><br>        prompt += <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>        inputs = <span class="hljs-variable language_">self</span>.tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>        outputs = <span class="hljs-variable language_">self</span>.model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">1024</span>)<br>        output = <span class="hljs-variable language_">self</span>.tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>这里我们传入 <code>Yuan2-2B-Mars</code> 的模型路径，新建一个 <code>LLM</code> 对象 <code>llm</code>。初始化时自动加载源大模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create Yuan2.0 LLM...&quot;</span>)<br>model_path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br>llm = LLM(model_path)<br></code></pre></td></tr></table></figure><p><code>LLM</code> 类的入口是生成函数 <code>generate()</code>，它有两个参数：</p><ul><li><code>question</code>: 用户提问，是一个str</li><li><code>context</code>: 检索到的上下文信息，是一个List，默认是[]，代表没有使用RAG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Without RAG:&#x27;</span>)<br>llm.generate(question, [])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; With RAG:&#x27;</span>)<br>llm.generate(question, context)<br></code></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">&gt;</span> <span class="hljs-string">Without RAG:</span><br><span class="hljs-attr">广州大学（Guangzhou</span> <span class="hljs-string">University）是广东省内一所综合性大学，位于中国广东省广州市。广州大学成立于1952年，前身为广州工学院，是中华人民共和国成立后创建的第一所高等工科院校。</span><br><span class="hljs-attr">广州大学坐落在广州市海珠区，占地面积广阔，校园环境优美。学校拥有多个校区，其中主校区位于广州市番禺区，其他校区分布在广州市的其他地区。学校占地面积约4000亩，拥有现代化的教学、实验和生活设施。</span><br><span class="hljs-attr">广州大学以培养人才为宗旨，注重理论与实践相结合的教学模式。学校开设了多个学院和专业，涵盖了工学、理学、文学、法学、经济学、管理学、艺术学等多个领域。学校现有本科专业近300个，研究生专业涵盖科学、工程、管理、文学、法学、艺术等多个领域。</span><br><span class="hljs-attr">广州大学注重国际交流与合作，积极推进国际化办学。学校与许多国际知名大学建立了合作关系，开展学术交流和合作研究。此外，学校还鼓励学生参与国际交流项目，提供海外实习和留学机会，提升学生的国际视野和能力。</span><br><span class="hljs-attr">广州大学一直以来致力于为学生提供优质的教育环境和丰富的学习资源。学校拥有先进的教学设施和实验室，以及图书馆、体育场馆、艺术工作室等丰富的学生课外活动设施。</span><br><span class="hljs-attr">广州大学以其优秀的教学质量、领先的科研水平和培养优秀学生的能力而闻名。学校致力于培养具有创新精神和社会责任感的高素质人才，为地方经济发展和社会进步做出贡献。&lt;eod&gt;</span><br><span class="hljs-attr">&gt;</span> <span class="hljs-string">With RAG:</span><br><span class="hljs-attr">广州大学是一所位于广东省广州市的全日制普通高等学校，实行省市共建、以市为主的办学体制。学校的办学历史可以追溯到1927年创办的私立广州大学，后来在1951年并入华南联合大学。1984年定名为广州大学。2000年，广州大学经过教育部批准，与广州教育学院、广州师范学院、华南建设学院西院、广州高等师范专科学校合并组建新的广州大学。&lt;eod&gt;</span><br></code></pre></td></tr></table></figure><h2 id="day03-微调技术原理与实践"><a href="#day03-微调技术原理与实践" class="headerlink" title="day03-微调技术原理与实践"></a>day03-微调技术原理与实践</h2><p>模型微调也被称为指令微调（Instruction Tuning）或者有监督微调（Supervised Fine-tuning, SFT），该方法利用成对的任务输入与预期输出数据，训练模型学会以问答的形式解答问题，从而解锁其任务解决潜能。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p><p>指令微调并非无中生有地传授新知，而是更多地扮演着催化剂的角色，激活模型内在的潜在能力，而非单纯地灌输信息。</p><p>相较于预训练所需的海量数据，指令微调所需数据量显著减少，从几十万到上百万条不等的数据，均可有效激发模型的通用任务解决能力。</p><h3 id="轻量化微调"><a href="#轻量化微调" class="headerlink" title="轻量化微调"></a>轻量化微调</h3><p>由于大模型的参数量巨大， 进行全量参数微调需要消耗非常多的算力。为了解决这一问题，研究者提出了参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning），这些方法通过训练极少的模型参数，同时保证微调后的模型表现可以与全量微调相媲美。</p><p>常用的轻量化微调技术有<code>LoRA</code>、<code>Adapter</code> 和 <code>Prompt Tuning</code>。</p><p>LoRA:<a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a></p><p>大模型轻量级微调（LoRA）：训练速度、显存占用分析:<a href="https://zhuanlan.zhihu.com/p/666000885">https://zhuanlan.zhihu.com/p/666000885</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#模型下载</span><br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br><span class="hljs-comment"># 导入环境</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer<br><span class="hljs-comment"># 读取数据</span><br>df = pd.read_json(<span class="hljs-string">&#x27;./data.json&#x27;</span>)<br>ds = Dataset.from_pandas(df)<br><span class="hljs-comment"># 加载 tokenizer</span><br>path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br>tokenizer.pad_token = tokenizer.eos_token<br><span class="hljs-comment"># 定义数据处理函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_func</span>(<span class="hljs-params">example</span>):<br>    MAX_LENGTH = <span class="hljs-number">384</span>    <span class="hljs-comment"># Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性</span><br><br>    instruction = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;input&#x27;</span>]&#125;</span>&lt;sep&gt;&quot;</span>)<br>    response = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&lt;eod&gt;&quot;</span>)<br>    input_ids = instruction[<span class="hljs-string">&quot;input_ids&quot;</span>] + response[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>    attention_mask = [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(input_ids) <br>    labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(instruction[<span class="hljs-string">&quot;input_ids&quot;</span>]) + response[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-comment"># instruction 不计算loss</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(input_ids) &gt; MAX_LENGTH:  <span class="hljs-comment"># 做一个截断</span><br>        input_ids = input_ids[:MAX_LENGTH]<br>        attention_mask = attention_mask[:MAX_LENGTH]<br>        labels = labels[:MAX_LENGTH]<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids,<br>        <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>        <span class="hljs-string">&quot;labels&quot;</span>: labels<br>    &#125;<br><span class="hljs-comment"># 处理数据集</span><br>tokenized_id = ds.<span class="hljs-built_in">map</span>(process_func, remove_columns=ds.column_names)<br>tokenized_id<br><span class="hljs-comment"># 数据检查</span><br>tokenizer.decode(tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>tokenizer.decode(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x != -<span class="hljs-number">100</span>, tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;labels&quot;</span>])))<br><span class="hljs-comment"># 模型加载</span><br>model = AutoModelForCausalLM.from_pretrained(path, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>)<br>model<br>model.enable_input_require_grads() <span class="hljs-comment"># 开启gradient_checkpointing时，要执行该方法</span><br><span class="hljs-comment"># 配置Lora</span><br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model<br><br>config = LoraConfig(<br>    task_type=TaskType.CAUSAL_LM, <br>    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>, <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],<br>    inference_mode=<span class="hljs-literal">False</span>, <span class="hljs-comment"># 训练模式</span><br>    r=<span class="hljs-number">8</span>, <span class="hljs-comment"># Lora 秩</span><br>    lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># Lora alaph，具体作用参见 Lora 原理</span><br>    lora_dropout=<span class="hljs-number">0.1</span><span class="hljs-comment"># Dropout 比例</span><br>)<br>config<br><span class="hljs-comment"># 构建PeftModel</span><br>model = get_peft_model(model, config)<br>model<br><span class="hljs-comment"># 设置训练参数</span><br>args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./output/Yuan2.0-2B_lora_bf16&quot;</span>,<br>    per_device_train_batch_size=<span class="hljs-number">12</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">1</span>,<br>    logging_steps=<span class="hljs-number">1</span>,<br>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    num_train_epochs=<span class="hljs-number">3</span>,<br>    learning_rate=<span class="hljs-number">5e-5</span>,<br>    save_on_each_node=<span class="hljs-literal">True</span>,<br>    gradient_checkpointing=<span class="hljs-literal">True</span>,<br>    bf16=<span class="hljs-literal">True</span><br>)<br><span class="hljs-comment"># 初始化Trainer</span><br>trainer = Trainer(<br>    model=model,<br>    args=args,<br>    train_dataset=tokenized_id,<br>    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="hljs-literal">True</span>),<br>)<br><span class="hljs-comment"># 模型训练</span><br>trainer.train()<br><span class="hljs-comment"># 定义生成函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">prompt</span>):<br>    prompt = prompt + <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>    inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>    outputs = model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">256</span>)<br>    output = tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br>    <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br>input_str = <span class="hljs-string">&#x27;张三，汉族，金融学硕士。&#x27;</span><br>prompt = template.replace(<span class="hljs-string">&#x27;input_str&#x27;</span>, input_str).strip()<br>generate(prompt)<br>&#123;<span class="hljs-string">&quot;姓名&quot;</span>: [<span class="hljs-string">&quot;张三&quot;</span>], <span class="hljs-string">&quot;国籍&quot;</span>: [<span class="hljs-string">&quot;汉族&quot;</span>], <span class="hljs-string">&quot;职位&quot;</span>: [<span class="hljs-string">&quot;金融学硕士&quot;</span>]&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
      <tag>部署</tag>
      
      <tag>微调技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据技术学习笔记</title>
    <link href="/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="大数据技术路线总揽"><a href="#大数据技术路线总揽" class="headerlink" title="大数据技术路线总揽"></a>大数据技术路线总揽</h2><h2 id="MapReduce编程思想"><a href="#MapReduce编程思想" class="headerlink" title="MapReduce编程思想"></a>MapReduce编程思想</h2><h2 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h2><h3 id="Google-files-system"><a href="#Google-files-system" class="headerlink" title="Google files system"></a>Google files system</h3><p>GFS直接以Linux为基础存储层，并且设计模式为单master模式。</p><p>另外一方面，GFS 还是采用了 Checkpoints、操作日志（Operation Logs）、影子Master（Shadow Master）等一系列的工程手段，来尽可能地保障整个系统的“可恢复（Recoverable）”，以及读层面的“可用性（Availability）”。</p>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
      <tag>数仓</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习基础</title>
    <link href="/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="Machine-Learning-Notebook"><a href="#Machine-Learning-Notebook" class="headerlink" title="Machine Learning-Notebook"></a>Machine Learning-Notebook</h1><center>Andrew Ng-吴恩达&copy;</center><center>Standford ONLINE & DeepLearning.AI</center><center>Mungeryang-杨桂淼总结</center><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>“Field of study that gives computers the ability to learn without being explicitly programmed.”——Arthur Samuel(1995)</p><p>  Practical advice for applying learning algorithm</p><h2 id="Basic-conception-cookbook"><a href="#Basic-conception-cookbook" class="headerlink" title="Basic conception cookbook"></a>Basic conception cookbook</h2><p>Data set:数据集包括训练集+测试集</p><p>Training set: 用于模型训练的数据集</p><p>Test set: 用于模型测试的数据集</p><p>Cost function: 衡量模型预测结果与真实结果之间的差异或误差</p><p>Gradient: 方向导数；梯度的大小将表示函数在某一方向上的变化率</p><p>Gradient descent:  一种最优化的算法，找到函数的局部最小值</p><p>Accurate : 所有分类（无论是正类还是负类）正确分类的比例。TP + TN &#x2F; TP+FP+TN+FN</p><p>Recall(also known as sensitivity): the fraction of relevant instances that were retrieved；所有实际正例被正确分类为正例的比例。TP&#x2F;TP+FN</p><p>Precision(also called positive predictive value) : the fraction of relevant instances among the retrieved instances;模型所有正分类分类中实际正分类的比例。TP&#x2F;TP+FP</p><p>F1: F1值 &#x3D; 正确率 * 召回率 * 2 &#x2F; (正确率 + 召回率)；F1值的范围在0和1之间，值越接近1表示模型的性能越好。F1值越大表示分类器在识别正例样本时取得了更好的平衡。</p><p>A <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><h2 id="Supervised-learning-监督学习"><a href="#Supervised-learning-监督学习" class="headerlink" title="Supervised learning-监督学习"></a>Supervised learning-监督学习</h2><p> Learns from being given “<strong>right answers</strong>(labels)”</p><img src="/img/fig/1.1.png" alt="s" style="text-align: center;"/><p>In all of these applications, we will first train our model with examples of inputs <strong>x</strong> and right answers, that is the labels <strong>y</strong>. After the model has learned from these input, output, or x and y pairs, they can take a brand new input x, something it has never seen before, and try to produce the appropriate corresponding output y.  </p><img src="/img/fig/1.2.jpg" alt="s" style="text-align: center;" /><p>  The task of the supervised learning algorithm is to produce more of these right answers based on labels.  </p><p>Classification is to predict categories,Regression is to predict a number. </p><img src="/img/fig/1.3.png" alt="s" style="text-align: center;" /><h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning-无监督学习"></a>Unsupervised learning-无监督学习</h2><p>Learns from being given “<strong>no-right answers</strong>(unlabeled data)”, data only comes with inputs x, but not output labels y. Algorithm has to find structure automatically in the data and automatically figure out whether the major types of individuals.</p><img src="/img/fig/1.4.png" style="text-align: center;" /><h2 id="Linear-Regression-with-one-variable"><a href="#Linear-Regression-with-one-variable" class="headerlink" title="Linear Regression with one variable"></a>Linear Regression with one variable</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Linear regression means fitting a <code>straight line</code> to the data. -&gt;linear regression  </p><img src="/img/fig/2.1.jpg" alt="s" style="text-align: center;"/><p>Regression model is to predict numbers</p><p>Classification model is to predicts categories</p><h3 id="Terminology-in-ML"><a href="#Terminology-in-ML" class="headerlink" title="Terminology in ML"></a><strong>Terminology in ML</strong></h3><p><code>Training dataset</code>-&gt;data used to train the model</p><p>$x&#x3D;$“input”variables feature</p><p>$y&#x3D;$“output ”variables or “target”variables</p><p>$(x,y)&#x3D;$single training example</p><p>$(x^{(i)},y^{(i)})&#x3D;i^{th}$ training example not exponent</p><img src="/img/fig/2.2.jpg" alt="s" style="text-align: center;"/><p>In the linear regression, we instantly believe the function is a linear function as follow:<br>$$<br>f_{w,b}(X)&#x3D;wX+b \<br>f(X)&#x3D;wX+b<br>$$<br>when we give a “x”as a input variable, we can get a “y-hat”variable as a result.</p><h3 id="triangular-ruler-Cost-function"><a href="#triangular-ruler-Cost-function" class="headerlink" title=":triangular_ruler:Cost function"></a>:triangular_ruler:Cost function</h3><p>squared error<br>$$<br>\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2<br>$$<br><img src="/img/fig/2.3.jpg" alt="s" style="text-align: center;" /></p><p>Model: $f_{w,b}(X)&#x3D;wX+b$.$w(slope),b(intersects)$ are parameters.<br>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{w.b}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2 \<br>&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2<br>$$<br>Our goal is to minimize the parameters to fit the model:<br>$$<br>\underset{w.b}{\min}J\left( w,b \right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#define cost function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    cost_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i]) ** <span class="hljs-number">2</span><br>        cost_sum += cost<br>    total_cost = (<span class="hljs-number">1</span>/(<span class="hljs-number">2</span>*m))*cost_sum<br>    <span class="hljs-keyword">return</span> total_cost<br></code></pre></td></tr></table></figure><h3 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent-梯度下降"></a>Gradient descent-梯度下降</h3><p><strong>Simultaneous update</strong> the parameters w and b until the cost function is <code>convergence</code>:</p><p>Simultaneous update the parameters is significant, we must focus on the order about the algorithm.</p><p>Correct order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>\<br>w&#x3D;tmp_w<br>\<br>b&#x3D;tmp_b<br>$$<br>Incorrect order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right) \<br>w&#x3D;tmp_w \<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right) \<br>b&#x3D;tmp_b \<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#compute gradient</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradient</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_dw = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_dw += dj_dw_i<br>        dj_db += dj_db_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></td></tr></table></figure><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>The choice of learning rate, alpha($\alpha$) will have a huge impact on the efficiency of our implementation of Gradient descent.<br>$$<br>w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>$$<br><img src="/img/fig/2.6.jpg" alt="s" style="text-align: center;" /></p><p>If we chose the alpha is too large, the fit efficiency is not very well. We can design the algorithm to decrease the alpha($\alpha$) following by the w and b.</p><img src="/img/fig/cost1.png" alt="s" style="text-align: center;" /><h2 id="Multiple-linear-regression"><a href="#Multiple-linear-regression" class="headerlink" title="Multiple linear regression"></a>Multiple linear regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>【Model】<br>$$<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n<br>$$<br><code>Dot product(inner product)</code> of two vectors about $w$ and $b$.<br>$$<br>\vec{w}&#x3D;\left[ w_1,w_2···,w_n \right]<br>\<br>\vec{x}&#x3D;\left[ x_1,x_2···,x_n \right]<br>$$<br>$x_j&#x3D;j^{th} features$</p><p>$n &#x3D; $ numbers of features</p><p>$\vec{x}^{(i)}$ &#x3D; features of $i^{th}$ training example</p><p>$\vec{x}^{(i)}_j$ &#x3D; value of feature j in  $i^{th}$ training example</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>We contrast the two process between vectorization and without vectorization.</p><p>without vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n):<br>    f = f + w[i]*x[i]<br>f = f + b<br></code></pre></td></tr></table></figure><p>If n is infinite, the algorithm is time consuming.</p><p>with vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>f = np.dot(w,b)<br></code></pre></td></tr></table></figure><p> Without vectorization, we just only use the for loop to calculate the results one by one. In the contrast, we can use the function numpy.dot() to calculate the result. Using numpy can decrease the time complexity and improve the algorithm efficiency. Numpy can use <code>parallel process</code> hardware to carry out the data.</p><p>Dot product of two vectors:<br>$$<br>a·b&#x3D;\underset{i&#x3D;0}{\overset{n-1}{\varSigma}}a_ib_i&#x3D;\left[ \begin{array}{l}<br>    ,, a_0\<br>    ,, a_1\<br>    ,,···\<br>    a_{n-1}\<br>\end{array} \right] \left[ \begin{array}{l}<br>    ,, b_0\<br>    ,, b_1\<br>    ,,···\<br>    b_{n-1}\<br>\end{array} \right] &#x3D;\left[ a_0b_0+a_1b_1+···+a_{n-1}b_{n-1} \right]<br>$$</p><h3 id="Gradient-descent-in-Multiple-regression"><a href="#Gradient-descent-in-Multiple-regression" class="headerlink" title="Gradient descent in Multiple regression"></a>Gradient descent in Multiple regression</h3><p>Parameters include $w_{1},w_{2},···w_{n}$ and $b$</p><p>Model is $f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n+b$</p><p>Cost function is $J\left( w_{1},w_{2},···w_{n},b \right)$</p><p>Gradient descent:</p><pre><code class="hljs">repeat&#123;</code></pre><p>$$<br>w_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>}</p><p><code>for i in range(1,n)</code>:<br>$$<br>\frac{\partial}{\partial w_{1}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{1}^{\left( i \right)} \<br>\frac{\partial}{\partial w_{n}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{n}^{\left( i \right)} \<br>\frac{\partial}{\partial b}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right)<br>$$<br>Normal equation:</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations</li></ul><p>Disadvantages:</p><ul><li>Does not generalize to other learning algorithm</li><li>Slow when number of features is large(n&gt;10000)</li></ul><h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The technology of Features scaling will enable Gradient descent to run much faster.</p><p>Z-score normalization:<br>$$<br>x_i&#x3D;\frac{x_i-\mu}{\sigma}<br>$$<br>$\mu$ is the sample average value, and $\sigma$ is standard error of sample.</p><p>Aim for about $-1\leqslant x_j\leqslant 1$ for $x_{j}$.</p><p>The objective of Gradient descent in Multiple regression is:<br>$$<br>\underset{\vec{w}.b}{\min},,J\left(\vec{w},b \right)<br>$$<br><img src="/img/fig/2.7.jpg" alt="s" style="text-align: center;" /></p><p>As the calculate process, $y&#x3D;J\left(\vec{w},b \right)$ is decreased until a low score. If we can get the similar the result in the experiment, the cost function is <code>convergence</code> and we can find the min $J\left(\vec{w},b \right)$.</p><p>【<strong>Skill</strong>】</p><p>At the beginning, we can set the learning rate($\alpha$) in a small value like 0.001. As running the algorithm, we can update the parameter $\alpha$ gradually.(0.001-&gt;0.01-&gt;0.1)</p><p><strong>Feature Engineering</strong> : Using <code>intuition</code> to design new features, by transforming or combining original features.</p><blockquote><p>[Wiki]:<strong>Feature engineering</strong> or <strong>feature extraction</strong> or <strong>feature discovery</strong> is the process of extracting features (characteristics, properties, attributes) from raw data.</p></blockquote><img src="/img/fig/2.8.jpg" style="text-align: center;" /><p>Except for using linear regression, we can also use <code>polynomial regression</code> to fit data. Through finding the segment of the data distribution(scatter), we can create special features($\sqrt{x}\quad x^3$) to fit data successfully.<br>$$<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+b\quad②<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+b\quad①<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2\sqrt{x}+b\quad③<br>$$</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>It turns out that, linear regression is not the good algorithm for classification. For classification, the output is not a continuous number. In the fact, it is a class variable like <strong>0(false-negative class)</strong> or <strong>1(true-positive class)</strong>.</p><img src="/img/fig/3.1.jpg" alt="s" style="text-align: center;" /><p>Sometimes, if we want to classify the output, the method of linear regression maybe not fit. Through the figure, the result of predict can be changed when we add a sample. The original fit result is the blue line, but, the new result is the green line. The standard of classification can be changed as the sample we adding.</p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>In this chapter, we will learn a useful algorithm model——<code>Logistic Regression</code> to solve the classification problem.</p><p>With linear regression method, the model is $f_{\vec{w},b}\left(x^{i} \right) &#x3D;\vec{w}·x^{i}+b$, to predict $y$ given $x$. However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</p><p>This can be accomplished by using a <code>&quot;sigmoid function&quot;</code> which maps all input values to values between 0 and 1.</p><p>【sigmoid function】</p><img src="/img/fig/3.2.jpg" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;\frac{1}{1+e^{-Z}},0&lt;g\left( Z \right) &lt;1<br>\<br>f_{w,b}\left( X^{\left( i \right)} \right) &#x3D;g\left( w·X^{\left( i \right)}+b \right)<br>\<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;g\left( \vec{w}·\vec{x}+b \right) &#x3D;\frac{1}{1+e^{-\left( \vec{w}·\vec{x}+b \right)}}<br>$$</p><p>We can calculate the Z-score to classify the predict value by the probability.</p><p>$$<br>P\left( y&#x3D;0 \right) +P\left( y&#x3D;1 \right) &#x3D;1<br>$$</p><img src="/img/fig/3.3.jpg" style="text-align: center;"/><p>$$<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;1|\vec{X};\vec{w},b \right)<br>\<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;0|\vec{X};\vec{w},b \right)<br>$$</p><p>Probability that $y&#x3D;1$  or $y&#x3D;0$, given input $\vec{x}$, parameters $w,b$.</p><p>If $y&#x3D;f_{\vec{w},b}\left( \vec{X} \right) &gt; 0.5$ -&gt; $g(Z)&gt;0.5$ -&gt; $Z&gt;0$ -&gt; $\vec{w}·\vec{x}+b &gt; 0$ &#x3D;&#x3D; YES $Y&#x3D;1$.</p><p>Else, NO $y &#x3D; 0$.</p><h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><img src="/img/fig/3.4.jpg" alt="s" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+b \right)<br>\<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b \right)\<br>boundary1&#x3D;w_1x_1+w_2x_2+b&#x3D;0<br>\<br>boundary2&#x3D;w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b&#x3D;0<br>$$</p><h3 id="Cost-function-for-regularized-logistic-regression"><a href="#Cost-function-for-regularized-logistic-regression" class="headerlink" title="Cost function for regularized logistic regression"></a>Cost function for regularized logistic regression</h3><p>Cost function:</p><p>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\frac{1}{2}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2&#x3D;L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right)<br>$$</p><p>Our goal is to minimize the parameters to fit the model:</p><p>$$<br>\underset{w.b}{\min},,J\left( w,b \right)<br>$$</p><p><strong>Simplified</strong> loss function:</p><p>$$<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;\begin{cases}<br>    -\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;1\<br>    -\log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;0\<br>\end{cases}\ \Longrightarrow \<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;-y^{\left( i \right)}\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) -\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right)<br>$$</p><p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}  \sum_{i&#x3D;0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2<br>$$</p><p>where:</p><p>$$<br>f_{\mathbf{w},b}(\mathbf{x}^{(i)}) &#x3D; sigmod(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)<br>$$</p><p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}\sum_{i&#x3D;0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]<br>$$</p><p>As was the case in linear regression above, the difference is the regularization term, which is   $\frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2$ </p><p>Gradient descent:</p><p>repeat{</p><p>$$<br>tw_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>tb&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>w&#x3D;tw_{j}\<br>b&#x3D;tb<br>$$</p><p>}</p><img src="/img/fig/gridant1.png" style="text-align: center;" /><p>To provide the overfitting problem, we apply the regularized method to add the penalty coefficient. That is why the we add the $\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}$ at the end of the formula.</p><img src="/img/fig/3.5.png" alt="s" style="text-align: center;" /><p><strong>Regularized</strong>:</p><p>$$<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}<br>\<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}b_{j}^{2}<br>$$</p><p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$.</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;&#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\ \<br>$$</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial b}  &#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})<br>$$</p><ul><li><p>m is the number of training examples in the data set      </p></li><li><p>$f_{\mathbf{w},b}(x^{(i)})$ is the model’s prediction, while $y^{(i)}$ is the target</p></li><li><p>For a  <span style="color:blue"> <strong>linear</strong> </span> regression model<br>  $f_{\mathbf{w},b}(x) &#x3D; \mathbf{w} \cdot \mathbf{x} + b$  </p></li><li><p>For a <span style="color:blue"> <strong>logistic</strong> </span> regression model<br>  $z &#x3D; \mathbf{w} \cdot \mathbf{x} + b$ \ $f_{\mathbf{w},b}(x) &#x3D; g(z)$<br>  where $g(z)$ is the sigmoid function:</p></li></ul><p>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#cpmpute cost function through itertion process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function</span>):<br>    w = copy.deepcopy(w_in)<br>    J_history = []<br>    p_history = []<br>    b = b_in <br>    w = w_in<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw,dj_db = gradient_function(x,y,w,b)<br>        <span class="hljs-comment"># update the parameters </span><br>        b = b - alpha * dj_db<br>        w = w - alpha * dj_dw<br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">100000</span>:<br>            J_history.append(cost_function(x,y,w,b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i % math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteraton <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> w,b,J_history,p_history<br></code></pre></td></tr></table></figure><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><h3 id="How-to-install-TensorFlow"><a href="#How-to-install-TensorFlow" class="headerlink" title="How to install TensorFlow"></a>How to install TensorFlow</h3><ul><li>conda install tensorflow</li><li>pip install tensorflow</li></ul><h3 id="How-the-brain-works"><a href="#How-the-brain-works" class="headerlink" title="How the brain works"></a>How the brain works</h3><p>The following picture shows the basic structure about neutral network. Like human&#96;s neutral cell, it passes information by layers, which every cell includes a logistic function.</p><img src="/img/fig/4.2.jpg" style="text-align: center;" /><h3 id="Data-format-in-Tensorflow"><a href="#Data-format-in-Tensorflow" class="headerlink" title="Data format in Tensorflow"></a>Data format in Tensorflow</h3><p><code>matrix and tensor</code></p><p>Numpy, a standard library created in 1970s, is used to calculate linear algebra in python(data analysis). Tensorflow is a machine learning framework created by Google.</p><p>In Tensorflow, the input format must <strong>a matrix</strong>. We should focus on the special characteristic in our work.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#一维数组</span><br>b = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<span class="hljs-comment">#一维矩阵</span><br>x = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">14</span>],<br>])<span class="hljs-comment">#3X3矩阵</span><br>Z = np.matmul(A_in,W) + B <span class="hljs-comment">#input matrix to simplify for loop</span><br></code></pre></td></tr></table></figure><h3 id="Build-a-Tensorflow"><a href="#Build-a-Tensorflow" class="headerlink" title="Build a Tensorflow"></a>Build a Tensorflow</h3><ol><li>build the structure of the model</li><li>compile the model</li><li>input training data</li><li>fit the model</li><li>predict the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span><br>layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_3 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1,layer_2,...layer_n])<br>----------------------------<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>])<br><span class="hljs-number">2.</span><br>model.<span class="hljs-built_in">compile</span>()<br><span class="hljs-number">3.</span><br>x = np.array([[<span class="hljs-number">0.</span>..,<span class="hljs-number">245</span>,...,<span class="hljs-number">17</span>],[<span class="hljs-number">0.</span>..,<span class="hljs-number">200</span>,...,<span class="hljs-number">284</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>])<br><span class="hljs-number">4.</span><br>model.fit<br><span class="hljs-number">5.</span><br>model.predict(x_new)<br></code></pre></td></tr></table></figure><h3 id="Implementation-of-the-preceding-communication"><a href="#Implementation-of-the-preceding-communication" class="headerlink" title="Implementation of the preceding communication"></a>Implementation of the preceding communication</h3><img src="/img/fig/4.4.png" alt="s" style="text-align: center;"/>$$\vec{w}_{1}^{\left[ 1 \right]}=\left[ \begin{array}{c}    1\\    2\\\end{array} \right] ,\vec{w}_{2}^{\left[ 1 \right]}=\left[ \begin{array}{c}    -3\\    4\\\end{array} \right] ,\vec{w}_{3}^{\left[ 1 \right]}=\left[ \begin{array}{c}    5\\    -6\\\end{array} \right] $$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([<br>    [<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,<span class="hljs-number">5</span>],<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,-<span class="hljs-number">6</span>]<br>]) <br>b = np.array([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>a_in = np.array([-<span class="hljs-number">2</span>,<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>Implement the coding of dense function with python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in,W,b,g</span>):<br>    <span class="hljs-comment">#units equals to the numbers of cols of W</span><br>    units = W.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment">#Create a matrix with the same size of units--[0,0,0]</span><br>    a_out = np.zeros(units)<br>    <span class="hljs-comment">#compete the g(z)</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x,W1,b1)<br>    a2 = dense(a1,W2,b2)<br>    a3 = desne(a2,W3,b3)<br>    a4 = dense(a3,W4,b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br>    <br>    <br></code></pre></td></tr></table></figure><h3 id="Choose-activation-function"><a href="#Choose-activation-function" class="headerlink" title="Choose activation function"></a>Choose activation function</h3><p>Three types activation function:</p><img src="/img/fig/4.3.jpg" alt="s" style="text-align: center;" /><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">activation=<span class="hljs-string">&quot;linear&quot;</span><br>activation=<span class="hljs-string">&quot;sigmoid&quot;</span><br>activation=<span class="hljs-string">&quot;relu&quot;</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>type y</th><th>0&#x2F;1</th><th>+&#x2F;-</th><th>0&#x2F;+</th></tr></thead><tbody><tr><td>sigmoid</td><td>binary classfication</td><td></td><td></td></tr><tr><td>linear</td><td></td><td>regression</td><td></td></tr><tr><td>relu</td><td></td><td></td><td>regression</td></tr></tbody></table><p>[why we need activation function?]</p><p>If we use the linear activation function in  hidden layers all the time, the predict results of neutral network equal to linear regression. that&#96;s why we should use sigmoid or relu function as the activation function to build the network.</p><h3 id="Soft-max-regression"><a href="#Soft-max-regression" class="headerlink" title="Soft-max regression"></a>Soft-max regression</h3><p>Softmax function has been used to solve the  multi-class problem. Essentially, it is still a <strong>classification problem</strong> <code>based on probability</code>. The expression of Softmax function as follow:<br>$$<br>z_j&#x3D;\vec{w}_j·\vec{x}+b_j\<br>a_j&#x3D;\frac{e^{z_j}}{\underset{k&#x3D;1}{\overset{n}{\varSigma}}e^{z_k}}&#x3D;P\left( y&#x3D;j|\vec{x} \right)<br>$$<br>a_j is the possibility of predict result.</p><p>The cost function of Soft-max function is:<br>$$<br>a_N&#x3D;\frac{e^{Z_N}}{e^{Z_1}+e^{Z_2}+\cdot \cdot \cdot +e^{Z_N}}&#x3D;P\left( y&#x3D;N|\vec{x} \right)<br>$$</p><p>$$<br>loss\left( a_1,…,a_N,y \right) &#x3D;\begin{cases}<br>    -\log a_1,,if,,y&#x3D;1\<br>    -\log a_2,,if,,y&#x3D;2\<br>    \vdots\<br>    -\log a_n,,if,,y&#x3D;n\<br>\end{cases}<br>$$<br>The result is <code>one of</code> the loss functions. </p><h3 id="Output-of-the-soft-max"><a href="#Output-of-the-soft-max" class="headerlink" title="Output of the soft-max"></a>Output of the soft-max</h3><img src="/img/fig/4.5.jpg" style="text-align: center;" /><p>The outcome of soft-max classification is multiply. Every outcome will be competed a score to predict the right answer. </p><p>Carefully, the loss function we must choose the <code>SparseCategoricalCrossentropy</code>.<br>$$<br>a_{N}^{\left[ l \right]}&#x3D;\frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>\<br>-\log a_{N}^{\left[ l \right]}\ne -\log \frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>$$<br>The difference of competing order can lead to the different outcome, which has different model accurancy.</p><h3 id="Improve-the-soft-max-model"><a href="#Improve-the-soft-max-model" class="headerlink" title="Improve the soft-max model"></a>Improve the soft-max model</h3><p>In order to improve the accuracy of calculations, we have made the following improvements to the model algorithm:</p><ul><li>In soft-max layer, we adopt the <code>linear</code> function as the activation.</li><li>In the process of compiling model, we add a parameter to improve the accurancy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#linear function as the activation function in the soft-max layer</span><br>model = sequential([<br>    Dense(units = <span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>)<br>])<br><span class="hljs-comment">#add parameter:from_logits=True </span><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br>logit = model(X)<br>f_x = tf.nn.sigmoid(logit)<br></code></pre></td></tr></table></figure><h3 id="MNIST-Adam"><a href="#MNIST-Adam" class="headerlink" title="MNIST Adam"></a>MNIST Adam</h3><h2 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h2><h3 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h3><p>Model fits the training data well(over-fit) but will fail to generalize to new examples not in the training set.</p><p>Hence, we need to <strong>partition the dataset</strong> to test the accurancy of the model.</p><p>The data set was divided into <code>test set</code> and <code>train set</code>.</p><p>【regression】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m_{train}}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{test}}\left[ \underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) -y</em>{test}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>compute train error:</p><p>$$<br>J_{train}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{train}}\left[ \underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) -y</em>{train}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>【train】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;-\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left[ y^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) \right] +\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2}<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}&#x3D;-\frac{1}{m_{test}}\underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left[ y_{test}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) \right)<br>+\left( 1-y</em>{test}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{test}^{\left( i \right)} \right) \right) \right]<br>$$</p><p>compute train error:</p><p>$$<br>-\frac{1}{m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left[ y_{train}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) \right) +\left( 1-y</em>{train}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{train}^{\left( i \right)} \right) \right) \right]<br>$$</p><h3 id="Model-selection-and-cross-validation"><a href="#Model-selection-and-cross-validation" class="headerlink" title="Model selection and cross validation"></a>Model selection and cross validation</h3><p>If we want to fit a function to predict a problem or classification, we often use test error $J_{test}$ to judge the accurancy of the model. But, the J test is likely to be an <code>optimistic estimate</code> of generalization error. Because, when we choose the degree of parameter d in polynomial fit.,This fit of J test may lower than the actual estimate. The optimistic estimate can lead to a low score of J_test.</p><p>So, we need to partition the dataset as three parts to avoid optimistic estimate:</p><ul><li>training set</li><li>cross validation set</li><li>test set</li></ul><p>compute cross validation error:</p><p>$$<br>J_{cv}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{cv}}\left[ \underset{i&#x3D;1}{\overset{m_{cv}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{cv}^{\left( i \right)} \right) -y</em>{cv}^{\left( i \right)} \right) ^2 \right]<br>$$</p><h3 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h3><p>Have idea-Train model-Diagnose bias and variance</p><p> $J_{train}$ reflects bias, and  $J_{cv}$ reflects variance. A perfect model has low  $J_{train}$ and low  $J_{cv}$.</p><p>As the increasing of degree of d, $J_{train}$ will typically go down. Meanwhile, $J_{cv}$ will also go down and then it will increase.$J_{cv}$ will have a min value for different degree of d.</p><img src="/img/fig/4.6.jpg" alt="s" style="text-align: center;" /><p>How do you tell if our algorithm has a bias or variance problem?</p><ul><li>High bias(under fit): $J_{train}$ will be high($J_{train}\approx J_{cv}$)</li><li>High variance(over fit): $J_{cv}$&gt;&gt;$J_{train}$($J_{train}$ may be low)</li><li>High bias and High variance $J_{train}$ will be high and $J_{cv}$&gt;&gt;$J_{train}$</li></ul><h3 id="Regularization-and-bias-variance"><a href="#Regularization-and-bias-variance" class="headerlink" title="Regularization and bias&#x2F;variance"></a>Regularization and bias&#x2F;variance</h3><p>A large neutral network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_1 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&quot;sigmoid&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>model = Sequential([layer_1,layer_2,layer_3])<br></code></pre></td></tr></table></figure><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>When we increase the size of training set, the train error will increase. But, the error of cross validation will decrease. As the increasing of sample points. a regression function(a line or a curve) cannot fit all the point.</p><img src="/img/fig/4.7.jpg" style="text-align: center;" /><p>If a algorithm suffers from high variance, getting more training data is <strong>likely</strong> to help.</p><p>If a algorithm suffers from high bias, getting more training data <strong>will not</strong> help much.</p><p>【Debugging algorithm】</p><p>we have implemented the regularized linear regression:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><table><thead><tr><th align="center">operation</th><th>what should do</th></tr></thead><tbody><tr><td align="center">get more training examples</td><td>fixes high variance</td></tr><tr><td align="center">try smaller sets of features</td><td>fixes high variance</td></tr><tr><td align="center">try getting additional features</td><td>fixes high bias</td></tr><tr><td align="center">try adding polynomial features</td><td>fixes high bias</td></tr><tr><td align="center">try decreasing  $\lambda$</td><td>fixes high bias</td></tr><tr><td align="center">try increasing  $\lambda$</td><td>fixes high variance</td></tr></tbody></table><p><code>Trade-off</code></p><p>Simple model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+b$) will get high bias VS complex model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+w_3x^3+w_4x^4+b$) will get high variance.</p><img src="/img/fig/4.8.jpg" style="text-align: center;" /><h3 id="Cycle-of-machine-learning"><a href="#Cycle-of-machine-learning" class="headerlink" title="Cycle of machine learning"></a>Cycle of machine learning</h3><p>The cycle of ML process:</p><img src="/img/fig/4.9.jpg" style="text-align: center;" /><img src="/img/fig/4.10.jpg" style="text-align: center;" /><p>How to apply the ML model to solve the actual problem in software engineering design?</p><img src="/img/fig/4.11.jpg" alt="s" style="text-align: center;" /><p>ML model is collected in the inference server. we use mobile app through API call to achieve these function.</p><h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><strong>Precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.</p><p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that were retrieved. </p><p>We design a <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><p>In the trade-off  between Precision(P) and Recall(R), we use F1 score to evaluate the efficiency about the model.</p><p>the trade-off  between Precision(P) and Recall(R) has shown in the figure:</p><img src="/img/fig/4.13.jpg" style="text-align: center;" /><p>$$<br>F1&#x3D;\frac{1}{\frac{1}{2}\left( \frac{1}{P}+\frac{1}{R} \right)}&#x3D;\frac{2PR}{P+R}<br>$$</p><h2 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h2><p>The structure of a decision tree:</p><img src="/img/fig/5.1.jpg" style="text-align: center;" /><h3 id="Methods-chosen"><a href="#Methods-chosen" class="headerlink" title="Methods chosen"></a>Methods chosen</h3><p>For Signal Decision tree, we should focus on the problem is that the data features.</p><p>If the data is  discrete(just like 0 or 1), we can build the Signal Decision tree model. But,a row data may includes more than two classes, in this situation we should use <code>one-hot encoding</code>.</p><blockquote><p>one-hot encoding only fit for the decision tree model.</p></blockquote><p>If the data has continuous data(not only just like 0 or 1), we should split on a continuous variance.</p><p>For Multiple trees, we can use <strong>Random Forest</strong> and <strong>XGboost</strong> algorithm to solve.</p><img src="/img/fig/5.3.jpg" style="text-align: center;" /><h3 id="Purity-entropy"><a href="#Purity-entropy" class="headerlink" title="Purity(entropy)"></a>Purity(entropy)</h3><p>$p_{1}$ &#x3D; fraction of examples that are True.</p><img src="/img/fig/5.4.png" style="text-align: center;" /><p>$$<br>H\left( p_1 \right) &#x3D;-p_1\log \left( p_1 \right) -p_0\log \left( p_0 \right)<br>\<br>&#x3D;-p_1\log \left( p_1 \right) -\left( 1-p_1 \right) \log \left( 1-p_1 \right)<br>$$</p><img src="/img/fig/5.5.jpg" style="text-align: center;" /><p>In this figure, $w^{left}$&#x3D;2&#x2F;5、 $w^{right}$&#x3D;3&#x2F;5、$p_{1}^{left}$&#x3D;5&#x2F;10、$p_{2}^{left}$&#x3D;5&#x2F;10.</p><p>Information Purity</p><p>$$<br>Information Purity &#x3D;H\left( p_{1}^{root} \right) -\left( w^{left}H\left( p_{1}^{left} \right) +w^{right}H\left( p_{1}^{right} \right) \right)<br>$$</p><p>We should choose the <code>max</code> value of the Information Purity to <strong>recursive</strong> the decision tree model, which is called <code>Information Gain</code>.</p><p>In the process of split on a continuous variance(<strong>Regression tree</strong>), we also choose the max decreasing variance result as a good fit model.</p><img src="/img/fig/5.2.jpg" alt="s" style="text-align: center;"/><p>The purity of regression tree(equal to information gain):</p><p>$$<br>D&#x3D;V^{root}-\left( w^{left}V^{left}+w^{right}V^{right} \right)<br>$$</p><p>V instead of <code>variance</code>.</p><h3 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h3><ul><li><p>Start with all examples at the root node</p></li><li><p>Calculate information gain for all features, and pick the one with the highest information gain</p></li><li><p>Split dataset according to selected features, and create left and right branches of the tree</p></li><li><p>Keep repeating splitting process until stopping criteria is met:</p><pre><code class="hljs">      <figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">when a node <span class="hljs-keyword">is</span> <span class="hljs-number">100</span>% one <span class="hljs-keyword">class</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">splitting</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">will</span> <span class="hljs-symbol">result</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">tree</span> <span class="hljs-symbol">exceeding</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">maximum</span> <span class="hljs-symbol">depth</span><br><br><span class="hljs-symbol">Information</span> <span class="hljs-symbol">gain</span> <span class="hljs-symbol">from</span> <span class="hljs-symbol">additional</span> <span class="hljs-symbol">splits</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">less</span> <span class="hljs-symbol">than</span> <span class="hljs-symbol">threshold</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">number</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">examples</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">below</span>  <span class="hljs-symbol">a</span> <span class="hljs-symbol">threshold</span><br></code></pre></td></tr></table></figure></code></pre></li></ul><h2 id="Decision-tree-VS-Neutral-network"><a href="#Decision-tree-VS-Neutral-network" class="headerlink" title="Decision tree VS Neutral network"></a>Decision tree VS Neutral network</h2><h3 id="Decision-tree-1"><a href="#Decision-tree-1" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul><li>Works well on tabular(structured) data</li><li>Not recommended for unstructured data(images,audios,text)</li><li>Small decision tree may be human interpretable</li></ul><h3 id="Neutral-network"><a href="#Neutral-network" class="headerlink" title="Neutral network"></a>Neutral network</h3><ul><li>Works well on all types of data,including tabular(structured) data and unstructured data(images,audios,text)</li><li>May be slower than decision tree</li><li>Works with transfer learning</li><li>When building a system of multiple models working together, it might be easier to string together multiple neutral network</li></ul><h2 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM-支持向量机"></a>SVM-支持向量机</h2><h2 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h2><p>L1、L2范数</p><p>正则化</p><p>归一化</p>]]></content>
    
    
    <categories>
      
      <category>人工智能基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>本科毕业论文致谢</title>
    <link href="/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
    <url>/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>时光匆匆，转眼间四年的本科生活即将接近尾声。在此，请允许我以一篇致谢，表达在这四年时光里最真挚的谢意！</p><p>感谢论文指导老师杨国庆教授的悉心指导。从2021年参加数学建模比赛与老师结缘，杨老师严谨的治学态度和科学的研究方法给了我极大的帮助和影响，由衷感谢杨老师对我的关心和指导！</p><p>同时，由衷感谢教过我的河北大学国际学院信管专业全体ISEC教师：杨秀丹老师、吴树芳老师、徐杰老师、史海燕老师、郝杰老师、史江兰老师、贾金英老师、宇文姝丽老师、郭海玲老师、崔广志老师、吴利明老师、韩倩老师、陈婷老师、张鑫老师(排名不分先后)，以及其他全部科任老师。</p><p>感谢我的教练吕旭老师。大一期间将我纳入冰雪运动队，从此便与体育运动结下不解之缘。吕旭老师积极的心态与干练的作风对我产生了深远影响。从教练身上，我也更加懂得了什么是责任，什么叫标杆。</p><p>感谢我的辅导员曹永姝老师。四年的本科生活，曹老师对我照顾有加，老师的隐忍与坚持对我产生了很大的影响。她了解我的脾气与性格，尊重我的选择。</p><p>感恩在河北大学遇到的老师们，师恩天大，永记心间。</p><p>感谢我的父亲杨海龙先生、母亲刘秋菊女士。二十四年的成长之路，你们敢于放手，让我自由生长、大胆试错。每当我面临人生抉择的时候，你们都是那样义无反顾地支持我的选择。</p><p>感谢我的舅舅刘运陶先生，他对于我的成长是特别的。舅舅是我们家族中的第一位本科生、研究生，他潜移默化地培养了我的阅读习惯、学习习惯，并用他四十多年走过的弯路为我规避错误，他一直是我的榜样。</p><p>感谢我的姥姥邵长芬女士、姥爷刘增才先生。从四岁上幼儿园开始我便离开父母跟随姥姥、姥爷生活，直到我十八岁离开小镇去沧州市第一中学读高中。十四年的时光大部分都是和姥姥、姥爷度过的，姥姥、姥爷的勤劳、隐忍、坚韧、厚道对我的性格塑造影响至深。感谢我的奶奶刘绪巧女士、爷爷杨春林先生。2017年的仲夏，我二叔的车祸离世对两位老人以及全家人造成了沉痛打击，我曾一度认为两位老人会无法走出晚年丧子之痛。但是时过境迁，二老以顽强不屈、坚忍不拔的精神面貌给儿孙们以希望。</p><p>感恩我的家人们，让我有足够的勇气去面对生活中的任何挫折与苦难。</p><p>最后，感谢这四年的自己。前路漫漫，未来还会有很长的路要走，还会有更多的挑战、磨难需要去面对，还会有更多的责任需要去承担。会遇到很多人，会经历更多的事。但是无论怎样，请不要丢掉良心和理想，都不要忘记抽时间回忆回忆在河北大学这四年的美好时光。加油，祝好！</p><p>感谢百忙之中参加答辩的各位领导、老师们！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell学习笔记</title>
    <link href="/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量定义的格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">变量名=变量值 #注意=左右两边不可以有空格<br><span class="hljs-meta prompt_">#</span><span class="language-bash">直接赋值</span><br>username=&quot;ygm&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">键盘赋值</span><br>read username<br><span class="hljs-meta prompt_">#</span><span class="language-bash">执行的命令结果赋值</span><br>str=$(pwd)<br>str=$(ll)<br>str=`ps -ef`<br></code></pre></td></tr></table></figure><p>变量的访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>echo $name<br>echo $echo&#123;name&#125;<br></code></pre></td></tr></table></figure><p>特殊变量</p><table><thead><tr><th>变量名</th><th>定义</th></tr></thead><tbody><tr><td>$#</td><td>命令行参数的个数</td></tr><tr><td>$?</td><td>前一个命令或函数的返回码</td></tr><tr><td>$n</td><td>$1表示第一个参数</td></tr><tr><td>$0</td><td>当前程序的名称</td></tr><tr><td>$*</td><td>以“参数1，参数2···”保存所有参数</td></tr></tbody></table><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>结论：推荐编程的时候使用双引号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">str=hello<br>str=`hello`<br>str=&quot;hello&quot;<br><br>str2=&#x27;I am $&#123;str&#125;&#x27;#单引号不会解释字符串里面的变量<br>str2=&quot;I am $&#123;str&#125;&quot;#双引号可以解释字符串里面的变量<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出字符串的长度</span><br>echo $&#123;#name&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">提取子字符串</span><br>echo $&#123;string:a:b&#125;#从索引(索引从0开始)为a个位置开始截取长度为b的子字符串<br></code></pre></td></tr></table></figure><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><p>算数运算在shell中要遵守严格的规范格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo `expr 2 + 3`#必须是反引号包裹、加号与数字之间留有空格<br>echo `expr 2 \* 3`#注意乘法和除法必须使用转义符号\前缀才会起作用<br>echo `expr 2 \% 4`<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此外，算数运算还可以用$(())和$[]表示-推荐方式</span><br>a=2<br>b=3<br>echo $((a+b))<br>echo $(($a+$b))<br>echo $[a+b]<br>echo $[$a+$b]<br></code></pre></td></tr></table></figure><h3 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h3><p>数字比较</p><ul><li>-eq：比较两个数是否相等，相等返回true</li><li>-ne：比较两个数是否不想等，不想等返回true</li><li>-gt：检测左边的数是否大于右边，若是返回true</li><li>-lt：检测左边的数是够小于右边，若是返回true</li><li>-ge：检测左边的数是否大于等于右边，若是返回true</li><li>-le：检测左边的数是否小于等于右边，若是返回true</li></ul><p>字符串比较：</p><ul><li>-z STRING：字符串长度为0</li><li>-n STRING：字符串长度不为0</li><li>&#x3D;：判断字符串长度是否相等</li><li>！&#x3D;：判断字符串长度是否不想等</li></ul><p>文件：</p><p>-f：存在且普通的文件</p><p>-e：文件存在</p><p>-d：存在且是目录</p><p>-h：存在且是链接</p><p>-r：存在且是只读</p><p>-w：存在且是可写</p><p>-x：存在且是可执行</p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组用小括号表示，中间元素用空格隔开，也可以直接定义数组中的每个元素的值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">array=(1 2 &quot;hello&quot; ygm)<br>array[4]=&quot;xyc&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取数组元素</span><br>echo $&#123;array[index]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取整个数组</span><br>echo $&#123;array[*]&#125;<br>echo $&#123;array[@]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">获取数组长度</span><br>echo $&#123;#array[*]&#125;<br><br></code></pre></td></tr></table></figure><h3 id="shell命令"><a href="#shell命令" class="headerlink" title="shell命令"></a>shell命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">逻辑运算符&amp;&amp;和||<br><br>&amp;&amp; 表示与，|| 表示或<br>二者具有短路原则：<br>expr1 &amp;&amp; expr2：当expr1为假时，直接忽略expr2<br>expr1 || expr2：当expr1为真时，直接忽略expr2<br>表达式的exit code为0，表示真；为非零，表示假。（与C/C++中的定义相反）<br><br></code></pre></td></tr></table></figure><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><p>if判断语句范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">if condition<br>then <br>···<br>else<br>···<br>fi<br><br>if[ &quot;a&quot; -lt &quot;b&quot; ] &amp;&amp; [ &quot;a&quot; -gt 2]<br>then <br>echo $&#123;a&#125;在范围内<br>fi<br><br>if[ $a -eq 2]<br>then <br>echo $&#123;a&#125;等于2<br>elif [ $a -eq 3]<br>then<br>echo $&#123;a&#125;等于3<br>else<br>echo 其他<br>fi<br></code></pre></td></tr></table></figure><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><p>for范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">for v in var1 var2 var3<br>do<br>echo $v<br>done<br><br>for flie in `ls`<br>do <br>echo $file<br>done<br><br>for i in $(seq 1 10)<br>do<br>echo $i<br>done<br><br>for ((i = 1;i&lt;10;i++))<br>do<br>echo $i<br>done<br></code></pre></td></tr></table></figure><p>while循环范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">while read name<br>do <br>echo $name<br>done<br><br>until [&quot;$&#123;word&#125;&quot; == &quot;yes&quot;] || [&quot;$&#123;word&#125;&quot; == &quot;YES&quot;]<br>do <br>read -p &quot;please input yse or YES to stop this program:&quot; word<br>done<br></code></pre></td></tr></table></figure><p>PS3使用方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;what is your favourite OS?&quot;<br>PS3=&quot;please enter your chose:&quot;<br>select var in &quot;linux&quot; &quot;windowns&quot; &quot;unix&quot;<br>do<br>break;<br>done<br>echo &quot;you have selected $var&quot;<br></code></pre></td></tr></table></figure><h2 id="一键安装JDK"><a href="#一键安装JDK" class="headerlink" title="一键安装JDK"></a>一键安装JDK</h2><p>使用shell脚本实现自动化部署</p><p>jdk_install.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">提示安装jdk</span><br>echo &quot;开始安装jdk&quot;<br>sleep 1<br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除自带的jdk</span><br>oldjdk=$(rpm -qa | grep jdk)<br>for old in $&#123;oldjdk&#125;<br>do<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-variable">$old</span></span><br>rpm -e --nodes $old<br>done<br><span class="hljs-meta prompt_">#</span><span class="language-bash">创建安装目录</span><br>java_path=$(/export/server)<br>if[ !-d $java_path]<br>then<br> mkdir -p $java_path<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">解压jdk安装包</span><br>tar -zxvf /export/softwore/jdk-8u241-linux-x64.tar.gz -C $java_path<br><span class="hljs-meta prompt_">#</span><span class="language-bash">设置环境变量</span><br>JAVA_HOME=&quot;/export/server/jdk1.8.0_241&quot;<br>grep &quot;JAVA_HOME&quot; /etc/profile<br>if[ #? -ne 0]<br>then<br><span class="hljs-meta prompt_">#</span><span class="language-bash">JAVA_HOME</span><br>echo &quot;---------JAVA_HOME-----------&quot;<br>echo `export JAVA_HOME=/export/server/jdk1.8.0_241` &gt;&gt; /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">PATH</span><br>echo &quot;----------PATH-----------&quot;<br>echo `export PATH=:$JAVA_HOME/bin:$PATH` &gt;&gt; /etc/profile<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">加载环境变量</span><br>sleep 1<br>source /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装完成提示</span><br>echo &quot;恭喜您jdk安装成功！&quot;<br>java -version<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>云计算</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cloud computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/08/05/hello-world/"/>
    <url>/2024/08/05/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! </p><p>值得纪念的日子，今天把之前的博客内容全部成功迁移到了新的平台上面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; welcome to hexo! &lt;&lt; endl;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>归家小记</title>
    <link href="/2024/04/24/%E5%BD%92%E5%AE%B6%E5%B0%8F%E8%AE%B0/"/>
    <url>/2024/04/24/%E5%BD%92%E5%AE%B6%E5%B0%8F%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>2024.04.13-2024.04.20</p><p>发小结婚，正好毕设初稿交上了，给自己放了一周的假期，回家搂席，调整一周。</p><p>返校回去的一个多月，自己明显懒惰了许多，每天都是睡到八点半，晚上也睡的很晚，生活并不规律。回家后的一个最明显的特征就是回来以后懒觉睡的少了。晚上十一点睡觉，早晨七点起床，睡的很踏实。</p><p>回家这几天，早起的习惯逐渐调节回来了，我希望自己回学校以后还能够继续保持好习惯。早上七点二十之前务必起床，晚上十一点半前务必睡觉。</p><p>周六晚上回来后，去爷爷家做了很久，和爷爷奶奶聊了很长时间的天；周一到周五在杜生陪姥姥姥爷了很长时间，感觉很幸福、很充实！</p><p>周日下午和桂一聊了一会，孩子正处在叛逆期，净和她妈妈对着干，上周在学校和老师发生了矛盾，到家后把自己关在屋子里不出来。其实想想自己当年那会，也是这股子劲头。为了二叔，我觉着，我作为大哥的有责任去教导它，开导他。聊天的最后，我给他写了八个大字：不要抱怨、不要逃避。希望他能记住我说的这几个字吧，算是过来人的教训。</p><p>周日晚上三叔请客吃饭，喝了不少啤酒，喝的肚子挺难受的。客人走后，三叔哭了，这应该我今生第一次看见三叔在我面前哭泣。他这个人，一生太过于小心翼翼，内心的自卑使他无法正常和亲人沟通感情，这是原生家庭和成长经历带给他的。三叔一定背负着太多的压力与重担，那一刻我想也是稍微的解脱，在大侄子面前卸下了平时严肃的面孔，终于吐露了心声。跟我说的最多的，也是当年我父亲跟我说的最多的：要努力。</p><p>爷爷一会也来了，我们爷三个，难得的机会坐在了一起。没有想象中的那么敞开心扉的畅聊，更多的有的却是沉默。婶婶说了一句幽默的话：小儿子，大孙子，老爷爷的命根子。</p><p>周二参加了好朋友的婚礼，快有5、6年没有正经参加过别人的婚礼了。现场的感觉真是不一般，说实话，很感动，想结婚哈哈。</p><p>周三晚上和俩挚友聚餐，三个人中今年只有我今年幸运保送研究生了，他们两个考研全部失败了。我一点没有沾沾自喜，跟多的是感同身受。三个人、三条道路、三种人生。那晚喝了点白酒，我们聊了很长时间的天。。。</p><p>现在是周五晚上了，想着赶紧把一周归家小记赶紧写出来。因为我知道，如果我再拖着，我肯定就不会再写了…</p><p>周六返校，周日又正式开工了。回去以后少拖延，少睡觉，多写代码，多看书，提升技术！</p><p>道路是曲折的，前途是光明的。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>湾区游记</title>
    <link href="/2024/02/23/%E6%B9%BE%E5%8C%BA%E6%B8%B8%E8%AE%B0/"/>
    <url>/2024/02/23/%E6%B9%BE%E5%8C%BA%E6%B8%B8%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>自驾一路向南，澳门、珠海、佛山、广州</p><p>2024.02.16-2024.02.22</p><p><img src="/img/life/zhuhai.jpg"></p><p>和老舅一家自驾大湾区(广州、珠海、佛山、澳门)。总体收获很大，一路上经历丰富，有高速堵车、昼夜奔袭、度假休息、患病感冒。这次出行，属实是把自己的身体耐造性提高性了一大截，不过代价就是回来之后感染了 新冠嗓子疼病毒 并且身体变虚了。。。开学晚回去几天，把身体调整调整，休整好身体之后再出发。</p><p><img src="/img/life/aomen.jpeg"></p><p>大湾区气候感觉还是潮湿，特别是珠海。一路上的风景也是很美的，我国的经济特区珠海、横琴，特别行政区澳门，还有超级都市群广州、佛山都特别开眼界。以前总觉得湖北、湖南地区属于我国的经济发达地区，总之要比我们河北省好。但是经过这次的实地调查后才发现，原来湖北、湖南大部分地区都依旧还是非常贫困的山区。高速公路也是非常不好走，只有双向4车道，特别是经过湖南地区的隧道的时候，几乎每年都要堵车。围绕着长沙的几个小城市还都是不错的，湘潭、株洲市都很干净，衡阳也可以。回来之后江西境内的高速都要顺畅很多，而且总体体验要比湖北、湖南地区丝滑很多。地势也没有绵延起伏，赣州以南有很多高山，赣州以北又有很多的平原。河南是个人口大省，人口密度大到难以想象，进入高速后根据车流量就可以判断出来。这片中原大地是我国的大粮仓，一路上除华北平原外少有的大平原开阔地带。</p><p><img src="/img/life/duche.jpeg"></p><p>这一路上和老舅、舅妈聊了很多，家庭、学业、事业。。。原来老舅和舅妈大二才确定关系、原来老妈年轻的时候并没有那么有自信和光环、原来我一直有偏见的LMJ在老舅眼里被降维打击般的望眼欲穿。老舅也和自己分享了很多学业、事业上的经历，对于<strong>顺境和逆境的分析</strong>、<strong>对于名和利的取舍</strong>、<strong>对于变通的处世方法</strong>。他将他自己宝贵的经验分享给了我，自己一定要认真复盘。</p><p>大湾区之旅，一生铭记，并且未完待续….</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>毛选及相关</title>
    <link href="/2023/12/26/%E6%AF%9B%E9%80%89%E5%8F%8A%E7%9B%B8%E5%85%B3/"/>
    <url>/2023/12/26/%E6%AF%9B%E9%80%89%E5%8F%8A%E7%9B%B8%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<div class="note note-danger">            <p>今日欢呼孙大圣，只缘妖雾又重来</p>          </div><p><img src="/img/life/mao.webp"></p><h1 id="唯物辩证法的基本规律"><a href="#唯物辩证法的基本规律" class="headerlink" title="唯物辩证法的基本规律"></a>唯物辩证法的基本规律</h1><p>《大众哲学》——艾思奇</p><p>唯物主义者必须让自己养成一种习惯：遇到任何事情、任何问题，都能够认真地做系统周密的调查研究，把事物本身的规律寻找出来，然后按照规律去办事，去解决问题，去指导工作，使事情办的好，问题得以顺利解决，工作做的有成绩，不至于碰钉子，或者至少不碰大钉子。</p><p>不如意和碰钉子的原因是因为我们的主观意愿和事物的客观规律不符合！</p><p>世界上一切事物，是在我们的主观心意之外独立存在着的，它们的发展变化情况不是依赖我们的心意为转移的。世界上的事情，绝对不是我们想要怎样就怎样的，相反的，常常是我们希望这样，而偏偏事实上就不会这样。考试失利、投资失败、分手失恋、失业…</p><p>“凡事不会无缘无故的发生”，这就是“无风不起浪”这一个成语包含的最主要的哲学思想，这种思想在每个人的头脑中普遍存在着，其实每个人都相信，一切事物的发生，它的出现总有一定的原因，一定道理。一切事物不会凭空产生和消失，任何事物都有它发生、发展和没落的过程。</p><h1 id="实践论"><a href="#实践论" class="headerlink" title="实践论"></a>实践论</h1><p>用联系和运动的观点看待事物的发展</p><h1 id="矛盾论"><a href="#矛盾论" class="headerlink" title="矛盾论"></a>矛盾论</h1><p>对立统一</p><p>在研究一切问题的时候，注意到事物发展的曲折性，不要把事物的发展，看作一条直线上升，而没有任何迂回、后退、停滞的形式，也不要把任何事物的发展看作是简单的循环，而不注意它由低级到高级的上升。事物的发展是曲折的，螺旋式上升的。</p><h2 id="战略战术分析"><a href="#战略战术分析" class="headerlink" title="战略战术分析"></a>战略战术分析</h2><p>中国红色政权存在的原因</p><p>井冈山斗争</p><p>论持久战</p><p>抗日游击战争的战略问题</p><h1 id="预见力与原则"><a href="#预见力与原则" class="headerlink" title="预见力与原则"></a>预见力与原则</h1><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><p>1.独立自主</p><p>独立自主、自力更生，在此基础之上来尽可能多的争取外援。合理的看待外援也是一种辩证法。</p><p>只有坚持独立自主，自己把自己照顾好，让自己强大起来，才能帮助到父母和家人，才能真正帮助到朋友。否则都是假的。</p><p>白手起家还需要独立自主。</p><p>爱别人，首先爱自己。</p><p>让步一定是积极的而不是消极的，本质是为了更好的合作，同样，让步也是有一定界限的。</p><p>2.实事求是</p><p>实事——客观存在的一切事物</p><p>求——调查研究</p><p>是——客观事物的内部联系，即规律性</p><p>实事：研究一切问题都要从实际事物出发，而不是从理论、概念、观点和定义出发。</p><p>求是：研究事物的规律，总结事物的规律，然后用规律指导行动。</p><p>3.群众路线</p><p>群众路线就是不脱离不脱离社会实际，因为群众是构成社会实际最基础的基础。</p><h2 id="预见力"><a href="#预见力" class="headerlink" title="预见力"></a>预见力</h2><p>什么事预见力？就是将矛盾发展演变成种种趋势加以判断，然后对自己主观能动性加以判断，最后得出事物发展的最大可能性。并且把这种可能性和必然性指出来，辅助以相应的对策，最后取得胜利。</p><p>在复杂的事物也是由简单事物的叠加组成的，提高预见力与洞察力，本质上就是需要我们擅长事物的矛盾分析和条件分析。</p><h1 id="成事的科学原理"><a href="#成事的科学原理" class="headerlink" title="成事的科学原理"></a>成事的科学原理</h1><p>分析清楚事物的矛盾，通过调查研究得出干这件事需要哪些条件，然后确定现在是否时机已经成熟们是否具备这样的条件，之后就是发挥主观能动行去创造条件，最终把事情尽可能的按照自己想的那样去办成。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>张宇智慧箴言</title>
    <link href="/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/"/>
    <url>/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>宝剑锋从磨砺出，梅花香自苦寒来</p>          </div><h2 id="别害怕"><a href="#别害怕" class="headerlink" title="别害怕"></a>别害怕</h2><p>不要试图掌控自己的奋斗过程，完美自己的各项计划，你会失落的，因为事实上它们并不听你的，你越跟它较劲，它越不听话。<br>不过这没什么，每一个成功，都是源于一段不完美的甚至是很狼狈的奋斗。<br>你只要，摈弃杂念，放下功利，全神贯注，尽力就好，学不完又怎样，考的上就行。好孩子，听话。</p><h2 id="拼搏"><a href="#拼搏" class="headerlink" title="拼搏"></a>拼搏</h2><p>年轻的时候，闯一闯，拼一拼，不要怕什么。实在扛不住了，哭一哭，喊一喊。大不了，回家。<br>不要把感情和情绪放在社会上、网络上，真正值得你在乎的，是表面上希望你出人头地，对你百般挑剔，但在心中却只为你祈求平安健康快乐的家人。<br>孩子们，听话，你逼自己的方法，可能不太对。放下包袱，放下功利，带着爱，带着对这个世界的好奇，去拼搏一下。累了，咱们回家就是了。<br><strong>健健康康，内心阳光</strong>，这八个字，可别弄丢了，丢了，会悔恨终身的。</p><h2 id="最大的敌人是自己"><a href="#最大的敌人是自己" class="headerlink" title="最大的敌人是自己"></a>最大的敌人是自己</h2><p>笛卡尔说，征服你自己，而不是征服世界。孩子们，晚安。</p><h2 id="嗜欲深者天机浅天机浅"><a href="#嗜欲深者天机浅天机浅" class="headerlink" title="嗜欲深者天机浅天机浅"></a>嗜欲深者天机浅天机浅</h2><p>古有云，<strong>无欲则刚</strong>。心思繁杂，欲望太多，则易焦虑，徘徊，无己见，恐非做事之正道。孩子们，务必静心，放下功利，用心为之，方可成之。</p><h2 id="基本功"><a href="#基本功" class="headerlink" title="基本功"></a>基本功</h2><p>有人问基础阶段以什么标准来看自己是不是真的懂了，这个问题我前面也回答过，这里详细说几句。我在课上提到一个人，叫做费曼，他是诺贝尔物理学奖获得者，人们试图研究费曼为什么对问题的理解总是透彻深刻，深得其精髓要义，事实上，费曼自己提出过一个观点：把你所学到的理解到的概念性质和方法讲给一个比如小孩子听（这里是为了确保聆听的人对你所讲述的知之甚少），而且让他尽量听懂，这时你无法用那些复杂的专业性的表达来叙述，在寻找简单通俗易懂的语言的时候，你的大脑在不断的深化对于知识的认识，迫使自己在更深层次上理解它们，这样你一定会停滞在很多地方，而这些地方就是你没有理解的地方。通过努力，把你的知识传递给这个孩子并让他听懂，就是你真的懂了，这就是过关。<br>我前面说到，你一定要会复述我课上对知识的讲解，甚至比我讲的还要简要精炼，这是你迅速融入这个学科思维状态的最佳途径。各位，我再写一会书，你们好好思考下我上面这段话，我是真的希望你们能够懂得如何学习，不要再功利的去卷了，真学东西，学真东西，学会怎么学，不仅为了考研，更为了自己的一生。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>前路漫漫亦灿灿</title>
    <link href="/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/"/>
    <url>/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>不要辜负这份好运气——写在顺利推免后读任正非先生<a href="https://baike.baidu.com/item/%E6%88%91%E7%9A%84%E7%88%B6%E4%BA%B2%E6%AF%8D%E4%BA%B2/6610022">《我的父亲母亲》</a>有感</p>          </div><p>2023年9月29日，下午14:50，故事开始了新的篇章。</p><p>从2023年2月15日离家返校，一直到10月1号，整整228天，家门未踏入一步。一直想带一个好的结果回家，经历磕磕绊绊，最终如愿以偿。</p><p>想说的东西、想表达的东西有好多，但却难以启齿、无从下笔。</p><p>回首这段岁月，感谢自己没有放弃自己，感谢暑假留校踏踏实实读书学习的沉淀。暑假留校两个月，自己在图书馆五楼靠窗户占了一整张桌子，五楼人烟稀少，八月份安安静静在那里度过了一个月充实的时光。早上八点半开馆准时到达，晚上在漆黑的五楼写读书笔记。如今再进入五楼，往事历历如过眼云烟般浮现在眼前。上一次有这种感觉，还是因为19年底口罩事情高三开学延期，20年初居家复习的岁月。这两段时光交织在一起，给我的反思就是，要想做成一件事情，首先要把自己调整成静音模式，踏踏实实忘我般的投入进去。想成事，不是每天炸炸呼呼，一天八条朋友圈，恨不能别人不知道我干啥。而是要沉淀、要安静、要忍耐、要坚持。</p><p>人性的弱点之一就是<strong>只看结果，不注重过程</strong>。</p><p>你考上研了，考上公了，周围都是欢呼、赞美之词。”牛逼、大佬、交给朋友吧…”诸如此类的话，会让你沉迷享受，会让你飘飘然忘乎所以。但是，当你没考上、没惊起波澜，周围人并不会关注，甚至会有讥讽、嘲笑不绝于耳。”他不行，他光玩，他就不是那块料…”。所以，自强不息、厚德载物，永远都是法宝，让自己强大起来，让自己内心强大起来，才会在面对这些情况的时候，做到多一份的从容、淡定与坦然。</p><p>带着好结果回家过节，家人们由衷地为我感到高兴、自豪。感谢家人，没有家人的理解与支持，我没有勇气走好今天的路。</p><p>我的父亲杨海龙，1976年出生于河北省沧州市一个普普通通的农村家庭。小时候学习成绩不错，凭借自己的努力一直读到了高中毕业。中考成绩很好，但是因为家里穷要继续供给我二叔、三叔读书。于是，父亲放弃了去县里读高中的机会，选择了在镇上完成学业，很多年前，他仍然自嘲:“以我当年的成绩，要是去县里上，考过河北工业绰绰有余”。高中毕业后在我二爷家工厂里工作，然后和爷爷、叔叔们创业。</p><p>我的母亲刘秋菊，1978年出生于河北省沧州市一个普普通通的农村家庭。小的时候学习成绩一般，肯定没有我父亲学习好，想考中专，结果也是为了弟弟妹妹的学业而放弃了自己的梦想。初中毕业后进入农村信用社当会计，后来在我姥爷的厂子里当会计，一干就是半辈子。母亲是一个聪明的人，积极、阳光、热爱学习，她算账、算数没出过错，爱写读书笔记，有阅读习惯，这些都润物细无声般影响着我。</p><p>两个有着相同命运路径的人走到了一起，养育了我、培养了我。特别是，今年因为面试忙于各个城市奔波，报辅导机构，发paper。以上种种，当我需要钱的时候，老父亲总是那么斩钉截铁地转给我，让我没有任何顾虑。钱是好东西，而且钱是干净的，脏的是人心。走正道，好好赚钱、多赚钱永远都是幸福的。</p><p>这些年，二叔的去世一直也是我前进的动力。我很少和外人去谈我二叔因为车祸去世的事情，我认为这是家族之殇。我作为家族中的长子长孙，在我17岁那年，他的死，使我对我的家庭、对我自己有了新的理解。挫折都是发人深省的，我庆幸自己没有被生活的重创打垮，我也庆幸，家里的老人没有因为生活的苦难而使他们丧失对生活的希望。在经历苦难之后，我看到的依然是他们积极的生活态度，没有怨天尤人、没有一蹶不振，永远都给子女树立了好的榜样。其实，这就是我遇到无数困难后，依然有勇气面对的底气来源。</p><p>今年还要感谢张宇(宇爹)，不确定的日子里一直在通过备考高数舒缓。激发了我对数学的热情，重塑了学科的理解。</p><p>没有轻舟已过万重山，唯有前路漫漫亦灿灿。逝去的已经逝去，活着的仍要前行。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>致谢</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
