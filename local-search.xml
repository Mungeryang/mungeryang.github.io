<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>李沐讲座</title>
    <link href="/2024/09/22/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/"/>
    <url>/2024/09/22/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-trends-Personal-Career-Choices"><a href="#LLM-trends-Personal-Career-Choices" class="headerlink" title="LLM trends &amp;&amp; Personal Career Choices"></a>LLM trends &amp;&amp; Personal Career Choices</h1><h1 id="——大语言模型的实践经验和未来预测"><a href="#——大语言模型的实践经验和未来预测" class="headerlink" title="——大语言模型的实践经验和未来预测"></a>——大语言模型的实践经验和未来预测</h1><h2 id="主讲人"><a href="#主讲人" class="headerlink" title="主讲人"></a>主讲人</h2><p>李沐：上海交大2011届计算机科学与工程系本硕系友。他曾担任亚马逊资深首席科学家，任加州大学伯克利分校和斯坦福大学的访问助理教授，是前Marianas Labs联合创始人。他的研究关注分布式系统和机器学习算法。发表了50余篇人工智能顶级会议论文，在CMU读博期间更是两年内发表了理论计算机领域的FOCS、神经网络领域的NIPS、数据挖掘领域的KDD和操作系统领域的OSDI等不同领域的顶级国际会议一作论文。他是深度学习框架Apache MXNet的创始人之一，合著了开源深度学习教材《动手学深度学习》。他目前是BosonAI的联合创始人。</p><p>时间地点：2024.08.25-上海交通大学计算机科学与技术系</p><h2 id="大语言模型与“炼丹”"><a href="#大语言模型与“炼丹”" class="headerlink" title="大语言模型与“炼丹”"></a>大语言模型与“炼丹”</h2><p>语言模型的三大核心组成：算力、算法、数据</p><p>把数据通过算力、算法压到模型里面，模型具有一定的能力，能够对一个新的数据，能够从原数据的找到相似的东西做一定的修改，输出你想要的东西。</p><p>沐神用“炼丹”的比喻将现在的语言模型就是一个炼丹的过程：数据是原材料，算力是设备(比如炼丹炉)，算法就是“丹方”</p><p>数据以参数的形式存储在LLM中，需要使用prompt工程进行数据的提取与使用。</p><h2 id="硬件的发展规律"><a href="#硬件的发展规律" class="headerlink" title="硬件的发展规律"></a>硬件的发展规律</h2><p>这一部分主要讲了两部分内容：</p><p>带宽是让芯片靠的更近一些</p><p>内存是制约模型的一大瓶颈</p><p>算力从长期来看会越来越便宜</p><p>思考：是否符合摩尔定律？</p><h2 id="多模态的趋势"><a href="#多模态的趋势" class="headerlink" title="多模态的趋势"></a>多模态的趋势</h2><h2 id="动机的来源"><a href="#动机的来源" class="headerlink" title="动机的来源"></a>动机的来源</h2><p>强烈的创业动机要么来自很深沉、很底层的欲望，要么来自很深的恐惧。</p><p>欲望与恐惧是动机的主要来源。</p><p>预训练是工程问题，后训练时技术问题</p><p>垂直模型也需要通用知识训练</p><p>数据决定模型的上限、算力决定模型的下限</p><p>神经符号系统</p><h2 id="打卡式人生"><a href="#打卡式人生" class="headerlink" title="打卡式人生"></a>打卡式人生</h2><h3 id="读PHD：真心热爱研究，不然难以坚持"><a href="#读PHD：真心热爱研究，不然难以坚持" class="headerlink" title="读PHD：真心热爱研究，不然难以坚持"></a>读PHD：真心热爱研究，不然难以坚持</h3><p>反思性写作与阶段式总结可以保持持续进步</p><p>PHD期间要有大部分时间都要花在写作和演讲上，努力提高自己的表达与写作的能力。</p><h2 id="数据要素-新增内容"><a href="#数据要素-新增内容" class="headerlink" title="数据要素(新增内容)"></a>数据要素(新增内容)</h2><p>国家大数据局</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>前沿讲座</tag>
      
      <tag>职业发展</tag>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Google File System</title>
    <link href="/2024/09/20/Google-File-System/"/>
    <url>/2024/09/20/Google-File-System/</url>
    
    <content type="html"><![CDATA[<h1 id="Google-File-System"><a href="#Google-File-System" class="headerlink" title="Google File System"></a>Google File System</h1><p>原文连接：</p><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf</a></p><p>论文翻译：</p><p><a href="https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/">https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/</a></p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG for LLM - A survey</title>
    <link href="/2024/09/19/RAG-for-LLM-A-survey/"/>
    <url>/2024/09/19/RAG-for-LLM-A-survey/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG-for-LLM-A-survey"><a href="#RAG-for-LLM-A-survey" class="headerlink" title="RAG for  LLM - A survey"></a>RAG for  LLM - A survey</h1><p>论文题目：</p><p>Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>论文链接：</p><p><a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></p><p>翻译：</p><p><a href="https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey">https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在LLM得到广泛应用的同时，对于LLM 幻觉、过时知识、不透明&#x2F;不可追踪的推理过程仍然面临巨大挑战。</p><p>可行的解决方式：RAG-检索生成增强</p><p>方式：连接外部数据库- RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases.</p><p>RAG paradigms（RAG三大范式）：</p><ul><li>Naive RAG</li><li>Advanced RAG</li><li>Modular RAG</li></ul><p>文章重点强调了嵌入在RAG系统中的关键组件，提高了对RAG系统的理解。此外，文章也给出了对于RAG系统的最新评估标准和框架。</p><h2 id="Introduction-RAG在大模型阶段的发展轨迹"><a href="#Introduction-RAG在大模型阶段的发展轨迹" class="headerlink" title="Introduction-RAG在大模型阶段的发展轨迹"></a>Introduction-RAG在大模型阶段的发展轨迹</h2><p>RAG技术最初与Transformer架构相吻合，通过增加外部知识库，在早期阶段用于细化预训练技术（pre-training）。</p><p>随着ChatGPT的兴起，大语言模型在长上下文的理解中展现出了强大能力。</p><p>RAG研究转向在推理阶段为LLM提供更好的信息，以回答更复杂和知识密集型的任务，导致RAG研究的快速发展。</p><p>后续随着研究进一步深入，RAG技术不再局限于推理阶段而是开始更多的和微调技术相结合。</p><p>总结：预训练-推理-微调</p><p>RAG技术本身经历了快速发展，但是目前仍然没有对RAG系统发展的清楚阐述，这篇文章的写作目的之一就是填补这一空白，去为读者绘制RAG发展路径并评估他未来的发展路径。</p><p>文章旨在阐明检索增强技术的演变，评估各种方法在各自上下文中的优点和缺点，并推测即将到来的趋势和创新。</p><p>本文的主要贡献：</p><ul><li>系统性回顾了RAG最新的方法、技术，描述了RAG三大范式的演变，将RAG研究置于LLM前景中</li><li>识别并阐明了RAG的三大核心技术：Retrieval、Generation、Augmentation，阐明了这些组件如何协同形成有效的RAG框架</li><li>总结了RAG的当前评估方法，涵盖了26个任务，近50个数据集，概述了评估目标和指标，以及当前的评估基准和工具。预计RAG的未来方向，强调应对当前挑战的潜在增强。</li></ul><p>通过认真阅读、分析原文后，应该可以对本文的三大贡献进行连贯复述。</p><h2 id="SectionII-RAG三大范式"><a href="#SectionII-RAG三大范式" class="headerlink" title="SectionII-RAG三大范式"></a>SectionII-RAG三大范式</h2><h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Indexing</p><p>索引编排阶段会将所有类型的文档（PDF、HTML、Word、Markdown）转换为统一的文本格式。之后，为了使用语言模型的文本限制，“格式化”后的文本会被切分成chunks，使用嵌入模型，这些chunks会被转换为向量存储在向量数据库中。</p><p>Retrieval</p><p>RAG系统会将用户请求(query)转换为一个向量表示，然后通过计算查询向量和chunks向量之间的相似度来检索与query最相近的top k个chunks。</p><p>Generation</p><p>将用户提出的查询和所选文档合成为一个连贯的提示，大型语言模型的任务是制定响应。在正在进行的对话情况下，任何现有的历史都会可以集成到提示中，是模型能够有效参与多轮对话交互。</p><p>检索阶段的挑战：准确率和召回率；导致错误或不相关的chunk，以及缺少关键信息。</p><p>生成阶段的困难：出现幻觉</p><p>增强阶段的障碍：检索信息与不同任务结合可能具有挑战性，有时会导致输出不连贯。对于复杂需求，基于最初查询的单一检索并不足以获取足够的上下文信息。</p><h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>相比于Naive RAG，Advanced RAG专注于提高检索质量，运用了pre-retrieval 和 post-retrieval策略。</p><p>pre-retrieval重点关注优化索引结构和原始查询，优化索引的目标是提高被索引的内容的质量。</p><p>post-retrieval阶段主要是整合有效查询，主要的方法包括chunks重排和上下文压缩。重新排序检索到的信息以将最相关的内容重新定位到提示的边缘是关键策略。</p><h3 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h3><p>相比于前两种范式，模块化RAG的适应性和多功能性得到提高。</p><p>方法：添加一个搜索模块进行相似度搜索，并通过微调细化检索器。重组RAG模块+重排RAG管道来解决目前新的挑战，引入额外组件提高检索和处理能力。</p><p>创新：</p><p>Rewrite-Retrieve-Read Model 利用LLM的能力通过重写模块和LM反馈机制来改进检索查询，以更新重写模型。，提高任务性能。</p><p>Generate-Read 用LLM生成的内容替换传统的检索</p><p>ReciteRead 强调从模型权重中检索，增强了模型处理知识密集型任务的能力</p><p>子查询和假设文档嵌入 (HyDE)旨在通过关注生成答案和真实文档之间的嵌入相似性来提高检索相关性</p><p>调整：Demonstrate-Search-Predict(DSP)框架和迭代的Retrieve-Read-Retrieve-Read流使用增强了模块的协同复杂理解。</p><h3 id="RAG与微调"><a href="#RAG与微调" class="headerlink" title="RAG与微调"></a>RAG与微调</h3><p>RAG通过提供实时知识更新和有效利用具有高可解释性的知识源，在动态环境中表现出色。然而，在检索方面有更高的延迟和伦理考虑。</p><p>RAG 和 微调技术之间的选择取决于应用程序上下文中数据动态、定制和计算能力的具体需求。RAG 和 微调技术不是互斥的，可以相互补充，增强了模型在不同层次上的能力。在某些情况下，它们的组合使用可能会导致最佳性能。涉及RAG和微调技术的优化过程可能需要多次迭代才能获得满意的结果。</p><h2 id="检索-RETTRIEVAL"><a href="#检索-RETTRIEVAL" class="headerlink" title="检索-RETTRIEVAL"></a>检索-RETTRIEVAL</h2><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="Semi-structured-data"><a href="#Semi-structured-data" class="headerlink" title="Semi-structured data"></a>Semi-structured data</h4><p>特别关注对于<strong>半结构化数据</strong>的处理分析。典型的半结构化数据例如PDF包括文本和表信息。</p><p>有两大原因导致传统RAG系统在处理半结构化数据时面临挑战：</p><ol><li>文本的切分会不经意地拆分表格，导致检索过程中的数据损坏</li><li>表与数据的结合会使语义相似度检索变得更加复杂</li></ol><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>所有的方法目前都不是最优的，该领域还有大量的研究机会。</p><p>关注北航关于Text-2-SQL的最新成果</p><h4 id="Structured-data"><a href="#Structured-data" class="headerlink" title="Structured data"></a>Structured data</h4><p>图检索使GNN神经网络、LLM与RAG相结合，通过LLM的软提示来增强图形理解和问答能力，并使用Prize-Collecting Steiner Tree进行优化图检索的结果。</p><h4 id="LLMs-Generated-Content"><a href="#LLMs-Generated-Content" class="headerlink" title="LLMs-Generated Content"></a>LLMs-Generated Content</h4><p>LLM 生成的上下文通常包含更准确的答案，因为其与因果语言建模的预训练目标能更好地对齐。</p><h3 id="Retrieval-Granularity"><a href="#Retrieval-Granularity" class="headerlink" title="Retrieval Granularity"></a>Retrieval Granularity</h3><p>粗粒度检索单元理论上可以为问题提供更多相关信息，但它们也可能包含冗余内容，这可能会分散下游任务中的检索器和语言模型。</p><p>另一方面，细粒度检索单元粒度增加了检索的负担，不能保证语义完整性并满足所需的知识。</p><p>选择恰当的检索粒度是一个简单的策略用来提高检索和下游任务的表现。</p><p>检索粒度从粗到细包括：Token, Phrase, Sentence, Proposition, Chunks, Document。将Propositions作为检索单元可以提高检索的相关性和精准度。</p><p>知识图谱的检索粒度包括：Entity, Triplet, and sub-Graph.</p><h3 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h3><p>在索引阶段，文档将被处理、分割并转换为要存储在向量数据库中的嵌入。索引的构造质量决定了是否可以在检索阶段获取正确的上下文。</p><h4 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h4><p>分块常用的方法是将文档分为不同的标记的chunks。</p><p>分块需要在语义完整性和上下文长度之间进行trade-off。更长的块会包含更长的上下文，但是噪声会增加、处理时间也会增加；短块包含的噪声小，但是能包含的上下文信息也更少。</p><p>于是后面提出了以句子作为检索单元，前后句被提供作为LLM的上下文内容。</p><h4 id="元数据附加"><a href="#元数据附加" class="headerlink" title="元数据附加"></a>元数据附加</h4><p>在检索过程中为文档时间戳分配不同的权重可以实现时间感知的RAG，确保知识的新鲜度并避免过时的信息。</p><p>元数据也可以被人工构建(添加段落摘要以及引入假设问题)。具体来说，使用 LLM 生成文档可以回答的问题，然后计算检索过程中原始问题和假设问题之间的相似度，以减少问题和答案之间的语义差距。</p><h4 id="结构化索引"><a href="#结构化索引" class="headerlink" title="结构化索引"></a>结构化索引</h4><p>层次索引结构和知识图谱都可以提高信息检索的准确率。</p><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>问题本身的复杂性、语义模糊性都是查询过程中遇到的困难。</p><p>查询优化的方法主要有三大类：</p><p>1.查询扩展</p><ul><li>多路查询</li><li>子查询</li><li>Chain-of-Verification(CoVe)</li></ul><p>2.查询转换-查询重写</p><p>3.查询路由</p><h3 id="嵌入-Embedding"><a href="#嵌入-Embedding" class="headerlink" title="嵌入-Embedding"></a>嵌入-Embedding</h3><p>怎么理解嵌入？</p><p>Embedding 是将离散的非结构化数据(图片、视频、音频、文本)通过Embedding Model转换为连续的向量表示的技术。</p><p>Embedding 常常用于将文本数据中的单词、句子或文档映射为固定长度的实数向量，使得文本数据能够在计算机中被更好地处理和理解。通过 Embedding，每个单词或句子都可以用一个实数向量来表示，这个向量中包含了该单词或句子的语义信息。</p><p>RAG系统中，检索是通过计算嵌入问题和文档块之间的相似度实现的。</p><p>没有最好的答案去回答“要使用哪一个嵌入模型”，然而，对于不同的问题可以有不同的方法。</p><h3 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h3><p>Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.</p><p>这篇文章介绍了一种通过指令微调将知识集成到白盒模型中的创新方法。检索器模块直接替换为根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难并提高模型性能。</p><h2 id="增强-AUGMENTATION-PROCESS-IN-RAG"><a href="#增强-AUGMENTATION-PROCESS-IN-RAG" class="headerlink" title="增强-AUGMENTATION PROCESS IN RAG"></a>增强-AUGMENTATION PROCESS IN RAG</h2><p>增强的过程主要侧重于优化检索的过程，这一部分介绍了三大检索增强的过程：迭代式检索、递归式检索、自适应检索。</p><p>迭代检索涉及检索和生成之间交替，允许在每一步从知识库中更丰富、更有针对性的上下文。递归检索涉及逐步细化用户查询并将问题分解为子问题，然后通过检索和生成不断解决复杂问题。自适应检索侧重于使 RAG 系统能够自主确定外部知识检索是否必要以及何时停止检索和生成，通常使用 LLM 生成的特殊标记来控制。</p><h2 id="生成-GENERATION"><a href="#生成-GENERATION" class="headerlink" title="生成-GENERATION"></a>生成-GENERATION</h2><p>在检索后，将所有检索到的信息直接输入到LLM中回答问题并不是一个好的实践。</p><h3 id="上下文配置"><a href="#上下文配置" class="headerlink" title="上下文配置"></a>上下文配置</h3><p>过长的上下文信息会导致LLM“忽视中间段落”，与人类一样，LLM 往往只关注长文本的开头和结尾，同时忘记中间部分。因此，在 RAG 系统中，我们通常需要进一步处理检索到的内容。</p><p>重排：从根本上重新排序对文档(chunks)块进行重新排序以首先突出最相关的结果，有效地减少整体文档池，在信息检索中切断双重目的，充当增强器和过滤器，为更精确的语言模型处理提供细化的输入。</p><p>上下文选择&#x2F;压缩：对RAG过程的一个误解是相信尽可能多地检索相关文档并用长检索提示包含他们是有好处的。然而，大量的信息也会带来大量的干扰，削弱大语言模型对于关键信息的敏锐度。</p><p>压缩方法包括检测并移除不重要的tokens，将上下文转化为人类很难理解但是LLM很好理解的形式。</p><p>小语言模型SLM用作过滤器，大语言模型LLM用作重排代理。在信息抽取任务中，指导LLM去重组被SLM识别的挑战性样本会导致效果显著提升。</p><h3 id="LLM微调"><a href="#LLM微调" class="headerlink" title="LLM微调"></a>LLM微调</h3><p>当LLM缺少特定领域的数据的时候，外部知识可以被提供通过微调技术。</p><p>微调的另一个好处是可以控制模型的输入和输出。它可以让LLM适应特定的数据格式和按照指示以特定风格生成响应。</p><p>通过强化学习将LLM输出与人类或检索器偏好对齐是一种潜在的方法。除了与人类偏好对齐外，还可以与微调模型和检索器的偏好保持一致。</p><p>当环境防止访问强大的专有模型或更大的参数开源模型时，一种简单有效的方法是提取更强大的模型（例如 GPT-4）。</p><h2 id="下游任务与评估"><a href="#下游任务与评估" class="headerlink" title="下游任务与评估"></a>下游任务与评估</h2><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>RAG的核心任务仍然是<strong>问答</strong>(QA)，QA包括传统的单步&#x2F;多步问答、多项选择、特定领域问答、长文本场景问答。</p><p>RAG也不断扩展到多个下游任务，如信息提取(IE)、对话生成、代码搜索等。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>上下文相关性和噪声鲁棒性对于评估检索质量很重要，而答案忠实度、答案相关性、负拒绝、信息集成和反事实鲁棒性对于评估生成质量很重要。</p><h2 id="RAG面临主要挑战和未来前景"><a href="#RAG面临主要挑战和未来前景" class="headerlink" title="RAG面临主要挑战和未来前景"></a>RAG面临主要挑战和未来前景</h2><h3 id="RAG-VS-长上下文"><a href="#RAG-VS-长上下文" class="headerlink" title="RAG VS 长上下文"></a>RAG VS 长上下文</h3><p>目前，LLM 可以毫不费力地管理超过 200,000 个标记的上下文。这也引发了关于当LLM不受上下文限制的时候是否还需要RAG的讨论。</p><p>对于RAG过程，整个检索和推理过程是可观察的，而仅依靠长上下文生成仍然是一个黑匣子。在超长上下文的背景下开发新的 RAG 方法是未来研究趋势之一。</p><h3 id="RAG鲁棒性"><a href="#RAG鲁棒性" class="headerlink" title="RAG鲁棒性"></a>RAG鲁棒性</h3><p>检索过程中噪声或矛盾信息的存在会对RAG的输出质量产生不利影响。这种情况被比喻地称为“Misinformation 可以比根本没有信息更糟糕”。</p><p>提高RAG对这种对抗性或反事实输入的抵抗力正在获得研究势头，已成为一个关键的性能指标。</p><p>The Power of Noise: Redefining Retrieval for RAG Systems.研究结果表明，包含不相关的文档可能会意外地提高准确性超过 30%，这与质量降低的初始假设相矛盾。</p><p>结果强调了开发专门的策略将检索与语言生成模型集成的重要性，强调了进一步研究和探索对 RAG 鲁棒性的必要性。</p><h3 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h3><p>将RAG与微调技术相结合正在成为领先策略，确定RAG与微调最佳整合取决于是顺序、交替还是端到端的训练。</p><p>另一个趋势是将特殊功能的小语言模型SLM引入RAG，并通过RAG系统进行微调。</p><h3 id="scaling-laws-of-RAG"><a href="#scaling-laws-of-RAG" class="headerlink" title="scaling laws of RAG"></a>scaling laws of RAG</h3><p>基于RAG的端到端模型和预训练模型仍然是当前研究人员的重点之一。</p><p>Kaplan, Jared, et al. “Scaling Laws for Neural Language Models.” arXiv: Learning,arXiv: Learning, Jan. 2020.</p><p>缩放规律已经被建立对LLM，但是应用目前尚未确定。</p><h3 id="Production-Ready-RAG"><a href="#Production-Ready-RAG" class="headerlink" title="Production-Ready RAG"></a>Production-Ready RAG</h3><p>然而，提高检索效率，提高大型知识库的文档召回率，保证数据安全，例如防止LLM中元数据和文档来源的无意披露，是仍然有待解决的挑战。</p><p>RAG技术在特殊方向的趋势：1.特定需求的定制化。2.降低最初的学习曲线来简化RAG使用。3.优化RAG来更好的服务生产环境。</p><h3 id="多模态RAG"><a href="#多模态RAG" class="headerlink" title="多模态RAG"></a>多模态RAG</h3><p>RAG在图像、音频视频、编码领域的应用。</p><h2 id="个人阅读总结"><a href="#个人阅读总结" class="headerlink" title="个人阅读总结"></a>个人阅读总结</h2><h3 id="通原理"><a href="#通原理" class="headerlink" title="通原理"></a>通原理</h3><p>对于RAG技术栈的原理自己在脑海中应该有了一定的轮廓，接下来就是重点学习一些技术细节，比如<strong>微调技术、嵌入技术</strong>，更多去关注下游任务实践与部署应用。</p><p>读完论文后要动手去尝试去实现一下RAG技术，通过动手实践真正对这门技术有一个具体的认识，而不是仅仅漂浮于理论上。练实践带动并反作用于学习，真正从实践中体会部署技术、微调技术、训练技术。</p><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><h3 id="知趋势"><a href="#知趋势" class="headerlink" title="知趋势"></a>知趋势</h3><p>读完论文后要对RAG的未来发展趋势自己心里要有一定的认知。这项技术已经做了解决了哪些问题？还能解决哪些问题？哪些方面还需提高或面临困难？对于上面三个问题，通过后期继续学习后要有清楚的认识。起步阶段不要着急，刚刚读了这个方向的一篇文章而已。</p><p>同样更重要的就是，了解趋势与发现问题后，如何去解决？如何转换为自己Idea，这也是关键。</p><h3 id="着力点"><a href="#着力点" class="headerlink" title="着力点"></a>着力点</h3><p>多模态RAG(Multi-modal RAG)、半结构化数据检索(处理半结构化数据的方法优化)、鲁棒性(RAG的对抗性或反事实输入的抵抗力)</p><p>对于半结构化数据的最新研究成果主要参考北京航空航天大学：<a href="https://arxiv.org/pdf/2408.16991">https://arxiv.org/pdf/2408.16991</a></p><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>LLM编码能力在数据库表上执行SQL和将表格转化为文本都不是最好的处理方法，所以对于LLM处理半结构化数据时，探索最优化方法仍然是趋势。</p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《在线》-读书笔记</title>
    <link href="/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>王坚院士《在线》书摘与读书笔记——互联网、数据和计算</p><p>我们很幸运，生活在一个创造决定未来的时代。但是创造能否决定未来取决于我们的信念和坚持，坚持相信的，相信坚持的。</p><p>计算，为了无法计算的价值</p>          </div><h2 id="榜样-王坚"><a href="#榜样-王坚" class="headerlink" title="榜样-王坚"></a>榜样-王坚</h2><p><strong>王坚</strong>（1962年10月—），中国信息技术专家、心理学家，阿里巴巴集团首席技术官，中国工程院院士，阿里云的创始人。曾主持研发中国唯一自研的云操作系统飞天。</p><img src="/img/bg/wangjian.jpg"/><h2 id="云计算的三次浪潮"><a href="#云计算的三次浪潮" class="headerlink" title="云计算的三次浪潮"></a>云计算的三次浪潮</h2><p>第一次浪潮中，云计算改变了今天全球互联网，奈飞、米哈游为代表的企业从第一天起100%完完整整就在云上。</p><p>第二次浪潮中，“传统企业”开始使用云计算。最标志性的事件是2022年北京冬奥会上，云计算第一次承载了赛事的核心系统。</p><p>人工智能和云计算的一次结合，这是云计算的第三次浪潮。</p><p>云计算的第三次浪潮，王坚认为，2023年人工智能和云计算有了一次集中的体现和爆发，GPT模型的出现使得计算机对科技创新的革命产生了非常重要的影响。云计算和GPT的关系就是电和电动机的关系。未来事实上云计算的算力都是会被这些在智能时代的电动机，就是被模型消耗掉。</p><h2 id="数据与计算"><a href="#数据与计算" class="headerlink" title="数据与计算"></a>数据与计算</h2><p>原子比特化，比特在线化</p><p>到底什么是<strong>在线</strong>？从思维发展与技术发展的角度来说，在线就是连接的属性，必须先连接起来才有在线这回事。从改变行业与生活的角度来说，连接就是在线的结果，在线后才能连接渗透社会生活的各个方面。</p><p>就像我们每天喝水、用电一样，计算也在逐步成为了一种新的公共服务。</p><p><strong>去IOE</strong>：摆脱企业对IBM小型机、Oracle数据库、EMC存储系统的依赖，拥抱云计算，拥抱互联网。</p><p>“<strong>大数据</strong>”这个词搞错了，数据不是因为大而产生的价值，而是因为在线上而产生了价值，因为数据从此可以在更大范围流动它产生的价值，这是真正的数据带来的巨大变化。</p><h2 id="在线定律"><a href="#在线定律" class="headerlink" title="在线定律"></a>在线定律</h2><p>定律一：每一个比特都在互联网上</p><p>定律二：每个比特都可以在互联网上流动</p><p>定律三：比特所代表的每个对象在互联网上都是可以计算的</p><h2 id="谈对技术的热爱"><a href="#谈对技术的热爱" class="headerlink" title="谈对技术的热爱"></a>谈对技术的热爱</h2><p>当我们谈对技术的热爱时，大家并没有真正意识到，我们要为之付出什么。一个不太恰当的比喻就是，我们与技术的关系就是农夫与蛇的关系。</p><p>什么是对技术的热爱？你真的相信技术会改变很多东西吗？有没有足够的自信和热爱，去捂暖这条蛇，哪怕它苏醒后会反咬你一口？当你真正热爱一个东西时，你很难预料最终的结果。当你把这条蛇揣在怀里时，你面对的最大考验是对你心脏的考验。你不知道这条蛇醒来以后是否会反咬你一口，也许你的身体足够强大，可以抵抗。</p><h2 id="人工智能、城市大脑——下一个10年的登月计划"><a href="#人工智能、城市大脑——下一个10年的登月计划" class="headerlink" title="人工智能、城市大脑——下一个10年的登月计划"></a>人工智能、城市大脑——下一个10年的登月计划</h2><p>世界上有三种智能：人的智能、机器的智能、动物的智能。</p><p>多数人谈及人工智能，考虑的都是人会不会被人工智能所取代，这是一个奇怪的逻辑。人们在让狗去寻找毒品的时候从来没有说过人类的鼻子被狗的鼻子取代了。人类要对机器有足够的尊重，要尊重机器在某些方面已经超过了人类。</p><p>互联网发展到今天，我们要做的，使用机器解决人类解决不了的问题。</p><p>今天，世界各国城市的可持续发展都面临着重大的挑战，这些挑战也到来了一个难得的机遇，就是用机器智能解决城市发展过程中产生的许多问题。</p><p>城市大脑要做的就是以互联网为基础设施，利用丰富的城市数据资源，对城市进行全局的实时分析，解决今天无法靠人脑解决的问题。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>杨力祥老师的编程哲学-把程序写成东西</title>
    <link href="/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/"/>
    <url>/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<img src="/img/life/oo.jpg"/><h2 id="把程序写成东西"><a href="#把程序写成东西" class="headerlink" title="把程序写成东西"></a>把程序写成东西</h2><p>感谢今生遇见杨力祥老师！！！活了20多年了，第一次遇到一位将面向对象思想讲解这么透彻的老师。</p><p>老师一上来的课程就非常硬核，告诉我们，<strong>面向对象就是把程序写成东西</strong></p><p>以麦当劳炸薯条的例子，教会我们区分面向过程和面向对象。面向过程就是流程化，第一步做什么第二部做什么。。。最后一步做什么。规则、时间、火候、尺寸必须一点都不能差。</p><p>而面向对象是将工作的各个流程打包成几个“东西”，任何人只要学会使用这个东西，就可以炸出麦当劳家的高质量薯条。</p><h2 id="复用"><a href="#复用" class="headerlink" title="复用"></a>复用</h2><p>为什么要学面向对象？有什么好处？</p><p>答案就是：<strong>复用</strong>，程序写一次，在不修改的情况下大量拷贝使用。而复用的本质是<strong>共性</strong>，程序只有写成东西才容易找到共性。</p><p>杨老师还从人类思维认知起源给我们讲起，人类最初学会的工作就是<code>分类</code>，引申出类和对象的关系。将分类比做一个多叉树，在实际思考时，是自下而上抽取共性，抽象就是脱离形式提取共性，但写的时候从上到下派生。</p><p>为什么把程序写成东西就容易复用？</p><p>因为人类祖先最早学会的技能就是分类，根据东西的<strong>共性</strong>进行分类，面向对象根本目的是提高复用率。</p><p>程序写成东西的唯一目的就是复用。复用就是自己可以作为贡献者也可以作为创新者。</p><p>不要做“万能工具箱类”，万能工具箱类不可以进行程序的复用，谁都可以拿来用。</p><h2 id="共性与个性"><a href="#共性与个性" class="headerlink" title="共性与个性"></a>共性与个性</h2><p>共性+差异&#x3D;个性</p><p>面向对象提高效率的理论基础与使用依据：<strong>共性远远远远大于差异，但是差异体现价值</strong>，自然界的事物亦如此。</p><p>继承派生的过程中加入了多态的思想，于是产生了个性的差异价值；如果只是单纯的继承派生无法体现出个性的差异价值。</p><p>虚函数是技术、多态是思想。</p><p>越能加入差异反而复用率越多，越能体现差异越能找到共性。</p><p>加机制产生变异，减机制适者生存。</p><h2 id="创新思维"><a href="#创新思维" class="headerlink" title="创新思维"></a>创新思维</h2><p>创新力与创新思维不是天生的，一定是要经过后天大量培训锻炼出来的。分析问题要抓住本质，这句话很空，但是很难做到。</p><p>作为未来的计算机从业者，凡事用数据和计算的思维去思考。</p><p>开源的东西想卡脖子也是很容易的，代码很难体现出思想上的变化。</p><h2 id="鉴赏力"><a href="#鉴赏力" class="headerlink" title="鉴赏力"></a>鉴赏力</h2><p>不管是机器还是人，提高鉴赏力尤为重要。</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>雁栖一年，科研三年</title>
    <link href="/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/"/>
    <url>/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="开学"><a href="#开学" class="headerlink" title="开学"></a>开学</h2><p>开学参加了很多次的入学讲座和培训，听了很多老师的演讲和致辞。</p><p>曹亚男老师结合她自己的经历，告诉我们要<code>做有品位的研究</code>。针对科学研究，提出了两大方向：针对开放问题的开创探讨和针对封闭数据的优先探索。问题的价值作为核心，与个人兴趣、资源、能力、知识储备紧密结合。做有意义的课题，要平衡好“我喜欢”和“我的能力”。在雁栖湖的一年，学好专业课程的同时，一定要定好自己的研究点(至少3个)，学硕开题前至少要投出去一篇。</p><p>林政老师在开学典礼的致辞中也告诫我们，研究生的学习过程中，要<code>多关注why而不是how</code>。对于新技术、新方法，主要要弄懂为什么这样用？为什么是这个结构？至于说怎么用，那是学习过程中需要解决的问题。林老师也提醒到，学习课程的同时，不要忘记培养自己的科研能力与科学素养。</p><h2 id="关于选课"><a href="#关于选课" class="headerlink" title="关于选课"></a>关于选课</h2><p>新学期选课工作全部结束啦，抢到了自己心仪的课程。新学期还是立足本专业，结合李老师的建议侧重数据库和人工智能相关。</p><p>核心课选了晓飞老师的机器学习、曹亚男老师的自然语言处理、沙老师的大数据管理与分析。专业课选了黄晶老师的大数据技术、计算机苏老师的数据库新技术。</p><p>希望自己好好利用在雁栖湖的这一年时光，多学点真本领、多培养一些科学素养、多积累一些科研经验。上好课是基本的态度，此外还要在立足上好专业课的同时多读读经典论文，毕竟自己是学术型硕士，科学研究的基本素养与方法论还是要多多去积累。</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>开学一周，结实了身边很多优秀的同学。依稀记得，第一次开班会的时候，大家在做自我介绍，每个人都会说自己的本科毕业院校是哪里。我留心关注了一下，河北大学应该是“倒数第三好”。说实话，本科院校说出来其实是不自信的，在我上台前我也犹豫过、摇摆过，到底要不要说。最终，我还是大大方方、很坦然的向大家做了介绍。此时此刻，彷佛就像刚步入高中，步入沧州市第一中学那个优秀的环境中。但是，唯一不同的是，我的心态和那时有了很大的变化。</p><p>刚从乡镇走出来，步入一中的时候，那时候身边都是比自己优秀的人。说实话，那三年过的非常迷茫和压抑。初中的时候，你在学校是“呼风唤雨的”，但是到了一个更大的环境中、一个竞争更激烈的环境中，你彷佛就迷失了自己。那几年你会发现，你好像无论怎么努力、怎么使劲学都没办法赶超周围的人，你一直在盲目的进行分数的“攀比”。学业学业跟不上、擅长的体育也不再是特长、个人感情不顺利、二叔车祸离世家庭带来创伤…真就想掉入深渊一样，周围全是黑暗，看不到一丝希望与光亮。</p><p>本科的时候，你考了一个相对来说正常发挥的分数，以专业第一的身份进入大学。在前三年没有懈怠，因为不装逼的说，当时确实还是有遗憾。推免完成后，顺利完成学业。这四年你的表现不错，通过积极体育锻炼、综合素质锻炼、遇到了一群好的室友，你的性格、心态有了很大改变。唯一重要的是，这四年，我觉着你一直在<code>找自己</code>。找自己就是客观坦然的面对自己，从容地面对自己的优点和缺点，三人行必有我师焉，你也在时时刻刻向身边优秀的人学习。真诚、勇敢、向上社交。</p><p>好了，话说回来了，来到研究生阶段，又是到了一个更大的环境中、一个竞争更激烈的环境中，彷佛是轮回。我想，此时的你一定不同于高中时期的你，你的心里一定多了几分淡定与从容。就随着这份淡定与从容，继续向前吧！千磨万击还坚劲，任尔东西南北风。走好自己的路，心安即强大。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>雁栖一年，科研三年。前路漫漫，忘君珍重、珍惜。</p><p>​                                                                                                                                                    2024年8月30日于国科大雁栖湖</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DataWhale AI夏令营-LLM实训</title>
    <link href="/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/"/>
    <url>/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/</url>
    
    <content type="html"><![CDATA[<h2 id="day01-baseline搭建"><a href="#day01-baseline搭建" class="headerlink" title="day01-baseline搭建"></a>day01-baseline搭建</h2><div class="note note-success">            <p>首先注册魔塔社区帐号，免费领取魔塔GPU算力资源</p>          </div><p>新建GPU算力环境，下载相关第三方库与拉取镜像资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 拉取git镜像</span></span><br>git lfs install<br>git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 安装第三方库</span></span><br>pip install streamlit==1.24.0<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 启动demo</span></span><br>streamlit run AICamp_yuan_baseline/Task\ 1：零基础玩转源大模型/web_demo_2b.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure><h2 id="day02-RAG原理与实践"><a href="#day02-RAG原理与实践" class="headerlink" title="day02-RAG原理与实践"></a>day02-RAG原理与实践</h2><p>检索增强生成 <strong>(Retrieval Augmented Generation,RAG)</strong> 是一种使用来自私有或专用数据源的信息来辅助文本生成的技术。它将检索模型(设计用于搜索大型数据集或知识库)和生成模型(例如大型语言模型 (LLM))，此类模型会使用检 索到的信息生成可供阅读的文本回复)结合在一起。</p><h3 id="LLM局限性"><a href="#LLM局限性" class="headerlink" title="LLM局限性"></a>LLM局限性</h3><p><a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p><p>这篇文章由来自Facebook AI Research、University College London、New York University三大科研教育机构的12名作者 (Patrick Lewis等)共同完成。文章主要介绍了一种新颖的检索增强生成(RAG)模型，该模型旨在解决预训练语言模型 在知识密集型NLP任务中的局限性，RAG技术被首次提出。</p><p>文章中阐述了传统大模型的局限性:传统的大型预训练模型虽然拥有存储大量事实知识的能力，但在 (query accuracy)和更 (knowledge updates)时存在不足。</p><p>同样，在实际业务场景中，通用的基础大模型可能存在无法满足我们需求的情况，主要有以下几方面原因：</p><ul><li><p>知识局限性：大模型的知识来源于训练数据，而这些数据主要来自于互联网上已经公开的资源，对于一些实时性的或者非公开的，由于大模型没有获取到相关数据，这部分知识也就无法被掌握。</p></li><li><p>数据安全性：为了使得大模型能够具备相应的知识，就需要将数据纳入到训练集进行训练。然而，对于企业来说，数据的安全性至关重要，任何形式的数据泄露都可能对企业构成致命的威胁。</p></li><li><p>大模型幻觉：由于大模型是基于概率统计进行构建的，其输出本质上是一系列数值运算。因此，有时会出现模型“一本正经地胡说八道”的情况，尤其是在大模型不具备的知识或不擅长的场景中。</p></li></ul><h3 id="RAG基本步骤"><a href="#RAG基本步骤" class="headerlink" title="RAG基本步骤"></a>RAG基本步骤</h3><p><img src="/img/post/rag-buzhou.png" alt="基本步骤"></p><ul><li><p>索引：将文档库分割成较短的 <strong>Chunk</strong>，即文本块或文档片段，然后构建成向量索引。</p></li><li><p>检索：计算问题和 Chunks 的相似度，检索出若干个相关的 Chunk。</p></li><li><p>生成：将检索到的Chunks作为背景信息，生成问题的回答。</p></li></ul><h3 id="RAG完整链路图"><a href="#RAG完整链路图" class="headerlink" title="RAG完整链路图"></a>RAG完整链路图</h3><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><p>图片来源:(<a href="https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png">https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png</a>)</p><p>用户进行query查询后，RAG会先进行检索，之后将检索到的 <strong><code>Chunks</code></strong> 和 <strong><code>query</code></strong> 一并输入到大模型，进而回答用户的问题。</p><p>为了完成检索，需要离线将文档（ppt、word、pdf等）经过解析、切割甚至OCR转写，然后进行向量化存入数据库(vector database)中。</p><h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h3><p>知识库中包含了多种类型的文件，如pdf、word、ppt等，这些 <code>文档</code>（Documents）需要提前被解析，然后切割成若干个较短的 <code>Chunk</code>，并且进行清洗和去重。</p><p>然后，我们会将知识库中的所有 <code>Chunk</code> 都转成向量，这一步也称为 <code>向量化</code>（Vectorization）或者 <code>索引</code>（Indexing）。<code>向量化</code> 需要事先构建一个 <code>向量模型</code>（Embedding Model），它的作用就是将一段 <code>Chunk</code> 转成 <code>向量</code>（Embedding）。</p><p>随着新知识的不断存储，向量的数量也会不断增加。这就需要将这些向量存储到 <code>数据库</code> （DataBase）中进行管理。</p><h3 id="在线计算"><a href="#在线计算" class="headerlink" title="在线计算"></a>在线计算</h3><p>在实际使用RAG系统时，当给定一条用户 <code>查询</code>（Query），需要先从知识库中找到所需的知识，这一步称为 <code>检索</code>（Retrieval）。在 <code>检索</code> 过程中，用户查询首先会经过向量模型得到相应的向量，然后与 <code>数据库</code> 中所有 <code>Chunk</code> 的向量计算相似度，最简单的例如  <code>余弦相似度</code>，然后得到最相近的一系列 <code>Chunk</code> 。</p><p>由于向量相似度的计算过程需要一定的时间，尤其是 <code>数据库</code> 非常大的时候。可以在检索之前进行 <code>召回</code>（Recall），即从 <code>数据库</code> 中快速获得大量大概率相关的 <code>Chunk</code>，然后只有这些 <code>Chunk</code> 会参与计算向量相似度。这样，计算的复杂度就从整个知识库降到了非常低。</p><p>随着知识库的增大，除了检索的速度变慢外，检索的效果也会出现退化。这是由于 <code>向量模型</code> 能力有限，而随着知识库的增大，已经超出了其容量，因此准确性就会下降。在这种情况下，相似度最高的结果可能并不是最优的。</p><p>为了解决这一问题，提升RAG效果，研究者提出增加一个二阶段检索——<code>重排</code> (Rerank)，即利用 <code>重排模型</code>（Reranker），使得越相似的结果排名更靠前。这样就能实现准确率稳定增长，即数据越多，效果越好（如上图中紫线所示）。</p><p>通常，为了与 <code>重排</code> 进行区分，一阶段检索有时也被称为 <code>精排</code> 。而在一些更复杂的系统中，在 <code>召回</code> 和 <code>精排</code> 之间还会添加一个 <code>粗排</code> 步骤，这里不再展开，感兴趣的同学可以自行搜索。综上所述，在整个 <code>检索</code> 过程中，计算量的顺序是 <code>召回</code> &gt; <code>精排</code> &gt; <code>重排</code>，而检索效果的顺序则是 <code>召回</code> &lt; <code>精排</code> &lt; <code>重排</code> 。</p><p>至此，一个完整的RAG链路就构建完毕了。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks </p><p>[2] Gao, Yunfan, et al. “Retrieval-augmented generation for large language models: A survey.” <strong>arXiv preprint arXiv:2312.10997</strong> (2023).</p><p>[3] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewriting for retrieval-augmented large language models,” <strong>arXiv preprint arXiv:2305.14283</strong>, 2023.</p><p>[4] QAnything: <a href="https://github.com/netease-youdao/QAnything">https://github.com/netease-youdao/QAnything</a></p><p>[5] When Large Language Models Meet Vector Databases: A Survey <a href="https://doi.org/10.48550/arXiv.2402.01763">https://doi.org/10.48550/arXiv.2402.01763</a> </p><h2 id="RAG技术实践"><a href="#RAG技术实践" class="headerlink" title="RAG技术实践"></a>RAG技术实践</h2><p>前置条件：使用day01搭建好的baseline环境</p><p>下载环境所需的任务包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git lfs install<br>git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-built_in">cp</span> AICamp_yuan_baseline/Task\ 3：源大模型RAG实战/* .<br></code></pre></td></tr></table></figure><p>双击打开<code>Task 3：源大模型RAG实战.ipynb</code>，然后运行所有单元格。</p><p>在环境中安装<code>streamlit</code>,为了后续进行模型微调以及Demo搭建(day01已经安装完毕)。</p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>在RAG实战过程中，需要构建一个向量模型。向量模型通常是一个BERT架构，是一个Transformer Encoder。</p><p>在本次学习中，选用基于BERT架构的向量模型 <code>bge-small-zh-v1.5</code>，它是一个4层的BERT模型，最大输入长度512，输出的向量维度也为512。</p><p>向量模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&quot;AI-ModelScope/bge-small-zh-v1.5&quot;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Yuan大模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>构造向量索引，分装一个向量模型类EmbeddingModel：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EmbeddingModel</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for EmbeddingModel</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(path)<br><br>        <span class="hljs-variable language_">self</span>.model = AutoModel.from_pretrained(path).cuda()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading EmbeddingModel from <span class="hljs-subst">&#123;path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">self, texts: <span class="hljs-type">List</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate embedding for text list</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        encoded_input = <span class="hljs-variable language_">self</span>.tokenizer(texts, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>        encoded_input = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()&#125;<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            model_output = <span class="hljs-variable language_">self</span>.model(**encoded_input)<br>            sentence_embeddings = model_output[<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>]<br>        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> sentence_embeddings.tolist()<br></code></pre></td></tr></table></figure><p>通过传入模型路径，新建一个 <code>EmbeddingModel</code> 对象 <code>embed_model</code>。初始化时自动加载向量模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create embedding model...&quot;</span>)<br>embed_model_path = <span class="hljs-string">&#x27;./AI-ModelScope/bge-small-zh-v1___5&#x27;</span><br>embed_model = EmbeddingModel(embed_model_path)<br></code></pre></td></tr></table></figure><p><code>EmbeddingModel</code> 类还有一个 <code>get_embeddings()</code> 函数，它可以获得输入文本的向量表示。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>为了实现向量检索，定义一个向量库索引类 <code>VectorStoreIndex</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量库索引类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorStoreIndex</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for VectorStoreIndex</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, doecment_path: <span class="hljs-built_in">str</span>, embed_model: EmbeddingModel</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.documents = []<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(doecment_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>):<br>            line = line.strip()<br>            <span class="hljs-variable language_">self</span>.documents.append(line)<br><br>        <span class="hljs-variable language_">self</span>.embed_model = embed_model<br>        <span class="hljs-variable language_">self</span>.vectors = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings(<span class="hljs-variable language_">self</span>.documents)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.documents)&#125;</span> documents for <span class="hljs-subst">&#123;doecment_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarity</span>(<span class="hljs-params">self, vector1: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], vector2: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate cosine similarity between two vectors</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dot_product = np.dot(vector1, vector2)<br>        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> magnitude:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> dot_product / magnitude<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, k: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        question_vector = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings([question])[<span class="hljs-number">0</span>]<br>        result = np.array([<span class="hljs-variable language_">self</span>.get_similarity(question_vector, vector) <span class="hljs-keyword">for</span> vector <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vectors])<br>        <span class="hljs-keyword">return</span> np.array(<span class="hljs-variable language_">self</span>.documents)[result.argsort()[-k:][::-<span class="hljs-number">1</span>]].tolist() <br></code></pre></td></tr></table></figure><p>类似地，通过传入知识库文件路径，新建一个 <code>VectorStoreIndex</code> 对象 <code>index</code>。初始化时会自动读取知识库的内容，然后传入向量模型，获得向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create index...&quot;</span>)<br>doecment_path = <span class="hljs-string">&#x27;./knowledge.txt&#x27;</span><br>index = VectorStoreIndex(doecment_path, embed_model)<br></code></pre></td></tr></table></figure><p>上文提到 <code>get_embeddings()</code> 函数支持一次性传入多条文本，但由于GPU的显存有限，输入的文本不宜太多。</p><p>所以，如果知识库很大，需要将知识库切分成多个batch，然后分批次送入向量模型。</p><p><code>VectorStoreIndex</code> 类还有一个 <code>get_similarity()</code> 函数，它用于计算两个向量之间的相似度，这里采用了余弦相似度。<code>VectorStoreIndex</code> 类的入口，即查询函数 <code>query()</code>。传入用户的提问后，首先会送入向量模型获得其向量表示，然后与知识库中的所有向量计算相似度，最后将 <code>k</code> 个最相似的文档按顺序返回，<code>k</code>默认为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">question = <span class="hljs-string">&#x27;介绍一下广州&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Question:&#x27;</span>, question)<br><br>context = index.query(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Context:&#x27;</span>, context)<br></code></pre></td></tr></table></figure><h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3><p>为了实现基于RAG的生成，我们还需要定义一个大语言模型类 <code>LLM</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义大语言模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLM</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for Yuan2.0 LLM</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat tokenizer...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat model...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading Yuan2.0 model from <span class="hljs-subst">&#123;model_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, context: <span class="hljs-type">List</span></span>):<br>        <span class="hljs-keyword">if</span> context:<br>            prompt = <span class="hljs-string">f&#x27;背景：<span class="hljs-subst">&#123;context&#125;</span>\n问题：<span class="hljs-subst">&#123;question&#125;</span>\n请基于背景，回答问题。&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            prompt = question<br><br>        prompt += <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>        inputs = <span class="hljs-variable language_">self</span>.tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>        outputs = <span class="hljs-variable language_">self</span>.model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">1024</span>)<br>        output = <span class="hljs-variable language_">self</span>.tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>这里我们传入 <code>Yuan2-2B-Mars</code> 的模型路径，新建一个 <code>LLM</code> 对象 <code>llm</code>。初始化时自动加载源大模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create Yuan2.0 LLM...&quot;</span>)<br>model_path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br>llm = LLM(model_path)<br></code></pre></td></tr></table></figure><p><code>LLM</code> 类的入口是生成函数 <code>generate()</code>，它有两个参数：</p><ul><li><code>question</code>: 用户提问，是一个str</li><li><code>context</code>: 检索到的上下文信息，是一个List，默认是[]，代表没有使用RAG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Without RAG:&#x27;</span>)<br>llm.generate(question, [])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; With RAG:&#x27;</span>)<br>llm.generate(question, context)<br></code></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">&gt;</span> <span class="hljs-string">Without RAG:</span><br><span class="hljs-attr">广州大学（Guangzhou</span> <span class="hljs-string">University）是广东省内一所综合性大学，位于中国广东省广州市。广州大学成立于1952年，前身为广州工学院，是中华人民共和国成立后创建的第一所高等工科院校。</span><br><span class="hljs-attr">广州大学坐落在广州市海珠区，占地面积广阔，校园环境优美。学校拥有多个校区，其中主校区位于广州市番禺区，其他校区分布在广州市的其他地区。学校占地面积约4000亩，拥有现代化的教学、实验和生活设施。</span><br><span class="hljs-attr">广州大学以培养人才为宗旨，注重理论与实践相结合的教学模式。学校开设了多个学院和专业，涵盖了工学、理学、文学、法学、经济学、管理学、艺术学等多个领域。学校现有本科专业近300个，研究生专业涵盖科学、工程、管理、文学、法学、艺术等多个领域。</span><br><span class="hljs-attr">广州大学注重国际交流与合作，积极推进国际化办学。学校与许多国际知名大学建立了合作关系，开展学术交流和合作研究。此外，学校还鼓励学生参与国际交流项目，提供海外实习和留学机会，提升学生的国际视野和能力。</span><br><span class="hljs-attr">广州大学一直以来致力于为学生提供优质的教育环境和丰富的学习资源。学校拥有先进的教学设施和实验室，以及图书馆、体育场馆、艺术工作室等丰富的学生课外活动设施。</span><br><span class="hljs-attr">广州大学以其优秀的教学质量、领先的科研水平和培养优秀学生的能力而闻名。学校致力于培养具有创新精神和社会责任感的高素质人才，为地方经济发展和社会进步做出贡献。&lt;eod&gt;</span><br><span class="hljs-attr">&gt;</span> <span class="hljs-string">With RAG:</span><br><span class="hljs-attr">广州大学是一所位于广东省广州市的全日制普通高等学校，实行省市共建、以市为主的办学体制。学校的办学历史可以追溯到1927年创办的私立广州大学，后来在1951年并入华南联合大学。1984年定名为广州大学。2000年，广州大学经过教育部批准，与广州教育学院、广州师范学院、华南建设学院西院、广州高等师范专科学校合并组建新的广州大学。&lt;eod&gt;</span><br></code></pre></td></tr></table></figure><h2 id="day03-微调技术原理与实践"><a href="#day03-微调技术原理与实践" class="headerlink" title="day03-微调技术原理与实践"></a>day03-微调技术原理与实践</h2><p>模型微调也被称为指令微调（Instruction Tuning）或者有监督微调（Supervised Fine-tuning, SFT），该方法利用成对的任务输入与预期输出数据，训练模型学会以问答的形式解答问题，从而解锁其任务解决潜能。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p><p>指令微调并非无中生有地传授新知，而是更多地扮演着催化剂的角色，激活模型内在的潜在能力，而非单纯地灌输信息。</p><p>相较于预训练所需的海量数据，指令微调所需数据量显著减少，从几十万到上百万条不等的数据，均可有效激发模型的通用任务解决能力。</p><h3 id="轻量化微调"><a href="#轻量化微调" class="headerlink" title="轻量化微调"></a>轻量化微调</h3><p>由于大模型的参数量巨大， 进行全量参数微调需要消耗非常多的算力。为了解决这一问题，研究者提出了参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning），这些方法通过训练极少的模型参数，同时保证微调后的模型表现可以与全量微调相媲美。</p><p>常用的轻量化微调技术有<code>LoRA</code>、<code>Adapter</code> 和 <code>Prompt Tuning</code>。</p><p>LoRA:<a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a></p><p>大模型轻量级微调（LoRA）：训练速度、显存占用分析:<a href="https://zhuanlan.zhihu.com/p/666000885">https://zhuanlan.zhihu.com/p/666000885</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#模型下载</span><br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br><span class="hljs-comment"># 导入环境</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer<br><span class="hljs-comment"># 读取数据</span><br>df = pd.read_json(<span class="hljs-string">&#x27;./data.json&#x27;</span>)<br>ds = Dataset.from_pandas(df)<br><span class="hljs-comment"># 加载 tokenizer</span><br>path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br>tokenizer.pad_token = tokenizer.eos_token<br><span class="hljs-comment"># 定义数据处理函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_func</span>(<span class="hljs-params">example</span>):<br>    MAX_LENGTH = <span class="hljs-number">384</span>    <span class="hljs-comment"># Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性</span><br><br>    instruction = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;input&#x27;</span>]&#125;</span>&lt;sep&gt;&quot;</span>)<br>    response = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&lt;eod&gt;&quot;</span>)<br>    input_ids = instruction[<span class="hljs-string">&quot;input_ids&quot;</span>] + response[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>    attention_mask = [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(input_ids) <br>    labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(instruction[<span class="hljs-string">&quot;input_ids&quot;</span>]) + response[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-comment"># instruction 不计算loss</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(input_ids) &gt; MAX_LENGTH:  <span class="hljs-comment"># 做一个截断</span><br>        input_ids = input_ids[:MAX_LENGTH]<br>        attention_mask = attention_mask[:MAX_LENGTH]<br>        labels = labels[:MAX_LENGTH]<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids,<br>        <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>        <span class="hljs-string">&quot;labels&quot;</span>: labels<br>    &#125;<br><span class="hljs-comment"># 处理数据集</span><br>tokenized_id = ds.<span class="hljs-built_in">map</span>(process_func, remove_columns=ds.column_names)<br>tokenized_id<br><span class="hljs-comment"># 数据检查</span><br>tokenizer.decode(tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>tokenizer.decode(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x != -<span class="hljs-number">100</span>, tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;labels&quot;</span>])))<br><span class="hljs-comment"># 模型加载</span><br>model = AutoModelForCausalLM.from_pretrained(path, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>)<br>model<br>model.enable_input_require_grads() <span class="hljs-comment"># 开启gradient_checkpointing时，要执行该方法</span><br><span class="hljs-comment"># 配置Lora</span><br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model<br><br>config = LoraConfig(<br>    task_type=TaskType.CAUSAL_LM, <br>    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>, <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],<br>    inference_mode=<span class="hljs-literal">False</span>, <span class="hljs-comment"># 训练模式</span><br>    r=<span class="hljs-number">8</span>, <span class="hljs-comment"># Lora 秩</span><br>    lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># Lora alaph，具体作用参见 Lora 原理</span><br>    lora_dropout=<span class="hljs-number">0.1</span><span class="hljs-comment"># Dropout 比例</span><br>)<br>config<br><span class="hljs-comment"># 构建PeftModel</span><br>model = get_peft_model(model, config)<br>model<br><span class="hljs-comment"># 设置训练参数</span><br>args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./output/Yuan2.0-2B_lora_bf16&quot;</span>,<br>    per_device_train_batch_size=<span class="hljs-number">12</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">1</span>,<br>    logging_steps=<span class="hljs-number">1</span>,<br>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    num_train_epochs=<span class="hljs-number">3</span>,<br>    learning_rate=<span class="hljs-number">5e-5</span>,<br>    save_on_each_node=<span class="hljs-literal">True</span>,<br>    gradient_checkpointing=<span class="hljs-literal">True</span>,<br>    bf16=<span class="hljs-literal">True</span><br>)<br><span class="hljs-comment"># 初始化Trainer</span><br>trainer = Trainer(<br>    model=model,<br>    args=args,<br>    train_dataset=tokenized_id,<br>    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="hljs-literal">True</span>),<br>)<br><span class="hljs-comment"># 模型训练</span><br>trainer.train()<br><span class="hljs-comment"># 定义生成函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">prompt</span>):<br>    prompt = prompt + <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>    inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>    outputs = model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">256</span>)<br>    output = tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br>    <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br>input_str = <span class="hljs-string">&#x27;张三，汉族，金融学硕士。&#x27;</span><br>prompt = template.replace(<span class="hljs-string">&#x27;input_str&#x27;</span>, input_str).strip()<br>generate(prompt)<br>&#123;<span class="hljs-string">&quot;姓名&quot;</span>: [<span class="hljs-string">&quot;张三&quot;</span>], <span class="hljs-string">&quot;国籍&quot;</span>: [<span class="hljs-string">&quot;汉族&quot;</span>], <span class="hljs-string">&quot;职位&quot;</span>: [<span class="hljs-string">&quot;金融学硕士&quot;</span>]&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型应用开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>部署</tag>
      
      <tag>微调技术</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据技术学习笔记</title>
    <link href="/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="大数据技术路线总揽"><a href="#大数据技术路线总揽" class="headerlink" title="大数据技术路线总揽"></a>大数据技术路线总揽</h2><h2 id="MapReduce编程思想"><a href="#MapReduce编程思想" class="headerlink" title="MapReduce编程思想"></a>MapReduce编程思想</h2><h2 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h2><h3 id="Google-files-system"><a href="#Google-files-system" class="headerlink" title="Google files system"></a>Google files system</h3><p>GFS直接以Linux为基础存储层，并且设计模式为单master模式。</p><p>另外一方面，GFS 还是采用了 Checkpoints、操作日志（Operation Logs）、影子Master（Shadow Master）等一系列的工程手段，来尽可能地保障整个系统的“可恢复（Recoverable）”，以及读层面的“可用性（Availability）”。</p>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
      <tag>数仓</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习基础</title>
    <link href="/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="Machine-Learning-Notebook"><a href="#Machine-Learning-Notebook" class="headerlink" title="Machine Learning-Notebook"></a>Machine Learning-Notebook</h1><center>Andrew Ng-吴恩达&copy;</center><center>Standford ONLINE & DeepLearning.AI</center><center>Mungeryang-杨桂淼总结</center><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>“Field of study that gives computers the ability to learn without being explicitly programmed.”——Arthur Samuel(1995)</p><p>  Practical advice for applying learning algorithm</p><h2 id="Basic-conception-cookbook"><a href="#Basic-conception-cookbook" class="headerlink" title="Basic conception cookbook"></a>Basic conception cookbook</h2><p>Data set:</p><p>Training set:</p><p>Test set:</p><p>Cost function:</p><p>Gradient:</p><p>Gradient descent:</p><p>Recall:</p><p>Precision:</p><h2 id="Supervised-learning-监督学习"><a href="#Supervised-learning-监督学习" class="headerlink" title="Supervised learning-监督学习"></a>Supervised learning-监督学习</h2><p> Learns from being given “<strong>right answers</strong>(labels)”</p><img src="/img/fig/1.1.png" alt="s" style="text-align: center;"/><p>In all of these applications, we will first train our model with examples of inputs <strong>x</strong> and right answers, that is the labels <strong>y</strong>. After the model has learned from these input, output, or x and y pairs, they can take a brand new input x, something it has never seen before, and try to produce the appropriate corresponding output y.  </p><img src="/img/fig/1.2.jpg" alt="s" style="text-align: center;" /><p>  The task of the supervised learning algorithm is to produce more of these right answers based on labels.  </p><p>Classification is to predict categories,Regression is to predict a number. </p><img src="/img/fig/1.3.png" alt="s" style="text-align: center;" /><h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning-无监督学习"></a>Unsupervised learning-无监督学习</h2><p>Learns from being given “<strong>no-right answers</strong>(unlabeled data)”, data only comes with inputs x, but not output labels y. Algorithm has to find structure automatically in the data and automatically figure out whether the major types of individuals.</p><img src="/img/fig/1.4.png" style="text-align: center;" /><h2 id="Linear-Regression-with-one-variable"><a href="#Linear-Regression-with-one-variable" class="headerlink" title="Linear Regression with one variable"></a>Linear Regression with one variable</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Linear regression means fitting a <code>straight line</code> to the data. -&gt;linear regression  </p><img src="/img/fig/2.1.jpg" alt="s" style="text-align: center;"/><p>Regression model is to predict numbers</p><p>Classification model is to predicts categories</p><h3 id="Terminology-in-ML"><a href="#Terminology-in-ML" class="headerlink" title="Terminology in ML"></a><strong>Terminology in ML</strong></h3><p><code>Training dataset</code>-&gt;data used to train the model</p><p>$x&#x3D;$“input”variables feature</p><p>$y&#x3D;$“output ”variables or “target”variables</p><p>$(x,y)&#x3D;$single training example</p><p>$(x^{(i)},y^{(i)})&#x3D;i^{th}$ training example not exponent</p><img src="/img/fig/2.2.jpg" alt="s" style="text-align: center;"/><p>In the linear regression, we instantly believe the function is a linear function as follow:<br>$$<br>f_{w,b}(X)&#x3D;wX+b \<br>f(X)&#x3D;wX+b<br>$$<br>when we give a “x”as a input variable, we can get a “y-hat”variable as a result.</p><h3 id="triangular-ruler-Cost-function"><a href="#triangular-ruler-Cost-function" class="headerlink" title=":triangular_ruler:Cost function"></a>:triangular_ruler:Cost function</h3><p>squared error<br>$$<br>\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2<br>$$<br><img src="/img/fig/2.3.jpg" alt="s" style="text-align: center;" /></p><p>Model: $f_{w,b}(X)&#x3D;wX+b$.$w(slope),b(intersects)$ are parameters.<br>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{w.b}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2 \<br>&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2<br>$$<br>Our goal is to minimize the parameters to fit the model:<br>$$<br>\underset{w.b}{\min}J\left( w,b \right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#define cost function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    cost_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i]) ** <span class="hljs-number">2</span><br>        cost_sum += cost<br>    total_cost = (<span class="hljs-number">1</span>/(<span class="hljs-number">2</span>*m))*cost_sum<br>    <span class="hljs-keyword">return</span> total_cost<br></code></pre></td></tr></table></figure><h3 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent-梯度下降"></a>Gradient descent-梯度下降</h3><p><strong>Simultaneous update</strong> the parameters w and b until the cost function is <code>convergence</code>:</p><p>Simultaneous update the parameters is significant, we must focus on the order about the algorithm.</p><p>Correct order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>\<br>w&#x3D;tmp_w<br>\<br>b&#x3D;tmp_b<br>$$<br>Incorrect order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right) \<br>w&#x3D;tmp_w \<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right) \<br>b&#x3D;tmp_b \<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#compute gradient</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradient</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_dw = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_dw += dj_dw_i<br>        dj_db += dj_db_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></td></tr></table></figure><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>The choice of learning rate, alpha($\alpha$) will have a huge impact on the efficiency of our implementation of Gradient descent.<br>$$<br>w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>$$<br><img src="/img/fig/2.6.jpg" alt="s" style="text-align: center;" /></p><p>If we chose the alpha is too large, the fit efficiency is not very well. We can design the algorithm to decrease the alpha($\alpha$) following by the w and b.</p><img src="/img/fig/cost1.png" alt="s" style="text-align: center;" /><h2 id="Multiple-linear-regression"><a href="#Multiple-linear-regression" class="headerlink" title="Multiple linear regression"></a>Multiple linear regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>【Model】<br>$$<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n<br>$$<br><code>Dot product(inner product)</code> of two vectors about $w$ and $b$.<br>$$<br>\vec{w}&#x3D;\left[ w_1,w_2···,w_n \right]<br>\<br>\vec{x}&#x3D;\left[ x_1,x_2···,x_n \right]<br>$$<br>$x_j&#x3D;j^{th} features$</p><p>$n &#x3D; $ numbers of features</p><p>$\vec{x}^{(i)}$ &#x3D; features of $i^{th}$ training example</p><p>$\vec{x}^{(i)}_j$ &#x3D; value of feature j in  $i^{th}$ training example</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>We contrast the two process between vectorization and without vectorization.</p><p>without vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n):<br>    f = f + w[i]*x[i]<br>f = f + b<br></code></pre></td></tr></table></figure><p>If n is infinite, the algorithm is time consuming.</p><p>with vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>f = np.dot(w,b)<br></code></pre></td></tr></table></figure><p> Without vectorization, we just only use the for loop to calculate the results one by one. In the contrast, we can use the function numpy.dot() to calculate the result. Using numpy can decrease the time complexity and improve the algorithm efficiency. Numpy can use <code>parallel process</code> hardware to carry out the data.</p><p>Dot product of two vectors:<br>$$<br>a·b&#x3D;\underset{i&#x3D;0}{\overset{n-1}{\varSigma}}a_ib_i&#x3D;\left[ \begin{array}{l}<br>    ,, a_0\<br>    ,, a_1\<br>    ,,···\<br>    a_{n-1}\<br>\end{array} \right] \left[ \begin{array}{l}<br>    ,, b_0\<br>    ,, b_1\<br>    ,,···\<br>    b_{n-1}\<br>\end{array} \right] &#x3D;\left[ a_0b_0+a_1b_1+···+a_{n-1}b_{n-1} \right]<br>$$</p><h3 id="Gradient-descent-in-Multiple-regression"><a href="#Gradient-descent-in-Multiple-regression" class="headerlink" title="Gradient descent in Multiple regression"></a>Gradient descent in Multiple regression</h3><p>Parameters include $w_{1},w_{2},···w_{n}$ and $b$</p><p>Model is $f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n+b$</p><p>Cost function is $J\left( w_{1},w_{2},···w_{n},b \right)$</p><p>Gradient descent:</p><pre><code class="hljs">repeat&#123;</code></pre><p>$$<br>w_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>}</p><p><code>for i in range(1,n)</code>:<br>$$<br>\frac{\partial}{\partial w_{1}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{1}^{\left( i \right)} \<br>\frac{\partial}{\partial w_{n}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{n}^{\left( i \right)} \<br>\frac{\partial}{\partial b}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right)<br>$$<br>Normal equation:</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations</li></ul><p>Disadvantages:</p><ul><li>Does not generalize to other learning algorithm</li><li>Slow when number of features is large(n&gt;10000)</li></ul><h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The technology of Features scaling will enable Gradient descent to run much faster.</p><p>Z-score normalization:<br>$$<br>x_i&#x3D;\frac{x_i-\mu}{\sigma}<br>$$<br>$\mu$ is the sample average value, and $\sigma$ is standard error of sample.</p><p>Aim for about $-1\leqslant x_j\leqslant 1$ for $x_{j}$.</p><p>The objective of Gradient descent in Multiple regression is:<br>$$<br>\underset{\vec{w}.b}{\min},,J\left(\vec{w},b \right)<br>$$<br><img src="/img/fig/2.7.jpg" alt="s" style="text-align: center;" /></p><p>As the calculate process, $y&#x3D;J\left(\vec{w},b \right)$ is decreased until a low score. If we can get the similar the result in the experiment, the cost function is <code>convergence</code> and we can find the min $J\left(\vec{w},b \right)$.</p><p>【<strong>Skill</strong>】</p><p>At the beginning, we can set the learning rate($\alpha$) in a small value like 0.001. As running the algorithm, we can update the parameter $\alpha$ gradually.(0.001-&gt;0.01-&gt;0.1)</p><p><strong>Feature Engineering</strong> : Using <code>intuition</code> to design new features, by transforming or combining original features.</p><blockquote><p>[Wiki]:<strong>Feature engineering</strong> or <strong>feature extraction</strong> or <strong>feature discovery</strong> is the process of extracting features (characteristics, properties, attributes) from raw data.</p></blockquote><img src="/img/fig/2.8.jpg" style="text-align: center;" /><p>Except for using linear regression, we can also use <code>polynomial regression</code> to fit data. Through finding the segment of the data distribution(scatter), we can create special features($\sqrt{x}\quad x^3$) to fit data successfully.<br>$$<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+b\quad②<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+b\quad①<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2\sqrt{x}+b\quad③<br>$$</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>It turns out that, linear regression is not the good algorithm for classification. For classification, the output is not a continuous number. In the fact, it is a class variable like <strong>0(false-negative class)</strong> or <strong>1(true-positive class)</strong>.</p><img src="/img/fig/3.1.jpg" alt="s" style="text-align: center;" /><p>Sometimes, if we want to classify the output, the method of linear regression maybe not fit. Through the figure, the result of predict can be changed when we add a sample. The original fit result is the blue line, but, the new result is the green line. The standard of classification can be changed as the sample we adding.</p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>In this chapter, we will learn a useful algorithm model——<code>Logistic Regression</code> to solve the classification problem.</p><p>With linear regression method, the model is $f_{\vec{w},b}\left(x^{i} \right) &#x3D;\vec{w}·x^{i}+b$, to predict $y$ given $x$. However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</p><p>This can be accomplished by using a <code>&quot;sigmoid function&quot;</code> which maps all input values to values between 0 and 1.</p><p>【sigmoid function】</p><img src="/img/fig/3.2.jpg" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;\frac{1}{1+e^{-Z}},0&lt;g\left( Z \right) &lt;1<br>\<br>f_{w,b}\left( X^{\left( i \right)} \right) &#x3D;g\left( w·X^{\left( i \right)}+b \right)<br>\<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;g\left( \vec{w}·\vec{x}+b \right) &#x3D;\frac{1}{1+e^{-\left( \vec{w}·\vec{x}+b \right)}}<br>$$</p><p>We can calculate the Z-score to classify the predict value by the probability.</p><p>$$<br>P\left( y&#x3D;0 \right) +P\left( y&#x3D;1 \right) &#x3D;1<br>$$</p><img src="/img/fig/3.3.jpg" style="text-align: center;"/><p>$$<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;1|\vec{X};\vec{w},b \right)<br>\<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;0|\vec{X};\vec{w},b \right)<br>$$</p><p>Probability that $y&#x3D;1$  or $y&#x3D;0$, given input $\vec{x}$, parameters $w,b$.</p><p>If $y&#x3D;f_{\vec{w},b}\left( \vec{X} \right) &gt; 0.5$ -&gt; $g(Z)&gt;0.5$ -&gt; $Z&gt;0$ -&gt; $\vec{w}·\vec{x}+b &gt; 0$ &#x3D;&#x3D; YES $Y&#x3D;1$.</p><p>Else, NO $y &#x3D; 0$.</p><h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><img src="/img/fig/3.4.jpg" alt="s" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+b \right)<br>\<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b \right)\<br>boundary1&#x3D;w_1x_1+w_2x_2+b&#x3D;0<br>\<br>boundary2&#x3D;w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b&#x3D;0<br>$$</p><h3 id="Cost-function-for-regularized-logistic-regression"><a href="#Cost-function-for-regularized-logistic-regression" class="headerlink" title="Cost function for regularized logistic regression"></a>Cost function for regularized logistic regression</h3><p>Cost function:</p><p>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\frac{1}{2}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2&#x3D;L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right)<br>$$</p><p>Our goal is to minimize the parameters to fit the model:</p><p>$$<br>\underset{w.b}{\min},,J\left( w,b \right)<br>$$</p><p><strong>Simplified</strong> loss function:</p><p>$$<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;\begin{cases}<br>    -\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;1\<br>    -\log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;0\<br>\end{cases}\ \Longrightarrow \<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;-y^{\left( i \right)}\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) -\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right)<br>$$</p><p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}  \sum_{i&#x3D;0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2<br>$$</p><p>where:</p><p>$$<br>f_{\mathbf{w},b}(\mathbf{x}^{(i)}) &#x3D; sigmod(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)<br>$$</p><p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}\sum_{i&#x3D;0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]<br>$$</p><p>As was the case in linear regression above, the difference is the regularization term, which is   $\frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2$ </p><p>Gradient descent:</p><p>repeat{</p><p>$$<br>tw_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>tb&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>w&#x3D;tw_{j}\<br>b&#x3D;tb<br>$$</p><p>}</p><img src="/img/fig/gridant1.png" style="text-align: center;" /><p>To provide the overfitting problem, we apply the regularized method to add the penalty coefficient. That is why the we add the $\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}$ at the end of the formula.</p><img src="/img/fig/3.5.png" alt="s" style="text-align: center;" /><p><strong>Regularized</strong>:</p><p>$$<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}<br>\<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}b_{j}^{2}<br>$$</p><p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$.</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;&#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\ \<br>$$</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial b}  &#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})<br>$$</p><ul><li><p>m is the number of training examples in the data set      </p></li><li><p>$f_{\mathbf{w},b}(x^{(i)})$ is the model’s prediction, while $y^{(i)}$ is the target</p></li><li><p>For a  <span style="color:blue"> <strong>linear</strong> </span> regression model<br>  $f_{\mathbf{w},b}(x) &#x3D; \mathbf{w} \cdot \mathbf{x} + b$  </p></li><li><p>For a <span style="color:blue"> <strong>logistic</strong> </span> regression model<br>  $z &#x3D; \mathbf{w} \cdot \mathbf{x} + b$ \ $f_{\mathbf{w},b}(x) &#x3D; g(z)$<br>  where $g(z)$ is the sigmoid function:</p></li></ul><p>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#cpmpute cost function through itertion process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function</span>):<br>    w = copy.deepcopy(w_in)<br>    J_history = []<br>    p_history = []<br>    b = b_in <br>    w = w_in<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw,dj_db = gradient_function(x,y,w,b)<br>        <span class="hljs-comment"># update the parameters </span><br>        b = b - alpha * dj_db<br>        w = w - alpha * dj_dw<br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">100000</span>:<br>            J_history.append(cost_function(x,y,w,b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i % math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteraton <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> w,b,J_history,p_history<br></code></pre></td></tr></table></figure><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><h3 id="How-to-install-TensorFlow"><a href="#How-to-install-TensorFlow" class="headerlink" title="How to install TensorFlow"></a>How to install TensorFlow</h3><ul><li>conda install tensorflow</li><li>pip install tensorflow</li></ul><h3 id="How-the-brain-works"><a href="#How-the-brain-works" class="headerlink" title="How the brain works"></a>How the brain works</h3><p>The following picture shows the basic structure about neutral network. Like human&#96;s neutral cell, it passes information by layers, which every cell includes a logistic function.</p><img src="/img/fig/4.2.jpg" style="text-align: center;" /><h3 id="Data-format-in-Tensorflow"><a href="#Data-format-in-Tensorflow" class="headerlink" title="Data format in Tensorflow"></a>Data format in Tensorflow</h3><p><code>matrix and tensor</code></p><p>Numpy, a standard library created in 1970s, is used to calculate linear algebra in python(data analysis). Tensorflow is a machine learning framework created by Google.</p><p>In Tensorflow, the input format must <strong>a matrix</strong>. We should focus on the special characteristic in our work.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#一维数组</span><br>b = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<span class="hljs-comment">#一维矩阵</span><br>x = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">14</span>],<br>])<span class="hljs-comment">#3X3矩阵</span><br>Z = np.matmul(A_in,W) + B <span class="hljs-comment">#input matrix to simplify for loop</span><br></code></pre></td></tr></table></figure><h3 id="Build-a-Tensorflow"><a href="#Build-a-Tensorflow" class="headerlink" title="Build a Tensorflow"></a>Build a Tensorflow</h3><ol><li>build the structure of the model</li><li>compile the model</li><li>input training data</li><li>fit the model</li><li>predict the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span><br>layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_3 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1,layer_2,...layer_n])<br>----------------------------<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>])<br><span class="hljs-number">2.</span><br>model.<span class="hljs-built_in">compile</span>()<br><span class="hljs-number">3.</span><br>x = np.array([[<span class="hljs-number">0.</span>..,<span class="hljs-number">245</span>,...,<span class="hljs-number">17</span>],[<span class="hljs-number">0.</span>..,<span class="hljs-number">200</span>,...,<span class="hljs-number">284</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>])<br><span class="hljs-number">4.</span><br>model.fit<br><span class="hljs-number">5.</span><br>model.predict(x_new)<br></code></pre></td></tr></table></figure><h3 id="Implementation-of-the-preceding-communication"><a href="#Implementation-of-the-preceding-communication" class="headerlink" title="Implementation of the preceding communication"></a>Implementation of the preceding communication</h3><img src="/img/fig/4.4.png" alt="s" style="text-align: center;"/>$$\vec{w}_{1}^{\left[ 1 \right]}=\left[ \begin{array}{c}    1\\    2\\\end{array} \right] ,\vec{w}_{2}^{\left[ 1 \right]}=\left[ \begin{array}{c}    -3\\    4\\\end{array} \right] ,\vec{w}_{3}^{\left[ 1 \right]}=\left[ \begin{array}{c}    5\\    -6\\\end{array} \right] $$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([<br>    [<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,<span class="hljs-number">5</span>],<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,-<span class="hljs-number">6</span>]<br>]) <br>b = np.array([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>a_in = np.array([-<span class="hljs-number">2</span>,<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>Implement the coding of dense function with python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in,W,b,g</span>):<br>    <span class="hljs-comment">#units equals to the numbers of cols of W</span><br>    units = W.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment">#Create a matrix with the same size of units--[0,0,0]</span><br>    a_out = np.zeros(units)<br>    <span class="hljs-comment">#compete the g(z)</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x,W1,b1)<br>    a2 = dense(a1,W2,b2)<br>    a3 = desne(a2,W3,b3)<br>    a4 = dense(a3,W4,b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br>    <br>    <br></code></pre></td></tr></table></figure><h3 id="Choose-activation-function"><a href="#Choose-activation-function" class="headerlink" title="Choose activation function"></a>Choose activation function</h3><p>Three types activation function:</p><img src="/img/fig/4.3.jpg" alt="s" style="text-align: center;" /><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">activation=<span class="hljs-string">&quot;linear&quot;</span><br>activation=<span class="hljs-string">&quot;sigmoid&quot;</span><br>activation=<span class="hljs-string">&quot;relu&quot;</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>type y</th><th>0&#x2F;1</th><th>+&#x2F;-</th><th>0&#x2F;+</th></tr></thead><tbody><tr><td>sigmoid</td><td>binary classfication</td><td></td><td></td></tr><tr><td>linear</td><td></td><td>regression</td><td></td></tr><tr><td>relu</td><td></td><td></td><td>regression</td></tr></tbody></table><p>[why we need activation function?]</p><p>If we use the linear activation function in  hidden layers all the time, the predict results of neutral network equal to linear regression. that&#96;s why we should use sigmoid or relu function as the activation function to build the network.</p><h3 id="Soft-max-regression"><a href="#Soft-max-regression" class="headerlink" title="Soft-max regression"></a>Soft-max regression</h3><p>Softmax function has been used to solve the  multi-class problem. Essentially, it is still a <strong>classification problem</strong> <code>based on probability</code>. The expression of Softmax function as follow:<br>$$<br>z_j&#x3D;\vec{w}_j·\vec{x}+b_j\<br>a_j&#x3D;\frac{e^{z_j}}{\underset{k&#x3D;1}{\overset{n}{\varSigma}}e^{z_k}}&#x3D;P\left( y&#x3D;j|\vec{x} \right)<br>$$<br>a_j is the possibility of predict result.</p><p>The cost function of Soft-max function is:<br>$$<br>a_N&#x3D;\frac{e^{Z_N}}{e^{Z_1}+e^{Z_2}+\cdot \cdot \cdot +e^{Z_N}}&#x3D;P\left( y&#x3D;N|\vec{x} \right)<br>$$</p><p>$$<br>loss\left( a_1,…,a_N,y \right) &#x3D;\begin{cases}<br>    -\log a_1,,if,,y&#x3D;1\<br>    -\log a_2,,if,,y&#x3D;2\<br>    \vdots\<br>    -\log a_n,,if,,y&#x3D;n\<br>\end{cases}<br>$$<br>The result is <code>one of</code> the loss functions. </p><h3 id="Output-of-the-soft-max"><a href="#Output-of-the-soft-max" class="headerlink" title="Output of the soft-max"></a>Output of the soft-max</h3><img src="/img/fig/4.5.jpg" style="text-align: center;" /><p>The outcome of soft-max classification is multiply. Every outcome will be competed a score to predict the right answer. </p><p>Carefully, the loss function we must choose the <code>SparseCategoricalCrossentropy</code>.<br>$$<br>a_{N}^{\left[ l \right]}&#x3D;\frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>\<br>-\log a_{N}^{\left[ l \right]}\ne -\log \frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>$$<br>The difference of competing order can lead to the different outcome, which has different model accurancy.</p><h3 id="Improve-the-soft-max-model"><a href="#Improve-the-soft-max-model" class="headerlink" title="Improve the soft-max model"></a>Improve the soft-max model</h3><p>In order to improve the accuracy of calculations, we have made the following improvements to the model algorithm:</p><ul><li>In soft-max layer, we adopt the <code>linear</code> function as the activation.</li><li>In the process of compiling model, we add a parameter to improve the accurancy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#linear function as the activation function in the soft-max layer</span><br>model = sequential([<br>    Dense(units = <span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>)<br>])<br><span class="hljs-comment">#add parameter:from_logits=True </span><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br>logit = model(X)<br>f_x = tf.nn.sigmoid(logit)<br></code></pre></td></tr></table></figure><h3 id="MNIST-Adam"><a href="#MNIST-Adam" class="headerlink" title="MNIST Adam"></a>MNIST Adam</h3><h2 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h2><h3 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h3><p>Model fits the training data well(over-fit) but will fail to generalize to new examples not in the training set.</p><p>Hence, we need to <strong>partition the dataset</strong> to test the accurancy of the model.</p><p>The data set was divided into <code>test set</code> and <code>train set</code>.</p><p>【regression】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m_{train}}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{test}}\left[ \underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) -y</em>{test}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>compute train error:</p><p>$$<br>J_{train}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{train}}\left[ \underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) -y</em>{train}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>【train】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;-\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left[ y^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) \right] +\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2}<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}&#x3D;-\frac{1}{m_{test}}\underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left[ y_{test}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) \right)<br>+\left( 1-y</em>{test}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{test}^{\left( i \right)} \right) \right) \right]<br>$$</p><p>compute train error:</p><p>$$<br>-\frac{1}{m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left[ y_{train}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) \right) +\left( 1-y</em>{train}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{train}^{\left( i \right)} \right) \right) \right]<br>$$</p><h3 id="Model-selection-and-cross-validation"><a href="#Model-selection-and-cross-validation" class="headerlink" title="Model selection and cross validation"></a>Model selection and cross validation</h3><p>If we want to fit a function to predict a problem or classification, we often use test error $J_{test}$ to judge the accurancy of the model. But, the J test is likely to be an <code>optimistic estimate</code> of generalization error. Because, when we choose the degree of parameter d in polynomial fit.,This fit of J test may lower than the actual estimate. The optimistic estimate can lead to a low score of J_test.</p><p>So, we need to partition the dataset as three parts to avoid optimistic estimate:</p><ul><li>training set</li><li>cross validation set</li><li>test set</li></ul><p>compute cross validation error:</p><p>$$<br>J_{cv}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{cv}}\left[ \underset{i&#x3D;1}{\overset{m_{cv}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{cv}^{\left( i \right)} \right) -y</em>{cv}^{\left( i \right)} \right) ^2 \right]<br>$$</p><h3 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h3><p>Have idea-Train model-Diagnose bias and variance</p><p> $J_{train}$ reflects bias, and  $J_{cv}$ reflects variance. A perfect model has low  $J_{train}$ and low  $J_{cv}$.</p><p>As the increasing of degree of d, $J_{train}$ will typically go down. Meanwhile, $J_{cv}$ will also go down and then it will increase.$J_{cv}$ will have a min value for different degree of d.</p><img src="/img/fig/4.6.jpg" alt="s" style="text-align: center;" /><p>How do you tell if our algorithm has a bias or variance problem?</p><ul><li>High bias(under fit): $J_{train}$ will be high($J_{train}\approx J_{cv}$)</li><li>High variance(over fit): $J_{cv}$&gt;&gt;$J_{train}$($J_{train}$ may be low)</li><li>High bias and High variance $J_{train}$ will be high and $J_{cv}$&gt;&gt;$J_{train}$</li></ul><h3 id="Regularization-and-bias-variance"><a href="#Regularization-and-bias-variance" class="headerlink" title="Regularization and bias&#x2F;variance"></a>Regularization and bias&#x2F;variance</h3><p>A large neutral network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_1 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&quot;sigmoid&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>model = Sequential([layer_1,layer_2,layer_3])<br></code></pre></td></tr></table></figure><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>When we increase the size of training set, the train error will increase. But, the error of cross validation will decrease. As the increasing of sample points. a regression function(a line or a curve) cannot fit all the point.</p><img src="/img/fig/4.7.jpg" style="text-align: center;" /><p>If a algorithm suffers from high variance, getting more training data is <strong>likely</strong> to help.</p><p>If a algorithm suffers from high bias, getting more training data <strong>will not</strong> help much.</p><p>【Debugging algorithm】</p><p>we have implemented the regularized linear regression:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><table><thead><tr><th align="center">operation</th><th>what should do</th></tr></thead><tbody><tr><td align="center">get more training examples</td><td>fixes high variance</td></tr><tr><td align="center">try smaller sets of features</td><td>fixes high variance</td></tr><tr><td align="center">try getting additional features</td><td>fixes high bias</td></tr><tr><td align="center">try adding polynomial features</td><td>fixes high bias</td></tr><tr><td align="center">try decreasing  $\lambda$</td><td>fixes high bias</td></tr><tr><td align="center">try increasing  $\lambda$</td><td>fixes high variance</td></tr></tbody></table><p><code>Trade-off</code></p><p>Simple model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+b$) will get high bias VS complex model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+w_3x^3+w_4x^4+b$) will get high variance.</p><img src="/img/fig/4.8.jpg" style="text-align: center;" /><h3 id="Cycle-of-machine-learning"><a href="#Cycle-of-machine-learning" class="headerlink" title="Cycle of machine learning"></a>Cycle of machine learning</h3><p>The cycle of ML process:</p><img src="/img/fig/4.9.jpg" style="text-align: center;" /><img src="/img/fig/4.10.jpg" style="text-align: center;" /><p>How to apply the ML model to solve the actual problem in software engineering design?</p><img src="/img/fig/4.11.jpg" alt="s" style="text-align: center;" /><p>ML model is collected in the inference server. we use mobile app through API call to achieve these function.</p><h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><strong>Precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.</p><p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that were retrieved. </p><p>We design a <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><p>In the trade-off  between Precision(P) and Recall(R), we use F1 score to evaluate the efficiency about the model.</p><p>the trade-off  between Precision(P) and Recall(R) has shown in the figure:</p><img src="/img/fig/4.13.jpg" style="text-align: center;" /><p>$$<br>F1&#x3D;\frac{1}{\frac{1}{2}\left( \frac{1}{P}+\frac{1}{R} \right)}&#x3D;\frac{2PR}{P+R}<br>$$</p><h2 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h2><p>The structure of a decision tree:</p><img src="/img/fig/5.1.jpg" style="text-align: center;" /><h3 id="Methods-chosen"><a href="#Methods-chosen" class="headerlink" title="Methods chosen"></a>Methods chosen</h3><p>For Signal Decision tree, we should focus on the problem is that the data features.</p><p>If the data is  discrete(just like 0 or 1), we can build the Signal Decision tree model. But,a row data may includes more than two classes, in this situation we should use <code>one-hot encoding</code>.</p><blockquote><p>one-hot encoding only fit for the decision tree model.</p></blockquote><p>If the data has continuous data(not only just like 0 or 1), we should split on a continuous variance.</p><p>For Multiple trees, we can use <strong>Random Forest</strong> and <strong>XGboost</strong> algorithm to solve.</p><img src="/img/fig/5.3.jpg" style="text-align: center;" /><h3 id="Purity-entropy"><a href="#Purity-entropy" class="headerlink" title="Purity(entropy)"></a>Purity(entropy)</h3><p>$p_{1}$ &#x3D; fraction of examples that are True.</p><img src="/img/fig/5.4.png" style="text-align: center;" /><p>$$<br>H\left( p_1 \right) &#x3D;-p_1\log \left( p_1 \right) -p_0\log \left( p_0 \right)<br>\<br>&#x3D;-p_1\log \left( p_1 \right) -\left( 1-p_1 \right) \log \left( 1-p_1 \right)<br>$$</p><img src="/img/fig/5.5.jpg" style="text-align: center;" /><p>In this figure, $w^{left}$&#x3D;2&#x2F;5、 $w^{right}$&#x3D;3&#x2F;5、$p_{1}^{left}$&#x3D;5&#x2F;10、$p_{2}^{left}$&#x3D;5&#x2F;10.</p><p>Information Purity</p><p>$$<br>Information Purity &#x3D;H\left( p_{1}^{root} \right) -\left( w^{left}H\left( p_{1}^{left} \right) +w^{right}H\left( p_{1}^{right} \right) \right)<br>$$</p><p>We should choose the <code>max</code> value of the Information Purity to <strong>recursive</strong> the decision tree model, which is called <code>Information Gain</code>.</p><p>In the process of split on a continuous variance(<strong>Regression tree</strong>), we also choose the max decreasing variance result as a good fit model.</p><img src="/img/fig/5.2.jpg" alt="s" style="text-align: center;"/><p>The purity of regression tree(equal to information gain):</p><p>$$<br>D&#x3D;V^{root}-\left( w^{left}V^{left}+w^{right}V^{right} \right)<br>$$</p><p>V instead of <code>variance</code>.</p><h3 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h3><ul><li><p>Start with all examples at the root node</p></li><li><p>Calculate information gain for all features, and pick the one with the highest information gain</p></li><li><p>Split dataset according to selected features, and create left and right branches of the tree</p></li><li><p>Keep repeating splitting process until stopping criteria is met:</p><pre><code class="hljs">      <figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">when a node <span class="hljs-keyword">is</span> <span class="hljs-number">100</span>% one <span class="hljs-keyword">class</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">splitting</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">will</span> <span class="hljs-symbol">result</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">tree</span> <span class="hljs-symbol">exceeding</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">maximum</span> <span class="hljs-symbol">depth</span><br><br><span class="hljs-symbol">Information</span> <span class="hljs-symbol">gain</span> <span class="hljs-symbol">from</span> <span class="hljs-symbol">additional</span> <span class="hljs-symbol">splits</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">less</span> <span class="hljs-symbol">than</span> <span class="hljs-symbol">threshold</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">number</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">examples</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">below</span>  <span class="hljs-symbol">a</span> <span class="hljs-symbol">threshold</span><br></code></pre></td></tr></table></figure></code></pre></li></ul><h2 id="Decision-tree-VS-Neutral-network"><a href="#Decision-tree-VS-Neutral-network" class="headerlink" title="Decision tree VS Neutral network"></a>Decision tree VS Neutral network</h2><h3 id="Decision-tree-1"><a href="#Decision-tree-1" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul><li>Works well on tabular(structured) data</li><li>Not recommended for unstructured data(images,audios,text)</li><li>Small decision tree may be human interpretable</li></ul><h3 id="Neutral-network"><a href="#Neutral-network" class="headerlink" title="Neutral network"></a>Neutral network</h3><ul><li>Works well on all types of data,including tabular(structured) data and unstructured data(images,audios,text)</li><li>May be slower than decision tree</li><li>Works with transfer learning</li><li>When building a system of multiple models working together, it might be easier to string together multiple neutral network</li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>本科毕业论文致谢</title>
    <link href="/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
    <url>/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>时光匆匆，转眼间四年的本科生活即将接近尾声。在此，请允许我以一篇致谢，表达在这四年时光里最真挚的谢意！</p><p>感谢论文指导老师杨国庆教授的悉心指导。从2021年参加数学建模比赛与老师结缘，杨老师严谨的治学态度和科学的研究方法给了我极大的帮助和影响，由衷感谢杨老师对我的关心和指导！</p><p>同时，由衷感谢教过我的河北大学国际学院信管专业全体ISEC教师：杨秀丹老师、吴树芳老师、徐杰老师、史海燕老师、郝杰老师、史江兰老师、贾金英老师、宇文姝丽老师、郭海玲老师、崔广志老师、吴利明老师、韩倩老师、陈婷老师、张鑫老师(排名不分先后)，以及其他全部科任老师。</p><p>感谢我的教练吕旭老师。大一期间将我纳入冰雪运动队，从此便与体育运动结下不解之缘。吕旭老师积极的心态与干练的作风对我产生了深远影响。从教练身上，我也更加懂得了什么是责任，什么叫标杆。</p><p>感谢我的辅导员曹永姝老师。四年的本科生活，曹老师对我照顾有加，老师的隐忍与坚持对我产生了很大的影响。她了解我的脾气与性格，尊重我的选择。</p><p>感恩在河北大学遇到的老师们，师恩天大，永记心间。</p><p>感谢我的父亲杨海龙先生、母亲刘秋菊女士。二十四年的成长之路，你们敢于放手，让我自由生长、大胆试错。每当我面临人生抉择的时候，你们都是那样义无反顾地支持我的选择。</p><p>感谢我的舅舅刘运陶先生，他对于我的成长是特别的。舅舅是我们家族中的第一位本科生、研究生，他潜移默化地培养了我的阅读习惯、学习习惯，并用他四十多年走过的弯路为我规避错误，他一直是我的榜样。</p><p>感谢我的姥姥邵长芬女士、姥爷刘增才先生。从四岁上幼儿园开始我便离开父母跟随姥姥、姥爷生活，直到我十八岁离开小镇去沧州市第一中学读高中。十四年的时光大部分都是和姥姥、姥爷度过的，姥姥、姥爷的勤劳、隐忍、坚韧、厚道对我的性格塑造影响至深。感谢我的奶奶刘绪巧女士、爷爷杨春林先生。2017年的仲夏，我二叔的车祸离世对两位老人以及全家人造成了沉痛打击，我曾一度认为两位老人会无法走出晚年丧子之痛。但是时过境迁，二老以顽强不屈、坚忍不拔的精神面貌给儿孙们以希望。</p><p>感恩我的家人们，让我有足够的勇气去面对生活中的任何挫折与苦难。</p><p>最后，感谢这四年的自己。前路漫漫，未来还会有很长的路要走，还会有更多的挑战、磨难需要去面对，还会有更多的责任需要去承担。会遇到很多人，会经历更多的事。但是无论怎样，请不要丢掉良心和理想，都不要忘记抽时间回忆回忆在河北大学这四年的美好时光。加油，祝好！</p><p>感谢百忙之中参加答辩的各位领导、老师们！</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell学习笔记</title>
    <link href="/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量定义的格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">变量名=变量值 #注意=左右两边不可以有空格<br><span class="hljs-meta prompt_">#</span><span class="language-bash">直接赋值</span><br>username=&quot;ygm&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">键盘赋值</span><br>read username<br><span class="hljs-meta prompt_">#</span><span class="language-bash">执行的命令结果赋值</span><br>str=$(pwd)<br>str=$(ll)<br>str=`ps -ef`<br></code></pre></td></tr></table></figure><p>变量的访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>echo $name<br>echo $echo&#123;name&#125;<br></code></pre></td></tr></table></figure><p>特殊变量</p><table><thead><tr><th>变量名</th><th>定义</th></tr></thead><tbody><tr><td>$#</td><td>命令行参数的个数</td></tr><tr><td>$?</td><td>前一个命令或函数的返回码</td></tr><tr><td>$n</td><td>$1表示第一个参数</td></tr><tr><td>$0</td><td>当前程序的名称</td></tr><tr><td>$*</td><td>以“参数1，参数2···”保存所有参数</td></tr></tbody></table><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>结论：推荐编程的时候使用双引号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">str=hello<br>str=`hello`<br>str=&quot;hello&quot;<br><br>str2=&#x27;I am $&#123;str&#125;&#x27;#单引号不会解释字符串里面的变量<br>str2=&quot;I am $&#123;str&#125;&quot;#双引号可以解释字符串里面的变量<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出字符串的长度</span><br>echo $&#123;#name&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">提取子字符串</span><br>echo $&#123;string:a:b&#125;#从索引(索引从0开始)为a个位置开始截取长度为b的子字符串<br></code></pre></td></tr></table></figure><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><p>算数运算在shell中要遵守严格的规范格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo `expr 2 + 3`#必须是反引号包裹、加号与数字之间留有空格<br>echo `expr 2 \* 3`#注意乘法和除法必须使用转义符号\前缀才会起作用<br>echo `expr 2 \% 4`<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此外，算数运算还可以用$(())和$[]表示-推荐方式</span><br>a=2<br>b=3<br>echo $((a+b))<br>echo $(($a+$b))<br>echo $[a+b]<br>echo $[$a+$b]<br></code></pre></td></tr></table></figure><h3 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h3><p>数字比较</p><ul><li>-eq：比较两个数是否相等，相等返回true</li><li>-ne：比较两个数是否不想等，不想等返回true</li><li>-gt：检测左边的数是否大于右边，若是返回true</li><li>-lt：检测左边的数是够小于右边，若是返回true</li><li>-ge：检测左边的数是否大于等于右边，若是返回true</li><li>-le：检测左边的数是否小于等于右边，若是返回true</li></ul><p>字符串比较：</p><ul><li>-z STRING：字符串长度为0</li><li>-n STRING：字符串长度不为0</li><li>&#x3D;：判断字符串长度是否相等</li><li>！&#x3D;：判断字符串长度是否不想等</li></ul><p>文件：</p><p>-f：存在且普通的文件</p><p>-e：文件存在</p><p>-d：存在且是目录</p><p>-h：存在且是链接</p><p>-r：存在且是只读</p><p>-w：存在且是可写</p><p>-x：存在且是可执行</p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组用小括号表示，中间元素用空格隔开，也可以直接定义数组中的每个元素的值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">array=(1 2 &quot;hello&quot; ygm)<br>array[4]=&quot;xyc&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取数组元素</span><br>echo $&#123;array[index]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取整个数组</span><br>echo $&#123;array[*]&#125;<br>echo $&#123;array[@]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">获取数组长度</span><br>echo $&#123;#array[*]&#125;<br><br></code></pre></td></tr></table></figure><h3 id="shell命令"><a href="#shell命令" class="headerlink" title="shell命令"></a>shell命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">逻辑运算符&amp;&amp;和||<br><br>&amp;&amp; 表示与，|| 表示或<br>二者具有短路原则：<br>expr1 &amp;&amp; expr2：当expr1为假时，直接忽略expr2<br>expr1 || expr2：当expr1为真时，直接忽略expr2<br>表达式的exit code为0，表示真；为非零，表示假。（与C/C++中的定义相反）<br><br></code></pre></td></tr></table></figure><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><p>if判断语句范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">if condition<br>then <br>···<br>else<br>···<br>fi<br><br>if[ &quot;a&quot; -lt &quot;b&quot; ] &amp;&amp; [ &quot;a&quot; -gt 2]<br>then <br>echo $&#123;a&#125;在范围内<br>fi<br><br>if[ $a -eq 2]<br>then <br>echo $&#123;a&#125;等于2<br>elif [ $a -eq 3]<br>then<br>echo $&#123;a&#125;等于3<br>else<br>echo 其他<br>fi<br></code></pre></td></tr></table></figure><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><p>for范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">for v in var1 var2 var3<br>do<br>echo $v<br>done<br><br>for flie in `ls`<br>do <br>echo $file<br>done<br><br>for i in $(seq 1 10)<br>do<br>echo $i<br>done<br><br>for ((i = 1;i&lt;10;i++))<br>do<br>echo $i<br>done<br></code></pre></td></tr></table></figure><p>while循环范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">while read name<br>do <br>echo $name<br>done<br><br>until [&quot;$&#123;word&#125;&quot; == &quot;yes&quot;] || [&quot;$&#123;word&#125;&quot; == &quot;YES&quot;]<br>do <br>read -p &quot;please input yse or YES to stop this program:&quot; word<br>done<br></code></pre></td></tr></table></figure><p>PS3使用方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;what is your favourite OS?&quot;<br>PS3=&quot;please enter your chose:&quot;<br>select var in &quot;linux&quot; &quot;windowns&quot; &quot;unix&quot;<br>do<br>break;<br>done<br>echo &quot;you have selected $var&quot;<br></code></pre></td></tr></table></figure><h2 id="一键安装JDK"><a href="#一键安装JDK" class="headerlink" title="一键安装JDK"></a>一键安装JDK</h2><p>使用shell脚本实现自动化部署</p><p>jdk_install.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">提示安装jdk</span><br>echo &quot;开始安装jdk&quot;<br>sleep 1<br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除自带的jdk</span><br>oldjdk=$(rpm -qa | grep jdk)<br>for old in $&#123;oldjdk&#125;<br>do<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-variable">$old</span></span><br>rpm -e --nodes $old<br>done<br><span class="hljs-meta prompt_">#</span><span class="language-bash">创建安装目录</span><br>java_path=$(/export/server)<br>if[ !-d $java_path]<br>then<br> mkdir -p $java_path<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">解压jdk安装包</span><br>tar -zxvf /export/softwore/jdk-8u241-linux-x64.tar.gz -C $java_path<br><span class="hljs-meta prompt_">#</span><span class="language-bash">设置环境变量</span><br>JAVA_HOME=&quot;/export/server/jdk1.8.0_241&quot;<br>grep &quot;JAVA_HOME&quot; /etc/profile<br>if[ #? -ne 0]<br>then<br><span class="hljs-meta prompt_">#</span><span class="language-bash">JAVA_HOME</span><br>echo &quot;---------JAVA_HOME-----------&quot;<br>echo `export JAVA_HOME=/export/server/jdk1.8.0_241` &gt;&gt; /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">PATH</span><br>echo &quot;----------PATH-----------&quot;<br>echo `export PATH=:$JAVA_HOME/bin:$PATH` &gt;&gt; /etc/profile<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">加载环境变量</span><br>sleep 1<br>source /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装完成提示</span><br>echo &quot;恭喜您jdk安装成功！&quot;<br>java -version<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>云计算</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cloud computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/08/05/hello-world/"/>
    <url>/2024/08/05/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! </p><p>值得纪念的日子，今天把之前的博客内容全部成功迁移到了新的平台上面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; welcome to hexo! &lt;&lt; endl;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>张宇智慧箴言</title>
    <link href="/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/"/>
    <url>/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>宝剑锋从磨砺出，梅花香自苦寒来</p>          </div><h2 id="别害怕"><a href="#别害怕" class="headerlink" title="别害怕"></a>别害怕</h2><p>不要试图掌控自己的奋斗过程，完美自己的各项计划，你会失落的，因为事实上它们并不听你的，你越跟它较劲，它越不听话。<br>不过这没什么，每一个成功，都是源于一段不完美的甚至是很狼狈的奋斗。<br>你只要，摈弃杂念，放下功利，全神贯注，尽力就好，学不完又怎样，考的上就行。好孩子，听话。</p><h2 id="拼搏"><a href="#拼搏" class="headerlink" title="拼搏"></a>拼搏</h2><p>年轻的时候，闯一闯，拼一拼，不要怕什么。实在扛不住了，哭一哭，喊一喊。大不了，回家。<br>不要把感情和情绪放在社会上、网络上，真正值得你在乎的，是表面上希望你出人头地，对你百般挑剔，但在心中却只为你祈求平安健康快乐的家人。<br>孩子们，听话，你逼自己的方法，可能不太对。放下包袱，放下功利，带着爱，带着对这个世界的好奇，去拼搏一下。累了，咱们回家就是了。<br><strong>健健康康，内心阳光</strong>，这八个字，可别弄丢了，丢了，会悔恨终身的。</p><h2 id="最大的敌人是自己"><a href="#最大的敌人是自己" class="headerlink" title="最大的敌人是自己"></a>最大的敌人是自己</h2><p>笛卡尔说，征服你自己，而不是征服世界。孩子们，晚安。</p><h2 id="嗜欲深者天机浅天机浅"><a href="#嗜欲深者天机浅天机浅" class="headerlink" title="嗜欲深者天机浅天机浅"></a>嗜欲深者天机浅天机浅</h2><p>古有云，<strong>无欲则刚</strong>。心思繁杂，欲望太多，则易焦虑，徘徊，无己见，恐非做事之正道。孩子们，务必静心，放下功利，用心为之，方可成之。</p><h2 id="基本功"><a href="#基本功" class="headerlink" title="基本功"></a>基本功</h2><p>有人问基础阶段以什么标准来看自己是不是真的懂了，这个问题我前面也回答过，这里详细说几句。我在课上提到一个人，叫做费曼，他是诺贝尔物理学奖获得者，人们试图研究费曼为什么对问题的理解总是透彻深刻，深得其精髓要义，事实上，费曼自己提出过一个观点：把你所学到的理解到的概念性质和方法讲给一个比如小孩子听（这里是为了确保聆听的人对你所讲述的知之甚少），而且让他尽量听懂，这时你无法用那些复杂的专业性的表达来叙述，在寻找简单通俗易懂的语言的时候，你的大脑在不断的深化对于知识的认识，迫使自己在更深层次上理解它们，这样你一定会停滞在很多地方，而这些地方就是你没有理解的地方。通过努力，把你的知识传递给这个孩子并让他听懂，就是你真的懂了，这就是过关。<br>我前面说到，你一定要会复述我课上对知识的讲解，甚至比我讲的还要简要精炼，这是你迅速融入这个学科思维状态的最佳途径。各位，我再写一会书，你们好好思考下我上面这段话，我是真的希望你们能够懂得如何学习，不要再功利的去卷了，真学东西，学真东西，学会怎么学，不仅为了考研，更为了自己的一生。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>前路漫漫亦灿灿</title>
    <link href="/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/"/>
    <url>/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>不要辜负这份好运气——写在顺利推免后读任正非先生<a href="https://baike.baidu.com/item/%E6%88%91%E7%9A%84%E7%88%B6%E4%BA%B2%E6%AF%8D%E4%BA%B2/6610022">《我的父亲母亲》</a>有感</p>          </div><p>2023年9月29日，下午14:50，故事开始了新的篇章。</p><p>从2023年2月15日离家返校，一直到10月1号，整整228天，家门未踏入一步。一直想带一个好的结果回家，经历磕磕绊绊，最终如愿以偿。</p><p>想说的东西、想表达的东西有好多，但却难以启齿、无从下笔。</p><p>回首这段岁月，感谢自己没有放弃自己，感谢暑假留校踏踏实实读书学习的沉淀。暑假留校两个月，自己在图书馆五楼靠窗户占了一整张桌子，五楼人烟稀少，八月份安安静静在那里度过了一个月充实的时光。早上八点半开馆准时到达，晚上在漆黑的五楼写读书笔记。如今再进入五楼，往事历历如过眼云烟般浮现在眼前。上一次有这种感觉，还是因为19年底口罩事情高三开学延期，20年初居家复习的岁月。这两段时光交织在一起，给我的反思就是，要想做成一件事情，首先要把自己调整成静音模式，踏踏实实忘我般的投入进去。想成事，不是每天炸炸呼呼，一天八条朋友圈，恨不能别人不知道我干啥。而是要沉淀、要安静、要忍耐、要坚持。</p><p>人性的弱点之一就是<strong>只看结果，不注重过程</strong>。</p><p>你考上研了，考上公了，周围都是欢呼、赞美之词。”牛逼、大佬、交给朋友吧…”诸如此类的话，会让你沉迷享受，会让你飘飘然忘乎所以。但是，当你没考上、没惊起波澜，周围人并不会关注，甚至会有讥讽、嘲笑不绝于耳。”他不行，他光玩，他就不是那块料…”。所以，自强不息、厚德载物，永远都是法宝，让自己强大起来，让自己内心强大起来，才会在面对这些情况的时候，做到多一份的从容、淡定与坦然。</p><p>带着好结果回家过节，家人们由衷地为我感到高兴、自豪。感谢家人，没有家人的理解与支持，我没有勇气走好今天的路。</p><p>我的父亲杨海龙，1976年出生于河北省沧州市一个普普通通的农村家庭。小时候学习成绩不错，凭借自己的努力一直读到了高中毕业。中考成绩很好，但是因为家里穷要继续供给我二叔、三叔读书。于是，父亲放弃了去县里读高中的机会，选择了在镇上完成学业，很多年前，他仍然自嘲:“以我当年的成绩，要是去县里上，考过河北工业绰绰有余”。高中毕业后在我二爷家工厂里工作，然后和爷爷、叔叔们创业。</p><p>我的母亲刘秋菊，1978年出生于河北省沧州市一个普普通通的农村家庭。小的时候学习成绩一般，肯定没有我父亲学习好，想考中专，结果也是为了弟弟妹妹的学业而放弃了自己的梦想。初中毕业后进入农村信用社当会计，后来在我姥爷的厂子里当会计，一干就是半辈子。母亲是一个聪明的人，积极、阳光、热爱学习，她算账、算数没出过错，爱写读书笔记，有阅读习惯，这些都润物细无声般影响着我。</p><p>两个有着相同命运路径的人走到了一起，养育了我、培养了我。特别是，今年因为面试忙于各个城市奔波，报辅导机构，发paper。以上种种，当我需要钱的时候，老父亲总是那么斩钉截铁地转给我，让我没有任何顾虑。钱是好东西，而且钱是干净的，脏的是人心。走正道，好好赚钱、多赚钱永远都是幸福的。</p><p>这些年，二叔的去世一直也是我前进的动力。我很少和外人去谈我二叔因为车祸去世的事情，我认为这是家族之殇。我作为家族中的长子长孙，在我17岁那年，他的死，使我对我的家庭、对我自己有了新的理解。挫折都是发人深省的，我庆幸自己没有被生活的重创打垮，我也庆幸，家里的老人没有因为生活的苦难而使他们丧失对生活的希望。在经历苦难之后，我看到的依然是他们积极的生活态度，没有怨天尤人、没有一蹶不振，永远都给子女树立了好的榜样。其实，这就是我遇到无数困难后，依然有勇气面对的底气来源。</p><p>今年还要感谢张宇(宇爹)，不确定的日子里一直在通过备考高数舒缓。激发了我对数学的热情，重塑了学科的理解。</p><p>没有轻舟已过万重山，唯有前路漫漫亦灿灿。逝去的已经逝去，活着的仍要前行。</p>]]></content>
    
    
    <categories>
      
      <category>生活随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>致谢</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
