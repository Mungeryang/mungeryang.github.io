<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025年3月阅读书目与摘录反思</title>
    <link href="/2025/03/24/2025%E5%B9%B43%E6%9C%88%E9%98%85%E8%AF%BB%E4%B9%A6%E7%9B%AE%E4%B8%8E%E6%91%98%E5%BD%95%E5%8F%8D%E6%80%9D/"/>
    <url>/2025/03/24/2025%E5%B9%B43%E6%9C%88%E9%98%85%E8%AF%BB%E4%B9%A6%E7%9B%AE%E4%B8%8E%E6%91%98%E5%BD%95%E5%8F%8D%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<p>2025年3月24日，星期一，晴天☀️，第55篇博客。</p><p>自从买了汉王的Clear6便对墨水屏爱不释手了，开通了微信读书会员，感觉以后会在买纸质书上省下一大笔开销。2025年争取把买汉王阅读器的钱读回来，先读699块钱的书再说，加油，坚持！✊</p><h2 id="3月读完"><a href="#3月读完" class="headerlink" title="3月读完"></a>3月读完</h2><ul><li>《沧浪之水》</li><li>《深度学习入门：基于Python的理论与实践》</li><li>《征服市场的人》</li><li>《高质量读研：教你如何写论文、做科研》</li></ul><h2 id="3月在读"><a href="#3月在读" class="headerlink" title="3月在读"></a>3月在读</h2><ul><li><p>《大明王朝1566》</p></li><li><p>《富兰克林自传》</p></li><li><p>《云计算(典藏版)》</p></li><li><p>《在线》(重读二刷)</p></li><li><p>《深度学习进阶：自然语言处理》</p></li></ul><h2 id="书摘总结"><a href="#书摘总结" class="headerlink" title="书摘总结"></a>书摘总结</h2><h3 id="《大明王朝1566》"><a href="#《大明王朝1566》" class="headerlink" title="《大明王朝1566》"></a>《大明王朝1566》</h3><p>寒假在家里先把电视剧版《大明王朝1566》刷了一遍，回学校后又在墨水屏上读了刘和平老师《大明王朝1566》的电纸书。</p><p>[吕芳]  什么叫‘三思’?‘三思’就是‘思危、思退、思变’！知道了危险就能躲开危险，这就叫‘思危’；躲到人家都不再注意你的地方这就叫‘思退’；退了下来就有机会，再慢慢看，慢慢想，自己以前哪儿错了，往后该怎么做，这就叫‘思变’！”</p><p>[胡宗宪] 平时叫你读《左传》《通鉴》，你不以为然，叫你读一读王阳明的书，你更不以为然。还说什么‘半部《论语》可治天下！’现在我问你，孔子说的‘知不可为而为之’是什么本意！孔子是告诉世人，<strong>做事时不问可不可能，但问应不应该！</strong>毁堤淹田，伤天害理，上误国家，下害百姓，也叫‘知不可为而为之’吗！</p><p>[杨金水] 有些事不上秤没有四两，上了秤一千斤也打不住。</p><h3 id="沧浪之水——鄙视迟大为、理解池大为、追随池大为"><a href="#沧浪之水——鄙视迟大为、理解池大为、追随池大为" class="headerlink" title="沧浪之水——鄙视迟大为、理解池大为、追随池大为"></a>沧浪之水——鄙视迟大为、理解池大为、追随池大为</h3><ul><li>沧浪之水清兮，可以濯吾缨；沧浪之水浊兮，可以濯吾足。</li><li>人生在世，就是跟世界打交道，口说无凭，都是泡沫，有东西才是真的。</li></ul><p>[原文] 在极度的痛苦中，经过了许多辗转反侧之夜，我意识到许小曼并不是属于我的，也许她现在也从浪漫而伟大的牺牲激情中省悟过来。毕竟，我们的血管里流着的是不同的血。事情就这么过去了，汪贵发等人模糊而明确地说着刻毒的话，我都装着听不懂，忍了，忍了。父亲当年不也是这么忍过来的？我还是感到了一点轻松，一点安慰，平民也可以坚守自己心灵的高贵。</p><p>[自评] 穷人最后的一丝精神上的高傲。作为读者我一直在反思：贫富差距所带来的认知上的差别，真的不能让人进行正常的沟通交流吗?固执的置气挽回的一丝尊严所带来的轻松、快感与抛开身份彻底的沟通到底哪个方式更划算。或许真当自己设身处地，那一丝面子和尊严也能就已经把你压死，你说不出口，她也听不进去。</p><p>[原文] 一个人是怎样的人，别人的眼都是雪亮的。有这点雪亮，这点理解，做个好人就并不吃亏，人间自有公道。</p><p>[自评] 立场和利益很多时候要大于为人和感情。徐在大为面前没有利益相关性，而且现阶段的立场和价值观相近，池本身也是善良实在的人。反观丁，只会紧跟能帮助自己的人，这么做没有对错之分，对于丁也要学着去理解，因为丁代表生活中的大多数，没准就是未来的你和我，谁也不用装孙子。</p><p>[原文]  庄子曾说到过两只龟，一只钻在污泥里，一身腥臭，可它是活的，一只死了被供在庙堂上，供帝王占卜之用，你说你愿钻在污泥里还是供在庙堂上？污泥里就不要说脸不脸了，一身腥臭还谈脸。</p><p>[锐评] 乌龟何来腥臭？腥臭只是人的感觉，管乌龟什么事？！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SmallCap Source Code</title>
    <link href="/2025/03/15/SmallCap-Source-Code/"/>
    <url>/2025/03/15/SmallCap-Source-Code/</url>
    
    <content type="html"><![CDATA[<p>2025年3月15日，星期六，雨夹雪，第54篇博客。三一五打假日～</p><h1 id="SmallCap复现与代码设计细节分析——基于Cursor-Claude3-7-sonnet"><a href="#SmallCap复现与代码设计细节分析——基于Cursor-Claude3-7-sonnet" class="headerlink" title="SmallCap复现与代码设计细节分析——基于Cursor Claude3.7-sonnet"></a>SmallCap复现与代码设计细节分析——基于Cursor Claude3.7-sonnet</h1><p>项目源代码：<a href="https://github.com/RitaRamo/smallcap">https://github.com/RitaRamo/smallcap</a></p><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs markdown">smallcap/<br>├── .DS<span class="hljs-emphasis">_Store</span><br><span class="hljs-emphasis">├── .gitignore</span><br><span class="hljs-emphasis">├── README.md</span><br><span class="hljs-emphasis">├── SmallCap_</span>demo.ipynb        # 模型演示笔记本<br>├── coco-caption/              # 模型评估工具目录<br>├── data/                      # 数据目录<br>├── infer.py                   # 模型推理脚本<br>├── requirements.txt           # 项目依赖包<br>├── src/                       # 核心源代码目录<br>└── train.py                   # 模型训练脚本<br><br>核心代码：<br>src/<br>├── <span class="hljs-strong">__init__</span>.py<br>├── extract<span class="hljs-emphasis">_features.py        # 特征提取工具</span><br><span class="hljs-emphasis">├── get_</span>indexed<span class="hljs-emphasis">_caps.py        # 描述索引处理工具</span><br><span class="hljs-emphasis">├── gpt2.py                    # GPT-2模型实现</span><br><span class="hljs-emphasis">├── opt.py                     # OPT模型实现</span><br><span class="hljs-emphasis">├── retrieve_</span>caps.py           # 检索增强功能实现<br>├── template.txt               # 检索提示模板<br>├── utils.py                   # 通用工具函数<br>├── vision<span class="hljs-emphasis">_encoder_</span>decoder.py  # 视觉-文本模型架构<br>└── xglm.py                    # XGLM模型实现<br><br>模型评估工具目录：<br>coco-caption/<br>├── README.md<br>├── annotations/               # 评估标准描述<br>├── cocoEvalCapDemo.ipynb      # 评估演示笔记本<br>├── get<span class="hljs-emphasis">_stanford_</span>models.sh     # Stanford模型下载脚本<br>├── license.txt<br>├── pycocoevalcap/             # 评估指标实现<br>├── pycocotools/               # COCO工具集<br>└── run<span class="hljs-emphasis">_eval.py                # 评估主脚本</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">数据目录：</span><br><span class="hljs-emphasis">data/</span><br><span class="hljs-emphasis">└── image_</span>truck.jpg            # 示例图像<br></code></pre></td></tr></table></figure><h2 id="依赖流"><a href="#依赖流" class="headerlink" title="依赖流"></a>依赖流</h2><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>train.py → src&#x2F;utils.py → src&#x2F;vision_encoder_decoder.py → 语言模型(gpt2.py&#x2F;opt.py&#x2F;xglm.py)</p><h3 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h3><p>infer.py → src&#x2F;utils.py → src&#x2F;vision_encoder_decoder.py → 语言模型</p><h3 id="检索增强流"><a href="#检索增强流" class="headerlink" title="检索增强流"></a>检索增强流</h3><p>src&#x2F;retrieve_caps.py → src&#x2F;get_indexed_caps.py → 训练&#x2F;推理流程</p><h3 id="评估流"><a href="#评估流" class="headerlink" title="评估流"></a>评估流</h3><p>infer.py(生成描述) → coco-caption&#x2F;run_eval.py(计算指标)</p><h2 id="关键功能"><a href="#关键功能" class="headerlink" title="关键功能"></a>关键功能</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| 文件 | 功能 |<br><br>|------|------|<br><br>| train.py | 训练主脚本，包含参数配置和训练逻辑 |<br><br>| infer.py | 推理主脚本，载入模型并生成描述 |<br><br>| src/vision<span class="hljs-emphasis">_encoder_</span>decoder.py | 核心模型架构，结合视觉编码器和文本解码器 |<br><br>| src/retrieve<span class="hljs-emphasis">_caps.py | 实现检索相似图像描述功能 |</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">| src/extract_</span>features.py | 提取和缓存图像特征 |<br><br>| src/utils.py | 数据处理和辅助函数 |<br><br>| SmallCap<span class="hljs-emphasis">_demo.ipynb | 演示模型使用方法和效果 |</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">| coco-caption/run_</span>eval.py | 评估生成描述质量 |<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从SmallCap出发切入Image Caption</title>
    <link href="/2025/03/08/%E4%BB%8ESmallCap%E5%87%BA%E5%8F%91%E5%88%87%E5%85%A5Image-Caption/"/>
    <url>/2025/03/08/%E4%BB%8ESmallCap%E5%87%BA%E5%8F%91%E5%88%87%E5%85%A5Image-Caption/</url>
    
    <content type="html"><![CDATA[<p>2025年3月8日，星期六，晴天☀️，第53篇博客。今天是3·8妇女节，恭祝所有女神节日快乐！！</p><h1>SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation</h1><center>CVPR 2023</center><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ol><li><strong>研究问题:</strong> 这篇文章要解决的问题是<strong>如何在减少计算资源消耗的情况下</strong>，<strong>生成高质量的Image Caption</strong>。当前的图像描述模型通常需要大规模的数据和模型，导致预训练和微调的成本很高。</li><li><strong>研究难点:</strong> 该问题的研究难点包括：1)如何在减少模型参数的同时保持模型的性能；2)如何在不进行微调的情况下，使模型能够适应新的领域；3)如何有效地利用大规模的外部数据进行训练。</li><li><strong>相关工作:</strong> 相关工作包括基于大规模数据训练的图像描述模型，如LEMONHuge、SimVLMHuge和BLIPCapFilt-L等。此外，还有一些工作尝试通过冻结部分模型参数来减少训练成本，如ClipCap和I-Tuning。检索增强生成（Retrieval-Augmented Generation）是另一个相关的研究方向，但在图像描述任务中尚未得到充分探索。</li></ol><h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><p>这篇论文提出了<code>SMALLCAP</code>模型，用于解决图像描述任务中的<em>计算资源消耗</em>问题。具体来说，SMALLCAP通过<strong>检索增强</strong>和<strong>轻量化设计</strong>来实现高效的图像描述生成。</p><ul><li><p><strong>模型架构:</strong> SMALLCAP结合了预训练的<strong>CLIP视觉编码器和GPT-2语言解码器</strong>。CLIP编码器用于<strong>处理输入图像</strong>，生成一系列图像块嵌入。GPT-2解码器<strong>用于生成描述文本</strong>。为了减少训练参数，SMALLCAP仅在CLIP编码器和GPT-2解码器之间引入了新的跨注意力层，这些层的参数是可以训练的。</p></li><li><p><strong>检索增强:</strong> SMALLCAP通过在训练和推理过程中<strong>使用检索到的相关描述</strong>来<strong>增强生成能力</strong>。具体来说，对于输入图像，模型会从数据存储库中检索k个相关的描述，并将这些描述作为提示输入到解码器中。解码器根据图像特征和检索到的描述生成最终的图像描述。</p></li><li><p><strong>轻量化设计:</strong> 为了进一步减少训练参数，SMALLCAP通过调整跨注意力层的投影矩阵维度来控制可训练参数的数量。默认情况下，投影矩阵的维度设置为64，但可以根据需要进行调整。</p></li></ul><p><img src="/./img/fig/smallcap-model.png" alt="smallcap-model"></p><h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><ol><li><strong>数据收集:</strong> 实验使用了COCO数据集进行训练和评估。此外，还在nocaps数据集上进行了跨领域评估。</li><li><strong>实验设置:</strong> SMALLCAP的编码器和解码器分别初始化自CLIP-ViT-B&#x2F;32和GPT-2Base。模型仅训练跨注意力层的参数，其他参数保持冻结。训练使用AdamW优化器，初始学习率为1e-4，批量大小为64，训练10个epoch。</li><li><strong>检索机制:</strong> 在训练过程中，模型使用CLIP-ResNet-50x64编码器对输入图像和数据存储库中的描述进行编码，并通过余弦相似度进行最近邻搜索，检索k个相关描述。推理过程中，模型使用beam search解码生成描述。</li></ol><h2 id="结果与分析"><a href="#结果与分析" class="headerlink" title="结果与分析"></a>结果与分析</h2><ol><li><strong>COCO数据集结果:</strong> 在COCO数据集上的实验结果表明，SMALLCAP在仅有7M可训练参数的情况下，性能与其他轻量级模型相当，甚至在某些指标上超过了更大的模型。</li><li><strong>nocaps数据集结果:</strong> 在nocaps数据集上的跨领域评估中，SMALLCAP表现出色，显著优于其他轻量级模型，并且在某些情况下接近甚至超过了经过微调的大规模模型。</li><li><strong>检索增强的影响:</strong> 实验结果表明，检索增强对SMALLCAP的性能有显著提升。通过检索相关描述，模型能够更好地适应不同的领域和任务。</li></ol><h2 id="总体结论"><a href="#总体结论" class="headerlink" title="总体结论"></a>总体结论</h2><p>这篇论文提出了SMALLCAP模型，通过检索增强和轻量化设计，实现了高效的图像描述生成。实验结果表明，SMALLCAP在减少计算资源消耗的同时，保持了较高的性能，并且能够在不同领域之间进行有效的迁移。未来的研究可以进一步探索检索增强在其他多模态任务中的应用，并扩展检索数据的使用规模。</p><h2 id="论纲梳理"><a href="#论纲梳理" class="headerlink" title="论纲梳理"></a>论纲梳理</h2><p>五步法：</p><ol><li>分析标题</li><li>目录概览</li><li>先读结论</li><li>再读导论</li><li>重点章节阅读</li></ol><p>实用技巧：将所有读过的内容用自己的话复述出来，最后是写下来！</p><h3 id="标题分析"><a href="#标题分析" class="headerlink" title="标题分析"></a>标题分析</h3><blockquote><p>SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation</p></blockquote><p>主标题：SmallCap</p><p>副标题：Lightweight Image Captioning Prompted with Retrieval Augmentation</p><p>作者起这样一个标题意味着什么呢？Cap大概率要和Image Caption有关，加上Small形容词是想说明更小巧的模型吗？再看副标题，Lightweight Image Captioning意味着要进行<strong>轻量化</strong>；Lightweight Image Captioning Prompted with Retrieval Augmentation-利用<strong>检索增强</strong>功能提示轻量级图像标题。也就是说作者想通过使用<strong>检索增强</strong>技术来轻量化Image Caption的过程。</p><div class="note note-success">            <p>根据题目展开的可能联想：</p><p>思考1：为什么要轻量化？轻量化是不是由于之前的研究造成了计算资源消耗大、计算成本大、数据规模大造成的？(当然这是通过阅读Abstract了解到的)</p><p>思考2：计算资源消耗在哪里了？检索增强技术是如何减少计算资源消耗的？</p><p>思考2：探究如何在减少计算资源消耗的情况下，生成高质量的Image Caption。</p>          </div><h3 id="先读Conclusion，自己复述"><a href="#先读Conclusion，自己复述" class="headerlink" title="先读Conclusion，自己复述"></a>先读Conclusion，自己复述</h3><p>In this paper, we propose SMALLCAP, an image captioning model augmented with retrieval, which is light to train and can be transferred across domains without retraining. Results on the COCO dataset show that SMALLCAP is competitive to other lightweight-training models despite having substantially less trainable parameters, instead leveraging non-parametric information from a datastore of text. Out-of-domain evaluations show that SMALLCAP can also perform training-free domain transfer when given access to a datastore with target-domain data. Our model further benefits from diverse web and human-labeled data in addition to or in place of target-domain data. We find that SMALLCAP benefits not just from access to image captions, but also to video and audio captions (resources neglected in image captioning work in the past).</p><p>本文提出了一种新型轻量级Image Caption模型-<code>SmallCap</code>，训练方式简单且方便跨领域使用。由于利用了<strong>文本数据存储的非参数信息</strong>，使得可训练参数大大减少，但是并没有影响SmallCap在COCO数据集上的良好表现。如果可以接入包含目标域的数据库，SmallCap可以实现免训练跨领域转换。Web数据和人类标签数据会使SmallCap更加多元化。SmallCap模型还可以从video and audio captions中获益。</p><p>SMALLCAP’s small size and impressive performance in out-of-domain settings attest to the potential of retrieval augmentation as an alternative to the expensive training found in large pre-trained vision-and-language models and the costly finetuning that even previous lightweight-training models require in order to adapt to different image captioning datasets. Future work can apply our retrieval augmentation approach to a wider range of multimodal tasks, and further explore the scalability of the data used for retrieval.</p><p>SmallCap<strong>小巧的体积</strong>和<strong>良好的表现</strong>证明了检索增强技术可以代替昂贵的VLM预训练和先前轻量级训练模型的高开销微调过程。未来可以将检索增强的方法用到更广泛的多模态任务中去，探索用于检索数据的可扩展性。</p><div class="note note-success">            <p>一句话总结：本文提出了一种轻量级Image Caption模型——SmallCap，相比于使用高成本的VLM预训练和高开销的lightweight-training模型微调(相比于预训练和微调的成本持续增加)，SmallCap结合检索增强技术大大降低了训练成本并在COCO数据集上取得良好的表现。</p>          </div><h3 id="再读导论"><a href="#再读导论" class="headerlink" title="再读导论"></a>再读导论</h3><p>先进的Image Caption是通过在大规模数据集上的大模型训练完成的。规模的扩大导致了更高计算量的预训练和微调成本。不同模型版本所需的视觉域和终端用户的实际应用也不同。</p><p>ClipCap 和 I-Tuning模型都对降低成本做出了努力，它们主要是使用了<strong>现成的</strong>预训练视觉编码、解码器。模型参数被预先冻结，只<strong>训练两者之间的映射</strong>来完成Image Caption任务。参数规模确实减少了、速度也有所提高了。虽然模型易于管理起来，但是仍然<strong>不适合前文提到的实际应用</strong>，因为模型对于每个用例都需要单独训练。</p><p>SmallCap使得lightweight training, training-free domain transfer,and exploitation of large data in a training-free fashion(轻量级训练，无需训练的域转移，以及以无需训练的方式利用大数据)成为可能。</p><p>结合<strong>检索增强提示</strong>的SmallCap克服了一个关键限制：<em>需要明确的微调以使用到新的领域</em>，这种方式证明了在多模态任务中的潜力。</p><div class="note note-success">            <p>导论首先介绍了当前Image Caption领域发展的主要趋势：数据规模变大，计算成本持续增加。而先前的模型如ClipCap 和 I-Tuning确实对降本有明显的提高，但是增效方面并没有取得良好的性能，并不能领过适配多种落地任务，也就是说可以降本但无法增效。而SmallCap的提出克服了需要多次明确微调以应用到新领域的限制，实现了降本增效的效果。</p>          </div><h3 id="最后读重要章节"><a href="#最后读重要章节" class="headerlink" title="最后读重要章节"></a>最后读重要章节</h3><h4 id="model"><a href="#model" class="headerlink" title="model"></a>model</h4><p>编码器：CLIP-ViT-B&#x2F;32，解码器：$GPT-2_{Base}$</p><p>难点理解 - multi-head cross-attention layer：</p><p>编码器与解码器模型的运算在两个向量空间维度，所以要通过<strong>multi-head cross-attention layer</strong>将它们连接起来，使得解码器的每一层都关注编码器的输出。</p><p>Small降本增效的方法：</p><ol><li>通过冻结编码-解码器，只训练他们之间的随机初始化交叉住意层来降低计算开销并提高泛化。</li><li>通过交叉注意力层的的投影矩阵维度(d)控制可训练参数的数量。</li></ol><h4 id="Prompting-with-Retrieved-Captions"><a href="#Prompting-with-Retrieved-Captions" class="headerlink" title="Prompting with Retrieved Captions"></a>Prompting with Retrieved Captions</h4><p>相比于先前的image2image检索方法，本文创新地运用了image2text方法。将数据集中的image captions作为外挂知识库。检索阶段使用完整的<strong>CLIP模型</strong>与视觉-文本编码器，将两种模态数据映射到共享的向量空间。</p><p>编码器会编码input image和数据存储的内容，通过cos相似度检索得到与input image最相似的k个文本项。</p><p>解码器会将检索阶段使用的prompt作为输入Tokens，然后依照特征图像和task demonstration生成captioin。</p><h2 id="实验复现"><a href="#实验复现" class="headerlink" title="实验复现"></a>实验复现</h2><p>论文源代码地址：<a href="https://github.com/RitaRamo/smallcap">https://github.com/RitaRamo/smallcap</a></p><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>代码环境基于python3.9</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n smallcap python=3.9<br>conda activate smallcap<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p><a href="https://www.kaggle.com/datasets/shtvkumar/karpathy-splits">https://www.kaggle.com/datasets/shtvkumar/karpathy-splits</a></p><p>从上面的链接中下载COCO Karpathy splits的dataset_coco.json文件并放置在data&#x2F;目录下</p><p><a href="https://cocodataset.org/#download">https://cocodataset.org/#download</a></p><p>从上面的链接中下载2017版本的COCO images(train、val、test)，并将文件放置在data&#x2F;images&#x2F;下</p><p><a href="http://images.cocodataset.org/zips/train2017.zip">http://images.cocodataset.org/zips/train2017.zip</a></p><p><a href="http://images.cocodataset.org/zips/val2017.zip">http://images.cocodataset.org/zips/val2017.zip</a></p><p><a href="http://images.cocodataset.org/zips/test2017.zip">http://images.cocodataset.org/zips/test2017.zip</a></p><h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>目前基于ResNet的CLIP模型仍然无法从Hugging Face下载，所以需要通过pip安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install git+https://github.com/openai/CLIP.git<br></code></pre></td></tr></table></figure><p>提取特征：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir features<br>python src/extract_features.py<br></code></pre></td></tr></table></figure><p>检索captions：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python src/retrieve_captions.py<br></code></pre></td></tr></table></figure><h4 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h4><p>完成前期所有准备后进行模型训练：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python train.py<br></code></pre></td></tr></table></figure><p>Training takes up to <strong>8 hours</strong> on a single <code>NVIDIA A100 GPU</code>, using <code>16 GB</code> of the available memory.</p><p>训练完成后，模型以名称<code>&lt;rag/norag&gt; _m</code>保存&#x2F;例如 RAG_7M，用于训练以检索增强和7m可训练参数训练的型号。</p><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>使用训练好的模型进行推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python infer.py --model_path &lt;MODEL_PATH&gt;<br></code></pre></td></tr></table></figure><p>如果还指定了 <code>--checkpoint_path</code> 则推理将仅使用该检查点运行。否则，将使用 –model_path 中的所有检查点。</p><p>如果指定<code>-infer_test</code>推理使用测试数据，则使用否则VAL数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python infer.py --model_path experiments/rag_7M --checkpoint_path checkpoint-<span class="hljs-number">17712</span> --infer_test<br></code></pre></td></tr></table></figure><p>在每个相应的checkpoint subdirectory中，将模型预测存储为<code>&lt;val/test&gt; _preds.json</code>。</p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王坚院士最新访谈录</title>
    <link href="/2025/03/04/%E7%8E%8B%E5%9D%9A%E9%99%A2%E5%A3%AB%E6%9C%80%E6%96%B0%E8%AE%BF%E8%B0%88%E5%BD%95/"/>
    <url>/2025/03/04/%E7%8E%8B%E5%9D%9A%E9%99%A2%E5%A3%AB%E6%9C%80%E6%96%B0%E8%AE%BF%E8%B0%88%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>2025年3月3日，星期一，阴天，第52篇博客。</p><h2 id="谈谈阿里-我眼中的"><a href="#谈谈阿里-我眼中的" class="headerlink" title="谈谈阿里(我眼中的)"></a>谈谈阿里(我眼中的)</h2><div class="note note-danger">            <p>王坚：中国工程院院士、之江实验室主任、阿里云创始人</p>          </div><p>我是一直看好阿里的，从本科接触到大数据技术以来，我就在一直慢慢探索大数据技术领域的领先技术。随着短视频爆火、电商领域重大变革，马老师退居二线“靠边站”，阿里系内部也有人事重大调整，很多人唱衰阿里。</p><p>但是我是一直看好阿里的，因为我知道阿里是懂技术、懂变革的，有真正懂技术、做技术的人，王坚院士便是首当其冲的一位。而不是像某公司，一开始说大模型开源毫无价值，但是当DeepSeek火起来了，它又拥抱开源，早干嘛去了！</p><p>阿里在国内<strong>云计算</strong>、<strong>AI</strong>领域处于绝对领先地位，毋庸置疑、不服不行。特别是阿里云，在阿里转型期起到决定性作用。马老师也因为阿里的转型成功，重新满血而归、出山。</p><h2 id="算力革命"><a href="#算力革命" class="headerlink" title="算力革命"></a>算力革命</h2><p>央视面对面栏目最新专访：<a href="https://www.bilibili.com/video/BV1xc98YAEwk/?spm_id_from=333.1007.tianma.1-1-1.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1xc98YAEwk/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><div class="note note-success">            <p>Get新观点</p><ol><li><p>一定条件下，<strong>车慢了</strong>才堵车，并不是<strong>车多了</strong>才堵车。大家都觉得堵车，所以车慢了；事实上是车慢了才堵车的。</p></li><li><p>黑土地这个词，很少人问为什么这样叫，大家只知道黑土地很肥沃。其实大家没搞明白，黑土地就是死下来的东西烂在这里才叫黑土地。创新，其实是是成千上万个公司、团队死在这里，才会涌现出的创新。就是死在这里，不能叫失败；叫失败就有价值判断了，没有失败；变成肥料也叫失败？！</p></li></ol>          </div><p>云计算就是帮助小公司去做大公司做的事情，创新就是一堆看似不起眼的人却做出来了很多变革的东西。</p><p>马力革命时代，人类驯服马匹作为提高生产力工具，伦敦享有“马粪城”的绰号。马力革命留下的基础设施是道路。马力革命时代不管怎么进步，本质上还是供给不足。</p><p>电力革命时代，城市文明就是电力革命，以纽约为代表的现代都市群发展，电力革命留下的基础设施是电网，新能源再怎么改进，电网没有改。电力时代不管怎么讲科技进步，最后落到一个关键点就是：人类消耗自然资源的能力大大增加，从而带来了一系列环境问题。</p><p>下一次：算力革命，互联网作为算力革命时代的基础设施——算力网。算力时代要做到的目标就是达到生活生平的提升而不增加自然资源的消耗。</p><h2 id="AI4S"><a href="#AI4S" class="headerlink" title="AI4S"></a>AI4S</h2>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年2月总结与展望</title>
    <link href="/2025/03/01/2025%E5%B9%B42%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/03/01/2025%E5%B9%B42%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>2025年3月1日，星期六，雾霾，第51篇博客。</p><h2 id="假期-开学"><a href="#假期-开学" class="headerlink" title="假期 -&gt; 开学"></a>假期 -&gt; 开学</h2><p>2月是过渡的一个月，从假期的状态慢慢恢复到了开学后的学习、工作状态。</p><p>2.24号开始，开学第一周。</p><p>结束完第一周的课程，感觉这半年又将是收获满满的一个学期，因为这学期选了几门感兴趣的硬核好课。</p><p>计算所陈云霁老师团队的《智能计算系统》当之无愧的好课，这门课很创新、很硬核。从软硬件、编程框架多角度剖析大模型时代下的计算系统；而且每周四下午还第一次开设了这门课程对应的实验《智能计算系统开发与实践》，对于我这个不务正业的二流选手，岂有不蹭之理。</p><p>计算所徐俊刚老师团队的《深度学习》，国科大每年爆火的一门课程。希望这学期能认认真真跟下来，把每个实验都扎扎实实做完，再把DP的整个框架梳理梳理，建立起知识体系。徐俊刚老师这学期还开了一门《高级数据管理》，但是和我的论文讨论课冲突了，想着如果后面大老师不来的话，就翘了去听一听《高级数据管理》这门课。</p><p>计算所陈世敏老师的《大数据系统与大规模数据分析》是一门很硬的课，感觉比上学期《大数据技术》还要硬核。陈世敏老师是清华本硕，CMU博士。感觉听陈老师的课，总能听到一名互联网老兵对于技术的读到见解。</p><p>自动化所张家俊老师的《大模型技术与应用》也很不错，蹭课选手计划前两节课一定去认真听完。感觉张老师讲课很干练，基本功很扎实，没有废话、直击要害，NLP领域宗成庆老师团队的人果然都是大佬啊。</p><p>软件所彭启民老师的《Java程序设计》也值得一听，打算这学期跟着老师，把Java基础再夯实一遍。</p><p>非常幸运抢到了朱天宇老师的篮球课，以后每周二都能去打篮球咯，这学期要把篮球的基本技术在提高提高～</p><p>珍惜雁栖湖的教育资源，别和上学期一样总是逃课了😂</p><h2 id="阅读收获"><a href="#阅读收获" class="headerlink" title="阅读收获"></a>阅读收获</h2><p>开学前购入了一款汉王clear6，墨水屏的小显示器像口袋书一样，完美地解决了我的阅读的需求。开通了微信读书会员，今年开始，要努力培养阅读习惯，多读点书。</p><p>《认知觉醒》</p><p>重新认识大脑：我们人类(这一物种)经过经过漫长的进化，大脑也是逐步进化的。从进化路径来看，大脑分为三种三范式演进过程：<strong>本能脑 → 情绪脑 → 理性脑</strong>。</p><ul><li><strong>本能脑</strong>源自于爬行时代，主管本能</li><li><strong>情绪脑</strong>源于哺乳动物时代，主管情绪</li><li><strong>理性脑</strong>源于灵长类动物时代，主管认知</li></ul><p>本能脑和情绪脑伴随人类发展进化的时间最长，从远古人类开始，就是本能脑和情绪脑在支配着我们的行为，<strong>避难趋易、急于求成</strong>、躲避危险都是人性最基本的行为模式。</p><p>焦虑的根源在于：<strong>想同时做很多事，又想立即见到效果</strong>。焦虑时人的天性，是人类的默认出厂设置。千百年来所有人都一样，所以，允许自己适当的焦虑，不要较劲。</p><ul><li>想同时做很多事：<strong>避难趋易</strong></li><li>又想立即见到效果：<strong>急于求成</strong></li></ul><p>快速进步的本质就是永远让自己始终处于<strong>学习拉伸区</strong>(舒适区与难度区的交界)。</p><p>刻意练习是在处于拉伸区的同时，要<strong>定义明确清晰的目标(学会任务拆解、拆分目标) - 极度专注(最触动你的地方) - 有效的反馈 - 在拉伸区刻意练习</strong>。</p><h2 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h2><ul><li><p>每个人的进度都是不同的，过分的关注别人的进度成果只会加剧自身的焦虑与迷茫，不要既想又想，还要必须有好结果。</p></li><li><p>做好科研的方法是什么？应该如何做有价值的科学研究？为了发论文而发，为了评奖评优而发？？底层逻辑必须想清楚。</p></li><li><p>这半年要一直去想：如何做好科研？那么多学术大佬，他们的方法论是什么？怎么建立起来的？技术与研究要两手抓、两手都要硬。</p></li></ul><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>2月份完成的任务：</p><ol><li>结合王鹏、谷清水、卞龙鹏新书《大模型RAG实战》梳理了RAG基础概念与范式变迁，完成对langchain框架学习。</li><li>动手实践了<strong>RAG-from-scratch</strong>的项目代码。</li><li>读了三篇关于Graph RAG的paper&#x2F;&#x2F; 但是仍然找不到什么好的idea。</li></ol><p>3月份目前计划要完成的任务：</p><ul><li>实践<strong>RAGflow</strong>项目，复盘下多模态RAG的实践项目</li><li>总结梳理langchain框架的核心组件与常用方法，熟悉LCEL编程范式</li><li>总结一下常用的Tokenizer(中文&#x2F;英文)，熟练掌握文本预处理的技术(以拿到文本快速适配任何E-D架构为目标)</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM开发必备</title>
    <link href="/2025/02/26/LLM%E5%BC%80%E5%8F%91%E5%BF%85%E5%A4%87/"/>
    <url>/2025/02/26/LLM%E5%BC%80%E5%8F%91%E5%BF%85%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>2025年2月26日，星期三，晴，第50篇博客。</p><div class="note note-danger">            <p>困扰的问题：在进行RAG、多模态检索的实践中难免要调用大模型，但是由于“网络”和注册等诸多因素，使得我们很难方便调用的Open API。</p>          </div><p>推荐一个API网站：<a href="https://www.dmxapi.com/">https://www.dmxapi.com/</a> ，网站集成了当前主流的大模型的调用方法。</p><h2 id="openai接口"><a href="#openai接口" class="headerlink" title="openai接口"></a>openai接口</h2><p>使用openai库调用大模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-comment"># 1. 创建客户端 -&gt; 将API密钥和base_url传给OpenAI对象</span><br>client = OpenAI(<br>    api_key=<span class="hljs-string">&quot;sk-........................&quot;</span>,<br>    base_url=<span class="hljs-string">&quot;https://www.dmxapi.com/v1&quot;</span><br>)<br><br><span class="hljs-comment"># 2. chat_completion -&gt; 实现chat </span><br>chat_completion = client.chat.completions.create(<br>    <span class="hljs-comment"># message的形式是以列表存储的json串</span><br>    messages=[<br>        &#123;<br>            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br>            <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;9.11和9.12哪个更大？&quot;</span>,<br>        &#125;<br>    ],<br>    model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span><br>)<br><br><span class="hljs-built_in">print</span>(chat_completion)<br></code></pre></td></tr></table></figure><p>Embedding模型调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> openai<br><br><span class="hljs-comment"># 设置openai密钥和基础URL</span><br>openai.api_key = <span class="hljs-string">&quot;sk-*********************&quot;</span><br>openai.base_url = <span class="hljs-string">&quot;https://www.dmxapi.com/v1/&quot;</span> <span class="hljs-comment"># 做嵌入时base_url后面要添加 /</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embedding</span>(<span class="hljs-params">text</span>):<br>    response = openai.embeddings.create(<br>        model=<span class="hljs-string">&quot;text-embedding-3-small&quot;</span>,<br>        <span class="hljs-built_in">input</span>=text<br>    )<br>    <span class="hljs-keyword">return</span> response.data[<span class="hljs-number">0</span>].embedding<br><br>text = <span class="hljs-string">&quot;你好世界，我爱你！&quot;</span><br><br><span class="hljs-comment"># 获取文本嵌入</span><br>embedding = get_embedding(text)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;文本：<span class="hljs-subst">&#123;text&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;嵌入的向量维度：<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(embedding)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;嵌入的向量前5个元素：<span class="hljs-subst">&#123;embedding[:<span class="hljs-number">6</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>文本：你好世界，我爱你！<br>嵌入的向量维度：1536<br>嵌入的向量前5个元素：[0.02944854088127613, -0.059522904455661774, -0.03096095100045204, 0.06727617979049683, 0.013377008028328419]</p><p>langchain - Ollama调用嵌入模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain_community.embeddings <span class="hljs-keyword">import</span> OllamaEmbeddings<br><br><span class="hljs-comment"># 使用本地Ollama运行嵌入模型</span><br>embed = OllamaEmbeddings(<br>    model=<span class="hljs-string">&#x27;milkey/dmeta-embedding-zh&#x27;</span>,<br>)<br></code></pre></td></tr></table></figure><h2 id="Deepseek-API"><a href="#Deepseek-API" class="headerlink" title="Deepseek API"></a>Deepseek API</h2><p>opanai库也提供了Deepseek API的调用方法。</p><p>通过使用Deepseek API调用Deepseek - R1模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">client = OpenAI(api_key=<span class="hljs-string">&quot;sk-********&quot;</span>,base_url=<span class="hljs-string">&quot;https://api.deepseek.com&quot;</span>)<br>messages = [&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;9.11 and 9.8,which is greater?&quot;</span>&#125;]<br>response = client.chat.completions.create(<br>    model=<span class="hljs-string">&quot;deepseek-chat&quot;</span>,<br>    messages=messages,<br>    stream=<span class="hljs-literal">False</span><br>)<br><span class="hljs-comment"># reasoning_content = response.choices[0].message.reasoning_content</span><br>content = response.choices[<span class="hljs-number">0</span>].message.content<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开工大吉</title>
    <link href="/2025/02/05/%E5%BC%80%E5%B7%A5%E5%A4%A7%E5%90%89/"/>
    <url>/2025/02/05/%E5%BC%80%E5%B7%A5%E5%A4%A7%E5%90%89/</url>
    
    <content type="html"><![CDATA[<p>2025年2月5日，乙巳年正月初八，星期三，晴，第49篇博客&#x2F;2025年第8篇博客，正式开工！！！</p><h1 id="关于DeepSeek以及未来1年有关LLM的一些观点和思考"><a href="#关于DeepSeek以及未来1年有关LLM的一些观点和思考" class="headerlink" title="关于DeepSeek以及未来1年有关LLM的一些观点和思考"></a>关于DeepSeek以及未来1年有关LLM的一些观点和思考</h1><p><img src="https://p4.itc.cn/images01/20230410/ae3dde350de54c24a22196a05fc7dd64.jpeg"></p><div class="note note-success">            <p>观点整理自刘焕勇老师，转发切记注明出处！</p>          </div><p>明天就开工了，针对deepseek，我们静下心来看看，这波造神热度迟早要消散或者翻车。LLM帮大家解决落地问题，让大家有饭吃，有活干，才能行。我倒觉得反而利好的是qwen跟豆包，因为后两者一直在做，deepseek那套grop，会被他们借鉴。然后，原有的基础跟基本盘会进一步牢固。等大家恢复理性之后，还是会乖乖用qwen阿里云这些。即便国家造神有单子，那也是deepseek自己，但独乐乐不如众乐乐，众乐乐才是真的乐。deepseek蒸馏版都70B，现在大家落地， qwen14B都部署费劲。所以，<strong>底层公司的刚需才是真需</strong>。</p><p>Guonei 流行造神跟毁神，比比皆是。deepseek也逃不出这个魔咒。</p><p>我觉得deepseek 只要不掉进舆论的坑里，就无所谓。他们开源的细节足以复现，因此会整体抬高中国大模型的实力。这点功不可没。至于后续，那就看谁能走的更靠前。而且，DS 打破了OpenAI 他们的神话，因此会给更多中国大模型厂商以启发，不一定非得复制OpenAI 才能成功，有更多的尝试，会有更多的成果。</p><p>所以，作为社区成员的大家，保持冷静，<strong>从解决落地问题的角度出发，从吃饭的角度出发</strong>。就会明朗平静很多。</p><p>从去年DS 的积累看，今年多模态应该没问题，所以今年肯定还会有新的模型出来，而且 R1 的开源启发了无数人的火花，今年看到 R2 甚至R3 都有可能。推理过程开放后，太有启发性了，等于给大模型开了一扇窗，可以用人能够理解的方式，让人们看到大模型在怎么运作，黑箱透明度的提高，意味着我们在大模型上的研究上可能会更有方向性。今年是个可以期待的一年。</p><p><strong>今年是落地年，技术上可以期待，但也仅仅是技术本身，还是要解决吃饭问题</strong>。</p><div class="note note-danger">            <p>自己的思考：今年务必多关注多模态、强化学习与RAG的结合，多做落地实践、多动手。</p>          </div><h1 id="关于LLM的一些通俗解释和理解"><a href="#关于LLM的一些通俗解释和理解" class="headerlink" title="关于LLM的一些通俗解释和理解"></a>关于LLM的一些通俗解释和理解</h1><div class="note note-success">            <p>读AI大佬Andrej Karpathy推文所记：<a href="https://x.com/karpathy/status/1885026028428681698">https://x.com/karpathy/status/1885026028428681698</a></p>          </div><p>费曼学习法：用自己话把问题和过程描述清楚，并建立关联。</p><p>我们可以将训练大型语言模型 (LLM) 的过程巧妙地比作教育学生，并以教科书的结构为框架，阐述了当前 LLM 训练的现状和未来方向。</p><p>当我们打开任何一本教科书，都会看到三种主要类型的信息：</p><p><strong>1. 背景信息 &#x2F; 阐述 (Background information &#x2F; exposition):</strong> 这是教科书的核心内容，用于解释各种概念和知识。</p><p>学生通过阅读和学习这些内容来构建知识体系，这就像是 <strong>LLM 的预训练 (pretraining)</strong> 阶段。在预训练阶段，模型通过阅读海量的互联网文本，学习语言的规律、世界的知识，积累广泛的背景知识，为后续的学习打下基础。</p><p><strong>2. 例题及解答 (Worked problems with solutions):</strong> 教科书会提供具体的例题，并详细展示专家如何解决这些问题。</p><p>这些例题是示范，引导学生模仿学习。这与 <strong>LLM 的 监督式微调 (supervised finetuning)</strong> 阶段相对应。在微调阶段，模型学习人类专家提供的“理想答案”，学习如何生成高质量、符合人类期望的回复，例如助手类应用的“理想回答”。</p><p><strong>3. 练习题 (Practice problems):</strong> 教科书每章节末尾通常会设置大量的练习题，这些题目往往只提供最终答案，而不给出详细的解题步骤。</p><p>练习题旨在引导学生通过 试错 (trial &amp; error) 的方式进行学习。学生需要尝试各种方法，才能找到正确的答案。Karpathy认为，这与 <strong>强化学习 (reinforcement learning)</strong> 的概念高度相似。</p><p>Karpathy强调，目前我们已经让 LLM 经历了大量的“阅读”和“示例学习”，也就是预训练和监督式微调，但对于“练习题”这一环节，也就是强化学习，我们还处于一个新兴的、尚待开发的阶段。</p><p>当我们为 LLM 创建数据集时，本质上与为它们编写教科书并无二致。为了让 LLM 真正“学会”，我们需要像编写教科书一样，提供这三种类型的数据：</p><p><strong>大量的背景知识 (Background information)：</strong> 对应预训练，让模型积累广泛的知识。</p><p><strong>示范性的例题 (Worked problems)：</strong> 对应监督式微调，让模型学习高质量的输出。</p><p><strong>大量的练习题 (Practice problems)：</strong> 对应强化学习，让模型在实践中学习，通过试错和反馈不断改进。</p><p><strong>LLM 需要阅读，更需要实践。</strong> 只有通过大量的实践练习，才能真正提升 LLM 的能力，让它们更好地理解世界、解决问题。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025年1月总结与展望</title>
    <link href="/2025/01/30/2025%E5%B9%B41%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2025/01/30/2025%E5%B9%B41%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<p>2025年1月30日，星期四，阴天，这是写的第48篇博客文章。</p><h2 id="这个月很忙，主旋律是考试与春节·过年。"><a href="#这个月很忙，主旋律是考试与春节·过年。" class="headerlink" title="这个月很忙，主旋律是考试与春节·过年。"></a>这个月很忙，主旋律是<strong>考试</strong>与<strong>春节·过年</strong>。</h2><p>咱中华文化就是博大精深，按理说，每年的1月、2月都应该是一年的开端，事情的开始阶段所有人的状态都应该鼓足干劲的。但是我们中国人不然，每年的1月、2月，中国人都要过自己的节日——<strong>春节</strong>。对于全体中国人来说，春节正式一年当中为数不多的放下手里工作回家团聚、休息调整的时光～</p><p>只有过了春节，才真正意味着中国人的一年正式拉开序幕了。所以，按照工作惯性与生活节奏，中国人的一年应该是从每年的2月中下旬开始才算真正的崭新一年。</p><p>1月份有一半的时间都是在复习备考中度过的，从在果壳跨年结束以后，一直持续到1月10号最后一门考试结束，然后16号离校。1月份也有意思，本应该是2025的开端，却和我的期末装了满怀，开始和结束“会面”。</p><p>期末考试整体还算说得过去，<strong>投入与回报成正比</strong>。分数高的课程确实花了很多精力在上面，分数低的课程(过了)要么是复习不到位、要么是知识体系没有建立(NLP和机器学习平时翘课太多了)。</p><p>NLP得分很低，是复习策略失误导致的；机器学习中规中矩，很多点其实也是眼高手低了。不过研究生的开始还好，不想本科那样去卷绩点、卷分数，更重要的还是对于自己能力的培养和提升。</p><p>腊月20回家，到家后和老于一起吃了个饭、喝喝酒叙叙旧，说好的过年后xdm一起聚聚，最终也没能聚齐。年前还感染了诺如病毒，上吐下泻折腾了不到四天，每年回家感冒发烧一次，很规律。</p><p>过年基本上还和往年一样，最有年味er的活动莫过于：<strong>上坟祭祖、年夜饭、拜年</strong>。</p><h2 id="这个月很爱刷剧"><a href="#这个月很爱刷剧" class="headerlink" title="这个月很爱刷剧"></a>这个月很爱刷剧</h2><p>可能也是考完试要放松了，这个月刷完了《大明王朝1566》、二刷了姜文和刘晓庆主演的《芙蓉镇》，正在刷《康熙王朝》和《雍正王朝》。假期计划把这几部历史巨著都刷一遍，再把王志文的几部经典也顺带看看。</p><h2 id="这个月很懒散"><a href="#这个月很懒散" class="headerlink" title="这个月很懒散"></a>这个月很懒散</h2><p>考完试伴随着放假，整个人就是非常的松散与懈怠，经常性的刷抖音一刷就是半天，然后刷完抖音就转战小红书继续刷，其他的事情提不起兴趣来，浪费了很多时间。</p><p>不要间歇性努力，持续性颓废哈哈哈，不过适当放松放松还是可以的。</p><h2 id="2月份，冲！"><a href="#2月份，冲！" class="headerlink" title="2月份，冲！"></a>2月份，冲！</h2><p>今年寒假考虑到天气、景区人数就不计划出游了，踏踏实实在家里沉淀吧，多学点技术、多读些书。</p><p>一直想在B站自己录点课，但是始终找不到动机。。。</p><p>抽空把李沐老师《动手学深度学习》的最后几节课程刷完，最后只剩下了BERT。论文课也要开始刷了。</p><p>2月份首先要从上个月的松散劲中走出来，尽最大努力坚持三大小改变：</p><ul><li><input disabled="" type="checkbox"> 早睡会(23:00-23:20),早起会(6:40-6:50)</li><li><input disabled="" type="checkbox"> 少刷会抖音、小红书</li><li><input disabled="" type="checkbox"> 坚持学英语、练口语和听力</li></ul><p>开学前(02.22)需要完成的的任务：</p><ul><li><input disabled="" type="checkbox"> Agentic RAG和Graph RAG技术栈梳理学习</li><li><input disabled="" type="checkbox"> RAG系统实践</li><li><input disabled="" type="checkbox"> Multi-Agent的多数据源RAG系统设计</li><li><input disabled="" type="checkbox"> Transformer架构完善和开源</li><li><input disabled="" type="checkbox"> 向量数据库代码实现与开源</li><li><input disabled="" type="checkbox"> 五子棋程序精进完善与开源</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梁文锋专访</title>
    <link href="/2025/01/29/%E6%A2%81%E6%96%87%E9%94%8B%E4%B8%93%E8%AE%BF/"/>
    <url>/2025/01/29/%E6%A2%81%E6%96%87%E9%94%8B%E4%B8%93%E8%AE%BF/</url>
    
    <content type="html"><![CDATA[<h1 id="暗涌Waves专访DeepSeek创始人梁文锋"><a href="#暗涌Waves专访DeepSeek创始人梁文锋" class="headerlink" title="暗涌Waves专访DeepSeek创始人梁文锋"></a>暗涌Waves专访DeepSeek创始人梁文锋</h1><p>暗涌：互联网和移动互联网时代留给大部分人的惯性认知是，美国擅长搞技术创新，中国更擅长做应用。</p><p>梁文锋：我们认为随着经济发展，<strong>中国也要逐步成为贡献者，而不是一直搭便车</strong>。过去三十多年 IT 浪潮里，我们基本没有参与到真正的技术创新里。我们已经习惯摩尔定律从天而降，躺在家里 18 个月就会出来更好的硬件和软件。<strong>Scaling Law 也在被如此对待</strong>。</p><p>但其实，这是西方主导的技术社区一代代孜孜不倦创造出来的，只因为之前我们没有参与这个过程，以至于忽视了它的存在。</p><div class="note note-danger">            <p>真正的差距是原创和模仿之差</p>          </div><p>暗涌：为什么 DeepSeek V2 会让硅谷的很多人惊讶？</p><p>梁文锋：在美国每天发生的大量创新里，这是非常普通的一个。他们之所以惊讶，是因为<strong>这是一个中国公司，在以创新贡献者的身份，加入到他们游戏里去</strong>。毕竟大部分中国公司习惯 follow，而不是创新。</p><p>暗涌：为什么中国公司——包括不缺钱的大厂，这么容易把快速商业化当第一要义？</p><p>梁文锋：<strong>过去三十年，我们都只强调赚钱，对创新是忽视的</strong>。创新不完全是商业驱动的，还需要好奇心和创造欲。我们只是被过去那种惯性束缚了，但它也是阶段性的。</p><p>暗涌：为什么你的定义是‘做研究、做探索’？</p><p>梁文锋：一种好奇心驱动。从远处说，我们想去验证一些猜想。比如我们理解人类智能本质可能就是语言，人的思维可能就是一个语言的过程。你以为你在思考，其实可能是你在脑子里编织语言。这意味着，在语言大模型上可能诞生出类人的人工智能（AGI）。从近处说，GPT4 还有很多待解之谜。我们去复刻的同时，也会做研究揭秘。</p><div class="note note-danger">            <p>万卡储备其实是好奇心驱动</p>          </div><p>暗涌：GPU是这次 ChatGPT 创业潮的稀缺品，你们在 2021 年就可以有先见之明，储备了 1 万枚。为什么？</p><p>梁文锋：其实从最早的 1 张卡，到 2015 年的 100 张卡、2019 年的 1000 张卡，再到一万张，这个过程是逐步发生的。几百张卡之前，我们托管在 IDC，规模再变大时，托管就没法满足要求了，就开始自建机房。很多人会以为这里边有一个不为人知的商业逻辑，但其实，主要是好奇心驱动。</p><p>暗涌：什么样的好奇心？</p><p>梁文锋：对 AI 能力边界的好奇。对很多行外人来说，ChatGPT 这波浪潮冲击特别大；但对行内人来说，2012 年 AlexNet 带来的冲击已经引领一个新的时代。AlexNet 的错误率远低于当时其他模型，复苏了沉睡几十年的神经网络研究。虽然具体技术方向一直在变，但模型、数据和算力这三者的组合是不变的，特别是当 2020 年 OpenAI 发布 GPT3 后，方向很清楚，需要大量算力；但即便 2021 年，我们投入建设萤火二号时，大部分人还是无法理解。</p><p>暗涌：所以 2012 年起，你们就开始关注到算力的储备？</p><p>梁文锋：<strong>对研究员来说，对算力的渴求是永无止境的</strong>。做了小规模实验后，总想做更大规模的实验。那之后，我们也会有意识地去部署尽可能多的算力。</p><div class="note note-danger">            <p>套路都是上一代的产物，未来不一定成立</p>          </div><p>暗涌：回到关于原创式创新的话题。现在经济开始进入下行，资本也进入冷周期，所以它对原创式创新是否会带来更多抑制？</p><p>梁文锋：我倒觉得未必。<strong>中国产业结构的调整，会更依赖硬核技术的创新</strong>。当很多人发现过去赚快钱很可能来自时代运气，就会更愿意俯身去做真正的创新。</p><p>暗涌：所以你对这件事也是乐观的？</p><p>梁文锋：我是八十年代在广东一个五线城市长大的。我的父亲是小学老师，九十年代，广东赚钱机会很多，当时有不少家长到我家里来，基本就是家长觉得读书没用。但现在回去看，观念都变了。因为钱不好赚了，连开出租车的机会可能都没了。一代人的时间就变了。</p><p><strong>以后硬核创新会越来越多</strong>。现在可能还不容易被理解，是因为整个社会群体需要被事实教育。<strong>当这个社会让硬核创新的人功成名就，群体性想法就会改变。我们只是还需要一堆事实和一个过程</strong>。</p><div class="note note-success">            <p>创新与赚快钱之间的抉择，过去赚快钱很可能是时代的馈赠，那么在经济低速发展期，没有赶上时代的红利的时候需要自己作出怎样的调整，这是至关重要的，需要不断实践不断思考，不断更新认知。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>LLM</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepSeek YYDS</title>
    <link href="/2025/01/27/DeepSeek-YYDS/"/>
    <url>/2025/01/27/DeepSeek-YYDS/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"></a>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><center>DeepSeek-AI</center><center>https://arxiv.org/pdf/2501.12948</center><center>多看论文原文，少看自媒体公众号文章的鼓吹宣传！</center><p><img src="https://i.bstr.es/drivingeco/2025/01/Deepseek-r1-logo.webp"></p><p>与普通LLM相比，推理LLM倾向于在回答之前讲问题分解为更小的步骤(推理步骤和思维链)，这类模型是在让模型学习如如何回答。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>文章介绍了DeepSeek-AI团队开发的第一代<strong>推理模型</strong>DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1是一种基于大规模强化学习训练的模型，无需监督微调作为初步步骤，展现出强大的推理能力。</p><ul><li><p>优势：推理能力强</p></li><li><p>挑战：可读性差、语言混合</p></li></ul><p><code>模型提出的标志性意义</code>：DeepSeek-R1<strong>迈出了使用纯强化学习 (RL) 提高语言模型推理能力的第一步</strong>，目标是<strong>探索 LLM 在没有任何监督数据的情况下开发推理能力的潜力</strong>，重点关注LLM<strong>通过纯 RL 过程进行模型自身进化</strong>。</p><p><img src="/./img/fig/deepseek.jpg" alt="deepseek-performance"></p><p>自身进化的同时提高了推理性能，同时，因为无需任何监督数据，极大地降低了成本。LLM拼多多称号当之无愧！国产骄傲！</p><p>DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks.</p><p>DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning.</p><p>DeepSeek-R1-Zero 是使用 DeepSeek-V3-Base 作为基础模型，结合GRPO作为RL框架经过数千步推理训练得到。然而DeepSeek-R1-Zero 遇到了可读性差、语言混合的困难，为了进一步提高推理性能，DeepSeek-AI团队由DeepSeek-R1-Zero模型改进到DeepSeek-R1模型，其结合了少量 cold-start data 和a multi-stage training pipeline。</p><p>在RL接近收敛的时候，通过对 RL 检查点进行rejection sampling来创建新的 SFT 数据，并结合 DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，然后重新训练 DeepSeek-V3-Base 模型。</p><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li><strong>Post - training</strong>: 在现有基础模型上进行大规模强化学习，该团队直接将RL应用于基础模型，而不依赖于监督微调（SFT）作为一个初步步骤。这种方法允许模型探索思想链（CoT）来解决复杂的问题，从而开发出DeepSeek-R1-Zero。团队也介绍了开发DeepSeek-R1的pipeline。该管道包含了两个RL阶段，以及两个SFT阶段，作为模型的推理和非推理能力的种子。</li><li><strong>Distillation</strong>: 蒸馏后使得小模型也具有大能量，该团队证明了更大模型的推理模式可以被提炼为更小的模型，与通过RL在小模型上发现的推理模式相比，从而获得更好的性能。</li></ol><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><ul><li><strong>General Capability:</strong> 目前，DeepSeek-R1在function calling, multi-turn, complex role-playing, and JSON output等任务上还不如DeepSeek-V3。接下来，我们计划探索可以利用多长时间的CoT来增强这些领域的任务。</li><li><strong>Language Mixing:</strong> DeepSeek-R1目前针对中文和英语进行了优化，这可能导致在处理其他语言的查询时出现语言混合问题。例如，DeepSeek-R1可能会使用英语进行推理和响应，即使查询使用的是英语或中文以外的语言。我们的目标是在未来的更新中解决这一限制。</li><li><strong>Prompting Engineering:</strong> 在评估DeepSeek-R1时，我们观察到它对提示很敏感。Few-shot的激励不断地降低了它的表现。因此，我们建议用户直接描述问题，并使用zero-shot指定输出格式，以获得最佳结果。</li><li><strong>Software Engineering Tasks:</strong> 由于评估时间长，影响了RL过程的效率，因此大规模的RL在软件工程任务中尚未得到广泛的应用。因此，DeepSeek-R1在软件工程基准测试上并没有显示出比DeepSeek-V3相比的巨大改进。未来的版本将通过对软件工程数据实施拒绝抽样或在RL过程中合并异步评估来提高效率来解决这个问题。</li></ul><h2 id="deepseek多模态LLM-Janus-Pro"><a href="#deepseek多模态LLM-Janus-Pro" class="headerlink" title="deepseek多模态LLM - Janus-Pro"></a>deepseek多模态LLM - Janus-Pro</h2><p>Unified Multimodal Understanding and Generation with Data and Model Scaling: 通过数据和模型扩展实现统一的多模态理解和生成。</p><p>论文解决的当前研究过程中的痛点是：使用相同的视觉编码器处理不同的任务会影响多模态理解的性能。Janus有成果，但是因为训练数据和模型容量有限存在不足。</p><p>Janus - Pro的创新在于改进了上述问题，提升了模型的性能，特别是在多模态理解和文本到图像的生成中表现出色。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2025/01/DeepSeek-Janus-Pro.webp" alt="model pk"></p><p>Janus - Pro作出的改进：</p><ol><li><p>优化训练策略：调整各个阶段的数据比例</p></li><li><p>扩展训练数据：涵盖多模态理解和生成领域</p></li><li><p>更大的模型扩展：扩展模型至7B</p><p><img src="https://aipapersacademy.com/wp-content/uploads/2025/01/Janus_figure3.png" alt="Method"></p></li></ol><p><strong>增强了文本到图像的稳定性</strong></p><h2 id="DeepSeek的习得过程"><a href="#DeepSeek的习得过程" class="headerlink" title="DeepSeek的习得过程"></a>DeepSeek的习得过程</h2><p>复现R1之前务必要把<strong>deepseek-v3</strong>搞懂！</p><p>DeepSeek是站在巨人的肩膀上，摸着OpenAI过河，不要去过分diss OpenAI。deepseek - R1是去“猜”o1的实现过程。DeepSeek是强在推理阶段，在SFT阶段未必是最优的解决方案。</p><p>目前市面上大部分指导集成基本上都是部署的蒸馏版的R1，本质上base还是Qwen。</p><h2 id="对DeepSeek的认知误区"><a href="#对DeepSeek的认知误区" class="headerlink" title="对DeepSeek的认知误区"></a>对DeepSeek的认知误区</h2><div class="note note-danger">            <p>误区：DeepSeek是全新物种，think是万能钥匙</p>          </div><p>R1本质上还是一个大模型，只不过是目前推理模型开源最强</p><div class="note note-danger">            <p>误区：DeepSeek-R1不需要提示词</p>          </div><p>只要是大模型，就一定需要提示词，官方已经提供了34种提示词。<strong>之前大模型解决不了的场景，DeepSeek-R1依旧不好解决。</strong></p><div class="note note-danger">            <p>误区：DeepSeek强化学习模型已经是最强模型，干死了OpenAI</p>          </div><p>DeepSeek的多模态能力不具备、处理一些极端长上下文表现不佳、较03模型基准还有不小差距、base模型好造就了推理能力好(SFT负责记忆，RL负责泛化)</p><h2 id="DeepSeek的场景机会"><a href="#DeepSeek的场景机会" class="headerlink" title="DeepSeek的场景机会"></a>DeepSeek的场景机会</h2><p>与RAG？与Agent(规划不靠谱)结合？</p><p>think是显示过程可以作为微调数据使用，蒸馏得到的qwen学到了一定的推理和数值计算能力</p><p>只会做纯量替换，使用其推理能力，目前并不会带来巨大个信。</p><h2 id="Ollama本地部署deepseek-R1"><a href="#Ollama本地部署deepseek-R1" class="headerlink" title="Ollama本地部署deepseek - R1"></a>Ollama本地部署deepseek - R1</h2><h3 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h3><p>电脑 - MacBook Pro </p><p>芯片 - Apple M3 Pro</p><p>运行内存 - 18GB</p><h3 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="安装Ollama"></a>安装Ollama</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -sSL https://ollama.com/download | bash<br></code></pre></td></tr></table></figure><p>通过curl直接安装即可</p><h3 id="ollama常用指令"><a href="#ollama常用指令" class="headerlink" title="ollama常用指令"></a>ollama常用指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动服务</span><br>ollama serve<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">显示模型信息</span><br>ollama show<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行模型</span><br>ollama run model<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">从注册表中拉取模型</span><br>ollama pull modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">列出模型</span><br>ollama list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除模型</span><br>ollama rm modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">模型使用提示符</span><br>/set | set session variables<br>/show | show model information<br>/load &lt;model&gt; | load a session or model<br>/save &lt;model&gt; | save your current session<br>/clear | clear session context<br>/bye | exit<br>/? /help | help for a command<br></code></pre></td></tr></table></figure><h3 id="下载deepseek-R1模型"><a href="#下载deepseek-R1模型" class="headerlink" title="下载deepseek-R1模型"></a>下载deepseek-R1模型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs shell">ollama pull deepseek-r1:7b<br> ~  ollama run deepseek-r1:7b                                     ok  20:40:25<br>pulling manifest<br>pulling 96c415656d37... 100% ▕████████████████▏ 4.7 GB<br>pulling 369ca498f347... 100% ▕████████████████▏  387 B<br>pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB<br>pulling f4d24e9138dd... 100% ▕████████████████▏  148 B<br>pulling 40fb844194b2... 100% ▕████████████████▏  487 B<br>verifying sha256 digest<br>writing manifest<br>success<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; 什么是信息差？</span><br>&lt;think&gt;<br>嗯，用户问的是“什么是信息差”，我之前已经给了一个比较详细的回答了。现在我要<br>模拟一个assistant对这个问题的思考过程。首先，我需要理解用户的问题到底是什么<br>。<br><br>信息差通常指的是信息不均衡的情况，可能是因为信息不对称或者获取渠道不同导致某<br>些人掌握更多的信息，而另一些人则没有。这在商业、经济和社会中都很常见，比如在<br>交易中卖家和买家的信息不对称。<br><br>然后，我要考虑用户为什么会问这个问题。可能是他们正在处理一个实际的问题，比如<br>在谈判中如何利用信息差，或者在投资时如何应对信息不对称带来的风险。也有可能他<br>们在学习相关知识，需要理解这个概念的基本含义。<br><br>接下来，我应该分析信息差的几个方面：定义、原因、影响和应用。这样可以让思考过<br>程更全面，覆盖不同的角度。此外，还可以举一些例子来说明信息差在不同场景中的表<br>现，比如股票交易中的庄家信息优势，或者在线销售中买家对产品的不了解带来的风险<br>。<br><br>另外，我还需要考虑用户可能的深层需求。他们可能不仅仅想知道定义，还想知道如何<br>利用或避免信息差带来的问题。因此，在思考过程中，应该包括一些关于应对策略的内<br>容，比如透明化的措施、对称信息的重要性以及信息获取成本的影响。<br><br>最后，总结一下信息差的重要性和其在现代经济中的角色，这样用户不仅了解了表面的<br>定义，还能理解其背后的现实意义和应用价值。<br>&lt;/think&gt;<br><br>信息差是指在信息不对称的情况下，某些人或组织掌握的信息比其他人更多、更全面或<br>更有优势，从而在决策或行动中占据一定的优势。这种差异可能来自于信息获取渠道的<br>不同、信息来源的不可靠性，或者信息本身的敏感性和隐私性。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的具体表现：</span></span><br>1. **信息不对称**：指一方掌握的信息多于另一方，导致后者处于不利地位。<br>2. **信息不对等**：一方获得大量优质信息，而另一方获取较少或低质量的信息。<br>3. **信息不透明**：某些信息被隐藏或限制，如商业机密、股票价格等。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的影响：</span></span><br>1. **经济影响**：在金融市场中，信息差可能导致市场操纵、价格误导和投资风险增<br>加。例如，庄家可能利用内部信息操控股价。<br>2. **社会影响**：信息差可能导致资源分配不公，甚至引发社会矛盾。<br>3. **个人决策**：在个人生活中，信息差可能导致不公平竞争或权力滥用。<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## 信息差的应对策略：</span></span><br>1. **透明化**：通过法律或制度限制信息不对称，确保信息公开和对称。<br>2. **教育与培训**：提高公众的信息素养，减少信息不对称的可能性。<br>3. **成本控制**：在获取和使用信息时，尽量降低信息不对称带来的风险。<br><br>理解信息差有助于我们更好地识别和管理信息不对称带来的挑战，在经济、社会和日常<br>生活中做出更明智的决策。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph RAG</title>
    <link href="/2025/01/24/Graph-RAG-A-Survey/"/>
    <url>/2025/01/24/Graph-RAG-A-Survey/</url>
    
    <content type="html"><![CDATA[<center>  <h1>Graph Retrieval-Augmented Generation: A Survey</h1>  https://www.arxiv.org/pdf/2408.08921</center>论文更多侧重理论概念层面，实践落地过程中需要自行体会各个阶段的实现过程。<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>通过引用外部知识库，RAG改进了LLM输出，有效地减轻了诸如“幻觉”、缺乏特定领域的知识和过时的信息等问题。但是，<strong>数据库中不同实体之间复杂的关系结构给RAG系统带来了挑战</strong>。</p><p><em>GraphRAG</em>利用<strong>跨实体</strong>的结构信息，以实现更精确和全面的检索，捕获关系知识，并促进更准确的、上下文感知的响应。</p><p>这篇文章是首次对于GraphRAG方法提供了全面的总结，规范了GraphRAG workflow(包括Graph-Based</p><p>Indexing、Graph-Guided Retrieval和Graph-Enhanced Generation)，强调了每一个阶段的核心技术和训练方法。此外，还对下有任务、应用领域、评估方法、工业案例进行阐述。</p><h2 id="工作流与关键技术"><a href="#工作流与关键技术" class="headerlink" title="工作流与关键技术"></a>工作流与关键技术</h2><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><div class="note note-success">            <p><strong>Graph-Based Indexing (G-Indexing) -&gt; Graph-Guided Retrieval (G-Retrieval) -&gt; Graph-Enhanced Generation (G-Generation)</strong> </p>          </div><p>图索引建立：包括识别或构建与下游任务对齐的图数据库，并在其上建立索引。图数据可以来自开放知识图谱或自建图数据。</p><p>图数据检索：从图数据库中提取相关信息以响应用户查询。涉及选择检索器、检索范式、检索粒度和增强技术。</p><p>图增强生成：基于检索到的图数据生成有意义的输出。涉及选择生成器、转换图数据格式和生成增强技术。</p><h3 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h3><ul><li><strong>图数据:</strong> 包括开放知识图谱和自建图数据。开放知识图谱如Wikidata、Freebase等，自建图数据则根据特定任务定制。</li></ul><p><img src="/img/fig/g-data.jpg"></p><ul><li><p><strong>索引方法:</strong> 包括图索引、文本索引和向量索引。混合索引结合了这些方法的优点。</p></li><li><p><strong>检索器:</strong> 分为非参数检索器、基于LM的检索器和基于GNN的检索器。检索范式包括一次检索、迭代检索和多阶段检索。</p></li></ul><p><img src="/img/fig/g-retrieval.jpg"></p><ul><li><strong>生成器:</strong> 包括GNNs、LMs和混合模型。生成增强技术包括预生成增强、中生成增强和后生成增强。</li></ul><p><img src="/img/fig/g-generation.jpg"></p><p><strong>GraphRAG在图索引（G-Indexing）阶段是如何构建和优化图数据库的？</strong></p><ol><li><strong>数据源选择</strong>：图数据库可以来自公共知识图谱（如Wikidata、Freebase、DBpedia等）或自构建的数据源（如文本、其他形式的数据等）。</li><li><strong>节点和边属性映射</strong>：将节点和边的属性进行映射，以便于后续的检索和生成操作。</li><li><strong>指针建立</strong>：在节点之间建立指针，以便于快速遍历和检索。</li><li><strong>数据组织</strong>：组织数据以支持快速遍历和检索操作，确保索引过程的高效性。</li></ol><p><strong>GraphRAG在图增强生成（G-Generation）阶段如何处理和转换检索到的图数据？</strong></p><p>在图增强生成（G-Generation）阶段，GraphRAG需要将检索到的图数据转换为生成器可以处理的格式。具体步骤如下：</p><ol><li><strong>数据转换</strong>：将检索到的图数据（如节点、边、路径、子图等）转换为生成器可以理解的格式。常见的转换方法包括邻接表、自然语言描述、代码形式、语法树和节点序列等。</li><li><strong>生成器输入</strong>：将转换后的图数据与查询一起作为输入提供给生成器。生成器可以是基于图神经网络（GNNs）的模型、判别式语言模型或生成式语言模型。</li><li><strong>生成响应</strong>：生成器根据输入的查询和图数据生成最终的响应。对于生成式语言模型，这通常涉及解码过程，以生成自然语言文本作为响应。</li></ol><h2 id="Future-Prospects"><a href="#Future-Prospects" class="headerlink" title="Future Prospects"></a>Future Prospects</h2><h3 id="多模态数据整合"><a href="#多模态数据整合" class="headerlink" title="多模态数据整合"></a>多模态数据整合</h3><p>知识图谱中目前存储的仍然是大量文本数据，对于多模态数据的整合仍然是挑战，知识图谱的规模升级是发展GraphRAG的底层刚需。</p><h3 id="检索机制"><a href="#检索机制" class="headerlink" title="检索机制"></a>检索机制</h3><p>当前的检索机制仍然是对于少量文本-实体的知识图谱作检索，而大量实体的复杂检索机制需要被探索。</p><h3 id="检索上下文的无损失压缩"><a href="#检索上下文的无损失压缩" class="headerlink" title="检索上下文的无损失压缩"></a>检索上下文的无损失压缩</h3><p>应用GraphRAG，检索得到信息包含大量的实体和关系，这些信息会被转换为长文本输入序列。但是长文本输入序列包含两个问题：LLMs很难解决长文本序列、对于个人用户而言推理阶段的大量计算是障碍。</p><p>因此，无损压缩对于GraphRAG是关键的。</p><h3 id="落地应用"><a href="#落地应用" class="headerlink" title="落地应用"></a>落地应用</h3><p>应用场景：客服服务系统、推荐系统、KBQA(知识问答系统)</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇调研文章全面的回顾了GraphRAG技术，系统地分类、组织了它的基本技术，训练方法和应用场景。GraphRAG通过使用从图数据集中派生出的关键知识，解决了传统RAG的关键性限制。</p><center>  <h1>Retrieval-Augmented Generation with Graph</h1>  https://arxiv.org/pdf/2501.00309</center>论文更多侧重理论概念层面，实践落地过程中需要自行体会各个阶段的实现过程。<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>与传统RAG相比，检索器、生成器和外部数据源可以做统一嵌入处理；Graph数据结构的独特性，在进行GraphRAG设计时面临着大量的挑战。</p><p>论文的主要工作：定义了包含关键组件在内的GraphRAG的总体架构(包括查询处理器、检索器、组织器、生成器和数据源)、总结了<strong>特定领域</strong>的GraphRAG技术、讨论了GraphRAG未来发展方向。</p><h2 id="图数据结构"><a href="#图数据结构" class="headerlink" title="图数据结构"></a>图数据结构</h2><p>内在的节点和边编码组成了大量异构和关系信息，是RAG中的黄金资源。</p><p><img src="/img/fig/rag-with-graph.png"></p><p>RAG处理文本和图像数据，这些数据可以统一格式化为1-D序列数据或2-D网格数据。相比之下，<strong>GraphRAG</strong>处理图结构数据，涵盖了多种格式并包含特定领域的关系信息。</p><h2 id="全面的GraphRAG架构"><a href="#全面的GraphRAG架构" class="headerlink" title="全面的GraphRAG架构"></a>全面的GraphRAG架构</h2><p>GraphRAG通过检索和生成技术来增强下游任务的执行，框架的核心在于利用图结构数据的特性，进行信息检索、数据挖掘。</p><p><img src="/img/fig/framework-of-GraphRAG.png"></p><p>GraphRAG框架的关键组件：</p><ol><li><strong>查询处理器（Query Processor）</strong>：负责预处理用户定义的查询，使其能够与图数据源进行交互。</li><li><strong>检索器（Retriever）</strong>：根据预处理后的查询从图数据源中检索相关内容。</li><li><strong>组织器（Organizer）</strong>：对检索到的内容进行整理和优化，以提高生成器的性能。</li><li><strong>生成器（Generator）</strong>：根据组织后的信息生成最终答案。</li></ol><h3 id="Query-Processor"><a href="#Query-Processor" class="headerlink" title="Query Processor"></a>Query Processor</h3><p>Query Processor是一个关键组件，它负责对用户输入的查询进行预处理，以便与图数据源进行交互。</p><p>查询处理器主要功能：</p><ul><li><strong>实体识别</strong>：从查询中识别出实体，并将其与图数据源中的节点相匹配。</li><li><strong>关系抽取</strong>：识别查询中的关系，并将其与图数据源中的边相匹配。</li><li><strong>查询结构调整</strong>：将自然语言查询转换为结构化查询，如GQL（Graph Query Language）。</li><li><strong>查询分解</strong>：将复杂的查询分解为多个子查询，以便进行多步推理。</li><li><strong>查询扩展</strong>：通过添加相关术语来丰富查询，以提高检索的准确性和相关性。</li></ul><h3 id="Retriever"><a href="#Retriever" class="headerlink" title="Retriever"></a>Retriever</h3><p>检索器（Retriever）是负责从图数据源中检索相关信息的关键组件。</p><p>检索器主要功能：</p><p><strong>检索内容</strong>：根据预处理后的查询从图数据源中检索相关内容。</p><p><strong>适应图结构数据</strong>：能够处理图结构数据的多样性和复杂性，包括不同格式和来源的信息。</p><p><strong>多跳遍历</strong>：支持多跳遍历，以捕获逻辑上相关的知识。</p><p><strong>领域特定设计</strong>：根据不同领域的特定需求进行设计，以提高检索的准确性和效率。</p><h3 id="Organizer"><a href="#Organizer" class="headerlink" title="Organizer"></a>Organizer</h3><p>负责处理检索器检索到的内容，将其与预处理后的查询结合，以生成更适应生成器（Generator）的格式。</p><p>主要功能：</p><p><strong>内容优化</strong>：对检索到的内容进行后处理和优化，以提高其质量。</p><p><strong>图结构处理</strong>：处理检索到的图结构数据，包括图剪枝、重排序和图增强。</p><p><strong>文本化</strong>：将检索到的图结构数据转换为文本格式，以便生成器可以处理。</p><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p><strong>生成器（Generator）</strong>是负责根据查询和检索到的信息生成最终答案的关键组件。生成器的任务是将组织者处理后的信息转化为具体的输出，这些输出可以是文本、图像、数值或其他形式的数据，取决于具体的应用场景。</p><p>生成器主要功能：</p><p><strong>生成最终答案</strong>：根据查询和检索到的信息生成具体的输出。</p><p><strong>适应不同任务</strong>：能够处理多种任务，如分类、生成、预测等。</p><p><strong>利用图结构信息</strong>：能够理解和利用图结构数据中的关系和模式。</p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>多模态</tag>
      
      <tag>Agentic RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agentic RAG:A Survey</title>
    <link href="/2025/01/20/Agentic-RAG-A-Survey/"/>
    <url>/2025/01/20/Agentic-RAG-A-Survey/</url>
    
    <content type="html"><![CDATA[<center>  <h1>AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON AGENTIC RAG</h1>  <h3>Aditi Singh | Abul Ehtesham | Saket Kumar | Tala Talaei Khoei</h3>  https://arxiv.org/abs/2501.09136</center><p>论文更多侧重于理论概念层面，实践中还会遇到很多问题和bug，需要自行体会。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>文章首先以LLM在<strong>文本生成</strong>和<strong>自然语言理解</strong>方面的革新引入。然后笔锋一转(however)，揭示了对依赖静态数据“革新”的局限性，静态训练数据使得LLM不能够及时地动态响应、处理实时需求。针对这一局限性，引出文章重点介绍的技术： <strong>Retrieval-Augmented Generation (RAG)</strong> 。RAG的出现使得LLM可以及时更新训练数据，利用最新数据作出实时响应。当然，传统的RAG技术在多步推理和复杂任务管理上同样存在缺陷。</p><p>Agentic RAG通过将AI agents嵌入到RAG pipeline中超越了传统RAG在多步推理和复杂任务管理上存在的缺陷。Agents利用代理式设计组件动态管理检索策略，迭代改善上下文理解，适应工作流对于复杂任务的需要。</p><p>本文从RAG范式演化与基础原则开始，提供了一个对于Agentic RAG的全面探索。对于Agentic RAG架构细节、下游任务应用，以及强调了在系统扩充、确保道德决策和优化现实应用程序的性能方面的挑战。</p><h2 id="RAG基础"><a href="#RAG基础" class="headerlink" title="RAG基础"></a>RAG基础</h2><p>RAG面临的限制：</p><p>缺乏上下文理解、无法进行多步推理、难以处理复杂任务</p><ol><li>outdated information</li><li>hallucinated responses</li><li>an inability to adapt to dynamic</li><li>real-world scenarios</li></ol><h3 id="Foundations-of-Retrieval-Augmented-Generation"><a href="#Foundations-of-Retrieval-Augmented-Generation" class="headerlink" title="Foundations of Retrieval-Augmented Generation"></a>Foundations of Retrieval-Augmented Generation</h3><div class="note note-danger">            <p>RAG &#x3D; LLM + Information retrieval</p>          </div><p>RAG就是融合LLM的生成能力和实时数据的信息检索能力</p><h3 id="Core-Components-of-RAG"><a href="#Core-Components-of-RAG" class="headerlink" title="Core Components of RAG"></a>Core Components of RAG</h3><ul><li><strong>Retrieveal</strong></li><li><strong>Augmentation</strong></li><li><strong>Generation</strong></li></ul><h2 id="AI-Agent"><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h2><h3 id="Components-of-AI-Agent"><a href="#Components-of-AI-Agent" class="headerlink" title="Components of  AI Agent"></a>Components of  AI Agent</h3><ol><li><strong>LLM (with defined Role and Task)</strong>: LLM用作核心推理引擎</li><li><strong>Memory (Short-Term and Long-Term)</strong>: 维护上下文信息</li><li><strong>Planning (Reflection &amp; Self-Critique)</strong>: 引导推理、拆分复杂任务</li><li><strong>Tools Vector Search, Web Search, APIs, etc.</strong>: 调用外部资源</li></ol><p><img src="/img/fig/Arag01.png"></p><p>Agentic工作流的四个核心模式：</p><ol><li>Reflection - 反省模式</li></ol><p><img src="/img/fig/Arag-reflection.png" alt="Reflection"></p><p>Agent根据输出相应作出自我评估，不断优化输出质量。</p><ol start="2"><li>Planning - 规划模式</li></ol><p><img src="/img/fig/Arag-planning.png" alt="Planning"></p><p>Agent制定任务执行计划，对复杂任务拆分、分配。</p><ol start="3"><li>Tool Use - 工具使用模式</li></ol><p><img src="/img/fig/Arag-tool.png" alt="Tool Use"></p><p>Agent会灵活调用APIs，扩展边界能力，同时调用数据库、数据统计分析工具、可视化工具等。</p><ol start="4"><li>Multi-Agent - 多代理协作模式</li></ol><p><img src="/img/fig/Arag-multiagent.png" alt="Multi-Agent"></p><p>多Agent协作配合，每个Agent负责特定任务，共同完成同一个目标任务。</p><h2 id="Agentic-RAG架构类别"><a href="#Agentic-RAG架构类别" class="headerlink" title="Agentic RAG架构类别"></a>Agentic RAG架构类别</h2><h3 id="Single-Agent-Agentic-RAG-Router"><a href="#Single-Agent-Agentic-RAG-Router" class="headerlink" title="Single-Agent Agentic RAG: Router"></a>Single-Agent Agentic RAG: Router</h3><p>Single-Agent架构是最基础的形式，Single-Agent可落地的功能：</p><ul><li>智能分析用户问题</li><li>选择最合适的信息源</li><li>整合多个数据库的内容</li><li>语义搜索和web搜索</li><li>推荐系统</li></ul><p><img src="/img/fig/single-agent.png"></p><p>Single-Agent架构的数据整合能力强</p><h3 id="Multi-Agent-Agentic-RAG"><a href="#Multi-Agent-Agentic-RAG" class="headerlink" title="Multi-Agent Agentic RAG"></a>Multi-Agent Agentic RAG</h3><p>多个Agent有明确分工：</p><ul><li>Agent 1：负责结构化数据查询</li><li>Agent 2：处理语义搜索</li><li>Agent 3：获取实时信息</li><li>Agent 4：负责给结构化数据查询</li><li>Agent 5：负责个性化推荐</li></ul><p><img src="/img/fig/multi-agent-rag.png"></p><h3 id="Hierarchical-Agentic-RAG-Systems"><a href="#Hierarchical-Agentic-RAG-Systems" class="headerlink" title="Hierarchical Agentic RAG Systems"></a>Hierarchical Agentic RAG Systems</h3><p>层级式架构类似于企业的组织结构，这种架构特别适合处理复杂的研究任务。</p><p><img src="/img/fig/Hierarchical-rag.png"></p><h3 id="Agentic-Corrective-RAG"><a href="#Agentic-Corrective-RAG" class="headerlink" title="Agentic Corrective RAG"></a>Agentic Corrective RAG</h3><p>自纠错架构，该架构引入了智能的质量控制机制。</p><p>相关性评估：确保检索内容的准确性</p><p>查询优化：动态调整搜索策略</p><p>外部知识补充：及时补充缺失信息</p><p>响应合成：生成高质量答案</p><p><img src="/img/fig/Agentic-Corrective-RAG.png"></p><h3 id="Adaptive-Agentic-RAG"><a href="#Adaptive-Agentic-RAG" class="headerlink" title="Adaptive Agentic RAG"></a>Adaptive Agentic RAG</h3><p>自适应架构，该架构能根据问题的复杂度动态调整处理策略</p><p><img src="/img/fig/Adaptive-Agentic-RAG.png"></p><h3 id="Graph-Based-Agentic-RAG"><a href="#Graph-Based-Agentic-RAG" class="headerlink" title="Graph-Based Agentic RAG"></a><strong>Graph-Based Agentic RAG</strong></h3><p>将检索增强生成技术与知识图谱结合起来，提升系统的推理能力</p><p>理解实体间的复杂关联、支持跨领域知识关联、优化知识组织方式</p><p><img src="/img/fig/Agent-G.png"></p><h3 id="Agentic-Document-Workflows-in-Agentic-RAG"><a href="#Agentic-Document-Workflows-in-Agentic-RAG" class="headerlink" title="Agentic Document Workflows in Agentic RAG"></a><strong>Agentic Document Workflows in Agentic RAG</strong></h3><p>文档工作流架构是企业级应用的选择</p><p><img src="/img/fig/agentic-document.png"></p><h2 id="Tool-and-FrameWork"><a href="#Tool-and-FrameWork" class="headerlink" title="Tool and FrameWork"></a>Tool and FrameWork</h2><ul><li>Langchain and LangGraph</li><li>LlamaIndex</li><li>Hugging Face Transformers and Qdrant</li><li>Neo4j and Vector databases</li></ul><h2 id="落地场景"><a href="#落地场景" class="headerlink" title="落地场景"></a>落地场景</h2><ol><li><strong>在教育与个性化学习领域中为不同学习者提供个性化和适应性强的学习体验</strong>。例如设计能够实时检索知识和提供个性化反馈的智能辅导器，根据学生进度和偏好生成定制的教育内容，多智能体系统用于协作学习模拟。</li><li><strong>在客户支持与虚拟助手领域提供与客户查询上下文相关且动态的响应。</strong> 例如构建基于人工智能的虚拟助手，用于实时客户支持；自适应系统，通过学习用户反馈改进响应；多智能体编排处理复杂的多查询互动。</li><li><strong>在多模态工作流程中增强图的应用程序领域处理需要关系理解和多模态数据集成的任务</strong>。例如，基于图的检索系统，用于连接结构化和非结构化数据；在科学研究、知识管理等领域中增强推理工作流程；将文本、图像和结构化数据的洞察综合起来，以生成可操作的输出。</li><li><strong>在以文档为中心的工作流领域自动化涉及文档解析、数据提取和多步骤推理的复杂工作流</strong>。</li></ol>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>多模态</tag>
      
      <tag>Agentic RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>寒假阅读书单</title>
    <link href="/2025/01/19/%E5%AF%92%E5%81%87%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95/"/>
    <url>/2025/01/19/%E5%AF%92%E5%81%87%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95/</url>
    
    <content type="html"><![CDATA[<h2 id="2025年第一个月的大部分时间都干了个啥"><a href="#2025年第一个月的大部分时间都干了个啥" class="headerlink" title="2025年第一个月的大部分时间都干了个啥"></a>2025年第一个月的大部分时间都干了个啥</h2><p>2024年12月31号在湖里度过了难忘的跨年晚会，进入2025年1月就完全进入了期末复习周，1月10号完成了期末考试，10-16号留校、回所和导师们汇报完这学期的进度以及后续安排，14号完成了晓飞老师《机器学习》课程的实验报告，16号离校去房山舅舅家住了两天，18号回家。忙忙碌碌地度过了2025年的1月份多半时光，伴随而来的寒假(大概率也是学生时代的最后一个完整寒假)正式开始了。</p><h2 id="趁着最后一个完整的寒假，多读点东西"><a href="#趁着最后一个完整的寒假，多读点东西" class="headerlink" title="趁着最后一个完整的寒假，多读点东西"></a>趁着最后一个完整的寒假，多读点东西</h2><p>论文必须要接着读哈哈哈，按着大老师的部署，继续向下推进任务，2025伴随着Agent的东风，争取做出些东西出来！！</p><p>技术相关：</p><ul><li><p>刘焕勇老师相关文章及推荐论文</p></li><li><p>从零构建向量数据库</p></li></ul><p>多方面：</p><ul><li>影响力</li><li>穷查理宝典</li><li>毛选</li><li>巴拉吉预言</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Attention is all you need</title>
    <link href="/2025/01/15/Attention-is-all-you-need/"/>
    <url>/2025/01/15/Attention-is-all-you-need/</url>
    
    <content type="html"><![CDATA[<div class="note note-danger">            <p>攻坚克难，务必拿下Transformer，彻底搞懂原理</p>          </div><p>参考教程：</p><p>[1] 动手学深度学习</p><p>[2] <a href="https://github.com/datawhalechina/fun-transformer/blob/main/docs/chapter1/introduction.md">Data Whale训练营-手撕Transformer</a></p><h2 id="Transformer名字的由来"><a href="#Transformer名字的由来" class="headerlink" title="Transformer名字的由来"></a>Transformer名字的由来</h2><p>论文的第一作者最喜欢的电影是《变形金刚》</p><div class="note note-success">            <p>论文的创新点：提出了一个新的简单的网络结构——Transformer，完全依赖于注意力机制而省去了卷积和循环</p>          </div><h2 id="注意力"><a href="#注意力" class="headerlink" title="注意力"></a>注意力</h2><p>就我个人而言，注意力对于我的作用就是提高效率，特别是提高做事情的效率。注意力提升了，在学习的时候就非常认真、专注，从而学习新的东西就会非常快。我发现一旦开始“使用”注意力，就能够快速对与外界刺激作出相应的反馈，无论是背诵、记忆、识别、计算，以上种种行为，在我注意力buff加持下都会比平时懒懒散散的状态下作出的反应要高效。</p><p>类比到机器身上，目前借鉴人类的行为模式，机器利用神经网络所从事的两大类主要任务是：运用<strong>视觉思维</strong>的识别与分类、运用<strong>语言思维</strong>的序列理解。对于这两种思维模式，在注意力buff加持下，执行任务的效率会显著提高。</p><p>而设计Transformer架构的主要任务，换言之，设计Transformer架构要解决的核心问题就是利用神经网络去提高机器翻译的准确率，设计的初衷是去解决<strong>机器翻译</strong>任务。</p><p>注意力有自主性和非自主性之分：<strong>非自主性</strong>提示是基于环境中物体的突出性和易见性(比如走在大街上在茫茫人海中一眼就发现了一个漂亮小姐姐…)、<strong>自主性</strong>是受到了认知和意识的控制。</p><h2 id="提问：如何进行机器翻译？—从Seq2Seq到Transformer"><a href="#提问：如何进行机器翻译？—从Seq2Seq到Transformer" class="headerlink" title="提问：如何进行机器翻译？—从Seq2Seq到Transformer"></a>提问：如何进行机器翻译？—从Seq2Seq到Transformer</h2><p>机器翻译是<strong>序列转换模型</strong>的一个核心问题， 其输入和输出都是长度可变的序列。</p><p>论文的摘要中提到了，当下主要的序列转换模型是基于包含一个编码器和一个解码器的复杂卷积神经网络和循环网络。表现最好的模型是使用注意力机制链接编码-解码架构。</p><div class="note note-danger">            <p>那么，编码-解码架构如何进行机器翻译呢？</p>          </div><p>这就不得不学习一下编码-解码架构：</p><p><img src="/./img/fig/encoder-d.jpg" alt="encoder-decoder"></p><p>编码器作用：接受一个长度可变的序列作为输入，将其转换为<strong>固定形式</strong>的编码状态。</p><p>解码器作用：将固定形式的编码状态<strong>映射</strong>到长度可变的变长序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Encoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Decoder, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, *args</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br><span class="hljs-comment">#@ EncoderDecoder基类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderDecoder</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,encoder,decoder</span>):<br>    <span class="hljs-built_in">super</span>(EncoderDecoder,<span class="hljs-variable language_">self</span>).__init__()<br>    <span class="hljs-variable language_">self</span>.encoder = encoder<br>    <span class="hljs-variable language_">self</span>.decoder = decoder<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forword</span>(<span class="hljs-params">self,enc_x,dec_x</span>):<br>    enc_output = <span class="hljs-variable language_">self</span>.encoder(enc_x)<br>    dec_state = <span class="hljs-variable language_">self</span>.decoder.init_state(enc_output)<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.decoder(dec_x,dec_state)<br></code></pre></td></tr></table></figure><p>给一个英文的输入序列：“我有一只笔”，首先，编码-解码架构会将这个变长的英语序列编码为一种“状态”，然后对这种状态进行解码，一个词元接着一个词元就生成被翻译的可变长度序列输出：“I have a pen”。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%881%EF%BC%89.PNG" alt="翻译过程"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;bos&gt;：Begin Of Sequence（BOS）<br>&lt;eos&gt;：End Of Sequence（EOS）<br>“BOS I have a pen EOS”，在这里，BOS 和 EOS 分别标记句子的开始和结束。<br></code></pre></td></tr></table></figure><p>在处理可变长度的序列时，使用 BOS 和 EOS 可以减少对 <strong>填充</strong>（<strong>padding</strong>）的依赖，因为模型可以根据这些标记来识别序列的边界。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%882%EF%BC%89.png" alt="seq2seq"></p><p>编码器所做的工作就是把一大段文字浓缩成一个简短的摘要，它把整个输入序列压缩成一个精华的上下文向量。然后，解码器就像一个作家，根据这个摘要重新创作出一篇完整的文章。</p><p>但是，上下文向量是固定维度的，这就导致会造成信息损失，特别是细粒度的丢失。</p><p>我们试图把一本厚厚的大百科全书的内容全部塞进一个张张小的记忆卡片里。Seq2Seq模型在做类似的事情，它需要把整个输入序列的信息压缩成一个固定大小的上下文向量。这就好比你只能记住百科全书的概要，而丢失了很多细致入微的细节。</p><p>同时，Seq2Seq模型有时候就像一个有短期记忆障碍的人，它很难回忆起很久以前发生的事情。序列虽然是变长的，但是长度过大模型往往难以捕捉到序列开始和结束之间的长期依赖关系，就像试图回忆一个长故事的每一个细节一样困难。</p><p>Teacher forcing：就像是考试时老师不断给你提示答案的一种训练方法</p><p>Exposure Bias：模型可能会过分依赖那些训练时的“提示”，而忽略了如何独立解决问题。</p><p><img src="/./img/fig/seq2seq.jpg" alt="简易的翻译流程图"></p><p>在Encoder-Decoder架构中，编码器和解码器之间使用一个<strong>固定长度</strong>的“上下文向量”来传递信息，这就好比通信机制中的“压缩-解压缩”的过程。</p><p>将一张将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 5000X5000 像素的图片也压缩到 100KB，看上去就模糊了。Encoder-Decoder也同样面临着这种困境：当输入的信息过长时(很长的上下文)，就会丢失掉一些信息。</p><p>基于这种缺陷，于是后面便创造出来Attention注意力机制。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><div class="note note-danger">            <p>编码-解码架构的痛点</p>          </div><p><code>长过程依赖</code>：输入句子很长的时候，编码器的理解后会创建一个糟糕的总结“上下文向量”。</p><p><code>梯度消失/爆炸</code>：RNN无法记住较长的句子和序列，LSTM被认为比RNN更能捕捉长依赖关系但是特定情况下容易遗忘。</p><p>那么，经过编码器创建上下文向量时，如何更小程度地保证句子中的信息损失较小呢？</p><p>于是，注意力机制应运而生。Bahdanau等人(2015)提出一个简单但优雅的想法：<strong>不仅在上下文向量中考虑所有的输入词，还可以赋予每个输入词相对重要性。</strong></p><p>注意力就是在编码器隐藏状态中搜索一组可获得最相关信息的位置。</p><p>Attention模型最大的特点就是不再将输入序列Encoder阶段生成<strong>固定长度</strong>的“content vector”，而是编码成一个向量的序列，解决固定长度而产生的“<strong>信息过长，信息丢失</strong>”问题。</p><p><img src="https://github.com/Spr1ng7/fun-transformer/raw/main/docs/chapter1/images/image%EF%BC%8810%EF%BC%89.png" alt="attention"></p><p>这样以来，输入是序列，输出是序列，而中间的过渡变量仍然是序列。<strong>变化一小步，进步一大步</strong>。</p><p>Attention机制就是<strong>将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。</strong></p><div class="note note-success">            <p>核心</p>          </div><p>Attention的核心工作是：<strong>保持专注，关注重点</strong>，抛给你一张图片，关注有效信息、有效特征；同时，尽可能地减少无效信息、无效特征的干扰。</p><p>Attention的核心思想是：<strong>加权求和</strong></p><blockquote><p>从上面的建模，我们可以大致感受到 Attention 的思路简单，四个字“<strong>带权求和</strong>”就可以高度概括，大道至简。做个不太恰当的类比，人类学习一门新语言基本经历四个阶段：死记硬背（通过阅读背诵学习语法练习语感）-&gt; 提纲挈领（简单对话靠听懂句子中的关键词汇准确理解核心意思）-&gt; 融会贯通（复杂对话懂得上下文指代、语言背后的联系，具备了举一反三的学习能力）-&gt; 登峰造极（沉浸地大量练习）。 这也如同attention的发展脉络，RNN 时代是死记硬背的时期，attention 的模型学会了提纲挈领，进化到 transformer，融汇贯通，具备优秀的表达学习能力，再到 GPT、BERT，通过多任务大规模学习积累实战经验，战斗力爆棚。 要回答为什么 attention 这么优秀？是因为它让模型开窍了，懂得了提纲挈领，学会了融会贯通。 — — 阿里技术</p></blockquote><div class="note note-success">            <p>全局注意力与局部注意力</p>          </div><p>受到计算机视觉的影响，我们将滤波器和感受野的思路引入注意力机制。</p><h2 id="Transformer模型结构"><a href="#Transformer模型结构" class="headerlink" title="Transformer模型结构"></a>Transformer模型结构</h2><p><img src="/./img/fig/transformer.jpg" alt="transformer"></p><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Encoder</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;init函数中实现对于各个层的定义&#x27;&#x27;&#x27;</span><br>        <span class="hljs-built_in">super</span>(EncoderLayer,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 定义注意力层</span><br>        <span class="hljs-variable language_">self</span>.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)<br>        <span class="hljs-comment"># 定义归一化层</span><br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(p=drop_prob)<br>        <br>        <span class="hljs-comment"># 定义前向传播层</span><br>        <span class="hljs-variable language_">self</span>.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)<br>        <span class="hljs-comment"># 定义归一化层</span><br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(p=drop_prob)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        residual_x = x <span class="hljs-comment"># 30 x 200 x 512 </span><br>        x = <span class="hljs-variable language_">self</span>.attention(x,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.dropout1(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.norm1(x + residual_x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        residual_x = x <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.ffn(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.dropout2(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        x = <span class="hljs-variable language_">self</span>.norm2(x + residual_x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,d_model,ffn_hidden,num_heads,drop_prob,num_layers</span>):<br>        <span class="hljs-built_in">super</span>(Encoder,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)<br>                                      <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layers(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><div class="note note-success">            <p>全局注意力与局部注意力</p>          </div><p>attention计算为什么要除以$\sqrt{d_k}$ ？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob</span>):<br>        <span class="hljs-built_in">super</span>(DecoderLayer,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-string">&#x27;&#x27;&#x27;init函数中实现对于各个层的定义&#x27;&#x27;&#x27;</span><br>        <span class="hljs-variable language_">self</span>.self_attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)<br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(p=drop_prob)<br>        <span class="hljs-variable language_">self</span>.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)<br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(p=drop_prob)<br>        <span class="hljs-variable language_">self</span>.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)<br>        <span class="hljs-variable language_">self</span>.norm3 = LayerNormalization(parameters_shape=[d_model])<br>        <span class="hljs-variable language_">self</span>.dropout3 = nn.Dropout(p=drop_prob)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y, decoder_mask</span>):<br>        _y = y <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.self_attention(y,mask=decoder_mask) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout1(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 1&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm1(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 1&quot;</span>)<br>        <br>        _y = y <span class="hljs-comment"># 30 x 200 x 512 </span><br>        y = <span class="hljs-variable language_">self</span>.encoder_decoder_attention(x,y,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout2(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 2&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm2(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 2&quot;</span>)<br>        <br>        _y = y <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.ffn(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        y = <span class="hljs-variable language_">self</span>.dropout3(y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;DROP OUT 3&quot;</span>)<br>        y = <span class="hljs-variable language_">self</span>.norm3(y + _y) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ADD + NORM 3&quot;</span>)<br>        <span class="hljs-keyword">return</span> y <span class="hljs-comment"># 30 x 200 x 512</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SequentialDecoder</span>(nn.Sequential):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *inputs</span>):<br>        x,y,mask = inputs<br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._modules.values():<br>            y = module(x,y,mask) <span class="hljs-comment">## 30 x 200 x 512</span><br>        <span class="hljs-keyword">return</span> y<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,ffn_hidden,num_heads,drop_prob,num_layers=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(Decoder,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)<br>                                          <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y, mask</span>):<br>        <span class="hljs-comment">#x : 30 x 200 x 512</span><br>        <span class="hljs-comment">#y : 30 x 200 x 512</span><br>        <span class="hljs-comment">#mask : 200 x 200</span><br>        y = <span class="hljs-variable language_">self</span>.layers(x,y,mask)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><h4 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入：需要经过softmax计算的序列</span><br><span class="hljs-comment"># 输出：经过softmax计算得到的序列</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (np.exp(x).T / np.<span class="hljs-built_in">sum</span>(np.exp(x), axis=-<span class="hljs-number">1</span>)).T<br><br><span class="hljs-comment"># 点积缩放注意力</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">scaled_dot_production_attention</span>(<span class="hljs-params">q,k,v,mask=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        实现点积缩放注意力机制</span><br><span class="hljs-string">            输入：</span><br><span class="hljs-string">                q: query</span><br><span class="hljs-string">                k: key</span><br><span class="hljs-string">                v: value</span><br><span class="hljs-string">                mask = None</span><br><span class="hljs-string">            输出：</span><br><span class="hljs-string">                out: attention * value</span><br><span class="hljs-string">                attention: attention score</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># q,k,v大小: 30 x 8 x 200 x 64</span><br>    <span class="hljs-comment"># mask大小: 200 x 200</span><br>    d_k = q.size()[-<span class="hljs-number">1</span>] <span class="hljs-comment"># 64</span><br>    scaled = torch.matmul(q, k.transpose(-<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)) / math.sqrt(d_k) <span class="hljs-comment"># 30 x 8 x 200 x 200</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;scaled.size() : <span class="hljs-subst">&#123;scaled.size()&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---ADDing MASK---&quot;</span>)<br>        scaled += mask <span class="hljs-comment"># 200 x 200 Broading add</span><br>    attention = F.softmax(scaled,dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># 30 x 8 x 200 x 200</span><br>    values = torch.matmul(attention,v) <span class="hljs-comment"># 30 x 8 x 200 x 64</span><br>    <span class="hljs-keyword">return</span> values, attention<br></code></pre></td></tr></table></figure><h4 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多头注意力机制   </span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;多头注意力机制的实现&#x27;&#x27;&#x27;</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        MultiHeadAttention input:</span><br><span class="hljs-string">            input_dim: 输入维度</span><br><span class="hljs-string">            d_model: 模型维度</span><br><span class="hljs-string">            num_heads: 注意力头数量</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        MultiHeadAttention output:</span><br><span class="hljs-string">            attention: 注意力分数</span><br><span class="hljs-string">            output: attention * v </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,d_model,num_heads</span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadAttention,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.d_model = d_model <span class="hljs-comment"># 512</span><br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads <span class="hljs-comment"># 8</span><br>        <span class="hljs-variable language_">self</span>.head_dim = d_model // num_heads <span class="hljs-comment"># 64</span><br>        <span class="hljs-comment"># 定义qkv层</span><br>        <span class="hljs-variable language_">self</span>.qkv_layer = nn.Linear(d_model, <span class="hljs-number">3</span> * d_model) <span class="hljs-comment"># 512 x 1536</span><br>        <span class="hljs-comment"># 定义线性层</span><br>        <span class="hljs-variable language_">self</span>.linear_layer = nn.Linear(d_model,d_model) <span class="hljs-comment"># 512 x 512</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,inputs,mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===MultiHeadAttention===&quot;</span>)<br>        batch_size,sequence_length,d_model = inputs.size() <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x.size() = :<span class="hljs-subst">&#123;inputs.size()&#125;</span>&quot;</span>)<br>        qkv = <span class="hljs-variable language_">self</span>.qkv_layer(inputs) <span class="hljs-comment"># 30 x 200 x 1536</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        qkv = qkv.reshape(batch_size,sequence_length,<span class="hljs-variable language_">self</span>.num_heads,<span class="hljs-number">3</span> * <span class="hljs-variable language_">self</span>.head_dim) <span class="hljs-comment"># 30 x 200 x 8 x 192</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv reshape size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        qkv = qkv = qkv.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>) <span class="hljs-comment"># 30 x 8 x 200 x 192</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;qkv permute size() = : <span class="hljs-subst">&#123;qkv.size()&#125;</span>&quot;</span>)<br>        q,k,v = qkv.chunk(<span class="hljs-number">3</span>,dim = -<span class="hljs-number">1</span>) <span class="hljs-comment"># each chunk: 30 x 8 x 200 x 64</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;q,k,v size() = : <span class="hljs-subst">&#123;q.size()&#125;</span>&quot;</span>)<br>        values,attention = scaled_dot_production_attention(q,k,v,mask=<span class="hljs-literal">None</span>) <span class="hljs-comment"># attention 30 x 8 x 200 x 200; value 30 x 8 x 200 x 64</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;attention size() = : <span class="hljs-subst">&#123;attention.size()&#125;</span>&quot;</span>)<br>        values = values.reshape(batch_size,sequence_length,<span class="hljs-variable language_">self</span>.num_heads * <span class="hljs-variable language_">self</span>.head_dim) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;values size() = : <span class="hljs-subst">&#123;values.size()&#125;</span>&quot;</span>)<br>        out = <span class="hljs-variable language_">self</span>.linear_layer(values) <span class="hljs-comment"># </span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 位置编码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositonalEncoding</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        PositionalEncoding input:</span><br><span class="hljs-string">            d_model:</span><br><span class="hljs-string">            max_sequence_length:</span><br><span class="hljs-string">        PositionalEncoding output:</span><br><span class="hljs-string">            PE: A matrix with positional score</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model,max_sequence_length</span>):<br>        <span class="hljs-built_in">super</span>(PositonalEncoding,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.max_sequence_length = max_sequence_length<br>        <span class="hljs-variable language_">self</span>.d_model = d_model<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        even_i = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-variable language_">self</span>.d_model,<span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>()<br>        denominator = torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">10000</span>,even_i/<span class="hljs-variable language_">self</span>.d_model)<br>        position = torch.arange(<span class="hljs-variable language_">self</span>.max_sequence_length).reshape(<span class="hljs-variable language_">self</span>.max_sequence_length,<span class="hljs-number">1</span>)<br>        even_PE = torch.sin(position / denominator)<br>        odd_PE = torch.cos(position / denominator)<br>        stacked = torch.stack([even_PE,odd_PE],dim = <span class="hljs-number">2</span>)<br>        PE = torch.flatten(stacked,start_dim = <span class="hljs-number">1</span>,end_dim =<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> PE<br></code></pre></td></tr></table></figure><h3 id="前向传播网络"><a href="#前向传播网络" class="headerlink" title="前向传播网络"></a>前向传播网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 前向传播网络   </span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionwiseFeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, hidden, drop_prob=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(PositionwiseFeedForward, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(d_model, hidden) <span class="hljs-comment"># 512 x 2048</span><br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(hidden, d_model) <span class="hljs-comment"># 2048 x 512</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=drop_prob)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===FeedForward===&quot;</span>)<br>        x = <span class="hljs-variable language_">self</span>.linear1(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x linear1 size() = : <span class="hljs-subst">&#123;x.size()&#125;</span>&quot;</span>)<br>        x = <span class="hljs-variable language_">self</span>.relu(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        x = <span class="hljs-variable language_">self</span>.dropout(x) <span class="hljs-comment"># 30 x 200 x 2048</span><br>        x = <span class="hljs-variable language_">self</span>.linear2(x) <span class="hljs-comment"># 30 x 200 x 512</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x linear2 size() = : <span class="hljs-subst">&#123;x.size()&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><h2 id="机器翻译任务实践"><a href="#机器翻译任务实践" class="headerlink" title="机器翻译任务实践"></a>机器翻译任务实践</h2>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GPT</tag>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年度总结</title>
    <link href="/2024/12/31/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    <url>/2024/12/31/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<center><h1>永远相信美好的事情即将发生</h1></center><div class="note note-danger">            <p>2025年 务必谦虚谨慎 艰苦奋斗 戒骄戒躁</p><p>写每一篇文章前都要牢牢把握好一个目的：<strong>写这个东西的动机是什么，要解决什么问题？</strong></p><p>毛主席讲过，做文章是要解决问题的。</p>          </div><center><img src="https://cdnv2.ruguoapp.com/FtwhP5bICelEIUZWWcJbhEnlIXLwv3.heic?imageMogr2/auto-orient/heic-exif/1/format/jpeg/thumbnail/1500x2000%3E/interlace/1" alt="永远相信美好的事情即将发生" style="zoom:37%;" /></center><h2 id="终点和起点，分别和相聚-2024大事记"><a href="#终点和起点，分别和相聚-2024大事记" class="headerlink" title="终点和起点，分别和相聚 - 2024大事记"></a>终点和起点，分别和相聚 - 2024大事记</h2><p>2024年主要经历了两件大事：本科毕业、研究生生涯开启，分别和相聚是这一年的主旋律。</p><p>2024年开始认真经营这个网站，发布了<strong>39篇</strong>博客文章，当然有好几篇文章的内容仍然需要完善修改。</p><p>2024年去的地方是这20多年里走过地方最多的一年：杭州、珠海、广州、澳门、佛山、正定、太原、五台、大同、济南、泰安、天津、上海、雄安新区；从小到大来了这么多次北京，市区里面新解锁的地方有慕田峪长城、雁栖湖、红螺寺、香山、大会堂、国博(这几个地方还真是第一次身临其境去参观)。</p><p>2024年的下半年来到果壳后，结实了很多更加优秀的朋友和老师，对于做事的动机和思维认知又有了一点新的提高。</p><p>2024年带着老妈和老弟去了趟上海，第一次做导游带着家人出去玩。</p><p>2024年奶奶住院，第一次在医院陪床，真实经历了晚上的医院到底是什么样子。</p><h2 id="技术路线更新"><a href="#技术路线更新" class="headerlink" title="技术路线更新"></a>技术路线更新</h2><p>2024年的上半年基本上属于<strong>half - GAP - year</strong>，本科这四年跑的快了，没来得及好好喘口气，今年上半年打算停下来歇一歇，走一走，深呼吸一下。</p><p>2024年下半年来到国科大后慢慢开始确定了研究方向，下半年其实主要就是学了深度学习的原理与使用，大模型领域的相关知识。</p><p>技术路线由原来的<strong>大数据技术路线</strong>逐渐转移、更新转化到<strong>人工智能、大模型开发</strong>上来了。大模型、数据存储、信息检索、多模态大数据分析是需要精进的技术。</p><h2 id="每一天都是崭新的，做行动的巨人"><a href="#每一天都是崭新的，做行动的巨人" class="headerlink" title="每一天都是崭新的，做行动的巨人"></a>每一天都是崭新的，做行动的巨人</h2><p>2024年懒了很多，可能也是和GAP year有关，勤奋和懒惰其实是相对的，不是每天11点睡7点起你就给出努力和勤奋，比起早睡早起，对于时间的利用效率更是关键，当然早睡早起的好习惯绝对是好的。</p><p>长远来看。<strong>系统性思维</strong>要优于<strong>目标性思维</strong>。长期坚持每天规律饮食、规律作息，长远来看要比短期减肥、减脂更有价值。</p><p>2025年希望自己继续保持前本科时代的生活习惯和作风，永远积极向上，做行动的巨人，不拖延。</p><h2 id="想探索的"><a href="#想探索的" class="headerlink" title="想探索的"></a>想探索的</h2><p>想买个相机，好好学一学摄影</p><p>想回所后再买辆公路车，把环线和北京的边边角角都走一遍</p><center><h1>2025年，继续做时间的朋友、做实践的朋友</h1></center><p>觉得罗振宇老师真的厉害，不知不觉的，跨年演讲坚持做了10年了。</p><p>其实我一直关注跨年演讲，吴晓波、罗振宇，包括何帆老师的《变量》系列书籍、吴军老师系列书籍，这些有价值的东西对我的认知和心态都有了很大的改进和提升。</p><p>摘抄里面的一些好的观点，希望用它们贯穿整个2025吧！</p><p>2025，请继续加油！</p><div class="note note-success">            <p>所有来自未来的好消息，都是我们今天努力的模样。</p><p>命运不在人的身上，而在人的周围。</p><p>悲观只是一种看法，乐观却是一种行动。</p><p>“买三送一”：凭借自己的努力多活三年，科技进步再帮助你多活一年</p><p>幸福总量新法则：挣得多不如活得长</p><p>所有的暴躁都是因为能力不够，所有的劳苦都是因为方法不对。</p>          </div><div class="note note-danger">            <p><strong>“发大愿，迈小步，走远路，磕长头，不停顿，不着急”。</strong></p>          </div>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年12月总结与展望</title>
    <link href="/2024/12/31/12%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/12/31/12%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<center><img src="https://hips.hearstchina.com/hmg-prod/images/%E5%9B%BE%E7%89%871-65af6a5382828.png?resize=640:*" alt="告别2024" style="zoom:90%;" />  </center><h2 id="11月任务回顾"><a href="#11月任务回顾" class="headerlink" title="11月任务回顾"></a>11月任务回顾</h2><p>2024年最后一个月，主要任务就一个：把之前<strong>深度学习</strong>所有相关知识整合、查漏补缺、复现Coding</p><p>今天5号了，这个月的任务除了复习最后的三门课之外，更多的精力要分配给如下任务：</p><ul><li>卷积神经网络学习(还剩VGG、ResNet没有实现)</li><li>语言神经网络学习</li><li>多读经典论文、多复现代码(循环神经网络、Attention、Transformer、BERT掌握)</li><li>精进python、C++(必须精通使用)</li></ul><h2 id="12月完成度"><a href="#12月完成度" class="headerlink" title="12月完成度"></a>12月完成度</h2><p>12与集中在RNN、注意力机制和Transformer复现上</p><ul><li><p>经典的卷积神经网络复现✅</p></li><li><p>语言神经网络学习 - <strong>仍在学习中</strong>，结合NLP期末复习</p></li><li><p>循环神经网络✅</p></li><li><p>Attention✅</p></li><li><p>Transformer✅</p></li><li><p>BERT(集中精力都放在了Transformer上) ❌</p></li></ul><h2 id="1月假期计划安排"><a href="#1月假期计划安排" class="headerlink" title="1月假期计划安排"></a>1月假期计划安排</h2><div class="note note-success">            <p>真正的高手都是把一个程序反复写，每一遍都和前一遍不一样，有提高有改进；做设计要优先于细节完善 —— 杨力祥</p><p>细节实现前，多画图多做设计！</p>          </div><p>假期需要完成的主要任务 - List：</p><ol><li><strong>继续完善</strong> 秋季学期所有的学科笔记与知识梳理&#x2F;真题复现，可选择性的放到CSDN或git上，想着做几个高质量有价值的git仓库或者CSDN文章</li><li><strong>继续完善</strong> C++课程的五子棋程序，争取学会如何在C++编程中使用pytorch，使用神经网络训练五子棋的AI</li><li><strong>继续完善</strong> 大数据作业实验，参考珊瑚书完成对分布式向量数据库的实现(大工程)</li><li><strong>继续完善</strong> 深度学习知识体系，对经典的代码反复研读复现，做一些小应用</li><li><strong>继续完善</strong> Transformer架构复现与应用，认真学一下数据如何在torch上进统一计算与模型训练代码的编写</li><li><strong>继续完善</strong> 博客网站上的全部文章，争取把所有内容做实</li><li><strong>继续学习</strong> 大模型领域的知识与实践：模型的学习(BERT、CLIP、BLIP)和Agentic RAG应用的技能修炼。RAG应用是关乎饭碗型的任务，务必持续学习深入，扎实提升技能！</li><li><strong>继续研读</strong> 领域内的经典论文</li></ol>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习与大数据管理总结复习</title>
    <link href="/2024/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>有题库就不再总结零散的知识点，重点看自己不熟悉的地方</p>          </div><p><strong>过拟合</strong>：一味追求提高对训练数据的预测能力，所选模型复杂度往往会比真实模型高。学习时选择的模型包含参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测的很差的现象。</p><p><strong>K折交叉验证</strong>：随机地将已给数据切分为K个互不相交的大小相同的子集；然后利用K-1个子集的数据训练模型，利用剩下的子集进行测试模拟。将这个过程对可能的K种选择重复进行，最后选出K次测评中平均测试误差最小的模型。</p><p>分类与回归本质上都是一种<strong>映射关系</strong>的建立，本质区别就是分类时离散数据、回归时连续数据。区别：分类强调依据类别标签y对样本x空间的划分，回归强调x与回归值的拟合</p><p>集成学习</p><p>数据维度</p><p>大数据的特征</p><p>数据科学的定义</p><h2 id="数据科学的生命周期"><a href="#数据科学的生命周期" class="headerlink" title="数据科学的生命周期"></a>数据科学的生命周期</h2><p>数据科学的生命周期、特点</p><p>数据科学项目的基本流程和特点</p><p>样本选择</p><p>批判性思维</p><p>判断逻辑谬误</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>数据采集的主要技术</p><p>缺失、冗余、噪声的数据处理</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>LDA(线性鉴别分析)：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到这条直线上，再根据投影点来判断新样本的类别。</p><p>决策树 - ID3</p><p>朴素贝叶斯</p><p>Logistics回归</p><p>k-近邻</p><p>SVM：支持向量机是一个分类算法，它的目标就是确定一个超平面，从而将不同类别的数据分隔开以达到分类的目标。当训练数据线性可分的时候，通过硬间隔最大化，学习一个线性分类器，即线性可分的支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，即软间隔支持向量机；当训练数据线性不可分时，通过使用核函数技巧或者软间隔最大化学习非线性支持向量机。</p><p>集成学习</p><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>划分聚类</p><p>层次聚类</p><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>一元</p><p>多元</p><p>非线性</p><p>逐步</p><h2 id="关联规则分析"><a href="#关联规则分析" class="headerlink" title="关联规则分析"></a>关联规则分析</h2><p>支持度、置信度</p><p>数据降维</p><p>PCA</p><p>时间序列分析</p><p>图数据挖掘</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 机器学习.周志华</p><p>[2] 统计学习方法. 李航</p><p>[3] 中国科学院大学机器学习课程资料(秋).  周晓飞</p><p>[4] 中国科学院大学机器学习课程资料(秋). 沙瀛、周川</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>期末总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自然语言处理基础总结复习</title>
    <link href="/2024/12/25/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/12/25/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="理解自然语言处理领域的基础技术与核心任务"><a href="#理解自然语言处理领域的基础技术与核心任务" class="headerlink" title="理解自然语言处理领域的基础技术与核心任务"></a>理解自然语言处理领域的基础技术与核心任务</h2><blockquote><p>什么是实体？怎样理解序列？</p><p>Entity vs Object；实体是“某种概念”的实例化，比如一个具体的地名、人名，我觉着实体其实和面向对象中对象的概念相似，就是一个<strong>东西</strong>，一个<strong>具体的东西</strong>。</p><p>广义的序列我认为可以理解为一串文字序列，一串经过嵌入后的文本向量。</p><p>词性标注：基于机器学习的方法，往往需要对分词后的词进行词性标注，词性一般包括动词、形容词、名词。标注的目的就是表征词的隐含状态，隐含状态构成了状态转移序列。</p><p>命名实体识别：一般指从文本中识别具有特定类别的实体，比如人名、地名、机构名、专有名词等。</p></blockquote><h3 id="核心任务"><a href="#核心任务" class="headerlink" title="核心任务"></a>核心任务</h3><div class="note note-success">            <p>六大核心任务与对应的11个子任务</p><p>文章顺序按照<strong>自顶向下</strong>进行总结梳理与复习</p>          </div><h4 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h4><ul><li>实体识别与抽取</li><li>实体消岐</li><li>关系抽取</li><li>事件抽取</li></ul><h4 id="归纳类别"><a href="#归纳类别" class="headerlink" title="归纳类别"></a>归纳类别</h4><p>文本分类</p><h4 id="情感识别"><a href="#情感识别" class="headerlink" title="情感识别"></a>情感识别</h4><p>情感分类</p><h4 id="回答问题"><a href="#回答问题" class="headerlink" title="回答问题"></a>回答问题</h4><ul><li>问答系统</li><li>机器阅读理解</li><li>智能对话</li></ul><h4 id="凝练内容"><a href="#凝练内容" class="headerlink" title="凝练内容"></a>凝练内容</h4><p>自动文摘</p><h4 id="语言翻译"><a href="#语言翻译" class="headerlink" title="语言翻译"></a>语言翻译</h4><p>机器翻译</p><h3 id="自然语言处理基础技术"><a href="#自然语言处理基础技术" class="headerlink" title="自然语言处理基础技术"></a>自然语言处理基础技术</h3><ul><li>分本分类</li></ul><p>利用计算机对大量文本的文档按照分类标准实现<strong>自动归档</strong>，目标就是将一段文字或者一个文本自动分配给一个或多个类别。</p><ul><li>分本匹配</li></ul><p>判断两个文本之间的“<strong>相似度</strong>”问题，并进一步地可以将两个文本进行对齐。</p><ul><li>序列标注</li></ul><p>对于给定的一个输入序列，使用模型对这个序列的每一个位置标注一个<strong>相应的标签</strong>。序列标注问题主要包括词性标注、语义角色标注、信息抽取(命名实体识别、关系抽取、事件抽取、信息集成)。</p><ul><li>序列生成</li></ul><p>根据输入内容序列和历史序列信息来<strong>预测序列的生成</strong>，序列标注可以看作是文本分类的推广。</p><h3 id="基于NLP领域内基础技术延伸出的基本模型方法与技术"><a href="#基于NLP领域内基础技术延伸出的基本模型方法与技术" class="headerlink" title="基于NLP领域内基础技术延伸出的基本模型方法与技术"></a>基于NLP领域内基础技术延伸出的基本模型方法与技术</h3><p>文本处理技术：分词技术、嵌入技术</p><p>文本分类：TF - IDF、CNN、RNN、LSTM</p><p>文本匹配：CBOW、Skip-gram</p><p>序列标注：隐马尔可夫模型HMM、维特比算法、RNN+CRF</p><p>序列生成：Seq2Seq、Attention、Transformer、BERT</p><h2 id="自然语言处理范式变迁"><a href="#自然语言处理范式变迁" class="headerlink" title="自然语言处理范式变迁"></a>自然语言处理范式变迁</h2><p>第一范式：特征工程+算法(概率统计时代)</p><p>第二范式：自动获取特征(表示学习)端到端分类</p><p>第三范式：预训练+精调范式</p><p>第四范式：预训练+提示+预测范式</p><p>第五范式：大模型</p><h2 id="基础概念复习"><a href="#基础概念复习" class="headerlink" title="基础概念复习"></a>基础概念复习</h2><h3 id="机器学习与深度学习基础相关"><a href="#机器学习与深度学习基础相关" class="headerlink" title="机器学习与深度学习基础相关"></a>机器学习与深度学习基础相关</h3><p>梯度消失的原因：在误差反向传播时，每一层都要乘以激活函数的导数，若导数值小于1，将导致误差越来越小，直至消失；如果导数过大，则会发生梯度爆炸。</p><p>激活函数的作用：增强网络表达能力，加入非线性因素</p><p>随机梯度下降与梯度下降区别：每次参数更新的数据量不同</p><h3 id="神经网络相关"><a href="#神经网络相关" class="headerlink" title="神经网络相关"></a>神经网络相关</h3><p>卷积层的作用：通过卷积操作减少参数</p><p>池化层的作用：通过下采样减少网络规模</p><p>全连接层：将池化层单元扁平化</p><p>RNN的参数训练方法：BPTT</p><h3 id="语言模型相关"><a href="#语言模型相关" class="headerlink" title="语言模型相关"></a>语言模型相关</h3><p>什么是词向量：将词转换为稠密向量的分布式表示</p><p>词向量的特征：语义相似的词，其词向量在空间距离上更相近</p><p>CBOW是训什么：用上下文预测中心词</p><p>Skip-Gram是训什么：用中心词预测上下文</p><p>Attention中普通模式和键值对模式异同：普通模式将输入信息直接作为键和值，键值对模式下“键”用于计算注意力分布，“值”用于生成最终的输出。</p><p>Prompt的核心思想是：将输入文本转换为模型可以理解的形式</p><p>什么是软注意力：选择的信息是所有输入信息在注意力分布下的期望</p><p>什么是硬注意力：只关注到<strong>某一个位置上</strong>的信息</p><p>什么是BERT：BERT通过使用双向Transformer编码器来捕捉文本中单词的上下文信息。</p><p>什么是BART：BART采用了一种编码-解码架构，编码器部分是双向的，类似于BERT，可以捕捉输入文本的上下文信息；解码器部分是自回归的，类似于传统的序列生成模型（如GPT），可以逐步生成输出序列。</p><p>GPT使用的是Transformer的什么：解码器部分</p><p>CRF的作用：CRF能够对整个序列进行全局优化，而不是单独地对每个元素进行标注，会考虑整个序列的上下文信息，使得标注结果更加一致和合理。</p><p>生成任务中常用的评价指标：BLUE、ROUGE</p><h2 id="2024年秋学期-期末考试真题回忆"><a href="#2024年秋学期-期末考试真题回忆" class="headerlink" title="2024年秋学期-期末考试真题回忆"></a>2024年秋学期-期末考试真题回忆</h2><p>家人们，能记住多少算多少哈哈哈哈~</p><p>考试时间：2025年1月6日</p><h3 id="选择题-10-1"><a href="#选择题-10-1" class="headerlink" title="选择题 10 * 1"></a>选择题 10 * 1</h3><ol><li><p>CBOW训练的是什么</p></li><li><p>skip_gram训练的是什么</p></li><li><p>激活函数有什么作用：增强网络表达能力</p></li><li><p>指针网络的作用：不仅可以用于机器翻译任务，还可以用于其他任务</p></li><li><p>词向量之间相似的原因解释</p></li><li><p>Attention的输入形式</p></li><li><p>top-k采样、贪心解码相关</p></li><li><p>Chatgpt相关的问题不正确的是：每次使用都是先微调</p></li></ol><p>还有两个记不起来了~</p><h3 id="填空题-10-2"><a href="#填空题-10-2" class="headerlink" title="填空题 10*2"></a>填空题 10*2</h3><p>   1.机器学习三要素</p><ol start="2"><li><p>池化层作用是什么</p></li><li><p>什么是硬注意力</p></li><li><p>BART中为什么要使用位置嵌入</p></li><li><p>Seq2Seq序列生成模型的三种类型</p></li><li><p>CRF的作用是什么</p></li><li><p>生成任务中常用的评价指标是什么</p></li><li><p>RNN训练方法是什么</p></li><li><p>文本匹配常用的建模方法</p></li><li><p>什么是曝光偏差</p></li></ol><h3 id="简答题-5-6"><a href="#简答题-5-6" class="headerlink" title="简答题 5*6"></a>简答题 5*6</h3><ol><li><p>什么是BPE算法？简要概述其工作原理</p></li><li><p>Transformer实现并行处理所用到的技术有哪些？</p></li><li><p>神经网络语言语言模型存在的问题？使用RNN+词向量的方式是如何解决这些问题的</p></li><li><p>简述第二、三、四、五范式特点和应用场景</p></li><li><p>简述传统的事件抽取任务，并给出传统的事件抽取的建模任务有哪些？具体实现步骤</p></li></ol><h3 id="计算题-10-1"><a href="#计算题-10-1" class="headerlink" title="计算题 10*1"></a>计算题 10*1</h3><p>维特比算法</p><h3 id="设计题-12-1"><a href="#设计题-12-1" class="headerlink" title="设计题 12*1"></a>设计题 12*1</h3><p>题干记得是：“这是一家正宗的俄式西餐，味道浓厚，环境很好”</p><p>(1) 写出属性情感输出</p><p>(2) 设计一个情感属性关联模型，解释其原理，画出模型图</p><h3 id="设计题-18-1"><a href="#设计题-18-1" class="headerlink" title="设计题 18*1"></a>设计题 18*1</h3><p>(1) 检索式问答系统包含哪几个部分？作用是什么？</p><p>(2) 设计一个检索式问答系统，并解释其原理</p>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>期末总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG综述一览</title>
    <link href="/2024/12/18/RAG%E7%BB%BC%E8%BF%B0%E4%B8%80%E8%A7%88/"/>
    <url>/2024/12/18/RAG%E7%BB%BC%E8%BF%B0%E4%B8%80%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p>参考文章：<a href="https://mp.weixin.qq.com/s/lbeeblGYvb1JeC0gW4AE9g">https://mp.weixin.qq.com/s/lbeeblGYvb1JeC0gW4AE9g</a></p><p>2025.02.23的自我批评：其实这篇综述文章早就应该写出来，但是就是一直拖着，结果一直到现在才想着去完善。最主要的原因是因为DataWhale发布了一篇推文，全面的梳理了RAG范式发展，与当初自己的思路不谋而合。所以，这学期一定不要拖延，想的事情就赶紧去做！！！</p><h2 id="三篇关键的综述"><a href="#三篇关键的综述" class="headerlink" title="三篇关键的综述"></a>三篇关键的综述</h2><p>[1] ZHAO P, ZHANG H, YU Q, 等. Retrieval-Augmented Generation for AI-Generated Content: A Survey[A&#x2F;OL]. arXiv, 2024[2024-06-21]. <a href="http://arxiv.org/abs/2402.19473">http://arxiv.org/abs/2402.19473</a>.　</p><p>[2] GAO Y, XIONG Y, GAO X, 等. Retrieval-Augmented Generation for Large Language Models: A Survey[A&#x2F;OL]. arXiv, 2024[2024-03-27]. <a href="http://arxiv.org/abs/2312.10997">http://arxiv.org/abs/2312.10997</a>.</p><p>[3] FAN W, DING Y, NING L, 等. A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models[A&#x2F;OL]. arXiv, 2024[2024-06-17]. <a href="http://arxiv.org/abs/2405.06211">http://arxiv.org/abs/2405.06211</a>. </p><p>三篇综述把RAG的三个基本范式，<strong>朴素RAG</strong>、<strong>高级RAG</strong>、<strong>模块化RAG</strong>介绍的非常清楚明了。</p><h2 id="范式发展"><a href="#范式发展" class="headerlink" title="范式发展"></a>范式发展</h2><div class="note note-success">            <p>Naive RAG -》Advanced RAG -》 Modular RAG -》Graph RAG -》 Agentic RAG</p>          </div><p>RAG技术起源于论文：LEWIS P, PEREZ E, PIKTUS A, 等. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks[A&#x2F;OL]. arXiv, 2021[2025-01-27]. <a href="http://arxiv.org/abs/2005.11401%EF%BC%9BRAG%E9%A6%96%E5%85%88%E8%A2%AB%E7%94%A8%E4%BA%8ELLMs%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E6%9D%A5%E5%A2%9E%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%9A%8F%E5%90%8E%E8%A2%AB%E7%94%A8%E4%BA%8E%E5%BE%AE%E8%B0%83%E4%B8%8E%E6%8E%A8%E7%90%86%E4%BB%BB%E5%8A%A1%E4%B8%AD%E3%80%82">http://arxiv.org/abs/2005.11401；RAG首先被用于LLMs的预训练阶段来增强语言模型，随后被用于微调与推理任务中。</a></p><h3 id="三范式阶段"><a href="#三范式阶段" class="headerlink" title="三范式阶段"></a>三范式阶段</h3><p>三种基本范式的演化历程是：NaiveRAG -》AdvancedRAG -〉 ModularRAG，2024年微软开源的GraphRAG开启了RAG的第四种范式，融合了知识图谱；在2024年下半年AgenticRAG出现，是前四种范式的集大成者，且具有自适应性。</p><p>NaiveRAG主要由三部分组成:索引、检索和生成。AdvancedRAG围绕检索前和检索后提出了多种优化策略，其过程与朴素RAG相似，仍然遵循链状结构。ModularRAG继承和发展了以前的范式，整体上展示了更大的灵活性。</p><ul><li><p>Advanced RAG代表作：<a href="http://arxiv.org/abs/2405.13576%EF%BC%8C%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E3%80%81%E6%A8%A1%E5%9D%97%E5%8C%96%E7%9A%84%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%8C%85FlashRAG%EF%BC%8C%E5%B7%A5%E5%85%B7%E5%8C%85%E5%AE%9E%E7%8E%B0%E4%BA%86">http://arxiv.org/abs/2405.13576，提出了一个高效、模块化的开源工具包FlashRAG，工具包实现了</a> 12 种先进的 RAG 方法，并收集和整理了 32 个基准数据集。</p></li><li><p>Modular RAG代表作：<a href="http://arxiv.org/abs/2407.21059%EF%BC%8C%E5%B0%86%E5%A4%8D%E6%9D%82%E7%9A%84RAG%E7%B3%BB%E7%BB%9F%E5%88%86%E8%A7%A3%E4%B8%BA%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%A8%A1%E5%9D%97%E5%92%8C%E4%B8%93%E9%97%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E7%AC%A6%EF%BC%8C%E5%BD%A2%E6%88%90%E4%B8%80%E4%B8%AA%E9%AB%98%E5%BA%A6%E5%8F%AF%E9%87%8D%E9%85%8D%E7%BD%AE%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82">http://arxiv.org/abs/2407.21059，将复杂的RAG系统分解为独立的模块和专门的操作符，形成一个高度可重配置的框架。</a></p></li></ul><p><img src="/img/fig/RAG-fanshi.png" alt="三种RAG范式之间的比较"></p><h3 id="Graph-RAG"><a href="#Graph-RAG" class="headerlink" title="Graph RAG"></a>Graph RAG</h3><p>图本身具有 “<strong>节点由边连接</strong> “的特性，可以编码大量的异构和关系信息，使其成为 RAG 在大量实际应用中的丰富资源。</p><p>针对LLMs在缺乏特定领域知识、实时更新信息和专有知识时遇到的问题，提出了GraphRAG作为一种解决上述问题的框架，通过结合<strong>图数据库</strong>中的结构化信息来增强LLMs的输出。</p><ul><li>LIANG L, SUN M, GUI Z, 等. <strong>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</strong>. arXiv, 2024[2024-11-12]. <a href="https://arxiv.org/abs/2409.13731">https://arxiv.org/abs/2409.13731</a>.</li></ul><p>论文引入了一个专业领域知识服务框架，称为知识增强生成（KAG）。为了充分发挥**知识图谱（KG）**和**向量检索**的优势，通过五个关键方面双向增强大型语言模型（LLM）和知识图谱（KG），从而提高生成和推理性能：（1）LLM友好的知识表示；（2）知识图谱和原始块之间的相互索引；（3）逻辑形式引导的混合推理引擎；（4）知识与语义推理的对齐；（5）KAG的模型能力增强。</p><p>一个专为大型语言模型（LLM）设计的友好型知识表示框架。LLMFriSPG将实例与概念区分开来，通过概念实现与 LLMs 的对接。SPG 的属性被划分为知识区和信息区，也就是静态区和动态区，分别与具有严格模式约束的决策专业知识以及具有开放信息表示的文档检索索引知识相兼容。</p><h3 id="Agentic-RAG"><a href="#Agentic-RAG" class="headerlink" title="Agentic RAG"></a>Agentic RAG</h3><p>Agentic RAG 将 ReACT 的推理能力与 Agent 的任务执行能力相结合，创建一个动态和自适应的系统。与遵循固定管道的传统 RAG 不同，Agentic RAG 通过使用 ReACT 根据用户查询的上下文动态协调 Agent，引入了灵活性。</p><ul><li>SINGH A, EHTESHAM A, KUMAR S, 等. Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG[A&#x2F;OL]. arXiv, 2025[2025-01-26]. <a href="http://arxiv.org/abs/2501.09136">http://arxiv.org/abs/2501.09136</a>.</li></ul><p>论文从RAG范式演化与基础原则开始，提供了一个对于Agentic RAG的全面探索。文章总结出了Agentic工作流的四个核心模式与七大Agentic-RAG架构。</p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-agent RAG Systems调研报告</title>
    <link href="/2024/12/18/Multi-agent-RAG-Systems%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/"/>
    <url>/2024/12/18/Multi-agent-RAG-Systems%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="Agentic-RAG调研报告—探索搭建适配多数据源的Multi-agent-RAG-System"><a href="#Agentic-RAG调研报告—探索搭建适配多数据源的Multi-agent-RAG-System" class="headerlink" title="Agentic RAG调研报告—探索搭建适配多数据源的Multi-agent RAG System"></a>Agentic RAG调研报告—探索搭建适配多数据源的Multi-agent RAG System</h1><h2 id="Multi-agent-RAG架构设计"><a href="#Multi-agent-RAG架构设计" class="headerlink" title="Multi-agent RAG架构设计"></a>Multi-agent RAG架构设计</h2><h3 id="应用场景与需求"><a href="#应用场景与需求" class="headerlink" title="应用场景与需求"></a>应用场景与需求</h3><p>根据不同的知识库与应用特点构造了不同的<strong>查询引擎</strong>，它们面向不同的领域知识，采用了不同的索引(比如VectorIndex与GraphIndex)。我们需要给使用者(可能是人或应用)提供一致性体验，他们无须关心后端使用的真实查询引擎是哪一个，只需要输入问题即可获得正确的答案。$^{[1]}$</p><p><img src="/./img/fig/agentrag.jpg" alt="Multi-agent RAG"></p><h3 id="适配多数据源"><a href="#适配多数据源" class="headerlink" title="适配多数据源"></a><strong>适配多数据源</strong></h3><p>计算机中的文档：<code>TXT文档</code>、<code>PDF文档</code>、<code>Office文档</code></p><p>存储在多种类型数据库中的结构化数据、半结构化数据：<code>MySQL</code>、<code>PostageSQL</code>、<code>Redis</code>、<code>MongDB</code></p><p>存储在图数据库中图数据：<code>Neo4j</code></p><p>存储在向量数据库中的向量数据：<code>Milvus</code>、<code>Chroma</code>、<code>Elastic</code></p><h2 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h2><ol><li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook/">https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook/</a> - OpenAI Agent + Query Engine Experimental Cookbook: 融合了vector_tool和sql_tool</li><li><a href="https://github.com/karthik-codex/Autogen_GraphRAG_Ollama">https://github.com/karthik-codex/Autogen_GraphRAG_Ollama</a> - GraphRAG + AutoGen + Ollama + Chainlit UI &#x3D; Local Multi-Agent RAG Superbot：这个应用程序将 <strong>GraphRAG</strong> 与 <strong>AutoGen</strong> 代理集成在一起，由 Ollama 的本地LLM提供支持，可实现免费的离线嵌入和推理。</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 基于大模型的RAG应用开发与优化—构建企业级LLM应用. 严灿平. 中国工信出版集团</p><p>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648407123&idx=2&sn=80ddf495e6c08af3355d8bf4a5109328&chksm=83836f06b4f4e610a99aa86e9882b7416c8f3ff42519c7aaff7d99a8ce936493e43a623f6e83&scene=178&cur_album_id=3276284267911856128#rd">RAG行业问答博客观点开源整理：从RAG评估、应用参考、开源框架到操作实践总结</a></p><p>[3] <a href="https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file">https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file</a></p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agentic RAG with LlamaIndex</title>
    <link href="/2024/12/10/Agentic-RAG-with-LlamaIndex/"/>
    <url>/2024/12/10/Agentic-RAG-with-LlamaIndex/</url>
    
    <content type="html"><![CDATA[<h2 id="传统RAG"><a href="#传统RAG" class="headerlink" title="传统RAG"></a>传统RAG</h2><p>作为一个首次接触RAG的开发者，简单理解RAG就是给LLM外挂一个知识库。</p><p>可笑的是记得第一次开组会给两位导师将MMLMs RAG System的综述博客，讲完以后导师问我如何通俗理解RAG，能不能举一个例子。我停顿了3秒，尴尬地说讲不出来。</p><p>讲不出实际的应用案例，说明你对这个领域内的知识、方法还不够理解。</p><p>老师随后说：LLM知道杨桂淼是谁吗？我说：不知道。老师接着说：如何事先已经把杨桂淼的基本信息都告诉了LLM，然后你在针对杨桂淼对LLM进行提问，那它可以回答出你的问题吗？我说：应该可以了。确实，外挂一个数据库，里面存放着所有有关杨桂淼信息的网页，当我query杨桂淼的相关问题时，LLM根据知识库返回的信息进行生产响应，这就是RAG。</p><div class="note note-success">            <p>RAG最核心的思想：给LLM补充外部知识以提高生成质量</p>          </div><p><img src="/./img/fig/rag2.jpg" alt="RAG-经典"></p><p>RAG应用整体上划分为两大阶段：</p><ol><li>数据索引：知识-加载-分割-嵌入-索引</li><li>数据查询：根据索引进行检索前处理-检索-检索后处理-生成</li></ol><p>RAG pipeline主要包括三大组件：</p><ol><li>外部知识源</li><li>提示模版</li><li>生成模型</li></ol><h2 id="微调与RAG的区别"><a href="#微调与RAG的区别" class="headerlink" title="微调与RAG的区别"></a>微调与RAG的区别</h2><p>还是以一个例子说明</p><p>如果把LLM比做一个优秀的学生正在参加一门考试，微调与RAG的区别如下：</p><p><strong>RAG</strong>：考试过程中给他提供这门课的全部参考资料，要求他现做现用，并争取写出答案</p><p><strong>微调</strong>：在考试前一天对他进行辅导，让他成为某个领域&#x2F;学科的专家，然后再去参加考试</p><h2 id="RAG与具有理解超长上下文能力的LLM"><a href="#RAG与具有理解超长上下文能力的LLM" class="headerlink" title="RAG与具有理解超长上下文能力的LLM"></a>RAG与具有理解超长上下文能力的LLM</h2><p>大模型的上下文窗口(content window)正在以不可思议的速度增大，在超长上下文中精确检索出特定位置的某个事实性知识(<code>大海捞针的能力</code>)也带来了一个争议性话题：<strong>如果未来能够把几百个文档一股脑式地全部丢进大模型的上下文窗口，并且大模型能够在其中检索出事实性知识，那么我们还有必要做外部索引与检索给大模型进行知识外挂吗？</strong></p><p>RAG的核心功能仍然是检索。事实上，大量实验测试表明，受限于主流LLM所依赖的底层Transformer架构的基本原理，当前理解超长上下文的能力并不像宣传的那样出色。</p><p>RAG与具有理解超长上下文能力LLM之间的trade-off是相辅相成、取长补短式发展的，不应该是谁取代谁。</p><h2 id="RAG演化三范式"><a href="#RAG演化三范式" class="headerlink" title="RAG演化三范式"></a>RAG演化三范式</h2><p><img src="/./img/fig/rag1.png" alt="RAG演化三范式"></p><h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Naive RAG中包含最基本的三个阶段：索引、检索、生成，是基本的链式结构。</p><h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>Advanced RAG在Naive RAG的基础之上，增加了Pre-Retrieval和Post-Retrivel两个过程。</p><p><em>Pre-retrieval process</em>：这个阶段主要关注的任务是<strong>优化索引结构</strong>和<strong>初始查询</strong>。优化索引的目标是提高被索引内容的质量，查询优化是使用户初始问题更加清晰并适用于检索任务。</p><p><em>Post-Retrieval Process</em>：这个阶段的主要任务包括chunk重排和内容压缩。重排检索信息取重定位最相关的内容对于边缘提示是关键策略。将所有相关文档全部喂给LLM会造成信息过载，聚焦最有用的信息并突出关键信息段压缩文本。</p><h3 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h3><p>改变了前两种模式的链式结构，转而模块化结构。增加了相似度检索的<strong>search module</strong>，通过微调重定义了<strong>retriever</strong>。同时支持序列化处理和end-to-end端到端的处理</p><h2 id="Agentic-RAG"><a href="#Agentic-RAG" class="headerlink" title="Agentic RAG"></a>Agentic RAG</h2><blockquote><p>Agentic RAG describes an AI agent-based implementation of RAG，<strong>the core of Agentic RAG is Agent.</strong></p><p>Agentic RAG—your super-smart digital library assistant!</p></blockquote><p>主动式RAG是基于AI agent的RAG实现，将AI agent 整合到RAG pipeline中，编排组件以执行简单的信息检索任务和生成之外的操作，以克服non-agentic pipeline的限制。</p><p>Agentic RAG更典型的是在检索组件的pipeline中使用agents。特别的，检索组件通过使用可访问不同检索器工具(tools)的检索agetns而成为agentic。</p><p>Agentic RAG应用推理的检索场景：</p><ol><li>Decide whether to retrieve information or not</li><li>Decide which tool to use to retrieve relevant information</li><li>Formulate the query itself</li><li>Evaluate the retrieved context and decide whether it needs to re-retrieve.</li></ol><p>Agentic RAG的主要工作流程：</p><div class="note note-success">            <p>Document Processing—Creating Embedings—Indexing—Retrieval—Agent based Reasoning—Generation</p>          </div><p><img src="/./img/fig/rag3.jpg" alt="Agentic RAG"></p><h3 id="Router-Engine"><a href="#Router-Engine" class="headerlink" title="Router Engine"></a>Router Engine</h3><p>在基础的RAG pipeline中，LLM仅仅用于整合生成答案。而在Agentic RAG中，LLM会被作为Agent使用在检索、增强、生成的各个阶段。</p><blockquote><p>Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific <strong>database searches</strong>, or <strong>merging different information streams</strong>. —RAG:A survey</p></blockquote><h3 id="Tool"><a href="#Tool" class="headerlink" title="Tool"></a>Tool</h3><p>检索组件的工具选择：</p><ul><li>Vector search Engine</li><li>Web search</li><li>Calculator</li><li>software(Email、chat programs)</li></ul><p>Agentic RAG属于RAG范式中的第三阶段：Modular RAG。</p><p>从文档的读取开始，处理流程基本上都是模块化层层封装好各个组件，已实现查询引擎的功能。</p><p>无论是引擎模块还是预测模块，最终都是将底层方法层层封装为<strong>工具方法</strong>，在将所有的工具方法整合为最终的引擎模块。</p><div class="note note-success">            <p>文档读取-文档分割-索引建立<strong>VectorIndex(nodes)</strong>-使用索引作为查询引擎<strong>index.as_query_engine</strong>-调用<strong>QueryEngineTool</strong>方法将查询引擎进行工具封装-将工具整合为LLM的<strong>predict_and_call</strong>模块进行生产应答</p>          </div><h2 id="Implementing-a-Basic-Multi-Document-Agentic-RAG"><a href="#Implementing-a-Basic-Multi-Document-Agentic-RAG" class="headerlink" title="Implementing a Basic Multi-Document Agentic RAG"></a>Implementing a Basic Multi-Document Agentic RAG</h2><p>课程源：<a href="https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex">deeplearning.ai短期课</a></p><p>博客链接：<a href="https://www.analyticsvidhya.com/blog/2024/09/multi-document-agentic-rag-using-llamaindex/">https://www.analyticsvidhya.com/blog/2024/09/multi-document-agentic-rag-using-llamaindex/</a></p><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>必要的库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> SimpleDirectoryReader<br><span class="hljs-keyword">from</span> llama_index.core.node_parser <span class="hljs-keyword">import</span> SentenceSplitter<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> Settings<br><span class="hljs-keyword">from</span> llama_index.llms.openai <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> llama_index.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbedding<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> SummaryIndex, VectorStoreIndex<br><span class="hljs-keyword">from</span> llama_index.core.tools <span class="hljs-keyword">import</span> QueryEngineTool<br><span class="hljs-keyword">from</span> llama_index.core.query_engine.router_query_engine <span class="hljs-keyword">import</span> RouterQueryEngine<br><span class="hljs-keyword">from</span> llama_index.core.selectors <span class="hljs-keyword">import</span> LLMSingleSelector<br></code></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>首先将文档转换为nodes插入到DocumentStore数据库中。</p><h3 id="对同一数据建立SummaryIndex和VectorIndex"><a href="#对同一数据建立SummaryIndex和VectorIndex" class="headerlink" title="对同一数据建立SummaryIndex和VectorIndex"></a>对同一数据建立SummaryIndex和VectorIndex</h3><h3 id="对索引建立查询引擎"><a href="#对索引建立查询引擎" class="headerlink" title="对索引建立查询引擎"></a>对索引建立查询引擎</h3><h3 id="定义检索增强的路由查询引擎"><a href="#定义检索增强的路由查询引擎" class="headerlink" title="定义检索增强的路由查询引擎"></a>定义检索增强的路由查询引擎</h3>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>Agent</tag>
      
      <tag>GPT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal RAG and Prompt Compression</title>
    <link href="/2024/12/08/Multimodal-RAG-and-Prompt-Compression/"/>
    <url>/2024/12/08/Multimodal-RAG-and-Prompt-Compression/</url>
    
    <content type="html"><![CDATA[<h1 id="Multimodal-RAG与Prompt-compression再梳理"><a href="#Multimodal-RAG与Prompt-compression再梳理" class="headerlink" title="Multimodal RAG与Prompt compression再梳理"></a>Multimodal RAG与Prompt compression再梳理</h1><h2 id="Multimodal-RAG-System-Work-Flow"><a href="#Multimodal-RAG-System-Work-Flow" class="headerlink" title="Multimodal RAG System Work Flow"></a>Multimodal RAG System Work Flow</h2><p>多模态RAG的完整工作流如下图所示：</p><p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-woXqKviW7JlYQlmaecD0QpvsiyX4PNJoAB6d4wEm_y4Ks6qXuJNTZ61KTOyJ1v8xi1k4CujEisnRNAWM4LIpgQs_HREGihbG3bquz1y-YXsNt84k7Y9jWy_Vip6yLPiIfNmnTAyI86xvEzbYFSpu69g?key=jCBGNvykPV6Ye85DC-Uxrg" alt="Multimodal RAG System"></p><p>再次梳理一遍多模态RAG系统的工作流程：</p><div class="note note-danger">            <p>1.多模态信息输入—2.特征提取—3.特征融合—4.信息检索—5.上下文构建—6.生成答案</p>          </div><p>1.加载所有文档，并使用类似unstructure.io的文档加载器提取文本块、图像和表格。</p><p>2.如有必要，将HTML表格转换为markdown；他们通常对LLM非常有效</p><p>3.将每个文本块、图像和表格传递到GPT-4o等多模式LLM中，并获得详细的摘要。</p><p>4.将摘要存储在向量数据库中，将原始文档片段存储在Redis等文档数据库中</p><p>5.使用多向量检索器使用公共document_id连接两个数据库，以识别哪个摘要映射到哪个原始文档块。</p><p>6.将这个多向量检索系统与GPT-4o等多模态LLM连接起来。</p><p>7.查询系统，并根据与查询类似的摘要，获取原始文档片段，包括表格和图像，作为上下文。</p><p>8.使用上述上下文，使用多模态LLM生成问题的答案。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><h4 id="多模态输入"><a href="#多模态输入" class="headerlink" title="多模态输入"></a>多模态输入</h4><p>在VQA系统中，<strong>文本+图像</strong>是常见的输入形式。</p><p>多模态RAG系统处理的是多种类型的数据，主要包括：</p><ul><li><strong>图像（Image）</strong>：可以是用户上传的图片、视频帧等视觉内容。</li><li><strong>文本（Text）</strong>：用户的问题、描述、上下文信息等语言内容。</li></ul><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p><strong>图像特征提取</strong>：</p><ul><li>使用预训练的卷积神经网络（如ResNet、EfficientNet）或专门的多模态模型（如CLIP、VisualBERT）提取图像特征。</li><li>图像特征通常表示为高维向量或特征图。</li></ul><p><strong>文本特征提取</strong>：</p><ul><li>使用基于Transformer的模型（如BERT、RoBERTa）或其他词嵌入技术（如Word2Vec、GloVe）将文本转换为嵌入向量。</li><li>文本特征表示为上下文相关的向量表示。</li></ul><h3 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h3><h4 id="联合表示（Joint-Representation）"><a href="#联合表示（Joint-Representation）" class="headerlink" title="联合表示（Joint Representation）"></a><strong>联合表示（Joint Representation）</strong></h4><p>将图像特征和文本特征结合成一个统一的表示，以便模型能够同时考虑两种模态的信息。</p><ul><li>拼接式融合（Concatenation）：<ul><li>直接将图像特征和文本特征向量拼接在一起。</li></ul></li><li>交互式融合（Interactive Fusion）：<ul><li>使用自注意力机制（Self-Attention）或跨注意力机制（Cross-Attention）在Transformer编码器中融合多模态特征。</li><li>例如，VisualBERT将图像区域特征和文本嵌入在同一序列中，通过自注意力层实现交互。</li></ul></li></ul><h4 id="联合嵌入（Joint-Embedding）"><a href="#联合嵌入（Joint-Embedding）" class="headerlink" title="联合嵌入（Joint Embedding）"></a><strong>联合嵌入（Joint Embedding）</strong></h4><p>通过多模态对齐技术（如对比学习）将不同模态的特征投射到同一语义空间中，使得跨模态检索和匹配更为有效。</p><ul><li>CLIP（Contrastive Language-Image Pretraining）：<ul><li>将图像和文本通过独立的编码器映射到共同的嵌入空间，通过对比损失优化使相关的图像和文本特征靠近，不相关的则远离。</li></ul></li><li>BLIP&#x2F;BLIP2</li></ul><h3 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h3><h4 id="检索模块"><a href="#检索模块" class="headerlink" title="检索模块"></a><strong>检索模块</strong></h4><p>基于融合后的多模态特征，从<strong>知识库</strong>或<strong>文档库</strong>中检索与输入相关的信息。</p><ul><li>相似度计算：<ul><li>计算输入特征与知识库中每个文档或片段的相似度（如余弦相似度）。</li></ul></li><li>索引与搜索：<ul><li>使用高效的向量索引技术（如FAISS、Annoy）加速大规模相似度搜索。</li></ul></li></ul><h4 id="检索结果过滤与排序"><a href="#检索结果过滤与排序" class="headerlink" title="检索结果过滤与排序"></a><strong>检索结果过滤与排序</strong></h4><ul><li>过滤：<ul><li>排除与查询不相关或低质量的检索结果。</li></ul></li><li>排序：<ul><li>根据相似度得分对检索结果进行排序，选取最相关的若干条信息作为生成答案的依据。</li></ul></li></ul><h3 id="上下文构建"><a href="#上下文构建" class="headerlink" title="上下文构建"></a>上下文构建</h3><h4 id="上下文整合"><a href="#上下文整合" class="headerlink" title="上下文整合"></a><strong>上下文整合</strong></h4><div class="note note-success">            <p>上下文的内容：检索到的相关信息与原始输入（图像和问题文本）进行整合后的内容</p>          </div><ul><li>上下文增强：<ul><li>将检索到的文本信息与输入问题结合，提供更丰富的背景知识。</li></ul></li><li>图文结合：<ul><li>将检索到的文本信息与图像特征共同输入生成模型，辅助生成更准确的答案。</li></ul></li></ul><h4 id="Prompt-Compression（提示压缩）"><a href="#Prompt-Compression（提示压缩）" class="headerlink" title="Prompt Compression（提示压缩）"></a><strong>Prompt Compression（提示压缩）</strong></h4><p>为了提高模型处理效率和响应速度，对输入提示进行压缩，将多模态信息以更紧凑的形式传递给生成模型。</p><ul><li>联合特征表示：<ul><li>使用预训练模型生成的联合嵌入向量作为压缩后的提示。</li></ul></li><li>模板化设计：<ul><li>设计高效的prompt模板，将图像和文本信息结构化地嵌入其中。</li></ul></li></ul><h3 id="答案生成"><a href="#答案生成" class="headerlink" title="答案生成"></a>答案生成</h3><h4 id="生成模块"><a href="#生成模块" class="headerlink" title="生成模块"></a><strong>生成模块</strong></h4><p>利用生成模型（如GPT、Mistral）基于整合后的上下文信息生成自然语言答案。</p><ul><li>自回归生成（Autoregressive Generation）：<ul><li>模型逐步生成答案，每一步依赖于之前生成的内容和上下文。</li></ul></li><li>条件生成（Conditional Generation）：<ul><li>生成过程受到输入条件（如问题文本和图像特征）的约束，确保生成内容与输入相关。</li></ul></li></ul><h4 id="生成优化"><a href="#生成优化" class="headerlink" title="生成优化"></a><strong>生成优化</strong></h4><ul><li>多样性与一致性：<ul><li>通过调整生成参数（如温度、顶层采样）控制答案的多样性和一致性。</li></ul></li><li>格式控制：<ul><li>使用特定的prompt模板引导生成内容的格式和风格。</li></ul></li></ul><p><img src="https://go2heart.github.io/echosight/static/images/teaser.png" alt="多模态RAG"></p><h2 id="Prompt的具体形式"><a href="#Prompt的具体形式" class="headerlink" title="Prompt的具体形式"></a>Prompt的具体形式</h2><h3 id="Multimodal-RAG-System博客里面的Prompt格式"><a href="#Multimodal-RAG-System博客里面的Prompt格式" class="headerlink" title="Multimodal RAG System博客里面的Prompt格式"></a><a href="https://www.analyticsvidhya.com/blog/2024/09/guide-to-building-multimodal-rag-systems/">Multimodal RAG System</a>博客里面的Prompt格式</h3><p>博客中prompt构建函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">multimodal_prompt_function</span>(<span class="hljs-params">data_dict</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Create a multimodal prompt with both text and image context.</span><br><span class="hljs-string">    This function formats the provided context from `data_dict`, which contains</span><br><span class="hljs-string">    text, tables, and base64-encoded images. It joins the text (with table) portions</span><br><span class="hljs-string">    and prepares the image(s) in a base64-encoded format to be included in a </span><br><span class="hljs-string">    message.</span><br><span class="hljs-string">    The formatted text and images (context) along with the user question are used to</span><br><span class="hljs-string">    construct a prompt for GPT-4o</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    formatted_texts = <span class="hljs-string">&quot;\n&quot;</span>.join(data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;texts&quot;</span>])<br>    messages = []<br>    <br>    <span class="hljs-comment"># Adding image(s) to the messages if present</span><br>    <span class="hljs-keyword">if</span> data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;images&quot;</span>]:<br>        <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> data_dict[<span class="hljs-string">&quot;context&quot;</span>][<span class="hljs-string">&quot;images&quot;</span>]:<br>            image_message = &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image_url&quot;</span>,<br>                <span class="hljs-string">&quot;image_url&quot;</span>: &#123;<span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">f&quot;data:image/jpeg;base64,<span class="hljs-subst">&#123;image&#125;</span>&quot;</span>&#125;,<br>            &#125;<br>            messages.append(image_message)<br>    <br>    <span class="hljs-comment"># Adding the text for analysis</span><br>    text_message = &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>,<br>        <span class="hljs-string">&quot;text&quot;</span>: (<br>            <span class="hljs-string">f&quot;&quot;&quot;You are an analyst tasked with understanding detailed information </span><br><span class="hljs-string">                and trends from text documents,</span><br><span class="hljs-string">                data tables, and charts and graphs in images.</span><br><span class="hljs-string">                You will be given context information below which will be a mix of </span><br><span class="hljs-string">                text, tables, and images usually of charts or graphs.</span><br><span class="hljs-string">                Use this information to provide answers related to the user </span><br><span class="hljs-string">                question.</span><br><span class="hljs-string">                Do not make up answers, use the provided context documents below and </span><br><span class="hljs-string">                answer the question to the best of your ability.</span><br><span class="hljs-string">                </span><br><span class="hljs-string">                User question:</span><br><span class="hljs-string">                <span class="hljs-subst">&#123;data_dict[<span class="hljs-string">&#x27;question&#x27;</span>]&#125;</span></span><br><span class="hljs-string">                </span><br><span class="hljs-string">                Context documents:</span><br><span class="hljs-string">                <span class="hljs-subst">&#123;formatted_texts&#125;</span></span><br><span class="hljs-string">                </span><br><span class="hljs-string">                Answer:</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>        ),<br>    &#125;<br>    messages.append(text_message)<br>    <span class="hljs-keyword">return</span> [HumanMessage(content=messages)]<br><br></code></pre></td></tr></table></figure><p>经过<code>multimodal_prompt_function</code>处理后的信息被整合为一个messages列表：messages &#x3D; [image_message,text_message]。</p><p>构建出的Prompt主要包含两部分：图像信息+文本信息</p><p>假设检索到的上下文包含两个文本段落和一张图像，用户问题为“Tell me detailed statistics of the top 5 years with largest wildfire acres burned”</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;image_url&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;image_url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...&quot;</span><span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;image_url&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;image_url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...&quot;</span><span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;text&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;You are an analyst tasked with understanding detailed information and trends from text documents, data tables, and charts and graphs in images. You will be given context information below which will be a mix of text, tables, and images usually of charts or graphs. Use this information to provide answers related to the user question. Do not make up answers, use the provided context documents below and answer the question to the best of your ability.\n\nUser question:\nTell me detailed statistics of the top 5 years with largest wildfire acres burned\n\nContext documents:\n[Text from first document]\n[Text from second document]\n\nAnswer:&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><h3 id="EchoSight论文里的Prompt格式"><a href="#EchoSight论文里的Prompt格式" class="headerlink" title="EchoSight论文里的Prompt格式"></a>EchoSight论文里的Prompt格式</h3><h4 id="最终的Prompt格式"><a href="#最终的Prompt格式" class="headerlink" title="最终的Prompt格式"></a>最终的Prompt格式</h4><p><strong>Context + Question</strong>：<code>prompt</code> 格式通常包括上下文和问题：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">&lt;维基百科文章或section&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>具体例子：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">Paris</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">capital</span> <span class="hljs-string">of</span> <span class="hljs-string">France,</span> <span class="hljs-string">located</span> <span class="hljs-string">in</span> <span class="hljs-string">the</span> <span class="hljs-string">northern</span> <span class="hljs-string">part</span> <span class="hljs-string">of</span> <span class="hljs-string">the</span> <span class="hljs-string">country.</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">What</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">capital</span> <span class="hljs-string">of</span> <span class="hljs-string">France?</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><h4 id="带图像的prompt"><a href="#带图像的prompt" class="headerlink" title="带图像的prompt"></a>带图像的prompt</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image features:</span> <span class="hljs-string">&lt;图像的特征向量或描述&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>可以将图像特征转化为文本描述：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Context:</span> <span class="hljs-string">This</span> <span class="hljs-string">is</span> <span class="hljs-string">an</span> <span class="hljs-string">image</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">cat</span> <span class="hljs-string">sitting</span> <span class="hljs-string">on</span> <span class="hljs-string">a</span> <span class="hljs-string">couch</span> <span class="hljs-string">next</span> <span class="hljs-string">to</span> <span class="hljs-string">a</span> <span class="hljs-string">window</span> <span class="hljs-string">with</span> <span class="hljs-string">sunlight</span> <span class="hljs-string">streaming</span> <span class="hljs-string">in.</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">What</span> <span class="hljs-string">is</span> <span class="hljs-string">the</span> <span class="hljs-string">cat</span> <span class="hljs-string">doing?</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>也可以将图像通过特征提取模型转换为特征向量，以特定格式嵌入到 <strong>prompt</strong> 中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image features:</span> [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-string">...</span>]  <span class="hljs-comment"># 图像特征向量</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><h4 id="图像-文本-Prompt-格式-联合表示形式"><a href="#图像-文本-Prompt-格式-联合表示形式" class="headerlink" title="图像+文本 Prompt 格式-联合表示形式"></a>图像+文本 Prompt 格式-联合表示形式</h4><p>将图像特征和文本信息结合成一个联合输入，可以使用 CLIP 这样的多模态模型将图像和文本映射到相同的嵌入空间</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Image and Text features:</span> <span class="hljs-string">&lt;联合特征表示&gt;</span><br><span class="hljs-attr">Question:</span> <span class="hljs-string">&lt;问题&gt;</span><br><span class="hljs-attr">The answer is:</span><br></code></pre></td></tr></table></figure><p>CLIP、VisualBERT、FLAVA等模型是基于 <strong>多模态数据联合训练</strong> 的。这些模型已经通过海量的图像-文本配对数据进行训练，能够在 <strong>文本</strong> 和 <strong>图像</strong> 之间建立语义连接。</p><p>Transformer模型的 <strong>自注意力机制</strong>（Self-attention）能够有效地对输入的不同部分进行加权，理解其中的关系。当我们输入 <strong>图像特征</strong> 和 <strong>问题文本</strong> 时，模型通过注意力机制结合这两部分信息，从而产生推理。</p><p>GPT、Mistral等非常擅长处理长文本和复杂的上下文关系。通过 <strong>自回归生成</strong> 或 <strong>条件生成</strong>，模型会基于已知的上下文信息生成答案。</p><h2 id="RAG链构建"><a href="#RAG链构建" class="headerlink" title="RAG链构建"></a>RAG链构建</h2><p>RAG链构建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">multimodal_rag = (<br>        &#123;<br>            <span class="hljs-string">&quot;context&quot;</span>: itemgetter(<span class="hljs-string">&#x27;context&#x27;</span>),<br>            <span class="hljs-string">&quot;question&quot;</span>: itemgetter(<span class="hljs-string">&#x27;input&#x27;</span>),<br>        &#125;<br>            |<br>        RunnableLambda(multimodal_prompt_function)<br>            |<br>        chatgpt<br>            |<br>        StrOutputParser()<br>)<br></code></pre></td></tr></table></figure><ol><li>使用<code>itemgetter</code>从输入数据中提取<code>&#39;input&#39;</code>（用户的问题）。</li><li>通过<code>RunnableLambda</code>运行<code>multimodal_prompt_function</code>，将提取的<code>context</code>和<code>question</code>传入，生成多模态prompt。</li><li>生成的prompt传递给<code>chatgpt</code>模型（假设为GPT-4模型的实例）进行回答生成。</li><li>使用<code>StrOutputParser</code>将模型生成的回答解析为字符串格式。</li></ol><p>信息检索模块构建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">retrieve_docs = (itemgetter(<span class="hljs-string">&#x27;input&#x27;</span>)<br>                    |<br>                retriever_multi_vector<br>                    |<br>                RunnableLambda(split_image_text_types))<br></code></pre></td></tr></table></figure><ol><li>使用<code>itemgetter</code>从输入数据中提取<code>&#39;input&#39;</code>（用户的问题）。</li><li><code>retriever_multi_vector</code>使用多向量检索技术（如基于CLIP的多模态向量检索）从知识库中检索与查询相关的文档和图像。</li><li>通过<code>RunnableLambda(split_image_text_types)</code>将检索到的内容分为文本和图像两类，便于后续处理。</li></ol><p>整合RAG链与检索模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)<br>                                               .assign(answer=multimodal_rag)<br>)<br></code></pre></td></tr></table></figure><ol><li>使用<code>RunnablePassthrough.assign</code>将<code>retrieve_docs</code>的输出赋值给<code>context</code>键。</li><li>使用<code>.assign</code>将先前定义的<code>multimodal_rag</code>链赋值给<code>answer</code>键。</li><li>通过这种方式，最终的<code>multimodal_rag_w_sources</code>链能够同时处理检索到的上下文和生成的答案。</li></ol>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学循环神经网络</title>
    <link href="/2024/12/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/12/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>由于pytorch已经将三个循环神经网络、LSTM进行了高度的封装，所以本节不再去关注网络结构的设计和复现，而是重点关注如何在自己设计的网络结构中将RNN和LSTM融入进去以实现特等的任务。</p><p>卷积神经网络是借鉴人类视觉的思想，教会计算机识别东西；从循环神经网络开始，我们的核心任务就是教会计算机理解序列数据。</p><p>人类并不是每时每刻都从头开始思考。 当我们阅读这篇文章时，会根据对前面单词的理解来理解每个单词。</p><h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络-RNN"></a>循环神经网络-RNN</h2><p>循环神经网络是神经网络的一种，为什么取名字要叫做循环呢？</p><p>就是因为它的神经元输出会在下一个时间步作为反馈进行输入，使整个网络具有处理<strong>序列数据</strong>的能力。</p><p>一个复杂的系统包含若干的最小单元组件，每个最小单元组件都会重复自身动作从而构成整个系统的运转，“循环”变应运而生。</p><p>普通的神经网络处理的是独立同分布数据，层与层之间就是简单的前馈链接关系，输入和输出长度是固定的。</p><p>而循环神经网络具有记忆能力，这种记忆能力体现在上层神经元的输出会作为下层神经元的输入，可以处理时序数据，而且输入输出长度不固定。<br>$$<br>h_{t}&#x3D;tanh(h_{t-1}W_{h}+x_{t}W_{x}+b)<br>$$<br>$$<br>&#x3D; tanh(h_{t-1}W_{hh}+b_{hh}+x_{t}W_{ih}+b_{ih})<br>$$</p><p>模型输入：input_size,hidden_size</p><p>模型输出：output_size,ht</p><p><img src="/img/fig/rnn.jpg" alt="RNN"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,input_size, hidden_size, num_layers=<span class="hljs-number">1</span>, batch_first=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br>        <span class="hljs-variable language_">self</span>.input_size = input_size<br>        <span class="hljs-variable language_">self</span>.bidirectional = <span class="hljs-literal">False</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.W_ih = nn.Parameter(torch.rand(<span class="hljs-variable language_">self</span>.input_size, <span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.W_hh = nn.Parameter(torch.rand(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.b_ih = nn.Parameter(torch.zeros(<span class="hljs-variable language_">self</span>.hidden_size))<br>        <span class="hljs-variable language_">self</span>.b_hh = nn.Parameter(torch.zeros(<span class="hljs-variable language_">self</span>.hidden_size))<br>        <br>    <span class="hljs-comment">#前向传播的两个参数分别为x_t(输入变量)和h_prev(隐藏变量)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x_t,h_prev=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># part 1: torch.matmul(x_t, self.W_ih)</span><br>        <span class="hljs-comment"># x_t包含多个时间步，形状为[batch_size, time_steps_num, input_dim]</span><br>        <span class="hljs-comment"># W_ih形状为[input_dim, hidden_size]</span><br>        <span class="hljs-comment"># torch.matmul(x_t, self.W_ih) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-comment"># part 2: torch.matmul(h_prev, self.W_hh)</span><br>        <span class="hljs-comment"># h_prev 形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-comment"># W_hh形状为[hidden_size, hidden_size]</span><br>        <span class="hljs-comment"># torch.matmul(h_prev, self.W_hh) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]</span><br>        <span class="hljs-keyword">if</span> h_prev == <span class="hljs-literal">None</span>:<br>             h_prev = torch.zeros( x_t.size(<span class="hljs-number">0</span>), <span class="hljs-variable language_">self</span>.hidden_size)<br>        output = torch.tanh(torch.matmul(x_t, <span class="hljs-variable language_">self</span>.W_ih) + <span class="hljs-variable language_">self</span>.b_ih + torch.matmul(h_prev, <span class="hljs-variable language_">self</span>.W_hh) + <span class="hljs-variable language_">self</span>.b_hh)<br>        <span class="hljs-keyword">return</span> output,output[:,-<span class="hljs-number">1</span>,:].unsqueeze(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><div class="note note-success">            <p>RNN的梯度消失&#x2F;爆炸</p>          </div><p>RNN的梯度消失或爆炸问题主要体现在“循环”过程中隐藏变量的迭代计算过程中。</p><ul><li>隐藏变量的参数&gt;1: 每迭代一次都会呈现幂律型增长，经过ReLU激活后会趋向于正无穷</li><li>隐藏变量的参数&lt;1: 每迭代一次都会呈现幂律型衰退，经过ReLU激活后始终等于0</li></ul><h2 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络-LSTM"></a>长短期记忆网络-LSTM</h2><p>网络的输入：隐藏变量H、记忆C、输入序列X</p><p>网络的输出：输出序列Y、隐藏变量H、新的记忆C</p><p><img src="/img/fig/lstm.jpg" alt="LSTM"></p><p>参数初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_paramaters</span>(<span class="hljs-params">vocab_size,hidden_units</span>):<br>    std = <span class="hljs-number">0.01</span><br>    input_units = output_units = vocab_size<br>    <br>    <span class="hljs-comment">#正态分布</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape) * std<br>    <br>    <span class="hljs-comment">#LSTM_cell_weights</span><br>    forget_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    input_gate_weights = normal((input_units+hidden_units,hidden_units))<br>    output_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    c_tilda_gate_weights = normal((input_units + hidden_units,hidden_units))<br>    <br>    <span class="hljs-comment"># 偏置项</span><br>    forget_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    input_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    output_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    c_tilda_gate_bias = torch.zeros((<span class="hljs-number">1</span>, hidden_units))<br>    <br>    <span class="hljs-comment">#输出层参数</span><br>    hidden_output_weights = normal((hidden_units, output_units))<br>    output_bias = torch.zeros((<span class="hljs-number">1</span>, output_units))<br>    <br>    <span class="hljs-comment"># 将所有参数添加到字典</span><br>    paramaters = &#123;<br>        <span class="hljs-string">&#x27;fgw&#x27;</span>: forget_gate_weights,<br>        <span class="hljs-string">&#x27;igw&#x27;</span>: input_gate_weights,<br>        <span class="hljs-string">&#x27;ogw&#x27;</span>: output_gate_weights,<br>        <span class="hljs-string">&#x27;cgw&#x27;</span>: c_tilda_gate_weights,<br>        <span class="hljs-string">&#x27;fgb&#x27;</span>: forget_gate_bias,<br>        <span class="hljs-string">&#x27;igb&#x27;</span>: input_gate_bias,<br>        <span class="hljs-string">&#x27;ogb&#x27;</span>: output_gate_bias,<br>        <span class="hljs-string">&#x27;cgb&#x27;</span>: c_tilda_gate_bias,<br>        <span class="hljs-string">&#x27;how&#x27;</span>: hidden_output_weights,<br>        <span class="hljs-string">&#x27;ob&#x27;</span>: output_bias<br>    &#125;<br>    <br>    <span class="hljs-comment"># 设置 requires_grad=True 以启用梯度计算</span><br>    <span class="hljs-comment"># 确保所有参数在反向传播中能够计算梯度</span><br>    <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> paramaters.values():<br>        para.requires_grad_(<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-keyword">return</span> paramaters<br></code></pre></td></tr></table></figure><p>网络结构定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LSTM</span>(<span class="hljs-params">inputs,state,params</span>):<br>    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,<br>     W_hq, b_q] = params <span class="hljs-comment">#各个门的参数</span><br>    (H,C) = state <span class="hljs-comment">#隐藏参数和记忆参数</span><br>    outputs = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs:<br>        I = torch.sigmoid((x @ W_xi) + (H @ W_hi) + b_i) <span class="hljs-comment">#输入门</span><br>        F = torch.sigmoid((x @ W_xf) + (H @ W_hf) + b_f) <span class="hljs-comment">#遗忘门</span><br>        O = torch.sigmoid((x @ W_xo) + (H @ W_ho) + b_o) <span class="hljs-comment">#输出门</span><br>        C_tilda = torch.tanh((x @ W_xc) + (H @ W_hc) + b_c) <span class="hljs-comment">#候选记忆</span><br>        C = F * C + I * C_tilda<br>        H = O * torch.tanh(C)<br>        Y = (H @ W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs,dim=<span class="hljs-number">0</span>),(H,C)<br></code></pre></td></tr></table></figure><p>对于每个LSTM门，它的输出是通过如下方式计算的：</p><ul><li>假设当前的输入向量为 xt&#x3D;[x1,x2,x3,x4]（维度是 <code>input_units = 4</code>），上一时刻的隐藏状态为 $h_{t-1} &#x3D; [h_1, h_2, h_3]$（维度是 <code>hidden_units = 3</code>）。</li><li>我们将这两个向量拼接起来，得到一个向量 [x1,x2,x3,x4,h1,h2,h3]，维度是7。</li><li>然后，我们将这个7维的向量与权重矩阵相乘，得到一个维度为3的向量（即隐藏状态的维度）。这个3维的向量就是该门的输出（例如，遗忘门的输出），它会被用来更新LSTM的状态。</li></ul><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>传统的循环神经网络（RNN）在处理长序列数据时常常遇到梯度消失或梯度爆炸的问题，这限制了它们捕捉长期依赖关系的能力。为了解决这些问题，长短期记忆网络（LSTM）被提出，它通过引入门控机制来控制信息的流动，从而有效缓解了梯度消失问题。GRU模型则是在LSTM的基础上进一步简化和发展的，它由Cho等人在2014年提出，旨在简化LSTM的结构，同时保持类似的性能。</p><p>GRU通过合并LSTM中的遗忘门和输入门为更新门，并引入重置门，同时合并单元状态和隐藏状态，使得模型更为简洁，训练速度更快。这种结构上的简化并没有牺牲性能，GRU在很多任务中的表现与LSTM相当，甚至在某些情况下更优。因此，GRU因其高效和简洁的特性，在自然语言处理、语音识别、时间序列预测等多个领域得到了广泛的应用。</p><p><img src="https://zh.d2l.ai/_images/gru-3.svg" alt="GRU"></p><p>参数初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device)*<span class="hljs-number">0.01</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">three</span>():<br>        <span class="hljs-keyword">return</span> (normal((num_inputs, num_hiddens)),<br>                normal((num_hiddens, num_hiddens)),<br>                torch.zeros(num_hiddens, device=device))<br><br>    W_xz, W_hz, b_z = three()  <span class="hljs-comment"># 更新门参数</span><br>    W_xr, W_hr, b_r = three()  <span class="hljs-comment"># 重置门参数</span><br>    W_xh, W_hh, b_h = three()  <span class="hljs-comment"># 候选隐状态参数</span><br>    <span class="hljs-comment"># 输出层参数</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></table></figure><p>模型定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gru</span>(<span class="hljs-params">inputs,state,params</span>):<br>    W_xr,W_xz,W_xh,b_z,b_h,b_r,W_hr,W_hz,W_hh,W_hq,b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)<br>        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)<br>        H_tilda = torch.tanh((X @ W_xh) + ((R * H)@W_hh) + b_h)<br>        H = Z * H + (<span class="hljs-number">1</span> - Z) * H_tilda<br>        Y = H @ W_hq + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs,dim=<span class="hljs-number">0</span>),(H,)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年11月总结与展望</title>
    <link href="/2024/12/05/11%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/12/05/11%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="消失的半个月，难得喘口气"><a href="#消失的半个月，难得喘口气" class="headerlink" title="消失的半个月，难得喘口气"></a>消失的半个月，难得喘口气</h2><p>折腾了半个月的分布式文件系统期末结课结课实验，终于汇报结束了。</p><p>第15周了，很多课程都是实验节课，最近半个月实验压力也别大。但是好在昨天把大数据的实验和C++的实验都完成了，周六还有一场英语考试，紧张刺激的15周就终于要迎来胜利的曙光了！</p><p>这半个月主要在忙这些事，科研搁置了很多。今天开始，我要把失去的，都补回来哈哈哈哈！！</p><p><code>最后一个月的工作重心转移到MLLMs RAG prompt与技术学习上来。</code></p><h2 id="做难事，收获颇丰"><a href="#做难事，收获颇丰" class="headerlink" title="做难事，收获颇丰"></a>做难事，收获颇丰</h2><p>本科之前从来没有接触过分布式，Java编程也是菜的一批…</p><p>其实这个大实验很早就着手准备了，但是一直理解不了架构，导致一拖再拖。最后的一周冲刺，每天晚上干到一点多，这种感觉前所未有。</p><p>马老师说过：梦想还是要有的，万一实现了呢</p><p><img src="/img/life/file.jpg" alt="难忘的下午"></p><p>上个月的主线任务完成度不错：务必11月要执行的-<strong>带着把程序写成东西的思想，疯狂Coding</strong>！！！！</p><p>11月完成的任务：</p><ol><li>循环神经网路、attention机制、Transformer架构基础知识理解(但是又快忘了😂)</li><li>三个卷积神经网络结构的编写</li><li>Java分布式文件系统项目(大部头费时费力收获颇丰)</li><li>C++五子棋程序(大部头费时费力收获颇丰)</li></ol><h2 id="切入"><a href="#切入" class="headerlink" title="切入"></a>切入</h2><p>要快速上手一个方向或者一个小领域，一般来说需要掌握一些基础知识（最基本的概念和原理）、前沿进展（近几年的研究方向与热点）和特定的工具（例如编程语言、框架）。</p><div class="note note-success">            <p>思考：深度学习领域基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p><p>思考：自然语言处理领域的基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p><p>思考：大模型领域基础知识是什么？前沿进展有哪些？需要学习哪些特定的工具去学习？</p>          </div><h2 id="客观分析一下自己目前的水平"><a href="#客观分析一下自己目前的水平" class="headerlink" title="客观分析一下自己目前的水平"></a>客观分析一下自己目前的水平</h2><div class="note note-success">            <p>革命尚未成功，同志还需努力</p>          </div><p>机器学习基础：吴恩达机器学习课程刷完，理解基本概念</p><p>深度学习：小土堆pytorch教程刷完，李沐的实战教程一直在断断续续看不系统。经典的网络架构梳理出来了，但是对于架构细节与实现还是<strong>眼高手低</strong>的状态。</p><p>NLP：这学期听了自然语言处理这门课，但是吸收效果一般，尚未开始系统学习。</p><h2 id="12月任务"><a href="#12月任务" class="headerlink" title="12月任务"></a>12月任务</h2><div class="note note-danger">            <p>不要吝惜代码，实现，虽然不是必要的，会很大程度上加深你的理解。</p>          </div><p>2024年最后一个月，主要任务就一个：把之前<strong>深度学习</strong>所有相关知识整合、查漏补缺、复现Coding</p><p>今天5号了，这个月的任务除了复习最后的三门课之外，更多的精力要分配给如下任务：</p><ul><li>卷积神经网络学习(还剩VGG、ResNet没有实现)</li><li>语言神经网络学习</li><li>多读经典论文、多复现代码(循环神经网络、Attention、Transformer、BERT掌握)</li><li>精进python、C++(必须精通使用)</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM训练常用操作-从训练到崩溃</title>
    <link href="/2024/11/22/LLM%E8%AE%AD%E7%BB%83%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB/"/>
    <url>/2024/11/22/LLM%E8%AE%AD%E7%BB%83%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="Conda相关"><a href="#Conda相关" class="headerlink" title="Conda相关"></a>Conda相关</h2><h3 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">创建虚拟环境(指定python版本)</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda create --name echosight python=3.10<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">激活虚拟环境</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda activate echosight<br>(echosight) root@I1dc83206c700201ce5:~/Echo/EchoSight#<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">退出虚拟环境</span><br>(echosight) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda deactivate echosight<br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight#<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看虚拟环境列表</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda env list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除虚拟环境</span><br>(base) root@I1dc83206c700201ce5:~/Echo/EchoSight# conda remove --name echosight --all<br><br><br></code></pre></td></tr></table></figure><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p>引用自ACwing笔记：</p><h3 id="git基本概念"><a href="#git基本概念" class="headerlink" title="git基本概念"></a>git基本概念</h3><p>工作区：仓库的目录。工作区是独立于各个分支的。</p><p>暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。</p><p>版本库：存放所有已经提交到本地仓库的代码版本。</p><p>版本结构：树结构，树中每个节点代表一个代码版本。</p><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">git config --global user.name xxx：设置全局用户名，信息记录在~/.gitconfig文件中<br>git config --global user.email xxx@xxx.com：设置全局邮箱地址，信息记录在~/.gitconfig文件中<br>git init：将当前目录配置成git仓库，信息记录在隐藏的.git文件夹中<br>git add XX：将XX文件添加到暂存区<br>git add .：将所有待加入暂存区的文件加入暂存区<br>git rm --cached XX：将文件从仓库索引目录中删掉<br>git commit -m &quot;给自己看的备注信息&quot;：将暂存区的内容提交到当前分支<br>git status：查看仓库状态<br>git diff XX：查看XX文件相对于暂存区修改了哪些内容<br>git log：查看当前分支的所有版本<br>git reflog：查看HEAD指针的移动历史（包括被回滚的版本）<br>git reset --hard HEAD^ 或 git reset --hard HEAD~：将代码库回滚到上一个版本<br>git reset --hard HEAD^^：往上回滚两次，以此类推<br>git reset --hard HEAD~100：往上回滚100个版本<br>git reset --hard 版本号：回滚到某一特定版本<br>git checkout — XX或git restore XX：将XX文件尚未加入暂存区的修改全部撤销<br>git remote add origin git@git.acwing.com:xxx/XXX.git：将本地仓库关联到远程仓库<br>git push -u (第一次需要-u以后不需要)：将当前分支推送到远程仓库<br>git push origin branch_name：将本地的某个分支推送到远程仓库<br>git clone git@git.acwing.com:xxx/XXX.git：将远程仓库XXX下载到当前目录下<br>git checkout -b branch_name：创建并切换到branch_name这个分支<br>git branch：查看所有分支和当前所处分支<br>git checkout branch_name：切换到branch_name这个分支<br>git merge branch_name：将分支branch_name合并到当前分支上<br>git branch -d branch_name：删除本地仓库的branch_name分支<br>git branch branch_name：创建新分支<br>git push --set-upstream origin branch_name：设置本地的branch_name分支对应远程仓库的branch_name分支<br> --set-upstream-to=origin/branch_name1 branch_name2：将远程的branch_name1分支与本地的branch_name2分支对应<br>git checkout -t origin/branch_name 将远程的branch_name分支拉取到本地<br>git stash：将工作区和暂存区中尚未提交的修改存入栈中<br>git stash apply：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素<br>git stash drop：删除栈顶存储的修改<br>git stash pop：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素<br>git stash list：查看栈中所有元素<br></code></pre></td></tr></table></figure><h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><h3 id="Access-authority"><a href="#Access-authority" class="headerlink" title="Access authority"></a>Access authority</h3><p>Hugging  Face拉取模型-恒源云服务器为例</p><p>服务器上要开启AI学术加速，对于GitHub、hugging face等网站可以实现上传、下载加速</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">(base) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/LLM-model#export https_proxy=http://turbo.gpushare.http_proxy=http://turbo.gpushare.com:30000<br>(base) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/LLM-model#export https_proxy=http://turbo2.gpushare.http_proxy=http://turbo2.gpushare.com:30000<br></code></pre></td></tr></table></figure><p>由于Hugging  Face属于国外网站，正常访问需要科学上网，所以使用git clone时同样也需要<strong>ssh认证</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">ssh秘钥创建</span><br>(base) root@I1dc83206c700201ce5:~/.ssh# ssh-keygen -t ed25519 -C &quot;youremail@gmail.com&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">复制公钥给Hugging Face设置</span><br>(base) root@I1dc83206c700201ce5:~/.ssh# ls<br>authorized_keys  id_ed25519  id_ed25519.pub<br>(base) root@I1dc83206c700201ce5:~/.ssh# vim id_ed25519.pub <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行报错</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">OSError: We couldn<span class="hljs-string">&#x27;t connect to &#x27;</span>https://huggingface.co<span class="hljs-string">&#x27; to load this file, couldn&#x27;</span>t find it <span class="hljs-keyword">in</span> the cached files and it looks like BAAI/EVA-CLIP-8B is not the path to a directory containing a file named config.json.</span><br>(echosight) root@I1dc83206c700201ce5:/hy-tmp/EchoProject/hf-llm/Mistral-7B-Instruct-v0.2# cd /usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/<br>(echosight)root@I1dc83206c700201ce5:/usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/# ls<br>(echosight)root@I1dc83206c700201ce5:/usr/local/miniconda3/envs/echosight/lib/python3.10/site-packages/huggingface_hub/# vim constants.py<br><span class="hljs-meta prompt_"># </span><span class="language-bash">修改constants.py里面的hugging face参数为镜像网站</span><br>HUGGINGFACE_CO_URL_HOME = &quot;https://hf-mirror.com/&quot;<br>_HF_DEFAULT_ENDPOINT = &quot;https://hf-mirror.com/&quot;<br></code></pre></td></tr></table></figure><h3 id="拉取仓库模型"><a href="#拉取仓库模型" class="headerlink" title="拉取仓库模型"></a>拉取仓库模型</h3><p>首先在hf官网创建model的token，之后提供两种方式下载模型到服务器或者本地</p><p>第一种方法：使用huggingface_hub直接下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download<br><span class="hljs-comment"># snapshot_download(repo_id=&quot;decapoda-research/llama-7b-hf&quot;)</span><br>snapshot_download(repo_id=<span class="hljs-string">&quot;THUDM/chatglm3-6b&quot;</span>)<br></code></pre></td></tr></table></figure><p>第一种方法：使用huggingface-cli下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">(echosight)root@I1dc83206c700201ce5:/hy-tmp/# huggingface-cli download --resume-download &lt;repo/name&gt; --local-dir &lt;path/to/local/dir&gt;<br></code></pre></td></tr></table></figure><ul><li>&lt;repo&#x2F;name&gt;：替换为你要下载的Hugging Face Hub上的模型或数据集的仓库名称</li><li>&lt;path&#x2F;to&#x2F;local&#x2F;dir&gt;：替换为你想要保存模型或数据集的本地目录路径</li><li><code>–resume-download</code>：可以尝试恢复一个之前中断的下载，这个选项特别有用，当你的网络连接不稳定，或者下载过程中因为某种原因被中断</li></ul><h2 id="Ollama常用操作"><a href="#Ollama常用操作" class="headerlink" title="Ollama常用操作"></a>Ollama常用操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动服务</span><br>ollama serve<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">显示模型信息</span><br>ollama show<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">运行模型</span><br>ollama run model<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">从注册表中拉取模型</span><br>ollama pull modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">列出模型</span><br>ollama list<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除模型</span><br>ollama rm modelname<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">模型使用提示符</span><br>/set | set session variables<br>/show | show model information<br>/load &lt;model&gt; | load a session or model<br>/save &lt;model&gt; | save your current session<br>/clear | clear session context<br>/bye | exit<br>/? /help | help for a command<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EchoSight论文研读复现</title>
    <link href="/2024/11/16/EchoSight%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E5%A4%8D%E7%8E%B0/"/>
    <url>/2024/11/16/EchoSight%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="EchoSight-Advancing-Visual-Language-Models-with-Wiki-Knowledge"><a href="#EchoSight-Advancing-Visual-Language-Models-with-Wiki-Knowledge" class="headerlink" title="EchoSight: Advancing Visual-Language Models with Wiki Knowledge"></a><em>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</em></h1><center> Yibin Yan && Weidi Xie </center><center> School of Artificial Intelligence, Shanghai Jiao Tong University </center> <center> 原文连接：https://arxiv.org/pdf/2407.12735 </center> <center> 论文主页：https://go2heart.github.io/echosight/ </center> <h2 id="Advancing-Visual-Language-Models"><a href="#Advancing-Visual-Language-Models" class="headerlink" title="Advancing Visual-Language Models"></a>Advancing Visual-Language Models</h2><div class="note note-success">            <p>推进VLM，推进点在哪里？</p>          </div><p>现有的视觉语言模型VLM(GPT-4V、Gemini、LLaVA、Phi-3-Vision)已经可以很好的解决Standard VQA任务，表现出来较强的图像分析和较准确的答案生成。</p><p>但是现有的VLM在KB-VQA(知识型视觉问答)任务上仍然存存在的问题：<strong>幻觉</strong>、<strong>不存在的文本与内部偏见</strong>和<strong>良好的检索机制</strong>。</p><p>本文的推进点就是设计并实现了一种新型的多模态RAG框架：<strong>EchoSight</strong>，这种方法显著增强了检索到的文本知识和视觉内容之间的对齐，从而提高了E-VQA和InfoSeek等基准测试的性能。</p><blockquote><p>Information Retrieval+LLM &#x3D; RAG<br>Information Retrieval+MLLM &#x3D; M-RAG</p></blockquote><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ul><li><p>EchoSight首先通过使用<em>visual-only</em> information的方法搜索wiki文章</p></li><li><p>根据相关性结合text-image query对候选文章进行重排序</p></li><li><p>整合了一个RAG的检索-重排过程</p></li></ul><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ul><li><p>提出了MLLM RAG框架：<strong>EchoSight</strong>，可以回答需要<strong>细粒度</strong>的百科全书知识的视觉问题</p></li><li><p>采取了一种<strong>retrieval-and-reranking scheme</strong>方法提升了检索效果</p></li><li><p>在Encyclopedic VQA和InfoSeek数据集上进行实验均取得的结果均优于现有的VLMs和其他RAG架构</p></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><div class="note note-success">            <p>检索增强VQA的问题定义-&gt;retrieval-and-reranking-&gt;答案生成</p>          </div><p>EchoSight的整体架构图如下所示：</p><p><img src="https://go2heart.github.io/echosight/static/images/overall.png" alt="EchoSight总体视图"></p><p><strong>4个主要组成</strong></p><ul><li><p>an external knowledge base(KB): Wiki</p></li><li><p>a retriever: $S_\Omega$</p></li><li><p>a reranker: $S_r^\text{sec}$ + $sec_{vl}$</p></li><li><p>an answer generator: LLM</p></li></ul><h3 id="Problem-Formation"><a href="#Problem-Formation" class="headerlink" title="Problem Formation"></a>Problem Formation</h3><p>输入：一张参考图片和一个自由形式的文本型问题</p><p>输出：答案</p><p>目标：建立一个视觉问答系统</p><p>辅助：接入外部知识源</p><p>知识源是一百万规模的实体文章及与它们相关的来自维基百科网页的图像</p><p>$$<br>B &#x3D; {(a_{1},I_{1}),(a_{2},I_{2})…(a_{n},I_{n})}<br>$$</p><p>整个过程从检索器开始，利用参考图像过滤并提取与相似图像有关的KB文档实体，之后重排器会<br>获取候选实体并使用它们的文本内容来重新排序，基于参考图片和文本问题之间的相关性；最后<br>将重排KB实体“喂”给答案生成器产生最终答案。</p><h3 id="Retrieval-and-Reranking"><a href="#Retrieval-and-Reranking" class="headerlink" title="Retrieval and Reranking"></a>Retrieval and Reranking</h3><p>这一阶段的主要目标就是根据给定的参考图像和文本问题从大规模外部数据集中识别相关实体。</p><h4 id="Visual-only-Research"><a href="#Visual-only-Research" class="headerlink" title="Visual-only Research"></a>Visual-only Research</h4><p>纯视觉搜索识别视觉上与查询图像相似的候选对象。Knowledge Base中存储格式：<strong>“image-article” pairs</strong>，优化图片检索是至关重要的。</p><p><strong>方法</strong>：将所有图片转换成向量，利用<em>cosine similarity</em>评估度量与参考图片的相似性。</p><p>$$<br>S_\Omega&#x3D;{s_i&#x3D;\left\langle\frac{v_r}{||v_r||}\cdot\frac{v_i}{||v_i||}\right\rangle,i&#x3D;1,\dots,n}<br>$$</p><p>$v_{r}$表示参考图片的嵌入；$v_{i}$表示数据库图像的嵌入。</p><p>FALSS库进行向量搜索，并保留Top-k最佳匹配图像和对应的文章实体。$\mathcal{E}_v&#x3D;{(a_1,I_1),\ldots,(a_k,I_k)},k\ll n$</p><h4 id="Multimodel-Reranking"><a href="#Multimodel-Reranking" class="headerlink" title="Multimodel Reranking"></a>Multimodel Reranking</h4><p>评估视觉和文本信息以重新排序检索实体。集成整合包含文本和视觉输入，来自多模态查询和top-k个检索得到的wiki文章实体，<br>确保相关性最高的文章排在顶部。</p><p><strong>方法</strong>：运用<em>Q-Former</em>架构从文本问题和参考图像中提取多模态信息，生成32query tokens。</p><p>$$<br>z_m^i&#x3D;\text{Q-Former}\left(I_\text{ref},Q\right)^i<br>$$</p><p>${z_m^i}$记为参考图片$I_{ref}$和文本问题$Q$的第$i_{th}$个query token嵌入。</p><p>在候选文章中，本论文将候选的wiki文章切分成以文章标题为前缀的sectioins：${sec_1^i,sec_2^i,\dots,sec_p^i}$。使用Q-Former文本编码器对sections进行编码，使用BLIP-2的权重初始化Q-Former，并微调除视觉编码器之外的所有参数。</p><p>重排得分：</p><p>$$<br>S_r^\text{sec}&#x3D;\max_{1\leq i\leq N_q}\left(\sin(z_m^i,z_s^\text{sec})\right)<br>$$</p><p>这种方法计算了在<strong>多模态query token嵌入</strong>和<strong>wiki文章section的token嵌入</strong>之间最高的成对相似性，每一个多模态query token嵌入${z_m^i}$都来自“img-question”对。</p><p>计算完重排得分后，最后一步是进行多模态重排，reranker将前一阶段视觉相似度得分和重排得分合成一个加权和：</p><p>$$<br>\begin{aligned}\sec_{vl}&amp;&#x3D;\arg\max_{\text{sec}\in a}(\alpha\cdot S_v^\text{sec}+(1-\alpha)\cdot S_r^\text{sec})\end{aligned}<br>$$</p><p>$sec_{vl}$表示的是经过reranker得到的最高排序实体<strong>section</strong>，注意这里是section而不是article。${\alpha}$是权重参数用来平衡视觉相似度得分$S_v^\text{sec}$和重排得分$S_r^\text{sec}$。</p><p>Note:$S_v^\text{sec}$有别于前文中的$S_\Omega$，它是在visual-only搜索阶段使用<strong>sec所属的wiki条目</strong>中的最佳匹配图像来计算的。</p><h4 id="Reranking-Training"><a href="#Reranking-Training" class="headerlink" title="Reranking Training"></a>Reranking Training</h4><p>前文中使用对比学习框架完成了硬负采样，负样本的选取是从视觉相似但是内容有别的例子中选取的，以至于最初的visual-only检索并不成功。训练重排器的目的就是为多模态queries选取最相关的文章以提高系统的整体的准确率和有效性。</p><p>训练的损失函数定义为：</p><p>$$<br>\mathcal{L}&#x3D;-\log\frac{\exp(\max_{1\leq i\leq N_q}\sin(z_m^i,z_s)&#x2F;\mathcal{T})}{\sum_{j&#x3D;1}^N\exp(\max_{1\leq i\leq N_q}\sin(z_m^i,z_s^j)&#x2F;\mathcal{T})}<br>$$</p><p>${z_s}$是positive section嵌入，$N$是全部的正负样本数量，$\mathcal{T}$是一个控制softmax平滑分布的超参数。</p><h3 id="Answer-Generation-with-LLMs"><a href="#Answer-Generation-with-LLMs" class="headerlink" title="Answer Generation with LLMs"></a>Answer Generation with LLMs</h3><p>一旦从KB中识别出相关条目，LLMs会整合所有的信息来生成answer，$A &#x3D; LLM(sec_{vl},Q)$。LLM是答案的genertor，$sec_{vl}$是检索得到的wiki文章section，$Q$是目标问题。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="两个数据集"><a href="#两个数据集" class="headerlink" title="两个数据集"></a>两个数据集</h3><p>Encyclopedic VQA 和 InfoSeek</p><p><strong>Encyclopedic VQA</strong>：contains 221k unique question and answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples</p><p><strong>InfoSeek</strong>：comprises 1.3M visual information-seeking questions, covering more than 11K visual entities from OVEN</p><h3 id="度量指标"><a href="#度量指标" class="headerlink" title="度量指标"></a>度量指标</h3><p>关注两个度量指标：检索和问题回答。</p><p>检索衡量的是在大规模多模态知识库中检索相关文章的准确性，问题回答衡量的是对于视觉问题提供准确回答的有效性。</p><h4 id="检索度量"><a href="#检索度量" class="headerlink" title="检索度量"></a>检索度量</h4><p>本文使用标准<strong>Recall@K</strong>，Recall@K评估是否正确的文章实体出现在top-k的检索结果中。</p><p>只有当一篇文章的URL与目标URL完全匹配时，它才被认为是正确的，这使得我们的检索评估与只匹配所检索到的文章的答案内容的方法相比更加严格和精确。</p><h4 id="问题回答的度量"><a href="#问题回答的度量" class="headerlink" title="问题回答的度量"></a>问题回答的度量</h4><p>E-VQA：BEM score</p><p>InfoSeek：VQA Accuracy and Relaxed Accuracy</p><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><h4 id="检索器"><a href="#检索器" class="headerlink" title="检索器"></a>检索器</h4><p>Eva-CLIP作为检索器，将参考图片和数据库图片进行视觉嵌入</p><h4 id="重排器"><a href="#重排器" class="headerlink" title="重排器"></a>重排器</h4><p>重排器使用带权重LAVIS的BLIP-2</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p>E-VQA：Mistral-7B-Instruct-v0.2</p><p>InfoSeek：LLaMA-8B-Instruct</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>很明显，我们提出的EchoSight框架实验结果远远超过了之前的工作，甚至接近了原始E-VQA(Mensink等人，2023)在基准报告的上行结果，其中采用了两个巨大的模型，即“Google Lens”用于知识检索，PaLM用于答案生成。</p><p><img src="https://go2heart.github.io/echosight/static/images/VQA.png" alt="实验结果"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>使用E-VQA数据集进行消融实验。</p><p>在检索方面，我们进行了以下分析：<br>(i)比较检索中不同的vision backbones<br>(ii)研究重新排序范围的影响<br>(iii)调查硬负抽样的重要性。</p><p>在最终答案生成时，我们进行了消融研究：<br>(i)各种语言模型的影响<br>(ii)在oracle设置下使用答案生成器进行实验。</p><h2 id="Conclusion、Limitations-and-Future-work"><a href="#Conclusion、Limitations-and-Future-work" class="headerlink" title="Conclusion、Limitations and Future work"></a>Conclusion、Limitations and Future work</h2><p>Conclusion:</p><p>在本文中，介绍了一种新的检索增强视觉语言系统EchoSight，用于解决基于知识的视觉问题回答(VQA)的挑战。通过改进的两阶段方法，EchoSight显示出显著的性能改善，E-VQA的准确率为41.8%，InfoSeek的准确率为31.3%。</p><p>EchoSight的成功强调了高效的检索过程和多模态信息的集成在基于知识的VQA任务中提高大型语言模型(LLMs)性能方面的重要性。</p><p>Limitations:</p><ul><li><p>严重依赖检索知识源的高质量和完整性；在特定领域上表现欠佳</p></li><li><p>多模态重排阶段需要消耗大量计算资源，耗时长，不利于实时应用</p></li></ul><p>Future work:</p><div class="note note-danger">            <p>优化计算开销能不能和prompt压缩和检索得分结合起来？</p>          </div><ul><li><p>提高知识源质量</p></li><li><p>优化多模态计算开销</p></li></ul><h2 id="复现代码的实验环境"><a href="#复现代码的实验环境" class="headerlink" title="复现代码的实验环境"></a>复现代码的实验环境</h2><p>GPU：Nvidia RTX 4090</p><p>显存：16GB</p><p>内存：50GB </p><h2 id="Git拉取源码仓库"><a href="#Git拉取源码仓库" class="headerlink" title="Git拉取源码仓库"></a>Git拉取源码仓库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp# git clone https://github.com/Go2Heart/EchoSight.git<br>Cloning into &#x27;EchoSight&#x27;...<br>remote: Enumerating objects: 532, done.<br>remote: Counting objects: 100% (130/130), done.<br>remote: Compressing objects: 100% (108/108), done.<br>remote: Total 532 (delta 66), reused 62 (delta 22), pack-reused 402 (from 1)<br>Receiving objects: 100% (532/532), 3.03 MiB | 3.11 MiB/s, done.<br>Resolving deltas: 100% (245/245), done.<br><br>root@I1dbca09acd00201b2e:/hy-tmp# cd EchoSight/<br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# ls<br>README.md  data_utils.py  demo                   lavis  requirements.txt  test<br>app.py     dataset        fine_tune_reranker.py  model  scripts           utils<br></code></pre></td></tr></table></figure><ul><li><p>model&#x2F;文件夹下存放着模型源代码</p></li><li><p>dataset&#x2F;文件夹下存放着wiki知识源</p></li><li><p>scripts&#x2F;文件夹下存放着训练脚本shell文件</p></li><li><p>requirements.txt文件存放着项目所需要的第三方库</p></li><li><p>app.py是前端检索的demo程序</p></li></ul><h2 id="Conda-创建虚拟环境"><a href="#Conda-创建虚拟环境" class="headerlink" title="Conda 创建虚拟环境"></a>Conda 创建虚拟环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">1.Create conda environment</span><br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# conda create -n echosight python=3.10<br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# conda activate echosight<br><span class="hljs-meta prompt_">#</span><span class="language-bash">2.Install the required packages</span><br>root@I1dbca09acd00201b2e:/hy-tmp/EchoSight# pip install -r requirements.txt<br><br>Successfully built antlr4-python3-runtime fairscale ffmpy iopath<br></code></pre></td></tr></table></figure><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><h3 id="Enclyclopedic-VQA"><a href="#Enclyclopedic-VQA" class="headerlink" title="Enclyclopedic-VQA"></a>Enclyclopedic-VQA</h3><ul><li><a href="https://storage.googleapis.com/encyclopedic-vqa/encyclopedic_kb_wiki.zip">Encylopedic-VQA’s 2M Knowledge Base</a>：4.9GB</li><li><a href="https://drive.google.com/file/d/1cQYul-my2FtqfCND2FeqgF9TCMU3u5xz/view?usp=drive_link">Enclopedic-VQA KB Images Faiss Index</a>：15GB</li></ul><h3 id="Infoseek"><a href="#Infoseek" class="headerlink" title="Infoseek"></a>Infoseek</h3><ul><li><a href="https://drive.google.com/file/d/1cIbKtYryD7XBAw0tjrrCvMCJC2rIzLM5/view?usp=drive_link">Our InfoSeek’s 100K Knowledge Base</a>：280M</li><li><a href="https://drive.google.com/file/d/1cDuL45c1iYwB0_BSlTmrMzbEE8ik2cVJ/view?usp=drive_link">InfoSeek KB Images Faiss Index</a>：1.2GB</li></ul><h3 id="将数据集上传到云服务器"><a href="#将数据集上传到云服务器" class="headerlink" title="将数据集上传到云服务器"></a>将数据集上传到云服务器</h3><p>使用<em>xftp</em>传输</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>EchoSight 的多模态重排序器使用 Encyclopedic-VQA 数据集和相应的 2M 知识库进行训练。要训练多模式重新排序器，请在更改必要的配置后运行 bash 脚本：</p><p>修改配置文件参数路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#vim train_reranker.sh<br><br>export CUDA_VISIBLE_DEVICES=0<br>python fine_tune_reranker.py \<br>   --blip-model-name &#x27;blip2_reranker&#x27; \<br>   --num-epochs 20 \<br>   --num-workers 4 \<br>   --learning-rate 1e-5 \<br>   --batch-size 6 \<br>   --transform targetpad \<br>   --target-ratio 1.25  \<br>   --save_frequency 50000 \<br>   --train_file /PATH/TO/TRAIN_FILE \<br>   --knowledge_base_file /PATH/TO/KNOWLEDGE_BASE_JSON_FILE \<br>   --negative_db_file /PATH/TO/NEGATIVE_DB_FILE \<br>   --inat_id2name /PATH/TO/INATURALISTS_ID2NAME \<br>   --save-training \<br><br></code></pre></td></tr></table></figure><p>–train_file：训练数据文件的路径。 训练文件应与 Encyclopedic-VQA 提供的格式相同。</p><p>–knowledge_base_file：JSON 格式的知识库文件的路径。 格式应与Encyclopedic-VQA 的格式相同。</p><p>–negative_db_file：用于训练的硬负样本数据库文件的路径。</p><p>–inat_id2name：iNaturalist ID 到名称映射文件的路径。</p><p>执行脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#bash scripts/test_reranker.sh<br></code></pre></td></tr></table></figure><p>test_reranker:</p><p>我们的重新排序模块权重可以在[Checkpoint]下载，要使用经过训练的模型进行推理，调整必要的参数后运行提供的 test_reranker.sh脚本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@I1dbca09acd00201b2e:/hy-tmp/EchoSight/scripts#vim test_reranker.sh<br><br>export CUDA_VISIBLE_DEVICES=0<br>python -m test.test_reranker \<br>    --test_file /PATH/TO/TESTFILE\<br>    --knowledge_base /PATH/TO/KNOWLEDGE_BASE_JSON_FILE\<br>    --faiss_index /PATH/TO/KNOWLEDGE_BASE_FAISS_INDEX\<br>    --retriever_vit eva-clip \<br>    --top_ks 1,5,10,20 \<br>    --retrieval_top_k 20\<br>    --perform_qformer_reranker \<br>    --qformer_ckpt_path /PATH/TO/QFORMER-CKPT\<br></code></pre></td></tr></table></figure><p>–test_file：测试文件的路径。</p><p>–knowledge_base：知识库 JSON 文件的路径。</p><p>–faiss_index：用于高效相似性搜索的 FAISS 索引文件的路径。</p><p>–save_result_path：保存结果 json 文件的路径。</p><h2 id="重点关注的问题"><a href="#重点关注的问题" class="headerlink" title="重点关注的问题"></a>重点关注的问题</h2><div class="note note-success">            <ol><li>prompt的具体格式是啥样的？-》看代码，做这一方面一定要对格式有一个清楚的了解</li><li>MLLMs RAG的prompt(最后生成答案前的一步)</li><li>重排的方法有哪些？可以调研一下</li></ol>          </div>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>MLLM</tag>
      
      <tag>VQA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典卷积神经网络复现</title>
    <link href="/2024/11/15/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    <url>/2024/11/15/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>基于<code>pytorch</code>将经典的卷积神经网络架构复现一下</p>          </div><h2 id="卷积神经网络基础知识"><a href="#卷积神经网络基础知识" class="headerlink" title="卷积神经网络基础知识"></a>卷积神经网络基础知识</h2><p>数据又表格数据过渡到图像的像素数据，网络结构由全连接的多层感知机过渡到卷积结构。</p><p>卷积神经网络就是将空间不变性这一概念系统化，基于这个模型使用较少的参数学习有用的表示。</p><p><img src="https://i.kinja-img.com/gawker-media/image/upload/w6gfkxdf7lyoygaog9d7.jpg" alt="where is Weldo?"></p><p>卷积神经网络的输入：n * n * 3的图片向量</p><p>卷积神经网络的输出：类别标签&#x2F;类别向量(经过softmax归一化后)</p><p>通道：图像一般包含3个通道&#x2F;3种原色(RGB)，一个颜色就是一个色彩通道。</p><p>感受野：卷积神经网络在图片上设定的区域大小</p><p>步幅：感受野在图片上移动的距离</p><p>填充：感受野在移动过程中可能会超出图片的像素范围，超出范围的地方用数字补值的方式就是填充</p><p>滤波器：让每个感受野都只有一组参数从而达到简化的方法。</p><p>卷积层：感受野+参数共享</p><p>下采样：把图像<strong>偶数列</strong>都拿掉，<strong>奇数行</strong>都拿掉，图像变成为原来的 1&#x2F;4，但是不会影响里面是什么</p><p>东西。</p><p>汇聚：汇聚没有参数只是一个操作，将滤波器产生的结果进行分组后按要求汇聚(取最大&#x2F;取平均)</p><p>池化层：取窗口中的最大值最为输出结果，然后滑动窗口减少其空间尺寸</p><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>卷积神经网络的开山之作，1998年由LeCun Yang提出。</p><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1TI1aGBZ4dybR6__DI9dzA.png" alt="LeNet-5"></p><h3 id="复现代码"><a href="#复现代码" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment">#LeNet-5一共有7层，输入图像大小为32*32*1，输出对应10个类别的条件概率</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet5</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet5,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layer1 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：1；输出channels：6，卷积核：5*5</span><br>          <span class="hljs-comment">#Conv2d(1,6,5,1,0)的参数依次是：输入通道数，输出通道数，卷积核大小、步长、填充</span><br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#2*2卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer2 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：6；输出channels：16，卷积核：5*5</span><br>            nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#2*2卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Flatten(),<span class="hljs-comment">#将卷积后的16@5*5图像输出展平</span><br>            nn.Linear(<span class="hljs-number">400</span>,<span class="hljs-number">120</span>),<span class="hljs-comment">#线性层</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),<span class="hljs-comment">#线性层</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>),<span class="hljs-comment">#线性层</span><br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x = <span class="hljs-variable language_">self</span>.layer2(x)<br>        <span class="hljs-built_in">print</span>(x.shape)<br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>2014年ImageNet比赛的冠军</p><h3 id="网络结构图-1"><a href="#网络结构图-1" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://i0.wp.com/bainsa.xyz/wp-content/uploads/2023/11/a1.png?fit=940,469&ssl=1" alt="AlexNet"></p><h3 id="结构细节"><a href="#结构细节" class="headerlink" title="结构细节"></a>结构细节</h3><div class="note note-danger">            <p>卷积核大小如何确定？</p>          </div><p>虽然说目前有比较火的研究方向针对这种自动神经网络结构搜索（NAS），这些自动搜索出来的网络在常规数据集上的建模结果显示（当然是达到一定的准确度）：自动搜索出来的网络中的卷积核的类别有包括各种常见的型号（3 * 3、5 * 5、7*7），且在网络中的前后排布没有规律。</p><div class="note note-danger">            <p>nn.BatchNorm2d()的作用？</p>          </div><p>在深度神经网络中，梯度消失是一个常见的问题。BatchNorm2d通过对激活函数前添加归一化层，抑制了梯度消失的问题，从而加速了优化过程。</p><p>BatchNorm2d通过对数据的归一化处理，使得权重初始化的影响减小，无论权重的初始值如何，都可以通过归一化和仿射变换得到较好的效果。</p><div class="note note-danger">            <p>nn.Dropout(0.5,inplace&#x3D;True)的作用？</p>          </div><p>nn.Dropout模块的作用是在训练过程中随机关闭一部分神经元，以增加模型的泛化能力。通常，我们在全连接层之后应用dropout，以避免破坏卷积层中重要的空间信息。</p><p>一般建议在10%-50%之间设置dropout比例</p><h3 id="复现代码-1"><a href="#复现代码-1" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment">#AlexNet的输入为224*224*3图像，输出为1000个类别的条件概率</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Neural network model consisting of layers proposed by AlexNet paper</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AlexNet,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layer1 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：3；输出channels：96，卷积核：11*11</span><br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">96</span>,<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">96</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer2 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：96；输出channels：256，卷积核：5*5</span><br>            nn.Conv2d(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer3 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：256；输出channels：384，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">394</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer4 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：384；输出channels：384，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">394</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.layer5 = nn.Sequential(<br>          <span class="hljs-comment">#输入channels：384；输出channels：256，卷积核：3*3</span><br>            nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>          <span class="hljs-comment">#3*3卷积核，步长为2进行最大池化</span><br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>    <br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Dropout(<span class="hljs-number">0.5</span>,inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">6</span>*<span class="hljs-number">6</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Dropout(<span class="hljs-number">0.5</span>,inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">1000</span>),<br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x = <span class="hljs-variable language_">self</span>.layer2(x)<br>        x = <span class="hljs-variable language_">self</span>.layer4(x)<br>        x = <span class="hljs-variable language_">self</span>.layer5(x)<br>        output = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>2014年ImageNet比赛的亚军</p><h3 id="网络结构图-2"><a href="#网络结构图-2" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://production-media.paperswithcode.com/methods/vgg_7mT4DML.png" alt="VGG"></p><p>每一个长方体“板子”就是一张图片数据的一个tensor，搭建网络的过程中关注每一块“板子”之间的变化即可。</p><h3 id="复现代码-2"><a href="#复现代码-2" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">基于pytorch搭建VGG卷积神经网络模型-16层网络结构</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGGmodel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(VGGmodel,<span class="hljs-variable language_">self</span>).__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.features = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">1000</span>)<br>        )<br>        <span class="hljs-variable language_">self</span>._initialize_weights()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.features(x)<br>        x = nn.Flatten(x)<br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>VGG网络只是对网络的层书进行堆叠，并没有进行结构性的创新，不过加深网络深度确实可以提高模型效果。</p><h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>GoogleNet也被叫做InceptionNet，它解决了一个核心问题是选择多大的卷积核是最合适的问题。在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。</p><p><img src="https://zh.d2l.ai/_images/inception.svg" alt="Inception块的架构"></p><p>动手学深度学习：Inception块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。 </p><ol><li>难点1是如何并行设计出网络结构？</li><li>难点2是并行输入输出的通道数如何计算？</li></ol><h3 id="网络结构图-3"><a href="#网络结构图-3" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://zh.d2l.ai/_images/inception-full.svg" alt="GoogleNet"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment">#channel1-channel4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels,channel1, channel2,channel3,channel4,**kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception,<span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-comment">#路径1:单层1*1卷积层</span><br>        <span class="hljs-variable language_">self</span>.path1 = nn.Conv2d(in_channels,channel1,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#路径2:1x1卷积层后接3x3卷积层</span><br>        <span class="hljs-variable language_">self</span>.path2_1 = nn.Conv2d(in_channels,channel2[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path2_2 = nn.Conv2d(channel2[<span class="hljs-number">0</span>],channel2[<span class="hljs-number">1</span>],<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#路径3:1x1卷积层后接5x5卷积层</span><br>        <span class="hljs-variable language_">self</span>.path3_1 = nn.Conv2d(in_channels,channel3[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path3_2 = nn.Conv2d(channel3[<span class="hljs-number">0</span>],channel3[<span class="hljs-number">1</span>],<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>        <span class="hljs-comment">#路径4:3x3最大汇聚层后接1x1卷积层</span><br>        <span class="hljs-variable language_">self</span>.path4_1 = nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.path4_2 = nn.Conv2d(in_channels,channel4,<span class="hljs-number">1</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        p1 = nn.ReLU(<span class="hljs-variable language_">self</span>.path1(x))<br>        p2 = nn.ReLU(<span class="hljs-variable language_">self</span>.path2_2(nn.ReLU(<span class="hljs-variable language_">self</span>.path2_1(x))))<br>        p3 = nn.ReLU(<span class="hljs-variable language_">self</span>.path3_2(nn.ReLU(<span class="hljs-variable language_">self</span>.path3_1(x))))<br>        p4 = nn.ReLU(<span class="hljs-variable language_">self</span>.path4_2(<span class="hljs-variable language_">self</span>.path4_2(x)))<br>        <span class="hljs-keyword">return</span> torch.cat((p1,p2,p3,p4),dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-comment">#第一个模块使用64个通道、7*7卷积层</span><br>        <span class="hljs-variable language_">self</span>.b1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第二个模块使用两个卷积层：第一个卷积层是64个通道、1*1卷积层；第二个卷积层使用将通道数量增加三倍的卷积层</span><br>        <span class="hljs-variable language_">self</span>.b2 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">192</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第三个模块串联两个完整的Inception块</span><br>        <span class="hljs-comment">#难点是理解各个路径通道数的计算</span><br>        <span class="hljs-variable language_">self</span>.b3 = nn.Sequential(<br>            Inception(<span class="hljs-number">192</span>,<span class="hljs-number">64</span>,(<span class="hljs-number">96</span>,<span class="hljs-number">128</span>),(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>)),<br>            Inception(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>,(<span class="hljs-number">128</span>,<span class="hljs-number">92</span>),(<span class="hljs-number">32</span>,<span class="hljs-number">94</span>)),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第四模块串联了5个Inception块</span><br>        <span class="hljs-variable language_">self</span>.b4 = nn.Sequential(<br>            Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>            Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment">#第五模块包含两个Inception块</span><br>        <span class="hljs-variable language_">self</span>.b5 = nn.Sequential(<br>            Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>            nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            nn.Flatten()<br>        )<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">10</span>)<br>        <br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = nn.Sequential(<span class="hljs-variable language_">self</span>.b1,<span class="hljs-variable language_">self</span>.b2,<span class="hljs-variable language_">self</span>.b3,<span class="hljs-variable language_">self</span>.b4,<span class="hljs-variable language_">self</span>.classifier)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>2015年ImageNet比赛的冠军，通过残差模块能够成功训练出152层深度的残差网络。</p><p>设计的灵感来源于：加深神经网络的时候会出现一个Degradation，准确率上升到达饱和，在持续增加深度会导致模型的准确率下降。</p><p>假设一个比较浅的网络达到了饱和准确率，那么在它后面加上几层恒等映射层误差不会增加，也就是说更深的模型不会是模型的效果下降。恒等映射的思想就是ResNet的灵感来源。</p><h3 id="网络结构图-4"><a href="#网络结构图-4" class="headerlink" title="网络结构图"></a>网络结构图</h3><p><img src="https://miro.medium.com/v2/resize:fit:2000/1*6hF97Upuqg_LdsqWY6n_wg.png" alt="ResNet"></p><h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">ResNet网络结构复现</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#残差块的定义</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResiDual</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels,num_channels,use_1X1conv = <span class="hljs-literal">False</span>,stride = <span class="hljs-number">1</span>,*kwargs</span>):<br>        <span class="hljs-built_in">super</span>(ResiDual,<span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-comment">#定义第一个3*3的卷积层</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels,num_channels,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">#定义第二个3*3的卷积层</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(num_channels,num_channels,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> use_1X1conv:<br>            <span class="hljs-variable language_">self</span>.conv3 = nn.Conv2d(in_channels,num_channels,<span class="hljs-number">1</span>,stride=stride)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.conv3 = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">#定义批量规范化层1</span><br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(num_channels)<br>        <span class="hljs-comment">#定义批量规范化层2</span><br>        <span class="hljs-variable language_">self</span>.bn2 = nn.BatchNorm2d(num_channels)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        y = nn.ReLU(<span class="hljs-variable language_">self</span>.bn1(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        y = <span class="hljs-variable language_">self</span>.bn2(<span class="hljs-variable language_">self</span>.conv2(y))<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.conv3:<br>            x = <span class="hljs-variable language_">self</span>.conv3(x)<br>        y += x<br>        <span class="hljs-keyword">return</span> F.relu(y)<br><br><span class="hljs-comment">#定义ResNet架构</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(ResNet,<span class="hljs-variable language_">self</span>).__init__(*args, **kwargs)<br>        <span class="hljs-comment"># 初始化与GoogleNet相似</span><br>        <span class="hljs-variable language_">self</span>.b1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment"># 定义残差块</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">ResBlock</span>(<span class="hljs-params">in_channels,num_channels,num_residuals,first_block=<span class="hljs-literal">False</span></span>):<br>            block = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>                <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>                    block.append(ResiDual(in_channels,num_channels,use_1X1conv=<span class="hljs-literal">True</span>,stride=<span class="hljs-number">2</span>))<br>                <span class="hljs-keyword">else</span>:<br>                    block.append(ResiDual(num_channels,num_channels))<br>            <span class="hljs-keyword">return</span> block<br>        <br>        <span class="hljs-comment"># 在ResNet加入所有残差块</span><br>        <span class="hljs-variable language_">self</span>.b2 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,first_block=<span class="hljs-literal">True</span>),<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b3 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b4 = nn.Sequential(<br>            *ResBlock(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>        <span class="hljs-variable language_">self</span>.b5 = nn.Sequential(<br>            *ResNet(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>)<br>        )<br>        <br>    <br>    <span class="hljs-comment">#前向传播计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forword</span>(<span class="hljs-params">self,x</span>):<br>        x = nn.Sequential(<br>            <span class="hljs-variable language_">self</span>.b1,<span class="hljs-variable language_">self</span>.b2,<span class="hljs-variable language_">self</span>.b3,<span class="hljs-variable language_">self</span>.b4,<span class="hljs-variable language_">self</span>.b5,<br>            nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>        )<br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="note note-danger">            <p>1.卷积神经网络的基本概念一定要熟练掌握</p><p>2.对于使用pytorch搭建神经网络模型要熟练掌握代码编写的过程与逻辑</p><p>3.对于步长和填充值的计算要理解，特别是GoogleNet和ResNet的padding和stride值的计算</p><p>4.如何根据网络架构图取复现代码是理解网络结构和pytorch工具箱的结合</p>          </div>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++五子棋</title>
    <link href="/2024/11/11/C-%E4%BA%94%E5%AD%90%E6%A3%8B/"/>
    <url>/2024/11/11/C-%E4%BA%94%E5%AD%90%E6%A3%8B/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>在Windows中一切矩形的东西都是窗口</p>          </div><p>微软MFC的CPoint类库</p><h2 id="五子棋设计"><a href="#五子棋设计" class="headerlink" title="五子棋设计"></a>五子棋设计</h2><p>需要的类和方法：</p><p>棋子：黑棋、白棋、最后一步</p><p>点类：参考微软MFC的CPoint类库，看似细节很见水平</p><p>棋盘：数组（GetAt()、SetAt()）、init、display函数、输入函数、输出</p><p>玩家：分类（人人、人机），需要创建裁判对象、AI对象（大部分AI事都是它的事）</p><p>AI：计算连子数、推荐落子、评分、随机（分值相等且对称的地方随机）</p><p>裁判：计算输赢、违规-virtual虚函数（叠子、出界、禁手）裁判要先合规再计算输赢</p><p>工头：棋盘、棋子、玩家、时钟对象的初始化，猜先、写棋谱、交换手</p><p>时钟：超时</p><p>开始游戏-选择人人对战、人机对战</p><h2 id="棋子的设计"><a href="#棋子的设计" class="headerlink" title="棋子的设计"></a>棋子的设计</h2><p>0: 空白&#x2F;没有落子</p><p>1:黑子</p><p>2:白子</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年10月总结与展望</title>
    <link href="/2024/11/03/10%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/11/03/10%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="得与失"><a href="#得与失" class="headerlink" title="得与失"></a>得与失</h2><p>得：心情上不再纠缠与患得患失了，月下旬相比于月初和月中，要平和了许多。学习上稳扎稳打吧，慢慢的在往前走。</p><p>失：李沐的课程并没有按进度跟上学习、一些经典的论文都没有读；这个月旷课不少。</p><p>有得有失：这个月罕见地勇敢了一次，不知怎的。但是方式方法我认为并不妥当，会很冒失、唐突，但是于我本人，我绝对真诚。真诚有用吗？说不好。以后还要继续真诚吗？还会的。阅读能学到东西，同样地，经历、体验也能学到东西，看自己咯，无论顺利还是失败，开心还是失落，总能收获些什么，学到些什么，知道自己还能在哪方面提高些什么。杨力祥老师上周说了句话很值得思考：优秀的本质就是要学会随时改变自己的想法。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>💡想法转变：老师不再是传统意义上的“传道授业解惑”，要解放思想。老师与学生更像是“买卖”的商业关系，老师“卖”知识、经验，学生是“客户”。顾客就是上帝，不能总想靠考试难为学生，课就是卖点，学生也不都是傻子(虽然现在大学生都很好骗包括我自己)，东西好不好上一节课就知道。</p><p>天下没有什么东西是不变的，优秀的人一定会随时改变自己的思路与想法，这是优秀的标志，不要咬死了死脑筋，那是愚蠢</p><h2 id="11月务必"><a href="#11月务必" class="headerlink" title="11月务必"></a>11月务必</h2><p>务必11月要执行的：<strong>带着把程序写成东西的思想，疯狂Coding</strong>！！！！</p><h2 id="缺点改正"><a href="#缺点改正" class="headerlink" title="缺点改正"></a>缺点改正</h2><p>⚠️效率不高</p><p>⚠️早睡早起的习惯很难保持了</p><h2 id="💡一个想法"><a href="#💡一个想法" class="headerlink" title="💡一个想法"></a>💡一个想法</h2><p>最近在系统梳理<strong>深度学习</strong>的知识体系，想着把所学的做一个主动输出，正好自己也有Mac本。目前有个想法就是自己在B站上录课，去把自己所学的讲出来，别管有没有人看，自己主动去把知识讲出来，做一个费曼学习法。</p><p>知识体系梳理详细内容可以参考本人的历史文章中拙作<a href="https://mungeryang.top/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/">《我们是怎么学习的？机器是如何学习我们的学习的？》</a>（还在补充更新中）。</p><h2 id="观影"><a href="#观影" class="headerlink" title="观影"></a>观影</h2><p>勇敢的心这个网名用了快10年了，但是都没看过勇敢的心这部电影。</p><p>前天晚上抽时间认真看完了这部电影，这今年看的难得的好片，不愧是奥斯卡金像奖。</p><p>这是一部任务纪传型电影，主人公是苏格兰民族独立英雄<strong>威廉·华莱士</strong>，印象最深的一句台词是，最后他被处死时高喊的那句：<strong>Freedom</strong>！</p><p>平民抗争、权贵压迫、国家观、民族观在这部电影里都体现的淋漓尽致。</p><p>不由得想起一句话：<strong>由此上溯到一千八百四十年，从那时为了反对内外敌人、争取民族，在历次斗争的牺牲的人民英雄们永垂不朽！</strong></p><h2 id="🌲11月学习与工作部署"><a href="#🌲11月学习与工作部署" class="headerlink" title="🌲11月学习与工作部署"></a>🌲11月学习与工作部署</h2><p>重点任务：继续梳理深度学习知识框架，按着杨力祥老师上课讲的思路去摸索、整理、关联</p><p>需要做的：这个月要<strong>多动手去Code</strong>了，全力Code</p><p>需要Code的任务(按任务优先级)：</p><ol><li>PythonMLLMs RAG训练</li><li>C++复现Deep Learning、Java实现分布式文件系统模拟</li><li>Hive大数据实验</li></ol><p>11.03-11.07：</p><ul><li>着手对LLM RAG、MLLMs RAG进行实操训练，技术流程要做到心里有数，能get到大老师的点</li><li>别忘了大老师给的任务点，以及上篇论文的盲点梳理。</li></ul><p>11.08-11.14:</p><ul><li>梳理完深度学习框架，动手实践复现框架</li><li>老师idea复现</li><li>月中总结+休息调整</li></ul><p>11.18-11.25:</p><ul><li>复现完成经典的CNN模型</li><li>EchoSight论文研读复现</li><li>要梳理好Transformer、Attention的核心架构</li><li>Java分布式文件系统着手编写</li></ul><p>11.26-11.30:</p><ul><li>Rag VL论文研读复现</li><li>C++五子棋程序要完成一半</li><li>争取完成Java分布式文件系统</li></ul><h2 id="希望这个月（非鸡汤）"><a href="#希望这个月（非鸡汤）" class="headerlink" title="希望这个月（非鸡汤）"></a>希望这个月（非鸡汤）</h2><p>知难上，戒焦躁</p><p>常自省，穷途明</p><p>事毕于今，不溺过往</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM basic knowledge</title>
    <link href="/2024/10/28/LLM-basic-knowledge/"/>
    <url>/2024/10/28/LLM-basic-knowledge/</url>
    
    <content type="html"><![CDATA[<p>【<a href="https://datawhalechina.github.io/llm-cookbook/#/">面向开发者的LLM教程</a>】</p><p>LLM是通过预测下一个词的的监督学习方式进行训练的。具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入 Context 预测下一个词的概率分布。通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异,语言模型逐步掌握了语言的规律，学会了预测下一个词。</p><p>在训练过程中,研究人员会准备大量句子或句子片段作为训练样本,要求模型一次次预测下一个词，通过反复训练促使模型参数收敛，使其预测能力不断提高。经过在海量文本数据集上的训练，语言模型可以达到十分准确地预测下一个词的效果。这种<strong>以预测下一个词为训练目标的方法使得语言模型获得强大的语言生成能力</strong>。</p><div class="note note-danger">            <p>思考：这个和当初<strong>Alpha Go</strong>生成下一步棋的方式是不是有异曲同工之妙？</p>          </div><p><a href="https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file">https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file</a></p><p>input (prompt) -&gt; fn{LLM} -&gt; output</p><h2 id="历史发展"><a href="#历史发展" class="headerlink" title="历史发展"></a>历史发展</h2><p>大语言模型发展主要经历了统计语言模型 –&gt; 神经语言模型 –&gt; 预训练语言模型 –&gt; 大语言模型四个阶段</p><p>图片引用自<a href="https://cleaner.love/">Cleaner 知识库</a></p><p><img src="/img/fig/history-LLM.png" alt="语言模型发展史"></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h3><p>Token的概念相当于文字，一个中文文字对应一个token，比如我爱你，对应三个token：我+爱+你；一个英文的字符对应一个token，比如love，而icecream对应两个token：ice+cream。</p><p>联系理解，计算机理解数据的形式是<strong>比特流</strong>，那么类比联想理解，LLM理解数据的“字节”就是一个Token。</p><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>使用大语言模型在<strong>大规模语料</strong>上进行预训练，大语言模型可以获得通用的语言理解与生成能力，掌握较为广泛的世界知识，具备解决众多下游任务的性能潜力。</p><p>预训练大模型类比机器学习的训练模式，其实就是通过算法将模型在训练集上先进行训练</p><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><p>指令微调是指使用自然语言形式的数据对预训练后的大语言模型进行<strong>参数微调</strong>，它是增强和激活大语言模型特定能力的重要方法之一。通过使用任务输入与输出的配对数据进行模型训练，可以使语言模型掌握通过问答形式进行任务求解的能力和较强的指令遵循能力，并且能够无需下游任务的训练样本或者示例就可以解决训练中未见过的任务。</p><h3 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h3><p>经过大规模的预训练和有监督指令微调，大语言模型具备了解决各种任务的通用能力和指令遵循能力，但是同时也可能生成<code>有偏见的</code>、<code>冒犯的</code>以及<code>事实错误的</code>文本内容。这些潜在的有害行为，可能在下游应用中产生严重的影响与危害，进一步被恶意使用者进行放大与利用。因此，在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个 关键研究问题，通常称这一研究问题为<strong>人类对齐</strong>。</p><h3 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h3><p>模型泛化是指一些模型可以应用（泛化）到其他场景，通常为采用迁移学习、微调等手段实现泛化。</p><h3 id="涌现"><a href="#涌现" class="headerlink" title="涌现"></a>涌现</h3><p>模型规模达到一定阈值以上后，会在多步算术、大学考试、单词释义等场景的准确性显著提升，称为涌现。</p><h2 id="RAG与微调"><a href="#RAG与微调" class="headerlink" title="RAG与微调"></a>RAG与微调</h2><p>目前的大模型都是预训练语言模型 LLaMA、GPT4o、o1-mini</p><ul><li>互联网公开的海量数据</li><li>私有化数据</li></ul><p>针对互联网数据无法及时更新与专业领域私有化数据的解决方案：</p><ul><li>RAG-检索增强生成：数据（语料）数据大部分是文献资料、文档</li><li>SFT-微调：QA-法律咨询、心理诊断等一问一答数据标签归类，微调出一个适用场景的垂直场景</li></ul><p>RAG缺点：</p><p>检索出来的文档片段不完整，重排序</p><p>对于系统延迟低场景(检索器检索)</p><p>Embeding Model-嵌入</p><p>将文本转换成一组N维的浮点数，文本向量又叫做Embedings</p><p>向量之间可以计算距离，距离的远近对应语义的相似性</p><p>以前的工程中，keywords、sql &#x3D; 精确匹配，使用关系型数据库进行存储。现在文本嵌入、图片嵌入等 &#x3D; 距离相似度匹配，使用向量数据库存储</p><p>原始文档 -》embedded</p><p>query  -》embedded</p><p>必须使用同一个Embedded model</p><h2 id="参考学习的资料："><a href="#参考学习的资料：" class="headerlink" title="参考学习的资料："></a>参考学习的资料：</h2><p>[1] 面向开发者的LLM教程:<a href="https://datawhalechina.github.io/llm-cookbook/#/">https://datawhalechina.github.io/llm-cookbook/#/</a></p><p>[2] LLMBOOK:<a href="https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file">https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file</a></p><p>[3] <a href="https://developer.aliyun.com/article/1361737">https://developer.aliyun.com/article/1361737</a></p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>抉择本身就是向前吧</title>
    <link href="/2024/10/24/%E6%8A%89%E6%8B%A9/"/>
    <url>/2024/10/24/%E6%8A%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h2 id="选择与看待结果"><a href="#选择与看待结果" class="headerlink" title="选择与看待结果"></a>选择与看待结果</h2><p>选择不分对错，抉择本身就是向前吧！</p><p>健健康康、内心阳光；事物都是在波动、曲折中发展的，无论结果如何，都要保持好心态。</p><p>尊重不同的声音，调整自己的状态。</p><p>结果当然重要，符合自己的预期，那得开心死；但是，结果如果不达预期，就不正常了吗？就不合理了吗？存在即合理啊，只是没达到你自己的预期，你会失落、会感到遗憾、会不开心。但是，没有达到预期，自己就没有一点问题吗？我想自身还是有很大问题的，而且不要过多的去抱怨周围的环境。</p><h2 id="要勇敢"><a href="#要勇敢" class="headerlink" title="要勇敢"></a>要勇敢</h2><p>要勇敢，但是勇敢的方式不要有攻击性，这还是要慢慢改、慢慢学的。</p><h2 id="爱"><a href="#爱" class="headerlink" title="爱"></a>爱</h2><p>好好爱自己、好好爱家人，爱人先爱己，勿忘心安。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal RAG System Survey</title>
    <link href="/2024/10/20/Multimodal-RAG-System-Survey/"/>
    <url>/2024/10/20/Multimodal-RAG-System-Survey/</url>
    
    <content type="html"><![CDATA[<h1 id="A-Comprehensive-Guide-to-Building-Multimodal-RAG-Systems"><a href="#A-Comprehensive-Guide-to-Building-Multimodal-RAG-Systems" class="headerlink" title="A Comprehensive Guide to Building Multimodal RAG Systems"></a>A Comprehensive Guide to Building Multimodal RAG Systems</h1><p>本篇博文的主要内容是对一篇综述性博客文章()的翻译阅读与总结。通过阅读本篇文章，理清楚多模态RAG的基本概念与应用场景、部署方式、工作流程。</p><p>RAG系统的优势是可以整合自身数据并增强LLM的智力，对于问题给出更多精确的回答。然而，关键的限制就是RAG系统仅仅是它仅适用于文本数据。 许多现实世界的数据本质上是多模态的，是文本、图像、表格等形式的混合。</p><p>该篇综述的主要目的就是着眼构建一个多模态的RAG系统，使用智能数据转换和多模态LLM解决混合数据格式。</p><h2 id="传统的RAG系统"><a href="#传统的RAG系统" class="headerlink" title="传统的RAG系统"></a>传统的RAG系统</h2><p>RAG系统架构通常包括：</p><ol><li>数据处理和索引</li><li>检索和响应生成</li></ol><h3 id="数据处理和索引"><a href="#数据处理和索引" class="headerlink" title="数据处理和索引"></a>数据处理和索引</h3><p>加载文档中的文本内容-&gt;将大型文本元素拆分为较小的块-&gt;使用嵌入器模型将它们转换为嵌入-&gt;将块和嵌入存储到矢量数据库中</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-5.webp" alt="建立索引"></p><h3 id="检索和响应生成"><a href="#检索和响应生成" class="headerlink" title="检索和响应生成"></a>检索和响应生成</h3><p>用户提问-&gt;文档检索-&gt;提示-&gt;LLM-&gt;回答</p><p>从向量数据库中检索与输入问题相似的相关文本文档块，然后将问题和上下文文档块发送到LLM以生成人类。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-1-3.webp" alt="检索生成"></p><h2 id="传统RAG系统的限制"><a href="#传统RAG系统的限制" class="headerlink" title="传统RAG系统的限制"></a>传统RAG系统的限制</h2><ul><li>无法获悉实时数据</li><li>系统仅会利用好我们自身向量数据库中的数据</li><li>大部分RAG系统仅仅致力于文本数据的检索和生成</li><li>传统的LLMs 仅仅处理文本内容去生成回答</li><li>无法利用多模态数据进行工作</li></ul><h2 id="什么是多模态？"><a href="#什么是多模态？" class="headerlink" title="什么是多模态？"></a>什么是多模态？</h2><p>Multimodal data is essentially data belonging to multiple modalities.</p><p>多模态数据本质上是属于多种模态的数据。 模态的正式定义来自人机交互 (HCI) 系统的背景，其中模态被称为计算机和人类之间的单个独立输入和输出通道的分类（更多详细信息请参阅维基百科）。 常见的计算机-人类模式包括以下内容：</p><p>文本：Input and output through written language(e.g., chat interfaces).</p><p>语音：Voice-based interaction(e.g., voice assistants).</p><p>视频：Image and video processing for visual recognition (e.g., face detection).</p><p>手势：Hand and body movement tracking(e.g., gesture controls).</p><p>触控：Haptic feedback and touchscreens.</p><p>音频：Sound-based signals (e.g., music recognition, alerts).</p><p>生物识别：Interaction through physiological data (e.g., eye-tracking, fingerprints).</p><p>本综述重点关注处理文本、图像和表格。 理解此类数据所需的关键组件之一是多模态大语言模型 (LLM)。</p><p>多模态的本质是混合了多种多种模式和格式的数据：</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-2_11zon.webp" alt="多模态数据"></p><h2 id="什么是多模态大语言模型？"><a href="#什么是多模态大语言模型？" class="headerlink" title="什么是多模态大语言模型？"></a>什么是多模态大语言模型？</h2><p>多模态大语言模型 (LLM) 本质上是基于 Transformer 的 LLM，已针对多模态数据进行预训练和微调，以分析和理解各种数据格式，包括文本、图像、表格、音频和视频。 </p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-3-2.webp" alt="MLLMs"></p><p> 理想情况下，真正的多模态模型不仅应该能够理解混合数据格式，还应该生成相同的数据格式。</p><p>文生图、图生文、文生音、文生视频</p><p>多模态编码阶段：</p><p>利用现有的成熟模型对各种模式的输入进行编码。</p><p>LLM理解和响应阶段</p><p>LLM被用作NExT GPT的核心代理。从技术上讲，他们采用Vicuna LLM，该LLM<strong>将来自不同模态的表示作为输入</strong>，并对<strong>输入进行语义理解和推理</strong>。它输出1）直接的文本响应和2）每种模态的<strong>信号标记</strong>，这些标记用作指示<strong>解码层是否生成多模态内容以及如果是则生成什么内容的指令</strong>。</p><p>多模态生成阶段</p><p>从LLM（如果有的话）<strong>接收具有特定指令的多模态信号</strong>，<strong>基于Transformer的输出投影层将信号令牌表示映射到后续多模态解码器可以理解的表示中</strong>。从技术上讲，他们采用了不同模态代的现成潜在条件<strong>扩散模型</strong>，即用于图像合成的稳定扩散（SD）、用于视频合成的Zeroscope和用于音频合成的AudioLDM。</p><p>目前，大多数可供实际使用的多模态LLM都是片面的，这意味着它们可以理解混合数据格式，但只能生成文本响应。</p><p><strong>GPT-4V &amp; GPT-4o</strong> (OpenAI):</p><p><strong>Gemini</strong> (Google):</p><p><strong>Claude</strong> (Anthropic):</p><p>想要构建完全开源的解决方案或担心数据隐私或延迟并更喜欢在内部本地托管所有内容，可以借助目前最流行的开放和开源多式联运模型构建：</p><p>LLaVA-NeXT: An open-source multimodal model with capabilities to work with text, images and also video, which an improvement on top of the popular LLaVa model</p><p>PaliGemma: A vision-language model from Google that integrates both image and text processing, designed for tasks like optical character recognition (OCR), object detection, and visual question answering (VQA). </p><p>Pixtral 12B: An advanced multimodal model from Mistral AI with 12 billion parameters that can process both images and text. Built on Mistral’s Nemo architecture, Pixtral 12B excels in tasks like image captioning and object recognition.</p><p>目前为止最为强大的多模态模型是Open-AI的GPT-4.o</p><h2 id="多模态RAG系统的工作流"><a href="#多模态RAG系统的工作流" class="headerlink" title="多模态RAG系统的工作流"></a>多模态RAG系统的工作流</h2><p>多模态RAG系统的构建流程与本篇博客的构建选择：</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-4-2.webp" alt="workflow"></p><p>不好理解的一部分？</p><p>端到端的工作流：</p><p>如何去理解Summary？为什么图像、表格、和文本中在嵌入的时候都变成了Image Summary、Table Summary、Text Summary？</p><p>Option 1：</p><p>选择1缺陷的原因是多模态嵌入模型通常无法对这些视觉图像中的数字等粒度信息进行编码，并将其压缩为有意义的嵌入。</p><p>Option 2：</p><p>选项 2 受到严重限制，因为我们最终不会在该系统中使用图像，即使它可能包含有价值的信息，并且它不是真正的多模式 RAG 系统。 </p><p>Option 3：</p><p>将继续使用选项 3 作为我们的多模态RAG 系统工作流程。 在此工作流程中，我们将从图像、表格以及可选的文本块中创建摘要，并使用多向量检索器，这可以帮助根据相应的原始图像、表格和文本元素映射和检索原始图像、表格和文本元素。 </p><h2 id="Multi-Vector-Retrieval-Workflow"><a href="#Multi-Vector-Retrieval-Workflow" class="headerlink" title="Multi-Vector Retrieval Workflow"></a>Multi-Vector Retrieval Workflow</h2><p>考虑到我们将在上一节中讨论的工作流程，对于我们的检索工作流程，我们将使用如下图所示的多向量检索器，正如 LangChain 博客中推荐和提到的那样。 多向量检索器的主要目的是充当包装器，并帮助将每个文本块、表格和图像摘要<strong>映射到</strong>实际的文本块、表格和图像元素，然后可以在检索期间获得这些内容。</p><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/unnamed-5-2.webp" alt="多向量检索器"></p><p>首先使用 Unstructed 等文档解析工具分别提取文本、表格和图像元素。 然后，我们将<strong>把每个提取的元素传递到 LLM 中，并生成详细的文本摘要</strong>，如上所示。 接下来，我们将使用任何流行的<strong>嵌入器模型</strong>（例如 OpenAI Embedders）将<strong>摘要及其嵌入存储到向量数据库</strong>中。 我们还将每个摘要的相应原始文档元素（文本、表格、图像）存储在文档存储中，文档存储可以是任何数据库平台，例如 Redis。</p><p>当用户提出问题时，首先，多向量检索器检索相关摘要，这些摘要在语义（嵌入）相似性方面与问题相似，然后使用公共的 doc_ids、原始文本、表格和图像 返回的元素将进一步传递给 RAG 系统的 LLM 作为回答用户问题的上下文。</p><h2 id="多模态RAG的架构细节"><a href="#多模态RAG的架构细节" class="headerlink" title="多模态RAG的架构细节"></a>多模态RAG的架构细节</h2><ol><li>Load all documents and use a document loader like <strong>unstructured.io</strong> to extract text chunks, image, and tables.</li><li>If necessary, convert HTML tables to markdown; they are often very effective with LLMs</li><li>Pass each text chunk, image, and table into a multimodal LLM like GPT-4o and get a detailed summary.</li><li>Store summaries in a vector DB and the raw document pieces in a document DB like Redis</li><li>Connect the two databases with a common document_id using a multi-vector retriever to identify which summary maps to which raw document piece.</li><li>Connect this multi-vector retrieval system with a multimodal LLM like GPT-4o.</li><li>Query the system, and based on similar summaries to the query, get the raw document pieces, including tables and images, as the context.</li><li>Using the above context, generate a response using the multimodal LLM for the question.</li></ol><hr><p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-woXqKviW7JlYQlmaecD0QpvsiyX4PNJoAB6d4wEm_y4Ks6qXuJNTZ61KTOyJ1v8xi1k4CujEisnRNAWM4LIpgQs_HREGihbG3bquz1y-YXsNt84k7Y9jWy_Vip6yLPiIfNmnTAyI86xvEzbYFSpu69g?key=jCBGNvykPV6Ye85DC-Uxrg" alt="架构"></p><p>1.加载所有文档，并使用类似unstructure.io的文档加载器提取文本块、图像和表格。</p><p>2.如有必要，将HTML表格转换为markdown；他们通常对LLM非常有效</p><p>3.将每个文本块、图像和表格传递到GPT-4o等多模式LLM中，并获得详细的摘要。</p><p>4.将摘要存储在向量数据库中，将原始文档片段存储在Redis等文档数据库中</p><p>5.使用多向量检索器使用公共document_id连接两个数据库，以识别哪个摘要映射到哪个原始文档块。</p><p>6.将这个多向量检索系统与GPT-4o等多模态LLM连接起来。</p><p>7.查询系统，并根据与查询类似的摘要，获取原始文档片段，包括表格和图像，作为上下文。</p><p>8.使用上述上下文，使用多模态LLM生成问题的答案。</p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>人工智能与深度学习</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我们是怎么学习的？机器是如何学习我们的学习的？</title>
    <link href="/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/"/>
    <url>/2024/10/18/%E6%83%B3%E6%95%B4%E6%98%8E%E7%99%BD%E6%88%91%E4%BB%AC%E6%98%AF%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E6%88%91%E4%BB%AC%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>写代码和写情书是一样的，得多练 ——杨力祥</p>          </div><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>杨力祥老师上节课讲了，之所以自己职业生涯在很早的时候投入到AI领域，包括复现Alpho Go；在弄清楚机器是如何学习的同时，也想知道人自身是怎么学习的？</p><p><strong>关联、体系</strong>是学习的关键，不要被一些看似“懂得多”、“行家”某些人忽悠。学东西，任何一门东西，都要有体系的去学、有结构的去学。不要想玩沙子一样，看似懂得很多、知道的很多，但是只要一追问、深究，或者问问A和那个B之间有啥联系，他便说不上来了。</p><p>力祥哲学(把程序写成东西—面向对象思想的精髓)同样适合用于学习人工神经网络的体系架构与技术细节。</p><p>按着<strong>把程序写成东西</strong>的思路，思考如何学习CNN、RNN、GAN呢？——还是去找“像什么”？每一层的具体细节可能有所差别，但是层与层之间的结构可以抽取共性。于是可以<strong>抽象出Layer类</strong>，基于Layer类去做派生，实现不同层本身之间的差异。</p><p><img src="/img/fig/history-dl.jpg" alt="深度学习发展史"></p><p>图片引自$^{[2]}$</p><h2 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h2><p>杨老师上课一直在无形地传授给我们如何学习？如何思考？</p><p>但是自己一直想不明白，于是昨天晚上下课，去找杨老师交流了一下他的学习方法。（2024.11.04）</p><p>杨老师和我说，他小时候正赶上“文革”时期，和我爷爷奶奶姥姥姥爷差不多，小时候没有书可看，所以很多时候就靠想，想不出来也要想。</p><p>按着把程序写成东西的思想，切入到一个新领域，要从这个领域内最基础的例子出发，也就是说，往这件事的根上倒。杨老师带团队做操作系统，一开始就是找的Linux内核v0.01版本，Linux内核最古老最原始的一版本；和女儿学人工智能的时候，咨询了业内的大佬，也是从书写数字识别开始入手的。问题导向，而且是可以揭示这个领域内最本质的问题出发开始往后面派生。</p><p>最后，杨老师还问了我一个问题：<strong>人为什么会问问题？</strong>老师让我把这个问题整明白，这个问题思考清楚了，怎么学习也就差不多快弄明白了。而且老师还举了个例子，人类在教黑猩猩说话的过程中，花费大量的时间确实教会了黑猩猩哑语，但是这个过程中黑猩猩始终没有提问题，想想这是为什么？</p><p>想要快速切入进一个领域应该做到：</p><ol><li>想，人家能想出来，你也可以；只是你的大脑从来就没有经过“想”的训练过程</li><li>案例导向，往根上倒，找最基本最经典的案例做切入点</li><li>能发现并且会提出切入领域内的问题，会提问</li></ol><h2 id="认知-人与机器"><a href="#认知-人与机器" class="headerlink" title="认知-人与机器"></a>认知-人与机器</h2><p>认知科学、认知心理学</p><p>杨老师说，咱们的祖先从猿猴进化为人类，其中学会的一个最重要的技能就是学会了<strong>分类</strong>。在课堂上，讲到面向对象的核心思想的时候，杨老师也是按照分类的思想，引申出类和对象的关系。分类本质上就是一颗二叉树&#x2F;多叉树，在实际思考时，是自下而上(树的层次结构)<strong>抽取共性</strong>，抽象就是脱离形式提取共性，但写的时候从上到下派生(树的层次结构)。</p><p><img src="/img/fig/nao.jpg" alt="认识与分类"></p><p>好了，上面介绍了人类认知的起源过程。OK，那么机器实现分类靠的是什么？机器如果学会了分类是不是就可以慢慢地开始学习人类的所有的思维模式呢？</p><p>初学机器学习，我们就学了回归和分类。回归可以用于预测<em>多少</em>的问题，但是分类问题不是问你“多少”，而是问你“哪一个”。</p><p>回归方法中最基础的方法就是线性回归，分类问题中最基础的方法就是softmax。</p><p>通常，机器学习实践者用<em>分类</em>这个词来描述两个有微妙差别的问题： </p><ol><li>我们只对样本的“硬性”类别感兴趣，即属于哪个类别； </li><li>我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。$^{[1]}$</li></ol><div class="note note-success">            <p>能搞明白<strong>怎么去分类</strong>，就可以慢慢地学着和人类一样去学习了。</p>          </div><h3 id="Sigmod"><a href="#Sigmod" class="headerlink" title="Sigmod"></a>Sigmod</h3><p>Sigmoid 函数常用于<code>二分类问题</code>的激活函数。</p><p>函数的定义域为所有实数，通常输出范围为 <strong>0 到 1</strong>，但某些变体（如<a href="https://en.wikipedia.org/wiki/Hyperbolic_functions">双曲正切</a>）的输出值介于 -1 和 1 之间。</p><p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p><p><img src="/img/fig/sigmod.png"></p><h3 id="Softmax-归一化指数函数"><a href="#Softmax-归一化指数函数" class="headerlink" title="Softmax-归一化指数函数"></a>Softmax-归一化指数函数</h3><p>本质上是一个<code>多分类</code>激活函数<br>$$<br>\text{softmax}(z_i) &#x3D; \frac{e^{z_i}}{\sum_{j&#x3D;1}^{n} e^{z_j}}<br>$$</p><p><img src="/img/fig/softmax.png"></p><p> softmax 函数通常用于<strong>分类任务</strong>的<code>神经网络模型</code>的<strong>最后一层</strong>，它通过取每个输出的指数并通过除以所有指数的总和来标准化这些值，<code>将原始输出分数（也称为 logits）转换为概率</code>。此过程可确保输出值在 (0,1) 范围内且总和为 1，使其可解释为概率。</p><p>神经网络的输出格式通常为one-hot格式，one-hot是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。（神经网络的输出格式）</p><p>Softmax的作用是可以将输入转换为概率分布，确保所有输出的总和为1</p><h3 id="Softmax函数和Sigmoid函数有何区别？"><a href="#Softmax函数和Sigmoid函数有何区别？" class="headerlink" title="Softmax函数和Sigmoid函数有何区别？"></a>Softmax函数和Sigmoid函数有何区别？</h3><p>单看函数图像，貌似Softmax函数和Sigmoid函数没有任何区别呀！</p><p>Softmax函数适用于<strong>多分类</strong>问题，将多个类别的概率进行归一化；而Sigmoid函数主要应用于<strong>二分类</strong>问题，将实数映射到0到1之间的概率。</p><h4 id="二分类情况下"><a href="#二分类情况下" class="headerlink" title="二分类情况下"></a>二分类情况下</h4><p>Softmax 和 Sigmoid 实际上是等价的，它们的图像几乎完全相同。</p><h4 id="多分类状况下"><a href="#多分类状况下" class="headerlink" title="多分类状况下"></a>多分类状况下</h4><ul><li>Softmax 考虑所有类别之间的关系，每个类别的概率受到其他类别的影响。</li><li>Sigmoid 则是独立计算每个类别的概率，不保证概率和为 1。</li></ul><h2 id="脑科学"><a href="#脑科学" class="headerlink" title="脑科学"></a>脑科学</h2><p>[引自weibo博主blog：<strong>indigo</strong>]</p><p>推荐最近读过的最棒的一本书《A Brief History of Intelligence》，作者 Max Bennett 不是学者和生物学专家，而是一位创业者和 AI 专家，他联合创办了估值过十亿美金的 AI 公司 Bluecore。 这本“智能简史”我认为可以和 Yuval Harari 的《人类简史》比肩，从地球生物视角纵览了智能的演化，全书建立一个描述生物大脑进化的新框架，作者称其为“五次突破”，这对现在 AI 与具身智能的研究有巨大的参考价值。</p><p><strong>关联</strong>的学习方法很符合大脑结构，<strong>神经元之间的连接</strong>正好反映出了这种关联关系。</p><p>下面这部分，结合大脑的结构，我们从最简单的线性神经网络、感知机开始本次“元学习”之旅～</p><h3 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h3><p>通过softmax回归构建出单层线性神经网络：<br>$$<br>o_1 &#x3D;x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1 \<br>$$</p><p>$$<br>o_2 &#x3D;x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2 \<br>$$</p><p>$$<br>o_3 &#x3D;x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3 \<br>$$</p><p><img src="/img/fig/softnet.jpg" alt="单层线性网络"></p><p>$$<br>\mathbf{O}&#x3D;\mathbf{X}·\mathbf{W}+\mathbf{b} \<br>$$</p><p>$$<br>\hat{\mathbf{Y}}&#x3D;\mathrm{softmax}(\mathbf{O})&#x3D;\frac{\exp(o_j)}{\sum_k\exp(o_k)} \<br>$$</p><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的<strong>仿射</strong>变换决定。 因此，softmax回归是一个<em>线性模型</em>。</p><h3 id="计算图和感知机"><a href="#计算图和感知机" class="headerlink" title="计算图和感知机"></a>计算图和感知机</h3><div class="note note-success">            <p>感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p>          </div><h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p>感知机是神经网络的原型，顾名思义，一种具有感知能力的机器。</p><p>有感知这件事可不一般吧，有感知以后，当接受到外界刺激后，便会对刺激产生反应了。</p><p>老师上课点名，听到名字后会立即喊“到”（如果不翘课的话嘿嘿），触摸到尖锐的物体或高温热水，会下意识的缩手；都是反应和感知。</p><p>在深度学习领域，感知机的本质其实是一种运用<strong>Logistic回归</strong>的二分类模型。它只能处理二分类问题而且必须是线性可分的问题。</p><p>由简单的线性回归网络过渡延伸到多层感知机，本质上就是网络中间的隐藏层的层数增加了。</p><p><img src="/img/fig/duoceng.jpg" alt="多层多分类感知机"> </p><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>Sigmod函数的计算图过程：</p><p><img src="/img/fig/jisuantu.png" alt="计算图流程-邱锡鹏"></p><h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><blockquote><p> 建立一个体系，将知识建立起联系：<strong>softmax</strong>、<strong>sigmod</strong>、<strong>感知机</strong>、<strong>计算图</strong>、<strong>激活函数</strong>、<strong>反向传播</strong></p></blockquote><p>我们是怎么学习的？始终不要忘记这个任务主线。</p><p>那么，我是怎么认识的呢？</p><p>反正我最初是被家大人带着认识东西的。</p><p>就我个人而言，从我出生，就是大人们带着我认识各种东西。一个典型的例子，刚出生没多久，学说话。老妈就对我说：“叫妈妈”、老爸就对我说：“叫爸爸”。从小开始，就学着认识人、认识各种东西。如果将我这个“婴儿”比作一个机器人（有点抽象哈哈），那么，父母带着我识人识物的过程不就是对我的一个训练过程吗？！</p><p>随着我慢慢长大，我在最初的被人带着学（学会了一些东西、认识了一些人和东西）的基础之上，我开始自学主动去探索、认识更多的东西。这期间我也会犯错，但是，每犯错一次，都会加深我的印象，纠正的我之前的想法与认识。</p><p>慢慢地，随着我认识的越多、学到的东西越多、对事物的分类也愈来愈清晰以后，我逐渐会自己学习、自己产出、自己创造生成了。</p><p>别人问我：“1+1&#x3D;？”，我不仅可以准确的告诉她答案，我也会不由自主的<strong>联想</strong>到“1*1”、“1&#x2F;1”、“1-1”。</p><div class="note note-success">            <p>错误的认识或分类，会作为一个刺激信号在大脑皮层中进行一次<strong>反向传播</strong>，调整参数，然后让我们更新认知，纠正错误。</p><p>这是我理解的人的学习方式，抛开基于统计，对于“机器学习的过程”，我猜亦如此</p>          </div><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNet</span><br>&#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-built_in">MyNet</span>();<br>  ~<span class="hljs-built_in">MyNet</span>();<br>  <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">InintInput</span><span class="hljs-params">(<span class="hljs-type">int</span> nodes)</span></span>;<br>  <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">InitOutput</span><span class="hljs-params">(<span class="hljs-type">int</span> nodes)</span></span>;<br>  <span class="hljs-built_in">FunctionChoice</span>(string s);<br>  <span class="hljs-built_in">Back</span>();<br>  <span class="hljs-built_in">Build</span>(vector&lt;<span class="hljs-type">float</span>&gt; x,vector&lt;<span class="hljs-type">float</span>&gt; w,vector&lt;<span class="hljs-type">float</span>&gt; b);<br><span class="hljs-keyword">private</span>:<br>  vector&lt;<span class="hljs-type">float</span>&gt; w;<br>  vector&lt;<span class="hljs-type">float</span>&gt; b;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="剖析神经元结构"><a href="#剖析神经元结构" class="headerlink" title="剖析神经元结构"></a>剖析神经元结构</h3><p><img src="/img/fig/shenjing.jpg" alt="神经元结构"> </p><p>输入：自变量&#x2F;参数 x列向量</p><p>激活函数:在计算网络中， 一个节点的<strong>激活函数</strong>定义了该节点在给定的输入或输入的集合下的输出。$^{[WiKi]}$</p><p>参数：w,b向量</p><p>输出：output列向量</p><h3 id="OO思想看神经网络"><a href="#OO思想看神经网络" class="headerlink" title="OO思想看神经网络"></a>OO思想看神经网络</h3><div class="note note-success">            <p>神经元网络能看成东西吗？用面向对象思想怎么玩？—本质上还是在<strong>搭积木</strong></p><p>高级思考：设计好的网络结构内存开哪？每一层要不要自己开内存？每层开内存也是共性</p>          </div><p>用带参数的构造函数初始化一个小网络，宽度、层数</p><p>1.动手写sin函数-泰勒展开，精度多少看rn项&#x2F;误差项0.000001</p><p>生成式最先淘汰的就是程序员，因为程序员最会训练程序员。</p><p>GitHub是黑窝。</p><p>操作系统和编译器能不能变成面向对象的方法写呢？</p><h3 id="学习的动机-提出好问题"><a href="#学习的动机-提出好问题" class="headerlink" title="学习的动机-提出好问题"></a>学习的动机-提出好问题</h3><div class="note note-success">            <p>怎么才能提出高水平的问题？</p>          </div><h3 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播-Back Propagation"></a>反向传播-Back Propagation</h3><p>反向传播的思想就是网络结构一般以随机参数开始，伴随着不尽如人意的初始效果，通过“学习”（反向传播）赋予网络“穿越”的能力，使其自身不断自我更新，从而达到一个理想的结果。$^{[4]}$</p><p>以实际正确类别与预测类别的损失为基础，以终为始，反向更新权重参数。</p><div class="note note-success">            <p>做题做错了，对答案，将错题的正解记录到错题本上，下次再遇见条件反射出错题本上的正确解题步骤修正自己之前的惯性错误方法。</p>          </div><p><strong>C++复现反向传播函数：</strong></p><h2 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN-卷积神经网络"></a>CNN-卷积神经网络</h2><p>参见文章《经典卷积神经网路的复现》</p><h2 id="RNN-循环神经网络"><a href="#RNN-循环神经网络" class="headerlink" title="RNN-循环神经网络"></a>RNN-循环神经网络</h2><p>参见文章《都手学循环神经网络》</p><h2 id="Attention-is-all-you-Need"><a href="#Attention-is-all-you-Need" class="headerlink" title="Attention is all you Need"></a>Attention is all you Need</h2><p>参见文章《Transformer复现》</p><p>思考：注意力机制为什么没有和神经网络语言模型和概率统计模型没有关联关系？</p><p>注意力机制就是一个加权求和机制，应用非常灵活，应用面非常之广</p><p>参见文章《Transformer复现》</p><p>可以尝试复现Transformer，大作业之一</p><p>出题：填空、简答之类的概念性；设计题（应用）</p><p>文本匹配任务的建模方法：孪生网络和交互聚合</p><p>序列标注任务的建模方法</p><h2 id="参考文献与资料"><a href="#参考文献与资料" class="headerlink" title="参考文献与资料"></a>参考文献与资料</h2><p>[1]动手学深度学习：<a href="https://zh.d2l.ai/">https://zh.d2l.ai/</a></p><p>[2]动手实践机器学习：<a href="https://aibydoing.com/">https://aibydoing.com/</a></p><p>[3]<strong>中国科学院大学.杨力祥老师课程《C++面向对象程序设计》</strong></p><p>[4]耿直哥随笔.<a href="https://gengzhige-essay.readthedocs.io/docs/03%20%E6%84%9F%E7%9F%A5%E6%9C%BA/3-3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3.html">反向传播的深入理解</a></p><p>[5] <a href="https://xpqiu.github.io/">邱锡鹏</a>，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/">https://nndl.github.io/</a>, 2020.</p><div class="note note-danger">            <p>记住特征-学会分类-类别中抽取共性-共性中总结概念-概念指导解决问题-解决问题过程中总结方法-方法加深特征印象</p>          </div>]]></content>
    
    
    <categories>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>创新思维</tag>
      
      <tag>力祥哲学</tag>
      
      <tag>系统论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>恐惧还是贪婪</title>
    <link href="/2024/10/14/%E6%81%90%E6%83%A7%E8%BF%98%E6%98%AF%E8%B4%AA%E5%A9%AA/"/>
    <url>/2024/10/14/%E6%81%90%E6%83%A7%E8%BF%98%E6%98%AF%E8%B4%AA%E5%A9%AA/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>别人恐惧时你要贪婪，别人贪婪时你要恐惧—沃伦·巴菲特</p>          </div><h2 id="对JJ要有信心—做时间的朋友、长期主义"><a href="#对JJ要有信心—做时间的朋友、长期主义" class="headerlink" title="对JJ要有信心—做时间的朋友、长期主义"></a>对JJ要有信心—做时间的朋友、长期主义</h2><p>今年总体是赚钱的，见好就收，还有两个月过年，猥琐发育，坚持住✊！今年的机会也差不多就到这了，没那么多心思去关注了，学习快跟不上了哈哈哈～</p><p>国庆节后，心情就是过山车。。。最重要的原因就是因为<strong>ZB市场</strong>的起起伏伏。</p><p><img src="/img/life/184.jpeg"></p><p>（当时心想，我要是有100个w，一周的操作足以半生衣食无忧，人在市场面前还是好渺小。。。）</p><p>当年明月在《明朝那些事儿》里说的非常好，<strong>我们这个古老的民族，五千多年生生不息，只要自己人不折腾自己人，日子肯定会愈过愈好。</strong></p><p>都说这几年大环境不好、jj不好，但是实话实讲，今年开始，只要动机正有耐心，还是慢慢能赚到钱的。</p><p><img src="/img/life/jj1.jpeg"></p><p>直到上周，才真正体会到了老舅前一阵子的感慨：压力最大的不是下跌你陪了多少进去，而是疯涨的时候你不知道什么时候收手，人性都是贪婪的，而且是常态。</p><p><img src="/img/life/jj2.jpeg"></p><p>我是国庆节前的一个周五下午，看了zzj会议释放的信号，和微博、水木社区的某些评论，决定赌一把，想着投机赚一笔。雷总说的好：“<strong>机会来了，猪都能飞啊</strong>！”。真是第一次感到市场的力量，无形的手，指数级增长的魅力。</p><h2 id="理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是"><a href="#理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是" class="headerlink" title="理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是"></a>理性：多数情况你只是运气好，不要觉着自己有很强能力，也别辜负好运气——实事求是</h2><p>三年多了，感谢国家让我体会到了什么叫<strong>指数级增长</strong>，一颗小草根伴随着时代的飓风呼啸而过。</p><p>斐波那契数列真不是吹的！</p><h3 id="创业板"><a href="#创业板" class="headerlink" title="创业板"></a>创业板</h3><p>涨的最疯狂的板块，说实话，<strong>不懂，赚钱纯靠投机</strong>。所以到年底，<strong>抽出来，别被套进去，见好就收</strong>。后期无论再怎么疯狂，都首先观望，不着急入手，维持在500以下<strong>求稳定长期回报</strong>。</p><p><img src="/img/life/jj-chuangye.jpeg"></p><h3 id="永利债券"><a href="#永利债券" class="headerlink" title="永利债券"></a>永利债券</h3><p>目前为止，持有时间最长的一支基金，也是体会到了时间的朋友。这波高潮没有赶上，在高潮结束后，市场情绪稳定后，我决定加码永利。毕竟债券的风险要低与股票型基金，只有长期持有，才有回报的可能，被总想着挣快钱。</p><p><img src="/img/life/jj-zhaiquan.jpeg"></p><h3 id="芯片半导体"><a href="#芯片半导体" class="headerlink" title="芯片半导体"></a>芯片半导体</h3><p>我个人长期<strong>看好芯片半导体</strong>，毕竟现在自己从事AI相关的行当。但是具体规则目前还没学明白，未来趋势看不清，我想买未来，但是风险难把握。所以，高潮小赚一笔过后，抽离，维持在500以下<strong>求稳定长期回报</strong>。</p><p><img src="/img/life/jj-xinpian.jpeg"></p><p>今天JF军对湾湾进行联合利剑-2024B JS演习，军工大涨，对于这种机会就属于转瞬即逝，想抓住很难。</p><h1 id="日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾"><a href="#日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾" class="headerlink" title="日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾"></a>日益增长的对美好生活的需求，同不充分、不平衡发展之间的矛盾</h1><p>现在愈来愈对这句话感同身受，能总结出这句话来的人真是牛逼！</p><p>从本科开始试水理财，加上疫情两年多消费不大，而且也没谈对象，自己还是在支付宝上攒下了一笔钱。时至今天回头再看，当初的这个攒钱决策时正确的，因为能给今天带来很大的自由空间。那么下一步就是要思考，如何在一个更好的平台上去发现更多的资源，创造更多的财富。注意，这里说的是财富，不是钱。</p><p>ZB市场起起伏伏，恰如生活一样，都是<strong>波动的</strong>，事事如你所愿，很多时候是异想天开。唯一能做的也只是把握好能把握住的：真诚待人、踏实工作、精进技能、爱自己的家人和爱的人、锻炼好身体、保持好心态；能做到上述这些的，已是凤毛麟角。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop集群配置汇总</title>
    <link href="/2024/10/12/Hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B1%87%E6%80%BB/"/>
    <url>/2024/10/12/Hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="汇总目的"><a href="#汇总目的" class="headerlink" title="汇总目的"></a>汇总目的</h2><ol><li>老师给的参考教程看的头大</li><li>畅哥在用Ubuntu，之前自己都很懒用的配置好的Centos7，所以决定学一学Ubuntu系统</li></ol><h2 id="啥是伪分布式？与集群有啥区别？"><a href="#啥是伪分布式？与集群有啥区别？" class="headerlink" title="啥是伪分布式？与集群有啥区别？"></a>啥是伪分布式？与集群有啥区别？</h2><p>伪分布式模式是一种在单节点上模拟多节点运行的方式，‌Hadoop进程以分离的‌Java进程运行，节点既作为‌NameNode也作为DataNode，同时读取‌HDFS中的文件。这种模式虽然不是真正的分布式，但可以在单台机器上模拟多节点环境，方便调试和理解分布式系统的运行机制。</p><p>在分布式部署中，Hadoop集群由多台计算机组成，每台计算机都可以独立运行Hadoop的各个组件，如HDFS（Hadoop分布式文件系统）和MapReduce。数据会被分散存储在不同的节点上，任务也会并行执行在不同的节点上，从而提高了整个系统的性能和可靠性。</p><p>我是这样理解的：严格意义上来讲，一台计算机中开多台虚拟机属于伪分布式操作；但是多虚拟机恰恰模拟出了多计算机的效果，所以叫它集群配置。真正的分布式最好的例子就是阿里云买三台服务器，统一配置，最直观，可惜成本最大。。。。</p><h2 id="操作系统的选择"><a href="#操作系统的选择" class="headerlink" title="操作系统的选择"></a>操作系统的选择</h2><p>Ubuntu18.04&#x2F;22.04</p><p>Centos7</p><h2 id="虚拟机的选择"><a href="#虚拟机的选择" class="headerlink" title="虚拟机的选择"></a>虚拟机的选择</h2><p>Vmare</p><p>Virtual Box</p><p>云服务器</p><p>推荐的组合：Centos7+Vmare、Vmare+Ubuntu、云服务器(单机版，集群费用成本太高)</p><h1 id="以下都是高质量教程-我的除外"><a href="#以下都是高质量教程-我的除外" class="headerlink" title="以下都是高质量教程(我的除外)"></a>以下都是高质量教程(我的除外)</h1><h2 id="基于Ubuntu的集群搭建详细教程"><a href="#基于Ubuntu的集群搭建详细教程" class="headerlink" title="基于Ubuntu的集群搭建详细教程"></a>基于Ubuntu的集群搭建详细教程</h2><h3 id="高质量博客推荐"><a href="#高质量博客推荐" class="headerlink" title="高质量博客推荐"></a>高质量博客推荐</h3><p>厦门大学林子雨老师的大数据博客网站非常不错，里面文章内容也非常有料：</p><p><a href="https://dblab.xmu.edu.cn/blog/category/big-data/">https://dblab.xmu.edu.cn/blog/category/big-data/</a></p><p>老师秉持着开源精神，将大部分资源全部开源了，推荐给每一个我的博客的读者！！！羊毛该薅就得薅</p><h3 id="简略版本："><a href="#简略版本：" class="headerlink" title="简略版本："></a>简略版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/94/">https://dblab.xmu.edu.cn/blog/94/</a></p><h3 id="集群版本："><a href="#集群版本：" class="headerlink" title="集群版本："></a>集群版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/8/">https://dblab.xmu.edu.cn/blog/8/</a></p><h3 id="伪分布式版本："><a href="#伪分布式版本：" class="headerlink" title="伪分布式版本："></a>伪分布式版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/7/">https://dblab.xmu.edu.cn/blog/7/</a></p><h3 id="CSDN-blog教程："><a href="#CSDN-blog教程：" class="headerlink" title="CSDN-blog教程："></a>CSDN-blog教程：</h3><p>金州饿霸: <a href="https://blog.csdn.net/wangjian530/article/details/142781899">https://blog.csdn.net/wangjian530/article/details/142781899</a></p><h2 id="基于Centos7的集群搭建详细教程"><a href="#基于Centos7的集群搭建详细教程" class="headerlink" title="基于Centos7的集群搭建详细教程"></a>基于Centos7的集群搭建详细教程</h2><h3 id="伪分布式版本：-1"><a href="#伪分布式版本：-1" class="headerlink" title="伪分布式版本："></a>伪分布式版本：</h3><p><a href="https://dblab.xmu.edu.cn/blog/190/">https://dblab.xmu.edu.cn/blog/190/</a></p><h3 id="我自己的配置-不详细、慎重参考"><a href="#我自己的配置-不详细、慎重参考" class="headerlink" title="我自己的配置(不详细、慎重参考)"></a>我自己的配置(不详细、慎重参考)</h3><h4 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim core-site.xml<br>&lt;configuration&gt;<br>&lt;!-- 指定HDFS 中NameNode 的地址--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>    &lt;value&gt;hdfs://node1:8020&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- 指定Hadoop 运行时产生文件的存储目录--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>    &lt;value&gt;/export/server/hadoop-3.3.0/data&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- 在web UI访问HDFS使用的用户名 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br>    &lt;value&gt;root&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p>配置hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim hdfs-site.xml<br>&lt;!-- 指定Hadoop 辅助名称节点主机配置--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>    &lt;value&gt;node2:9868&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><h4 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim mapred-site.xml<br>&lt;configuration&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;!-- 指定MR 运行在YARN 上--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>    &lt;value&gt;yarn&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- MR App Master环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- MR MapTask环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!-- MR ReduceTask环境变量 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br>    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt; <br></code></pre></td></tr></table></figure><h4 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim yarn-site.xml<br>&lt;configuration&gt;<br>&lt;!-- Reducer 获取数据的方式--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 指定YARN 的ResourceManager 的地址--&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>    &lt;value&gt;node1&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 服务器请求最小内存 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;<br>    &lt;value&gt;512&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 服务器请求最大内存 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;<br>    &lt;value&gt;2048&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 容器虚拟内存与物理内存之间的比率 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;<br>    &lt;value&gt;4&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h4 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim workers <br>node1<br>node2<br>node3<br></code></pre></td></tr></table></figure><h4 id="配置hadoop-env-sh"><a href="#配置hadoop-env-sh" class="headerlink" title="配置hadoop-env.sh"></a>配置hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim hadoop-env.sh<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br><br>export HDFS_NAMENODE_USER=root<br>export HDFS_DATANODE_USER=root<br>export HDFS_SECONDARYNAMENODE_USER=root<br>export YARN_RESOURCEMANAGER_USER=root<br>export YARN_NODEMANAGER_USER=root<br></code></pre></td></tr></table></figure><h4 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 hadoop]# vim /etc/profile<br>export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin<br></code></pre></td></tr></table></figure><h4 id="配置文件传输到其他节点"><a href="#配置文件传输到其他节点" class="headerlink" title="配置文件传输到其他节点"></a>配置文件传输到其他节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@node1 server]# scp -r /etc/profile node2:$PWD  <br>[root@node1 server]# scp -r /etc/profile node3:$PWD<br>[root@node1 server]# scp /etc/profile node2:/etc <br>[root@node1 server]# scp /etc/profile node3:/etc<br></code></pre></td></tr></table></figure><p>格式化与一键启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">只能格式化一次</span><br>[root@node1 hadoop-3.3.0]# bin/hdfs namenode -format<br><br>[root@node1 sbin]# start-dfs.sh<br>[root@node1 sbin]# stop-dfs.sh<br>[root@node1 sbin]# start-yarn.sh<br>[root@node1 sbin]# stop-yarn.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">一键启动YARN、HDFS</span><br>start-all.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">一键关闭YARN、HDFS</span><br>start-all.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">单独启动或关闭集群</span><br>hdfs -daemon start namenode<br>hdfs -daemon stop namenode<br>hdfs -daemon start datanode<br>hdfs -daemon stop datanode<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年8-9月总结与展望</title>
    <link href="/2024/09/30/8-9%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/"/>
    <url>/2024/09/30/8-9%E6%9C%88%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="论文研读"><a href="#论文研读" class="headerlink" title="论文研读"></a>论文研读</h2><p>这个月只读了一篇关于RAG的综述文章，详细的笔记可以参考RAG for LLM：A survey这篇博客文章。</p><p>通过浏览该领域以及拓宽知识面，又找到了如下几个感兴趣的点：</p><ul><li><p>RAG</p></li><li><p>Text2SQL</p></li><li><p>多模态检索</p></li><li><p>Agent</p></li></ul><p>读文献的能力、论文写作的能力是需要一步步培养、积累、学些的，不能放松对写作能力的培养，<strong>写作和表达</strong>是这一年在雁栖湖要努力下功夫去打磨的本领。</p><h2 id="技术学习"><a href="#技术学习" class="headerlink" title="技术学习"></a>技术学习</h2><p>【重要】小土堆-Pytorch</p><p>【重要】李沐-动手学深度学习</p><p>ziliz-milvus数据库检索</p><p>黑马程序员-Spark、Flink、NoSQL</p><p>个人认为：这半年最主要的任务就是打好机器学习和深度学习的基础，一定要对原理有深刻的理解，对动手实现做到熟练运用。</p><p>不要着急，多下功夫。</p><h2 id="适应节奏"><a href="#适应节奏" class="headerlink" title="适应节奏"></a>适应节奏</h2><p>正式开学的一周，很不适应。时间安排上、上课的节奏上、作息规律上都不是非常适应。好在我认为我的适应能力还是很强的，从第二周开始，慢慢调整过来了，主要是中秋节去了趟雄安，和好朋友们毕业后聚了一聚，很多开学之初困扰的心结慢慢打开了。这个心结有学业方面的，生活方面的，感情方面的。聊了很多，也听他们聊了很多。虽人生南北多歧路，君向潇湘我向秦，但是最终的目标都是一样的。为了未来更好的生活而努力奋斗。</p><p>回来以后我整个人状态好了很多，最明显的变化就是食欲大增，心情好多了，更专注于自己的事情了。不要着急，给自己一个缓冲时间，慢慢发力。</p><p>现在变得越来越像一个夜猫子了，睡的越来越晚，早上也是不能早起了。我觉着要相对调整生物钟，和本科的时候不一样了，不要强迫自己，而必须保证一天最好的状态，效率第一。</p><h2 id="遇见良师"><a href="#遇见良师" class="headerlink" title="遇见良师"></a>遇见良师</h2><p>这个月和杨力祥老师课后长谈了两次，一次是在9.18晚上下课后，一次是在今天9.30号晚上下课后。两次长谈都是我们下课主动去找老师问问题，然后老师根据问题展开的多方面的讨论。</p><p>这是我在果壳遇到的难得的良师益友，他从思想上改变了我对计算机、对人工智能、对如何思考的观点与思维模式。每次和老师聊完，心中彷佛都燃烧了一团火，像雷总说的那样，要做世界上伟大的程序员。</p><p>鉴赏力的培养、创新力的训练、如何想问题、如何做关联、预见力的洞察、跨学科的素养，这些都是大学问，值得深思。<strong>不要盲目努力、勤奋，先想明白了再去做，再去执行</strong>。</p><h2 id="10月文献阅读list"><a href="#10月文献阅读list" class="headerlink" title="10月文献阅读list"></a>10月文献阅读list</h2><p>基于对LLM RAG - A Survey的论文分析研读，总结出以下可以展开的研究点： </p><p>RAG与NLP</p><p>检索</p><h3 id="Multimodel-RAG-Systems"><a href="#Multimodel-RAG-Systems" class="headerlink" title="Multimodel RAG Systems"></a>Multimodel RAG Systems</h3><p>周五(2024.10.25)完成了Multimodel RAG System的汇报，梳理了一下这个领域目前所需的技术栈和任务流程。</p><p>对于prompt compression方向要深入了解，同时，李老师让举一个基本的例子，没有回答上来，说明还是对基本概念的理解有所欠缺。大老师在方向的深度上给了帮助，需要深入推进下去。</p><p>要好好梳理一下LLM的知识框架了，所涉及到的基本知识，脑子里要有一个体系结构。</p><p>按部就班的跟着大老师的节奏走，争取11.22完成既定任务。</p><h3 id="杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha-Go"><a href="#杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha-Go" class="headerlink" title="杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha Go"></a>杨力祥老师推荐：理解生成式AI最好的起源paper-Alpha Go</h3><p>诺奖开奖！</p><p>优质博客：<a href="http://xtf615.com/2018/02/10/AlphaGo/">http://xtf615.com/2018/02/10/AlphaGo/</a></p><p>Mastering the game of Go with deep neural networks and tree search：<a href="https://www.nature.com/articles/nature16961">https://www.nature.com/articles/nature16961</a></p><p><a href="https://arxiv.org/pdf/1706.04052">https://arxiv.org/pdf/1706.04052</a></p><h3 id="李沐的课要坚持刷完，动手学深度学习"><a href="#李沐的课要坚持刷完，动手学深度学习" class="headerlink" title="李沐的课要坚持刷完，动手学深度学习"></a>李沐的课要坚持刷完，动手学深度学习</h3><p>10.14-10.21:完成感知机到kaggle01实训的学习，一定要多动手实践！！</p><p>10.26-10.31:完成LLM基本知识框架的梳理、神经网络结构理解、注意力机制</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>希望未来一个月有所提高的方面：</p><ul><li>文献阅读能力：把我核心能力、代码复现能力、找idea</li><li>深度学习实践：未来一个月多动手学深度学习、多实操、多总结</li><li>科学论文写作与英语：英语听说读写的精进，论文、文章的写作能力怎么样提升，思考</li></ul>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI大模型时代人才需求与技能培养</title>
    <link href="/2024/09/29/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%BA%BA%E6%89%8D%E9%9C%80%E6%B1%82%E4%B8%8E%E6%8A%80%E8%83%BD%E5%9F%B9%E5%85%BB/"/>
    <url>/2024/09/29/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%BA%BA%E6%89%8D%E9%9C%80%E6%B1%82%E4%B8%8E%E6%8A%80%E8%83%BD%E5%9F%B9%E5%85%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="AI大模型需要的三类人才"><a href="#AI大模型需要的三类人才" class="headerlink" title="AI大模型需要的三类人才"></a>AI大模型需要的三类人才</h2><p>核心基础理论与架构、大模型+X、支持大模型的计算机系统</p><h2 id="技能与素质需求"><a href="#技能与素质需求" class="headerlink" title="技能与素质需求"></a>技能与素质需求</h2><h3 id="大模型核心基础理论与架构"><a href="#大模型核心基础理论与架构" class="headerlink" title="大模型核心基础理论与架构"></a>大模型核心基础理论与架构</h3><p>机器学习基础：熟悉机器学习基础算法，线性回归、决策树、随机向量机等，要理解过拟合、正则化、交叉验证等概念</p><p>深度学习理论：神经网络的基本结构和工作原理，熟练使用深度学习框架，如：tensorfllow、pytorch，理解卷积神经网络CNN、循环神经网络RNN、Transformer等结构</p><h3 id="大模型-X"><a href="#大模型-X" class="headerlink" title="大模型+X"></a>大模型+X</h3><p>交叉学科，行业赋能，落地应用。大模型在各行各业的落地使用。</p><h3 id="支持大模型的计算机系统"><a href="#支持大模型的计算机系统" class="headerlink" title="支持大模型的计算机系统"></a>支持大模型的计算机系统</h3><p>不做模型本身，但是也要了解大模型的原理、机器学习与深度学习。更多地侧重对于大模型的训练、部署与使用层面，了解如何评估模型的性能，掌握云计算与大数据技术，能够处理大规模数据集，会使用云服务对打模型进行部署服务与运用。</p><p>引用自：郑纬民院士</p><p>视频源：<a href="https://www.bilibili.com/video/BV1jUtKekEkR/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1jUtKekEkR/?spm_id_from=333.1007.tianma.2-2-5.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>人工智能</tag>
      
      <tag>职业发展</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch and Deep-learning NoteBook</title>
    <link href="/2024/09/28/Pytorch-and-Deep-learning-NoteBook/"/>
    <url>/2024/09/28/Pytorch-and-Deep-learning-NoteBook/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>PyTorch与深度学习笔记，持续更新….</p><p>不要着急，扎扎实实打好基本功！</p>          </div><h2 id="参考学习资料"><a href="#参考学习资料" class="headerlink" title="参考学习资料"></a>参考学习资料</h2><p>小土堆-pytorch学习教程：<a href="https://www.bilibili.com/video/BV1hE411t7RN/?p=10&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1hE411t7RN/?p=10&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p>李沐-动手学深度学习：<a href="https://www.bilibili.com/video/BV1wM4m117mp/?spm_id_from=333.337.search-card.all.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1wM4m117mp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p>李宏毅深度学习教程LeeDL-Tutorial（苹果书）：<a href="https://github.com/datawhalechina/leedl-tutorial?tab=readme-ov-file">https://github.com/datawhalechina/leedl-tutorial?tab=readme-ov-file</a></p><p>张贤同学pytorch学习笔记：<a href="https://pytorch.zhangxiann.com/">https://pytorch.zhangxiann.com/</a></p><p>《pytorch实用教程(第二版)》<a href="https://tingsongyu.github.io/PyTorch-Tutorial-2nd/">https://tingsongyu.github.io/PyTorch-Tutorial-2nd/</a></p><h2 id="conda常用指令"><a href="#conda常用指令" class="headerlink" title="conda常用指令"></a>conda常用指令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建环境</span><br>conda create -n your_env_name python=x.x(x.x为python版本)<br>eg: conda create -n pytorch_tutorial python=3.7<br><span class="hljs-meta prompt_"># </span><span class="language-bash">激活环境</span><br>source avtivate your_env_name<br>eg: source activate pytorch_tutorial<br><span class="hljs-meta prompt_"># </span><span class="language-bash">推出环境</span><br>source deactivate<br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除环境</span><br>conda remove -n your_env_name -all<br>eg: conda remove -n pytorch_tutorial -all<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看已有的虚拟环境</span><br>conda env list / conda info -e<br></code></pre></td></tr></table></figure><h2 id="PyTorch-CookBook"><a href="#PyTorch-CookBook" class="headerlink" title="PyTorch  CookBook"></a>PyTorch  CookBook</h2><h3 id="PyTorch安装"><a href="#PyTorch安装" class="headerlink" title="PyTorch安装"></a>PyTorch安装</h3><p>CPU版本的安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install torch torchvision torchaudio<br></code></pre></td></tr></table></figure><p>GPU版本：</p><p>安装指令可以去官网查看与之适配的CUDA版本，选择合适的操作系统、编程语言与CUDA版本安装即可</p><p><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p><h3 id="PyTorch常用子文件"><a href="#PyTorch常用子文件" class="headerlink" title="PyTorch常用子文件"></a>PyTorch常用子文件</h3><p>执行完pip install torch后，Torch的安装路径为：&#x2F;Users&#x2F;mungeryang&#x2F;anaconda3&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;torch，在程序中import torch其实是从安装路径中进行导入。</p><p>torch中重要的组件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">_pycache: 该文件夹存放python解释器生成的字节码,目的是通过牺牲一定的存储空间来提高加载速度<br>_C: 辅助C语言代码调用的一个模块,PyTorch的底层计算代码采用的是C++语言编写，并封装成库，供pytorch的python语言进行调用<br>include: 存放着上面描述底层计算所需的C++代码<br>lib: lib文件夹下包含大量的.lib .dll文件（分别是静态链接库和动态链接库）,torch文件夹中98%内容都在lib中<br>autograd: pytorch的核心模块与概念，它实现了梯度的自动求导，开发人员只需编写前向传播代码，反向传播部分由autograd自动实现<br>nn: 使用频率最高的模块，搭建网络的网络层就在nn.modules里边<br>onnx: pytorch模型转换到onnx模型表示的核心模块<br>optim: 优化使用的方法函数，都在optim文件夹中<br>utils: utils是各种软件工程中常见的文件夹，包含了各类常用工具，其中比较关键的是data文件夹，tensorboard文件夹<br>datasets: 常用的数据集写的数据读取函数，例如常见的cifar, coco, mnist,svhn,voc都是有对应的函数支持<br>torchvision:<br>models: 它里边存放了经典的、可复现的、有训练权重参数可下载的深度学习模型<br>ops: 视觉任务特殊的功能函数<br>transforms: transforms是pytorch自带的图像预处理、增强、转换工具<br></code></pre></td></tr></table></figure><p><em>菜鸟使用pytorch实现图像分类任务时可能考虑到的任务点</em>：</p><p>图像数据是哪用一行代码读取进来的？</p><p>transforms.Compose是如何工作对图像数据进行转换的？</p><p>ToTensor又有哪些操作？</p><p>如何编写Dataset？</p><p>DataLoader有什么功能？如何使用？有什么需要注意的？</p><p>模型如何按自己的数据流程搭建？</p><p>nn有哪些网络层可以调用？</p><p>损失函数有哪些？</p><p>优化器是如何更新model参数的？</p><p>学习率调整有哪些方法？如何设置它们的参数？</p><p>model.train()与model.eval()作用是什么？</p><p>optimizer.zero_grad()是做什么？为什么要梯度清零？</p><p>scheduler.step() 作用是什么？应该放在哪个for循环里？</p><h2 id="核心板块-Tensor"><a href="#核心板块-Tensor" class="headerlink" title="核心板块 - Tensor"></a>核心板块 - Tensor</h2><p>Pytorch中核心的数据结构就是张量——Tensor</p><p>张量表示的是一个多维数组，它是标量、向量、矩阵的拓展。标量(Scaler)是零维张量，向量(Vector)是一维张量，矩阵(Matrix)是二维张量，一个RGB数组是三维张量。</p><p>torch.Tensor的八大属性：</p><ol><li>data: 多维数组</li><li>dtype: 多维数组的数据类型</li><li>shape: 多维数组的形状</li><li>device: tensor所在设备，cpu或GPU</li><li>grad,  grad_fn, is_leaf, requires_grad: 梯度计算中需要的</li></ol><p>Pytorch训练的五大要素：数据、模型、损失函数、优化器、迭代训练</p><p>多维数组是主要的数据处理形式</p><h2 id="数据模块-DataSet、DataLoader、transforms"><a href="#数据模块-DataSet、DataLoader、transforms" class="headerlink" title="数据模块 - DataSet、DataLoader、transforms"></a>数据模块 - DataSet、DataLoader、transforms</h2><p>DataSet是一个抽象基类，提供给用户自定义数据读取方式，核心在于<strong>getitem</strong>方法中对数据的处理。</p><p>DataLoader是pytorch数据加载核心，包括了大乱数据、采样机制、多进程数据加载、组装Batch。</p><h3 id="DataSet类代码实战"><a href="#DataSet类代码实战" class="headerlink" title="DataSet类代码实战"></a>DataSet类代码实战</h3><p>虽然DataLoader是pytorch核心数据模块，但是改动最多与原数据最为接近的是DataSet。</p><p>DataSet类中核心功能：</p><ol><li><strong>getitem</strong>：实现读取一个样本的功能。通常是传入索引，然后实现从磁盘中读取数据，并行处理数据，返回样本数据。getitem返回的数据会在dataloader中组装成一个batch。</li><li><strong>len</strong>：返回数据集大小</li></ol><p>Dataset负责与磁盘打交道，将磁盘上的数据读取并预处理好，提供给DataLoader，而DataLoader只需要关心如何组装成批数据，以及如何采样。</p><h3 id="DataLoader类代码实战"><a href="#DataLoader类代码实战" class="headerlink" title="DataLoader类代码实战"></a>DataLoader类代码实战</h3><h2 id="模型板块"><a href="#模型板块" class="headerlink" title="模型板块"></a>模型板块</h2><p>nn.Module</p><h2 id="创建简单的神经网络"><a href="#创建简单的神经网络" class="headerlink" title="创建简单的神经网络"></a>创建简单的神经网络</h2><p>使用Pytorch创建一个神经网络意味着创建一个新的类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicNet</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(BasicNet,<span class="hljs-variable language_">self</span>).__init__()<br>    <br>    <span class="hljs-comment">#初始化所有的权重和偏差</span><br>    <span class="hljs-variable language_">self</span>.w00 = nn.Parameter(torch.tensor(<span class="hljs-number">1.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b00 = nn.Parameter(torch.tensor(-<span class="hljs-number">0.85</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w01 = nn.Parameter(torch.tensor(-<span class="hljs-number">40.8</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.w10 = nn.Parameter(torch.tensor(<span class="hljs-number">12.6</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b10 = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w11 = nn.Parameter(torch.tensor(<span class="hljs-number">2.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.final_bias = nn.Parameter(torch.tensor(-<span class="hljs-number">16.0</span>), requires_grad = <span class="hljs-literal">False</span>)<br>    <br>  <span class="hljs-comment"># 前向传播</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>    <br>    input_to_top_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w00 + <span class="hljs-variable language_">self</span>.b00<br>    top_relu_output = F.relu(input_to_top_relu)<br>    scaled_top_relu_output = top_relu_output * <span class="hljs-variable language_">self</span>.w01<br>    <br>    input_to_buttom_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w10 + <span class="hljs-variable language_">self</span>.b10<br>    buttom_relu_output = F.relu(input_to_buttom_relu)<br>    scaled_buttom_relu_output = buttom_relu_output * <span class="hljs-variable language_">self</span>.w11<br>    <br>    input_to_final_relu = scaled_top_relu_output + scaled_buttom_relu_output + <span class="hljs-variable language_">self</span>.final_bias<br>    output = F.relu(input_to_final_relu)<br>    <br>    <span class="hljs-keyword">return</span> output<br>  <br>input_doses = torch.linspace(start = <span class="hljs-number">0</span>,end = <span class="hljs-number">1</span>,step = <span class="hljs-number">11</span>)<br>model = BasicNet()<br>output = model(input_doses)<br></code></pre></td></tr></table></figure><p>从创建网络到训练的完整流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicNet</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(BasicNet,<span class="hljs-variable language_">self</span>).__init__()<br>    <br>    <span class="hljs-comment">#初始化所有的权重和偏差</span><br>    <span class="hljs-variable language_">self</span>.w00 = nn.Parameter(torch.tensor(<span class="hljs-number">1.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b00 = nn.Parameter(torch.tensor(-<span class="hljs-number">0.85</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w01 = nn.Parameter(torch.tensor(-<span class="hljs-number">40.8</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.w10 = nn.Parameter(torch.tensor(<span class="hljs-number">12.6</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.b10 = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.w11 = nn.Parameter(torch.tensor(<span class="hljs-number">2.7</span>), requires_gard = <span class="hljs-literal">False</span>)<br>    <br>    <span class="hljs-variable language_">self</span>.final_bias = nn.Parameter(torch.tensor(<span class="hljs-number">0.0</span>), requires_grad = <span class="hljs-literal">True</span>)<br>    <br>  <span class="hljs-comment"># 前向传播</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>    <br>    input_to_top_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w00 + <span class="hljs-variable language_">self</span>.b00<br>    top_relu_output = F.relu(input_to_top_relu)<br>    scaled_top_relu_output = top_relu_output * <span class="hljs-variable language_">self</span>.w01<br>    <br>    input_to_buttom_relu = <span class="hljs-built_in">input</span> * <span class="hljs-variable language_">self</span>.w10 + <span class="hljs-variable language_">self</span>.b10<br>    buttom_relu_output = F.relu(input_to_buttom_relu)<br>    scaled_buttom_relu_output = buttom_relu_output * <span class="hljs-variable language_">self</span>.w11<br>    <br>    input_to_final_relu = scaled_top_relu_output + scaled_buttom_relu_output + <span class="hljs-variable language_">self</span>.final_bias<br>    output = F.relu(input_to_final_relu)<br>    <br>    <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = SGD(model.parameters(),lr = <span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>  <br>  total_loss = <span class="hljs-number">0.0</span><br>  <br>  <span class="hljs-keyword">for</span> iteration <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(inputs)):<br>    input_i = inputs[iteration]<br>    label_i = labels[iteration]<br>    output_i = model(input_i)<br>    <br>    loss = (output_i - label_i) ** <span class="hljs-number">2</span><br>    <br>    loss.backward()<br>    total_loss += <span class="hljs-built_in">float</span>(loss)<br>    <br><span class="hljs-keyword">if</span>(total_loss &lt; <span class="hljs-number">0.0001</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Num steps:&quot;</span> + <span class="hljs-built_in">str</span>(epoch))<br>  <span class="hljs-keyword">break</span><br>  optimizer.step()<br>  optimizer.zero_grad()<br>  <br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step: &quot;</span> + <span class="hljs-built_in">str</span>(epoch) + <span class="hljs-string">&quot;Final bias: &quot;</span> + <span class="hljs-built_in">str</span>(model.final_bias) + <span class="hljs-string">&quot;\n&quot;</span>)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能与深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李沐交大讲座-大语言模型的实践经验和未来预测</title>
    <link href="/2024/09/23/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/"/>
    <url>/2024/09/23/%E6%9D%8E%E6%B2%90%E8%AE%B2%E5%BA%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-trends-Personal-Career-Choices"><a href="#LLM-trends-Personal-Career-Choices" class="headerlink" title="LLM trends &amp;&amp; Personal Career Choices"></a>LLM trends &amp;&amp; Personal Career Choices</h1><h1 id="——大语言模型的实践经验和未来预测"><a href="#——大语言模型的实践经验和未来预测" class="headerlink" title="——大语言模型的实践经验和未来预测"></a>——大语言模型的实践经验和未来预测</h1><h2 id="主讲人"><a href="#主讲人" class="headerlink" title="主讲人"></a>主讲人</h2><p>李沐：上海交大2011届计算机科学与工程系本硕系友。他曾担任亚马逊资深首席科学家，任加州大学伯克利分校和斯坦福大学的访问助理教授，是前Marianas Labs联合创始人。他的研究关注分布式系统和机器学习算法。发表了50余篇人工智能顶级会议论文，在CMU读博期间更是两年内发表了理论计算机领域的FOCS、神经网络领域的NIPS、数据挖掘领域的KDD和操作系统领域的OSDI等不同领域的顶级国际会议一作论文。他是深度学习框架Apache MXNet的创始人之一，合著了开源深度学习教材《动手学深度学习》。他目前是BosonAI的联合创始人。</p><p>时间地点：2024.08.25-上海交通大学计算机科学与技术系</p><h2 id="大语言模型与“炼丹”"><a href="#大语言模型与“炼丹”" class="headerlink" title="大语言模型与“炼丹”"></a>大语言模型与“炼丹”</h2><p>语言模型的三大核心组成：<strong>算力、算法、数据</strong></p><p>语言模型也好，整个机器学习模型也好，本质上就是把数据通过算力和算法压进中间那个模型里面，使得模型有一定的能力，在面对一个新的数据时，它能够在原数据里面找到相似的东西，然后做一定的修改，输出你要的东西。</p><p>沐神用“炼丹”的比喻将现在的语言模型就是一个炼丹的过程：数据是原材料，算力是设备(比如炼丹炉)，算法就是“丹方”。数据这个原材料不是很好搞，是个体力活；算力也很重要，要想火大一点，设备就要先进一点，设备越好练出来的东西质量也会更好。算法这个丹方，每年都在进步，就算有人指导你做，实际操作时候也会变得不一样。“像是徒手发火箭，发射前要动手调一调，没调好就会炸掉”</p><p>数据以参数的形式存储在LLM中，需要使用prompt工程进行数据的提取与使用。(LLM时代，大数据存储是以<strong>参数</strong>的形式进行的)</p><h2 id="硬件的发展规律，算力方面-LLM不是特别有性价比的东西"><a href="#硬件的发展规律，算力方面-LLM不是特别有性价比的东西" class="headerlink" title="硬件的发展规律，算力方面-LLM不是特别有性价比的东西"></a>硬件的发展规律，算力方面-LLM不是特别有性价比的东西</h2><h3 id="带宽"><a href="#带宽" class="headerlink" title="带宽"></a>带宽</h3><p>带宽是让芯片靠的更近一些</p><p>讲带宽是因为LLM训练要分布式进行，现在的带宽是一根光纤承载 400Gigabits，下一代就是 double，变成 800Gigabits。</p><p>数据中心采用水冷当时对机架降温可以提高信息传输效率，提升训练速度。</p><p>“英伟达的 GB200 这个卡就可以把 GPU 都放在一起，那么它们之间的通讯会变得更好一些。你可以理解成：之前我们做多核，把单核封装到一个芯片里面，现在是说多核不够，我要做多卡，多卡以前是分布在一个房间里面，现在是多卡也要尽量放在一起，这是一个趋势。就是一块芯片那么大，早就做不上去了，这是台积电等面临的工艺难题，现在是尽量把这些东西弄得近一些。”</p><h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><p>内存是制约模型的一大瓶颈</p><p><strong>现在的语言模型，核心是把整个世界的数据压进模型里面</strong>，但是这么做模型就会被搞的非常大，几百个G的样子。跑起来以后，参数和中间变量会更大，因此内存就需要很大。</p><p>但是内存大小这个东西快要遇到瓶颈了，因为它内存占面积，一块芯片就那么大，划一个区域给算力，划一块给内存，基本上就剩不下什么东西了。</p><p>沐神预测，一块芯片200G内存，基本上就要从工艺上就到瓶颈了。当然我不是搞硬件的，对于这些知识就是认真学习，在实践中去感受。</p><p>记得王坚院士去年在云栖大会上的演讲给出的观点是，目前的模型和算法并没有匹配上当前的算力资源。</p><h3 id="算力"><a href="#算力" class="headerlink" title="算力"></a>算力</h3><p>算力从长期来看会越来越便宜</p><p>模型做的越大，就会发现资源其实是问题。数据中心需要大量的供电；短期来看，算力翻倍，价格可能会有 1.4 倍的提升。但是长期来看，当竞争变得越来越激烈，摩尔定律会发挥作用，就是说算力翻倍，价格不一定变。所以<strong>长期来看算力会变得越来越便宜。</strong></p><p>关于算力的观点：你可以认为摩尔定律还是会发挥作用，就是训练会两倍两倍地变便宜。所以你今天训练一个模型，一年之后它的价值会减半。很多时候，大家不要去想我现在能搞多大的模型，一年之后，这个模型会贬值。我想说，大模型不是特别有性价比的东西。你要想清楚，<strong>从长期来看，你的模型能带来什么价值，让你能够保值</strong>。</p><h2 id="趋势：从语言到多模态"><a href="#趋势：从语言到多模态" class="headerlink" title="趋势：从语言到多模态"></a>趋势：从语言到多模态</h2><h3 id="语言模型：100B-500B参数会是主流"><a href="#语言模型：100B-500B参数会是主流" class="headerlink" title="语言模型：100B-500B参数会是主流"></a>语言模型：100B-500B参数会是主流</h3><p>每次预训练，无论是 OpenAI 还是别的模型，基本都是用 10T 到 50T token 做预训练。开源的话基本也在 10T token 以上。这个数据量我觉得差不多了，不会再往一个更大的尺寸去发展。原因是，人类历史上的数据比这个多是多，但是看多样性、质量的话，我觉得 10T 到 50T 这个规模就差不多了。</p><p>比较好的一线的模型就是 500B，超过 500B 不是训练不动，而是做 serving 很难。</p><h3 id="语音模型：延迟更低、信息更丰富"><a href="#语音模型：延迟更低、信息更丰富" class="headerlink" title="语音模型：延迟更低、信息更丰富"></a>语音模型：延迟更低、信息更丰富</h3><p>新的语音技术可以让语音直接进去，然后利用文本语言模型强大的能力去发掘里面的信息。在做输出的时候也是一样的，我的输出可以根据你的输入的个性化场景来变换语调、情绪。这是一点。</p><p>另一点是延迟更短。之前我要先输出一句话，再进到一个模型去把语音输出来，这个延迟可能是 1 秒。现在我们大概可以做到 300 毫秒。</p><h3 id="音乐模型：不是技术问题，而是商业问题"><a href="#音乐模型：不是技术问题，而是商业问题" class="headerlink" title="音乐模型：不是技术问题，而是商业问题"></a>音乐模型：不是技术问题，而是商业问题</h3><p>我觉得这一块的进展从来不是一个技术问题。它的技术其实比语音麻烦一点，因为音乐比人说话更复杂一点。但是实际上它还是一个版权的问题。</p><h3 id="图像模型：图像越来越有神韵"><a href="#图像模型：图像越来越有神韵" class="headerlink" title="图像模型：图像越来越有神韵"></a>图像模型：图像越来越有神韵</h3><p>图片应该是整个 AIGC 领域做得最早的，也是效果最好的。现在大家可以做到 100 万以上像素的图片的生成。大家说得最多的是图片要有灵魂。之前你去看那些文生图的工具，它的风格还是很假，但现在你会看到跟真的很接近，当然它还缺那么一点点灵魂，不过这一块说不定很快就有了。</p><h3 id="视频模型：尚属早期"><a href="#视频模型：尚属早期" class="headerlink" title="视频模型：尚属早期"></a>视频模型：尚属早期</h3><p>Sora 出来之后，大家非常关注视频模型。这个实际上还算比较早期，通用的 video 生成还是非常贵，因为 video 数据特别难弄。视频模型的训练成本很有可能低于数据处理的成本，所以你没有看到市面上有特别好的开源模型出来。问题在于生成一张图片容易，但生成一连串连贯的图片，并保持一致性是很难的。</p><h3 id="多模态模型：整合所有信息"><a href="#多模态模型：整合所有信息" class="headerlink" title="多模态模型：整合所有信息"></a>多模态模型：整合所有信息</h3><p><strong>目前存在一种趋势，即多模态</strong>。现如今，多模态技术的发展趋势在于整合不同类型的模态信息，尤其是文本信息，因为文本含有丰富的信息并且易于获取。通过利用在文本上学到的技能，可以将这些能力泛化到其他模态，如图片、视频和声音。</p><h3 id="交互方式的一点改变"><a href="#交互方式的一点改变" class="headerlink" title="交互方式的一点改变"></a>交互方式的一点改变</h3><p>比如在点菜时，在 ChatGPT 出来之前我们与手机的交互方式是刷刷刷和点点点，这是最简单的方式，对人类来说也不耗费精力，能不说就不说。但在 ChatGPT 出来之后，大家打破了这种观念，他们愿意去输入一段很长的文字去做事情，这是因为设计好的东西不一定满足我们的所有需求，可能满足了 80%，但没有满足对细节的需求，这时可以通过长文本，即输入很长的文字来解决。但输入长文字还是不如说话方便，所以在微信上很多人会说我语音留言会方便点。</p><p>未来大家可能会越来越能接受对方用一个很长的语音跟你描述一些事情，让你去完成。随着技术的发展，未来的语音控制系统将能够处理更加复杂和具体的任务，这种技术的自然和便捷性将显著提高。</p><h2 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h2><p>预训练时工程问题，后训练才是技术问题</p><p>垂直模型也需要通用知识</p><p>评估很难但是很重要</p><p>数据决定模型的上限</p><h2 id="打卡式人生"><a href="#打卡式人生" class="headerlink" title="打卡式人生"></a>打卡式人生</h2><p>从最基本的目标来说，去大公司，是为了升职加薪；读 PhD ，你要保证自己能毕业；而创业的目标是要能推出产品，要么上市，要么卖掉，这是每天都需要思考的。</p><h3 id="打工人"><a href="#打工人" class="headerlink" title="打工人"></a>打工人</h3><p>打工人的好处是，可以在一个相对简单的环境里学习各种从业知识，比如一个技术如何落地、产品怎么做出来、怎么设计、怎么运营、怎么管理。</p><p>那么打工人的坏处是什么？坏处就是停留在打工人或者职业经理人的思维。</p><h3 id="读PHD：真心热爱研究，不然难以坚持"><a href="#读PHD：真心热爱研究，不然难以坚持" class="headerlink" title="读PHD：真心热爱研究，不然难以坚持"></a>读PHD：真心热爱研究，不然难以坚持</h3><h4 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h4><p>读 PhD 的好处是，在几年的时间里可以专心探索某一个领域，反正也没钱赚，也没有升职加薪的机会。PHD期间要有大部分时间都要花在写作和演讲上，努力提高自己的表达与写作的能力。</p><p>还有一个好处，很多公司的研发职位要求就是 PhD。</p><h4 id="坏处"><a href="#坏处" class="headerlink" title="坏处"></a>坏处</h4><p>读 PhD 的坏处是什么？</p><p>首先，很少有实验室能参与大项目的研发。</p><p>其次是研究课题和导师风格都很挑人，需要你去适应，这个适应过程，要么看你的适应能力有多好，要么看你导师的适应能力有多好。你在公司里面还能够部门之间跳一跳，但读 PhD 就更难一些。</p><p>最后，<strong>要真的热爱研究，不然坚持不下去</strong>，你会觉得研究这个东西到底有什么意义，写这篇论文要干嘛。其实，你可以这样想：我写这篇文章就是为了练习写作，等到更厉害、更大的成果做出来后，写作不能给我拉后腿。你要有一个更远大的目标，是真的热爱它。</p><h3 id="动机的来源"><a href="#动机的来源" class="headerlink" title="动机的来源"></a>动机的来源</h3><p>强烈的创业动机要么来自很深沉、很底层的欲望，要么来自很深的恐惧。</p><p>欲望与恐惧是动机的主要来源。</p><p>欲望是越底层越好，名、利、权，都是底层的欲望，要直面自己的欲望，也要直面自己的恐惧，这种恐惧是可以让你抑郁的恐惧，也是让你感受到生死的恐惧。</p><p>你需要把欲望和恐惧转变成积极向上的动机，这一点很重要，你的动机一定是正确的，符合价值观的，因为逃避、放纵满足不了欲望，也缓解不了恐惧，唯一克服它的办法是，把它变成一个积极向上、符合社会价值的一个动机。</p><h2 id="持续进步的妙招"><a href="#持续进步的妙招" class="headerlink" title="持续进步的妙招"></a>持续进步的妙招</h2><p>反思性写作与阶段式总结可以保持持续进步。</p><p>你要形成一个习惯，定个闹钟，每周一晚上花 30 分钟对自己进行总结，每个季度要<strong>总结</strong>，翻看之前你的写的周记，看看这个季度的目标是否完成，下个季度要做什么。</p><p><strong>选择比努力更重要，但选择的前提是搞清楚你的目标是什么。</strong></p><div class="note note-success">            <p>最好的时代：新的技术带来了很多新的机会</p><p>最坏的时代：需要付出比上一代人更多</p>          </div><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>讲座视频：</p><p><a href="https://www.bilibili.com/video/BV1dHWkewEWz/?spm_id_from=333.337.search-card.all.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0">https://www.bilibili.com/video/BV1dHWkewEWz/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f093c3d64ba399e149cbffa6cd31a7b0</a></p><p><a href="https://www.youtube.com/watch?v=ziHUcDh0DwM">https://www.youtube.com/watch?v=ziHUcDh0DwM</a></p><p>阅读文章：</p><p><a href="https://mp.weixin.qq.com/s/FctMj2SXotdn8MlL9oSfAg">机器之心推文</a></p><p><a href="https://zhuanlan.zhihu.com/p/714533901">创业一年，人间三年</a></p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
      <tag>职业发展</tag>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
      <tag>前沿讲座</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Google File System</title>
    <link href="/2024/09/20/Google-File-System/"/>
    <url>/2024/09/20/Google-File-System/</url>
    
    <content type="html"><![CDATA[<h1 id="Google-File-System"><a href="#Google-File-System" class="headerlink" title="Google File System"></a>Google File System</h1><p>原文连接：</p><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf</a></p><p>论文翻译：</p><p><a href="https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/">https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/</a></p>]]></content>
    
    
    <categories>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG for LLM - A survey</title>
    <link href="/2024/09/19/RAG-for-LLM-A-survey/"/>
    <url>/2024/09/19/RAG-for-LLM-A-survey/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG-for-LLM-A-survey"><a href="#RAG-for-LLM-A-survey" class="headerlink" title="RAG for  LLM - A survey"></a>RAG for  LLM - A survey</h1><p>论文题目：</p><p>Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>论文链接：</p><p><a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></p><p>翻译：</p><p><a href="https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey">https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在LLM得到广泛应用的同时，对于LLM 幻觉、过时知识、不透明&#x2F;不可追踪的推理过程仍然面临巨大挑战。</p><p>可行的解决方式：RAG-检索生成增强</p><p>方式：连接外部数据库- RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases.</p><p>RAG paradigms（RAG三大范式）：</p><ul><li>Naive RAG</li><li>Advanced RAG</li><li>Modular RAG</li></ul><p>文章重点强调了嵌入在RAG系统中的关键组件，提高了对RAG系统的理解。此外，文章也给出了对于RAG系统的最新评估标准和框架。</p><h2 id="Introduction-RAG在大模型阶段的发展轨迹"><a href="#Introduction-RAG在大模型阶段的发展轨迹" class="headerlink" title="Introduction-RAG在大模型阶段的发展轨迹"></a>Introduction-RAG在大模型阶段的发展轨迹</h2><p>RAG技术最初与Transformer架构相吻合，通过增加外部知识库，在早期阶段用于细化预训练技术（pre-training）。</p><p>随着ChatGPT的兴起，大语言模型在长上下文的理解中展现出了强大能力。</p><p>RAG研究转向在推理阶段为LLM提供更好的信息，以回答更复杂和知识密集型的任务，导致RAG研究的快速发展。</p><p>后续随着研究进一步深入，RAG技术不再局限于推理阶段而是开始更多的和微调技术相结合。</p><p>总结：预训练-推理-微调</p><p>RAG技术本身经历了快速发展，但是目前仍然没有对RAG系统发展的清楚阐述，这篇文章的写作目的之一就是填补这一空白，去为读者绘制RAG发展路径并评估他未来的发展路径。</p><p>文章旨在阐明检索增强技术的演变，评估各种方法在各自上下文中的优点和缺点，并推测即将到来的趋势和创新。</p><p>本文的主要贡献：</p><ul><li>系统性回顾了RAG最新的方法、技术，描述了RAG三大范式的演变，将RAG研究置于LLM前景中</li><li>识别并阐明了RAG的三大核心技术：Retrieval、Generation、Augmentation，阐明了这些组件如何协同形成有效的RAG框架</li><li>总结了RAG的当前评估方法，涵盖了26个任务，近50个数据集，概述了评估目标和指标，以及当前的评估基准和工具。预计RAG的未来方向，强调应对当前挑战的潜在增强。</li></ul><p>通过认真阅读、分析原文后，应该可以对本文的三大贡献进行连贯复述。</p><h2 id="SectionII-RAG三大范式"><a href="#SectionII-RAG三大范式" class="headerlink" title="SectionII-RAG三大范式"></a>SectionII-RAG三大范式</h2><h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Indexing</p><p>索引编排阶段会将所有类型的文档（PDF、HTML、Word、Markdown）转换为统一的文本格式。之后，为了使用语言模型的文本限制，“格式化”后的文本会被切分成chunks，使用嵌入模型，这些chunks会被转换为向量存储在向量数据库中。</p><p>Retrieval</p><p>RAG系统会将用户请求(query)转换为一个向量表示，然后通过计算查询向量和chunks向量之间的相似度来检索与query最相近的top k个chunks。</p><p>Generation</p><p>将用户提出的查询和所选文档合成为一个连贯的提示，大型语言模型的任务是制定响应。在正在进行的对话情况下，任何现有的历史都会可以集成到提示中，是模型能够有效参与多轮对话交互。</p><p>检索阶段的挑战：准确率和召回率；导致错误或不相关的chunk，以及缺少关键信息。</p><p>生成阶段的困难：出现幻觉</p><p>增强阶段的障碍：检索信息与不同任务结合可能具有挑战性，有时会导致输出不连贯。对于复杂需求，基于最初查询的单一检索并不足以获取足够的上下文信息。</p><h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>相比于Naive RAG，Advanced RAG专注于提高检索质量，运用了pre-retrieval 和 post-retrieval策略。</p><p>pre-retrieval重点关注优化索引结构和原始查询，优化索引的目标是提高被索引的内容的质量。</p><p>post-retrieval阶段主要是整合有效查询，主要的方法包括chunks重排和上下文压缩。重新排序检索到的信息以将最相关的内容重新定位到提示的边缘是关键策略。</p><h3 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h3><p>相比于前两种范式，模块化RAG的适应性和多功能性得到提高。</p><p>方法：添加一个搜索模块进行相似度搜索，并通过微调细化检索器。重组RAG模块+重排RAG管道来解决目前新的挑战，引入额外组件提高检索和处理能力。</p><p>创新：</p><p>Rewrite-Retrieve-Read Model 利用LLM的能力通过重写模块和LM反馈机制来改进检索查询，以更新重写模型。，提高任务性能。</p><p>Generate-Read 用LLM生成的内容替换传统的检索</p><p>ReciteRead 强调从模型权重中检索，增强了模型处理知识密集型任务的能力</p><p>子查询和假设文档嵌入 (HyDE)旨在通过关注生成答案和真实文档之间的嵌入相似性来提高检索相关性</p><p>调整：Demonstrate-Search-Predict(DSP)框架和迭代的Retrieve-Read-Retrieve-Read流使用增强了模块的协同复杂理解。</p><h3 id="RAG与微调"><a href="#RAG与微调" class="headerlink" title="RAG与微调"></a>RAG与微调</h3><p>RAG通过提供实时知识更新和有效利用具有高可解释性的知识源，在动态环境中表现出色。然而，在检索方面有更高的延迟和伦理考虑。</p><p>RAG 和 微调技术之间的选择取决于应用程序上下文中数据动态、定制和计算能力的具体需求。RAG 和 微调技术不是互斥的，可以相互补充，增强了模型在不同层次上的能力。在某些情况下，它们的组合使用可能会导致最佳性能。涉及RAG和微调技术的优化过程可能需要多次迭代才能获得满意的结果。</p><h2 id="检索-RETTRIEVAL"><a href="#检索-RETTRIEVAL" class="headerlink" title="检索-RETTRIEVAL"></a>检索-RETTRIEVAL</h2><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="Semi-structured-data"><a href="#Semi-structured-data" class="headerlink" title="Semi-structured data"></a>Semi-structured data</h4><p>特别关注对于<strong>半结构化数据</strong>的处理分析。典型的半结构化数据例如PDF包括文本和表信息。</p><p>有两大原因导致传统RAG系统在处理半结构化数据时面临挑战：</p><ol><li>文本的切分会不经意地拆分表格，导致检索过程中的数据损坏</li><li>表与数据的结合会使语义相似度检索变得更加复杂</li></ol><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>所有的方法目前都不是最优的，该领域还有大量的研究机会。</p><p>关注北航关于Text-2-SQL的最新成果</p><h4 id="Structured-data"><a href="#Structured-data" class="headerlink" title="Structured data"></a>Structured data</h4><p>图检索使GNN神经网络、LLM与RAG相结合，通过LLM的软提示来增强图形理解和问答能力，并使用Prize-Collecting Steiner Tree进行优化图检索的结果。</p><h4 id="LLMs-Generated-Content"><a href="#LLMs-Generated-Content" class="headerlink" title="LLMs-Generated Content"></a>LLMs-Generated Content</h4><p>LLM 生成的上下文通常包含更准确的答案，因为其与因果语言建模的预训练目标能更好地对齐。</p><h3 id="Retrieval-Granularity"><a href="#Retrieval-Granularity" class="headerlink" title="Retrieval Granularity"></a>Retrieval Granularity</h3><p>粗粒度检索单元理论上可以为问题提供更多相关信息，但它们也可能包含冗余内容，这可能会分散下游任务中的检索器和语言模型。</p><p>另一方面，细粒度检索单元粒度增加了检索的负担，不能保证语义完整性并满足所需的知识。</p><p>选择恰当的检索粒度是一个简单的策略用来提高检索和下游任务的表现。</p><p>检索粒度从粗到细包括：Token, Phrase, Sentence, Proposition, Chunks, Document。将Propositions作为检索单元可以提高检索的相关性和精准度。</p><p>知识图谱的检索粒度包括：Entity, Triplet, and sub-Graph.</p><h3 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h3><p>在索引阶段，文档将被处理、分割并转换为要存储在向量数据库中的嵌入。索引的构造质量决定了是否可以在检索阶段获取正确的上下文。</p><h4 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h4><p>分块常用的方法是将文档分为不同的标记的chunks。</p><p>分块需要在语义完整性和上下文长度之间进行trade-off。更长的块会包含更长的上下文，但是噪声会增加、处理时间也会增加；短块包含的噪声小，但是能包含的上下文信息也更少。</p><p>于是后面提出了以句子作为检索单元，前后句被提供作为LLM的上下文内容。</p><h4 id="元数据附加"><a href="#元数据附加" class="headerlink" title="元数据附加"></a>元数据附加</h4><p>在检索过程中为文档时间戳分配不同的权重可以实现时间感知的RAG，确保知识的新鲜度并避免过时的信息。</p><p>元数据也可以被人工构建(添加段落摘要以及引入假设问题)。具体来说，使用 LLM 生成文档可以回答的问题，然后计算检索过程中原始问题和假设问题之间的相似度，以减少问题和答案之间的语义差距。</p><h4 id="结构化索引"><a href="#结构化索引" class="headerlink" title="结构化索引"></a>结构化索引</h4><p>层次索引结构和知识图谱都可以提高信息检索的准确率。</p><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>问题本身的复杂性、语义模糊性都是查询过程中遇到的困难。</p><p>查询优化的方法主要有三大类：</p><p>1.查询扩展</p><ul><li>多路查询</li><li>子查询</li><li>Chain-of-Verification(CoVe)</li></ul><p>2.查询转换-查询重写</p><p>3.查询路由</p><h3 id="嵌入-Embedding"><a href="#嵌入-Embedding" class="headerlink" title="嵌入-Embedding"></a>嵌入-Embedding</h3><p>怎么理解嵌入？</p><p>Embedding 是将离散的非结构化数据(图片、视频、音频、文本)通过Embedding Model转换为连续的向量表示的技术。</p><p>Embedding 常常用于将文本数据中的单词、句子或文档映射为固定长度的实数向量，使得文本数据能够在计算机中被更好地处理和理解。通过 Embedding，每个单词或句子都可以用一个实数向量来表示，这个向量中包含了该单词或句子的语义信息。</p><p>RAG系统中，检索是通过计算嵌入问题和文档块之间的相似度实现的。</p><p>没有最好的答案去回答“要使用哪一个嵌入模型”，然而，对于不同的问题可以有不同的方法。</p><h3 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h3><p>Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.</p><p>这篇文章介绍了一种通过指令微调将知识集成到白盒模型中的创新方法。检索器模块直接替换为根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难并提高模型性能。</p><h2 id="增强-AUGMENTATION-PROCESS-IN-RAG"><a href="#增强-AUGMENTATION-PROCESS-IN-RAG" class="headerlink" title="增强-AUGMENTATION PROCESS IN RAG"></a>增强-AUGMENTATION PROCESS IN RAG</h2><p>增强的过程主要侧重于优化检索的过程，这一部分介绍了三大检索增强的过程：迭代式检索、递归式检索、自适应检索。</p><p>迭代检索涉及检索和生成之间交替，允许在每一步从知识库中更丰富、更有针对性的上下文。递归检索涉及逐步细化用户查询并将问题分解为子问题，然后通过检索和生成不断解决复杂问题。自适应检索侧重于使 RAG 系统能够自主确定外部知识检索是否必要以及何时停止检索和生成，通常使用 LLM 生成的特殊标记来控制。</p><h2 id="生成-GENERATION"><a href="#生成-GENERATION" class="headerlink" title="生成-GENERATION"></a>生成-GENERATION</h2><p>在检索后，将所有检索到的信息直接输入到LLM中回答问题并不是一个好的实践。</p><h3 id="上下文配置"><a href="#上下文配置" class="headerlink" title="上下文配置"></a>上下文配置</h3><p>过长的上下文信息会导致LLM“忽视中间段落”，与人类一样，LLM 往往只关注长文本的开头和结尾，同时忘记中间部分。因此，在 RAG 系统中，我们通常需要进一步处理检索到的内容。</p><p>重排：从根本上重新排序对文档(chunks)块进行重新排序以首先突出最相关的结果，有效地减少整体文档池，在信息检索中切断双重目的，充当增强器和过滤器，为更精确的语言模型处理提供细化的输入。</p><p>上下文选择&#x2F;压缩：对RAG过程的一个误解是相信尽可能多地检索相关文档并用长检索提示包含他们是有好处的。然而，大量的信息也会带来大量的干扰，削弱大语言模型对于关键信息的敏锐度。</p><p>压缩方法包括检测并移除不重要的tokens，将上下文转化为人类很难理解但是LLM很好理解的形式。</p><p>小语言模型SLM用作过滤器，大语言模型LLM用作重排代理。在信息抽取任务中，指导LLM去重组被SLM识别的挑战性样本会导致效果显著提升。</p><h3 id="LLM微调"><a href="#LLM微调" class="headerlink" title="LLM微调"></a>LLM微调</h3><p>当LLM缺少特定领域的数据的时候，外部知识可以被提供通过微调技术。</p><p>微调的另一个好处是可以控制模型的输入和输出。它可以让LLM适应特定的数据格式和按照指示以特定风格生成响应。</p><p>通过强化学习将LLM输出与人类或检索器偏好对齐是一种潜在的方法。除了与人类偏好对齐外，还可以与微调模型和检索器的偏好保持一致。</p><p>当环境防止访问强大的专有模型或更大的参数开源模型时，一种简单有效的方法是提取更强大的模型（例如 GPT-4）。</p><h2 id="下游任务与评估"><a href="#下游任务与评估" class="headerlink" title="下游任务与评估"></a>下游任务与评估</h2><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>RAG的核心任务仍然是<strong>问答</strong>(QA)，QA包括传统的单步&#x2F;多步问答、多项选择、特定领域问答、长文本场景问答。</p><p>RAG也不断扩展到多个下游任务，如信息提取(IE)、对话生成、代码搜索等。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>上下文相关性和噪声鲁棒性对于评估检索质量很重要，而答案忠实度、答案相关性、负拒绝、信息集成和反事实鲁棒性对于评估生成质量很重要。</p><h2 id="RAG面临主要挑战和未来前景"><a href="#RAG面临主要挑战和未来前景" class="headerlink" title="RAG面临主要挑战和未来前景"></a>RAG面临主要挑战和未来前景</h2><h3 id="RAG-VS-长上下文"><a href="#RAG-VS-长上下文" class="headerlink" title="RAG VS 长上下文"></a>RAG VS 长上下文</h3><p>目前，LLM 可以毫不费力地管理超过 200,000 个标记的上下文。这也引发了关于当LLM不受上下文限制的时候是否还需要RAG的讨论。</p><p>对于RAG过程，整个检索和推理过程是可观察的，而仅依靠长上下文生成仍然是一个黑匣子。在超长上下文的背景下开发新的 RAG 方法是未来研究趋势之一。</p><h3 id="RAG鲁棒性"><a href="#RAG鲁棒性" class="headerlink" title="RAG鲁棒性"></a>RAG鲁棒性</h3><p>检索过程中噪声或矛盾信息的存在会对RAG的输出质量产生不利影响。这种情况被比喻地称为“Misinformation 可以比根本没有信息更糟糕”。</p><p>提高RAG对这种对抗性或反事实输入的抵抗力正在获得研究势头，已成为一个关键的性能指标。</p><p>The Power of Noise: Redefining Retrieval for RAG Systems.研究结果表明，包含不相关的文档可能会意外地提高准确性超过 30%，这与质量降低的初始假设相矛盾。</p><p>结果强调了开发专门的策略将检索与语言生成模型集成的重要性，强调了进一步研究和探索对 RAG 鲁棒性的必要性。</p><h3 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h3><p>将RAG与微调技术相结合正在成为领先策略，确定RAG与微调最佳整合取决于是顺序、交替还是端到端的训练。</p><p>另一个趋势是将特殊功能的小语言模型SLM引入RAG，并通过RAG系统进行微调。</p><h3 id="scaling-laws-of-RAG"><a href="#scaling-laws-of-RAG" class="headerlink" title="scaling laws of RAG"></a>scaling laws of RAG</h3><p>基于RAG的端到端模型和预训练模型仍然是当前研究人员的重点之一。</p><p>Kaplan, Jared, et al. “Scaling Laws for Neural Language Models.” arXiv: Learning,arXiv: Learning, Jan. 2020.</p><p>缩放规律已经被建立对LLM，但是应用目前尚未确定。</p><h3 id="Production-Ready-RAG"><a href="#Production-Ready-RAG" class="headerlink" title="Production-Ready RAG"></a>Production-Ready RAG</h3><p>然而，提高检索效率，提高大型知识库的文档召回率，保证数据安全，例如防止LLM中元数据和文档来源的无意披露，是仍然有待解决的挑战。</p><p>RAG技术在特殊方向的趋势：1.特定需求的定制化。2.降低最初的学习曲线来简化RAG使用。3.优化RAG来更好的服务生产环境。</p><h3 id="多模态RAG"><a href="#多模态RAG" class="headerlink" title="多模态RAG"></a>多模态RAG</h3><p>RAG在图像、音频视频、编码领域的应用。</p><h2 id="个人阅读总结"><a href="#个人阅读总结" class="headerlink" title="个人阅读总结"></a>个人阅读总结</h2><h3 id="通原理"><a href="#通原理" class="headerlink" title="通原理"></a>通原理</h3><p>对于RAG技术栈的原理自己在脑海中应该有了一定的轮廓，接下来就是重点学习一些技术细节，比如<strong>微调技术、嵌入技术</strong>，更多去关注下游任务实践与部署应用。</p><p>读完论文后要动手去尝试去实现一下RAG技术，通过动手实践真正对这门技术有一个具体的认识，而不是仅仅漂浮于理论上。练实践带动并反作用于学习，真正从实践中体会部署技术、微调技术、训练技术。</p><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><h3 id="知趋势"><a href="#知趋势" class="headerlink" title="知趋势"></a>知趋势</h3><p>读完论文后要对RAG的未来发展趋势自己心里要有一定的认知。这项技术已经做了解决了哪些问题？还能解决哪些问题？哪些方面还需提高或面临困难？对于上面三个问题，通过后期继续学习后要有清楚的认识。起步阶段不要着急，刚刚读了这个方向的一篇文章而已。</p><p>同样更重要的就是，了解趋势与发现问题后，如何去解决？如何转换为自己Idea，这也是关键。</p><h3 id="着力点"><a href="#着力点" class="headerlink" title="着力点"></a>着力点</h3><p>多模态RAG(Multi-modal RAG)、半结构化数据检索(处理半结构化数据的方法优化)、鲁棒性(RAG的对抗性或反事实输入的抵抗力)</p><p>对于半结构化数据的最新研究成果主要参考北京航空航天大学：<a href="https://arxiv.org/pdf/2408.16991">https://arxiv.org/pdf/2408.16991</a></p><p>目前，处理半结构化数据的方法是<strong>利用LLM的编码能力在数据库表上执行Text-2-SQL</strong>[Zha, Liangyu, et al. TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.]查询，或者<strong>将表格转化为文本格式</strong>[Luo, Ziyang, et al. Augmented Large Language Models with Parametric Knowledge Guiding.]，使用基于文本的处理方法进行分析。</p><p>LLM编码能力在数据库表上执行SQL和将表格转化为文本都不是最好的处理方法，所以对于LLM处理半结构化数据时，探索最优化方法仍然是趋势。</p>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
      <category>论文研读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《在线》-读书笔记</title>
    <link href="/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/09/18/%E3%80%8A%E5%9C%A8%E7%BA%BF%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>王坚院士《在线》书摘与读书笔记——互联网、数据和计算</p><p>我们很幸运，生活在一个创造决定未来的时代。但是创造能否决定未来取决于我们的信念和坚持，坚持相信的，相信坚持的。</p><p>计算，为了无法计算的价值</p>          </div><h2 id="榜样-王坚"><a href="#榜样-王坚" class="headerlink" title="榜样-王坚"></a>榜样-王坚</h2><p><strong>王坚</strong>（1962年10月—），中国信息技术专家、心理学家，阿里巴巴集团首席技术官，中国工程院院士，阿里云的创始人。曾主持研发中国唯一自研的云操作系统飞天。</p><img src="/img/bg/wangjian.jpg"/><h2 id="云计算的三次浪潮"><a href="#云计算的三次浪潮" class="headerlink" title="云计算的三次浪潮"></a>云计算的三次浪潮</h2><p>第一次浪潮中，云计算改变了今天全球互联网，奈飞、米哈游为代表的企业从第一天起100%完完整整就在云上。</p><p>第二次浪潮中，“传统企业”开始使用云计算。最标志性的事件是2022年北京冬奥会上，云计算第一次承载了赛事的核心系统。</p><p>人工智能和云计算的一次结合，这是云计算的第三次浪潮。</p><p>云计算的第三次浪潮，王坚认为，2023年人工智能和云计算有了一次集中的体现和爆发，GPT模型的出现使得计算机对科技创新的革命产生了非常重要的影响。云计算和GPT的关系就是电和电动机的关系。未来事实上云计算的算力都是会被这些在智能时代的电动机，就是被模型消耗掉。</p><h2 id="数据与计算"><a href="#数据与计算" class="headerlink" title="数据与计算"></a>数据与计算</h2><p>原子比特化，比特在线化</p><p>到底什么是<strong>在线</strong>？从思维发展与技术发展的角度来说，在线就是连接的属性，必须先连接起来才有在线这回事。从改变行业与生活的角度来说，连接就是在线的结果，在线后才能连接渗透社会生活的各个方面。</p><p>就像我们每天喝水、用电一样，计算也在逐步成为了一种新的公共服务。</p><p><strong>去IOE</strong>：摆脱企业对IBM小型机、Oracle数据库、EMC存储系统的依赖，拥抱云计算，拥抱互联网。</p><p>“<strong>大数据</strong>”这个词搞错了，数据不是因为大而产生的价值，而是因为在线上而产生了价值，因为数据从此可以在更大范围流动它产生的价值，这是真正的数据带来的巨大变化。</p><h2 id="在线定律"><a href="#在线定律" class="headerlink" title="在线定律"></a>在线定律</h2><p>定律一：每一个比特都在互联网上</p><p>定律二：每个比特都可以在互联网上流动</p><p>定律三：比特所代表的每个对象在互联网上都是可以计算的</p><h2 id="谈对技术的热爱"><a href="#谈对技术的热爱" class="headerlink" title="谈对技术的热爱"></a>谈对技术的热爱</h2><p>当我们谈对技术的热爱时，大家并没有真正意识到，我们要为之付出什么。一个不太恰当的比喻就是，我们与技术的关系就是农夫与蛇的关系。</p><p>什么是对技术的热爱？你真的相信技术会改变很多东西吗？有没有足够的自信和热爱，去捂暖这条蛇，哪怕它苏醒后会反咬你一口？当你真正热爱一个东西时，你很难预料最终的结果。当你把这条蛇揣在怀里时，你面对的最大考验是对你心脏的考验。你不知道这条蛇醒来以后是否会反咬你一口，也许你的身体足够强大，可以抵抗。</p><h2 id="人工智能、城市大脑——下一个10年的登月计划"><a href="#人工智能、城市大脑——下一个10年的登月计划" class="headerlink" title="人工智能、城市大脑——下一个10年的登月计划"></a>人工智能、城市大脑——下一个10年的登月计划</h2><p>世界上有三种智能：人的智能、机器的智能、动物的智能。</p><p>多数人谈及人工智能，考虑的都是人会不会被人工智能所取代，这是一个奇怪的逻辑。人们在让狗去寻找毒品的时候从来没有说过人类的鼻子被狗的鼻子取代了。人类要对机器有足够的尊重，要尊重机器在某些方面已经超过了人类。</p><p>互联网发展到今天，我们要做的，使用机器解决人类解决不了的问题。</p><p>今天，世界各国城市的可持续发展都面临着重大的挑战，这些挑战也到来了一个难得的机遇，就是用机器智能解决城市发展过程中产生的许多问题。</p><p>城市大脑要做的就是以互联网为基础设施，利用丰富的城市数据资源，对城市进行全局的实时分析，解决今天无法靠人脑解决的问题。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力祥哲学-把程序写成东西</title>
    <link href="/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/"/>
    <url>/2024/09/10/%E6%9D%A8%E5%8A%9B%E7%A5%A5%E8%80%81%E5%B8%88%E7%9A%84%E7%BC%96%E7%A8%8B%E5%93%B2%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>天下大事，必做于细</p><p>站在设计者的角度去思考问题</p>          </div><img src="/img/life/oo.jpg"/><h2 id="把程序写成东西"><a href="#把程序写成东西" class="headerlink" title="把程序写成东西"></a>把程序写成东西</h2><p>感谢今生遇见杨力祥老师！！！活了20多年了，第一次遇到一位将面向对象思想讲解这么透彻的老师。</p><p>老师一上来的课程就非常硬核，告诉我们，<strong>面向对象就是把程序写成东西</strong></p><p>以麦当劳炸薯条的例子，教会我们区分面向过程和面向对象。面向过程就是流程化，第一步做什么第二部做什么。。。最后一步做什么。规则、时间、火候、尺寸必须一点都不能差。</p><p>而面向对象是将工作的各个流程打包成几个“东西”，任何人只要学会使用这个东西，就可以炸出麦当劳家的高质量薯条。</p><h2 id="复用"><a href="#复用" class="headerlink" title="复用"></a>复用</h2><p>为什么要学面向对象？有什么好处？</p><p>答案就是：<strong>复用</strong>，程序写一次，在不修改的情况下大量拷贝使用。而复用的本质是<strong>共性</strong>，程序只有写成东西才容易找到共性。</p><p>杨老师还从人类思维认知起源给我们讲起，人类最初学会的工作就是<code>分类</code>，引申出类和对象的关系。将分类比做一个多叉树，在实际思考时，是自下而上抽取共性，抽象就是脱离形式提取共性，但写的时候从上到下派生。</p><p>为什么把程序写成东西就容易复用？</p><p>因为人类祖先最早学会的技能就是分类，根据东西的<strong>共性</strong>进行分类，面向对象根本目的是提高复用率。</p><p>程序写成东西的唯一目的就是复用。复用就是自己可以作为贡献者也可以作为创新者。</p><p>不要做“万能工具箱类”，万能工具箱类不可以进行程序的复用，谁都可以拿来用。</p><h2 id="共性与个性"><a href="#共性与个性" class="headerlink" title="共性与个性"></a>共性与个性</h2><p>共性+差异&#x3D;个性</p><p>面向对象提高效率的理论基础与使用依据：<strong>共性远远远远大于差异，但是差异体现价值</strong>，自然界的事物亦如此。</p><p>继承派生的过程中加入了多态的思想，于是产生了个性的差异价值；如果只是单纯的继承派生无法体现出个性的差异价值。</p><p>虚函数是技术、多态是思想。</p><p>越能加入差异反而复用率越多，越能体现差异越能找到共性。</p><p>加机制产生变异，减机制适者生存。</p><h2 id="创新思维"><a href="#创新思维" class="headerlink" title="创新思维"></a>创新思维</h2><p>创新力与创新思维不是天生的，一定是要经过后天大量培训锻炼出来的。分析问题要抓住本质，这句话很空，但是很难做到。</p><p>作为未来的计算机从业者，凡事用数据和计算的思维去思考。</p><p>开源的东西想卡脖子也是很容易的，代码很难体现出思想上的变化。</p><h2 id="鉴赏力"><a href="#鉴赏力" class="headerlink" title="鉴赏力"></a>鉴赏力</h2><p>不管是机器还是人，提高鉴赏力尤为重要。</p><h2 id="C语言拾遗"><a href="#C语言拾遗" class="headerlink" title="C语言拾遗"></a>C语言拾遗</h2><p>1.为什么要发明C++&#x2F;为什么要复用，单纯使用C语言不行吗？</p><p>面向对象就是把程序写成东西，东西可以复用。实际生产生活中，利益趋使、采取封装，采取找边界，采取把程序写成东西，可以省钱，这才是最重要的原因！</p><p>面向对象就是把程序写成东西，是东西就要有边界(水是唯一能想到的唯一没有边界的东西)。而硬件无法做到边界感，需要人为创造，编译器看着阻止越界。</p><p>边界靠编译器维护，所以不要耍小聪明越级跳过编译器。</p><p>2.C与C++区别与联系</p><p>class从c语言中struct衍生而来，只是多了成员函数与访问控制（public、private，protected）</p><p><strong>class is type</strong></p><p><strong>类作用域</strong>：function::method1()，过去函数库的玩法不行了，面向对象需要的是类库</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//a也是一个对象</span><br><span class="hljs-type">int</span>   a;<br><span class="hljs-comment">//CDate类的实例化</span><br>CDate odate;<br></code></pre></td></tr></table></figure><p>对象在内存中的存储：内存中只有对象：对象的内存结构是只存一份代码，但是存有n个数据</p><p>3.业界规范编码</p><p>每一个类写两个文件(以CDate类为例)：一个CDate.h文件放在Headers File文件夹下，另一个是CDate.cpp文件放在Source File文件夹下。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//Headers File/CDate.h</span><br><span class="hljs-comment">//头文件中只声明类和函数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CDate</span>&#123;<br><span class="hljs-keyword">private</span>:<span class="hljs-comment">//不允许从类的外部直接访问、修改成员变量</span><br>  <span class="hljs-type">int</span> m_iYear;<br>  <span class="hljs-type">int</span> m_iMonth;<br>  <span class="hljs-type">int</span> m_iDay;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">isLeapYear</span><span class="hljs-params">()</span></span>;<br>  <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Print</span><span class="hljs-params">()</span></span>;<br>  <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Set</span><span class="hljs-params">(<span class="hljs-type">int</span>,<span class="hljs-type">int</span>,<span class="hljs-type">int</span>)</span></span>;<br>&#125;;<br><br><span class="hljs-comment">//Source File/CDate.cpp</span><br><span class="hljs-comment">//头文件中只声明类和函数</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&quot;CDate.h&quot;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CDate::isLeapYear</span><span class="hljs-params">()</span></span>&#123;<br>  <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CDate::Print</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; m_iYear &lt;&lt; <span class="hljs-string">&quot;\&quot; &lt;&lt; m_iMonth &lt;&lt; &quot;</span>\<span class="hljs-string">&quot; &lt;&lt; m_iDay &lt;&lt; endl;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">void CDate::Set(int y,int m,int d)&#123;</span><br><span class="hljs-string">  //函数定义</span><br><span class="hljs-string">&#125;</span><br></code></pre></td></tr></table></figure><p>4.声明与定义</p><p>操作系统通电即有值（0010010101010….）</p><p>什么叫函数声明？-只给编译器看不占内存</p><p>函数声明的作用：函数的栈空间是靠函数的声明开辟的，被调函数的形参空间是主调函数做出来的。而空间大小有参数的类型决定，仅仅告诉编译器要撑多大空间。</p><p>函数外不能进行赋值运算，编译完是指令的（动作）必须进函数。无论是变量还是函数，只要<strong>定义就要在内存中出东西</strong>。</p><p>所有C语言编译完是指令的，一定要进函数，不能在全局位置，指令必须存在函数内部。函数调用必须也要在函数内部进行，main函数是出发点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-type">int</span> a;<span class="hljs-comment">//变量的声明，全局变量直接在内存中静态代码区赋值为0</span><br><span class="hljs-type">int</span> b;<br>b = <span class="hljs-number">5</span><span class="hljs-comment">//函数外的赋值操作，编译报错！！</span><br><span class="hljs-type">int</span> c = <span class="hljs-number">10</span>;<span class="hljs-comment">//定义了一个变量c并初始化为10，与前两行不等价</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  <span class="hljs-type">int</span> a = <span class="hljs-number">1</span>;<span class="hljs-comment">//变量的声明+定义 </span><br>  <br>&#125;<br></code></pre></td></tr></table></figure><p>main函数存在的意义：</p><p>禁止全局的语句进行运算操作，函数调用也是语句</p><p>1）编译完是指令的（动作）必须进函数</p><p>2）函数调用是指令也要进函数</p><p>3）总要有一个初始调用，于是mian函数来干这件事</p><p>5.狗屎值与野指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  <span class="hljs-type">int</span> a;<span class="hljs-comment">//生成狗屎值-只声明不赋值</span><br>  <span class="hljs-comment">//千万不要这么写</span><br><span class="hljs-type">int</span> *p;<span class="hljs-comment">//野指针-指向位置随机，危险系数极大</span><br>*p = <span class="hljs-number">5</span>;<span class="hljs-comment">//给一个随机位置赋值，会发生难以预料的危险，如果这个指针指向了内核</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>C++有两类人，一类是负责设计类机构的，另一部分是基于类架构写程序的人</p><p>C++语言的设计与演化</p><h2 id="接受工头的思想"><a href="#接受工头的思想" class="headerlink" title="接受工头的思想"></a>接受工头的思想</h2><p>工头就是抽取共性，事物都是共性大于差异。</p><p>可以从Vector类中的set、get方法出发，用public方法去修改private数据。</p><p>程序写成东西，东西就是要有一套数据和一套配套数据的操作，面向对象也是相对独立、完整的东西，就像每一个人一样。数据都要private，修改数据要用借助public方法。</p><p>五子棋的工头和围棋的工头抽取共性，然后从大共性中派生出象棋的工头、跳棋的工头。。。。</p><p>要把东西就要放在一眼看的范围</p><p>虚函数是C++面向对象程序设计中影响最显著的特性（影响的是复用）。</p><p>虚函数赋值0为纯虚函数，虚函数为抽象类，其不能创建对象。</p><p>添加差异获取复用共性的收益。</p><p>虚函数在基类和派生类中必须是一模一样！</p><p>对于一些程度词，读书的时候要重点关注，什么是好？什么是最显著？</p><h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><p>写程序前要成为行业专家，挣钱多是因为活不好干</p><p>面向对象的程序设计学会画两种设计图：</p><ul><li>类架构图</li><li>对象图</li></ul><p>会画图、会设计是理清思路的关键！</p><p>以银行系统为例：能把人民币取款和微信取款两个行为的共性抽取出来，写出来才是能力。</p><p>真正的共性是脱离了具象的，更接近本质，既体现了内在含义，又脱离了具体的对象，同时又是各个多态共有的东西。</p>]]></content>
    
    
    <categories>
      
      <category>cs基础</category>
      
      <category>力祥哲学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程思想</tag>
      
      <tag>创新思维</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>雁栖一年，科研三年</title>
    <link href="/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/"/>
    <url>/2024/08/30/%E9%9B%81%E6%A0%96%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%A7%91%E7%A0%94%E4%B8%89%E5%B9%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="开学"><a href="#开学" class="headerlink" title="开学"></a>开学</h2><p>开学参加了很多次的入学讲座和培训，听了很多老师的演讲和致辞。</p><p>曹亚男老师结合她自己的经历，告诉我们要<code>做有品位的研究</code>。针对科学研究，提出了两大方向：针对开放问题的开创探讨和针对封闭数据的优先探索。问题的价值作为核心，与个人兴趣、资源、能力、知识储备紧密结合。做有意义的课题，要平衡好“我喜欢”和“我的能力”。在雁栖湖的一年，学好专业课程的同时，一定要定好自己的研究点(至少3个)，学硕开题前至少要投出去一篇。</p><p>林政老师在开学典礼的致辞中也告诫我们，研究生的学习过程中，要<code>多关注why而不是how</code>。对于新技术、新方法，主要要弄懂为什么这样用？为什么是这个结构？至于说怎么用，那是学习过程中需要解决的问题。林老师也提醒到，学习课程的同时，不要忘记培养自己的科研能力与科学素养。</p><h2 id="关于选课"><a href="#关于选课" class="headerlink" title="关于选课"></a>关于选课</h2><p>新学期选课工作全部结束啦，抢到了自己心仪的课程。新学期还是立足本专业，结合李老师的建议侧重数据库和人工智能相关。</p><p>核心课选了晓飞老师的机器学习、曹亚男老师的自然语言处理、沙老师的大数据管理与分析。专业课选了黄晶老师的大数据技术、计算机苏老师的数据库新技术。</p><p>希望自己好好利用在雁栖湖的这一年时光，多学点真本领、多培养一些科学素养、多积累一些科研经验。上好课是基本的态度，此外还要在立足上好专业课的同时多读读经典论文，毕竟自己是学术型硕士，科学研究的基本素养与方法论还是要多多去积累。</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>开学一周，结实了身边很多优秀的同学。依稀记得，第一次开班会的时候，大家在做自我介绍，每个人都会说自己的本科毕业院校是哪里。我留心关注了一下，河北大学应该是“倒数第三好”。说实话，本科院校说出来其实是不自信的，在我上台前我也犹豫过、摇摆过，到底要不要说。最终，我还是大大方方、很坦然的向大家做了介绍。此时此刻，彷佛就像刚步入高中，步入沧州市第一中学那个优秀的环境中。但是，唯一不同的是，我的心态和那时有了很大的变化。</p><p>刚从乡镇走出来，步入一中的时候，那时候身边都是比自己优秀的人。说实话，那三年过的非常迷茫和压抑。初中的时候，你在学校是“呼风唤雨的”，但是到了一个更大的环境中、一个竞争更激烈的环境中，你彷佛就迷失了自己。那几年你会发现，你好像无论怎么努力、怎么使劲学都没办法赶超周围的人，你一直在盲目的进行分数的“攀比”。学业学业跟不上、擅长的体育也不再是特长、个人感情不顺利、二叔车祸离世家庭带来创伤…真就想掉入深渊一样，周围全是黑暗，看不到一丝希望与光亮。</p><p>本科的时候，你考了一个相对来说正常发挥的分数，以专业第一的身份进入大学。在前三年没有懈怠，因为不装逼的说，当时确实还是有遗憾。推免完成后，顺利完成学业。这四年你的表现不错，通过积极体育锻炼、综合素质锻炼、遇到了一群好的室友，你的性格、心态有了很大改变。唯一重要的是，这四年，我觉着你一直在<code>找自己</code>。找自己就是客观坦然的面对自己，从容地面对自己的优点和缺点，三人行必有我师焉，你也在时时刻刻向身边优秀的人学习。真诚、勇敢、向上社交。</p><p>好了，话说回来了，来到研究生阶段，又是到了一个更大的环境中、一个竞争更激烈的环境中，彷佛是轮回。我想，此时的你一定不同于高中时期的你，你的心里一定多了几分淡定与从容。就随着这份淡定与从容，继续向前吧！千磨万击还坚劲，任尔东西南北风。走好自己的路，心安即强大。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>雁栖一年，科研三年。前路漫漫，忘君珍重、珍惜。</p><p>​                                                                                                                                                    2024年8月30日于国科大雁栖湖</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科研心路</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DataWhale AI夏令营-LLM实训</title>
    <link href="/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/"/>
    <url>/2024/08/12/LLM%E5%AE%9E%E8%AE%AD/</url>
    
    <content type="html"><![CDATA[<h2 id="day01-baseline搭建"><a href="#day01-baseline搭建" class="headerlink" title="day01-baseline搭建"></a>day01-baseline搭建</h2><div class="note note-success">            <p>首先注册魔塔社区帐号，免费领取魔塔GPU算力资源</p>          </div><p>新建GPU算力环境，下载相关第三方库与拉取镜像资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 拉取git镜像</span></span><br>git lfs install<br>git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 安装第三方库</span></span><br>pip install streamlit==1.24.0<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 启动demo</span></span><br>streamlit run AICamp_yuan_baseline/Task\ 1：零基础玩转源大模型/web_demo_2b.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure><h2 id="day02-RAG原理与实践"><a href="#day02-RAG原理与实践" class="headerlink" title="day02-RAG原理与实践"></a>day02-RAG原理与实践</h2><p>检索增强生成 <strong>(Retrieval Augmented Generation,RAG)</strong> 是一种使用来自私有或专用数据源的信息来辅助文本生成的技术。它将检索模型(设计用于搜索大型数据集或知识库)和生成模型(例如大型语言模型 (LLM))，此类模型会使用检 索到的信息生成可供阅读的文本回复)结合在一起。</p><h3 id="LLM局限性"><a href="#LLM局限性" class="headerlink" title="LLM局限性"></a>LLM局限性</h3><p><a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p><p>这篇文章由来自Facebook AI Research、University College London、New York University三大科研教育机构的12名作者 (Patrick Lewis等)共同完成。文章主要介绍了一种新颖的检索增强生成(RAG)模型，该模型旨在解决预训练语言模型 在知识密集型NLP任务中的局限性，RAG技术被首次提出。</p><p>文章中阐述了传统大模型的局限性:传统的大型预训练模型虽然拥有存储大量事实知识的能力，但在 (query accuracy)和更 (knowledge updates)时存在不足。</p><p>同样，在实际业务场景中，通用的基础大模型可能存在无法满足我们需求的情况，主要有以下几方面原因：</p><ul><li><p>知识局限性：大模型的知识来源于训练数据，而这些数据主要来自于互联网上已经公开的资源，对于一些实时性的或者非公开的，由于大模型没有获取到相关数据，这部分知识也就无法被掌握。</p></li><li><p>数据安全性：为了使得大模型能够具备相应的知识，就需要将数据纳入到训练集进行训练。然而，对于企业来说，数据的安全性至关重要，任何形式的数据泄露都可能对企业构成致命的威胁。</p></li><li><p>大模型幻觉：由于大模型是基于概率统计进行构建的，其输出本质上是一系列数值运算。因此，有时会出现模型“一本正经地胡说八道”的情况，尤其是在大模型不具备的知识或不擅长的场景中。</p></li></ul><h3 id="RAG基本步骤"><a href="#RAG基本步骤" class="headerlink" title="RAG基本步骤"></a>RAG基本步骤</h3><p><img src="/img/post/rag-buzhou.png" alt="基本步骤"></p><ul><li><p>索引：将文档库分割成较短的 <strong>Chunk</strong>，即文本块或文档片段，然后构建成向量索引。</p></li><li><p>检索：计算问题和 Chunks 的相似度，检索出若干个相关的 Chunk。</p></li><li><p>生成：将检索到的Chunks作为背景信息，生成问题的回答。</p></li></ul><h3 id="RAG完整链路图"><a href="#RAG完整链路图" class="headerlink" title="RAG完整链路图"></a>RAG完整链路图</h3><p><img src="/img/post/lianlu.png" alt="RAG执行链路"></p><p>图片来源:(<a href="https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png">https://github.com/netease-youdao/QAnything/blob/master/docs/images/qanything_arch.png</a>)</p><p>用户进行query查询后，RAG会先进行检索，之后将检索到的 <strong><code>Chunks</code></strong> 和 <strong><code>query</code></strong> 一并输入到大模型，进而回答用户的问题。</p><p>为了完成检索，需要离线将文档（ppt、word、pdf等）经过解析、切割甚至OCR转写，然后进行向量化存入数据库(vector database)中。</p><h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h3><p>知识库中包含了多种类型的文件，如pdf、word、ppt等，这些 <code>文档</code>（Documents）需要提前被解析，然后切割成若干个较短的 <code>Chunk</code>，并且进行清洗和去重。</p><p>然后，我们会将知识库中的所有 <code>Chunk</code> 都转成向量，这一步也称为 <code>向量化</code>（Vectorization）或者 <code>索引</code>（Indexing）。<code>向量化</code> 需要事先构建一个 <code>向量模型</code>（Embedding Model），它的作用就是将一段 <code>Chunk</code> 转成 <code>向量</code>（Embedding）。</p><p>随着新知识的不断存储，向量的数量也会不断增加。这就需要将这些向量存储到 <code>数据库</code> （DataBase）中进行管理。</p><h3 id="在线计算"><a href="#在线计算" class="headerlink" title="在线计算"></a>在线计算</h3><p>在实际使用RAG系统时，当给定一条用户 <code>查询</code>（Query），需要先从知识库中找到所需的知识，这一步称为 <code>检索</code>（Retrieval）。在 <code>检索</code> 过程中，用户查询首先会经过向量模型得到相应的向量，然后与 <code>数据库</code> 中所有 <code>Chunk</code> 的向量计算相似度，最简单的例如  <code>余弦相似度</code>，然后得到最相近的一系列 <code>Chunk</code> 。</p><p>由于向量相似度的计算过程需要一定的时间，尤其是 <code>数据库</code> 非常大的时候。可以在检索之前进行 <code>召回</code>（Recall），即从 <code>数据库</code> 中快速获得大量大概率相关的 <code>Chunk</code>，然后只有这些 <code>Chunk</code> 会参与计算向量相似度。这样，计算的复杂度就从整个知识库降到了非常低。</p><p>随着知识库的增大，除了检索的速度变慢外，检索的效果也会出现退化。这是由于 <code>向量模型</code> 能力有限，而随着知识库的增大，已经超出了其容量，因此准确性就会下降。在这种情况下，相似度最高的结果可能并不是最优的。</p><p>为了解决这一问题，提升RAG效果，研究者提出增加一个二阶段检索——<code>重排</code> (Rerank)，即利用 <code>重排模型</code>（Reranker），使得越相似的结果排名更靠前。这样就能实现准确率稳定增长，即数据越多，效果越好（如上图中紫线所示）。</p><p>通常，为了与 <code>重排</code> 进行区分，一阶段检索有时也被称为 <code>精排</code> 。而在一些更复杂的系统中，在 <code>召回</code> 和 <code>精排</code> 之间还会添加一个 <code>粗排</code> 步骤，这里不再展开，感兴趣的同学可以自行搜索。综上所述，在整个 <code>检索</code> 过程中，计算量的顺序是 <code>召回</code> &gt; <code>精排</code> &gt; <code>重排</code>，而检索效果的顺序则是 <code>召回</code> &lt; <code>精排</code> &lt; <code>重排</code> 。</p><p>至此，一个完整的RAG链路就构建完毕了。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks </p><p>[2] Gao, Yunfan, et al. “Retrieval-augmented generation for large language models: A survey.” <strong>arXiv preprint arXiv:2312.10997</strong> (2023).</p><p>[3] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewriting for retrieval-augmented large language models,” <strong>arXiv preprint arXiv:2305.14283</strong>, 2023.</p><p>[4] QAnything: <a href="https://github.com/netease-youdao/QAnything">https://github.com/netease-youdao/QAnything</a></p><p>[5] When Large Language Models Meet Vector Databases: A Survey <a href="https://doi.org/10.48550/arXiv.2402.01763">https://doi.org/10.48550/arXiv.2402.01763</a> </p><h2 id="RAG技术实践"><a href="#RAG技术实践" class="headerlink" title="RAG技术实践"></a>RAG技术实践</h2><p>前置条件：使用day01搭建好的baseline环境</p><p>下载环境所需的任务包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git lfs install<br>git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git<br><span class="hljs-built_in">cp</span> AICamp_yuan_baseline/Task\ 3：源大模型RAG实战/* .<br></code></pre></td></tr></table></figure><p>双击打开<code>Task 3：源大模型RAG实战.ipynb</code>，然后运行所有单元格。</p><p>在环境中安装<code>streamlit</code>,为了后续进行模型微调以及Demo搭建(day01已经安装完毕)。</p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>在RAG实战过程中，需要构建一个向量模型。向量模型通常是一个BERT架构，是一个Transformer Encoder。</p><p>在本次学习中，选用基于BERT架构的向量模型 <code>bge-small-zh-v1.5</code>，它是一个4层的BERT模型，最大输入长度512，输出的向量维度也为512。</p><p>向量模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&quot;AI-ModelScope/bge-small-zh-v1.5&quot;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Yuan大模型下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>构造向量索引，分装一个向量模型类EmbeddingModel：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EmbeddingModel</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for EmbeddingModel</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(path)<br><br>        <span class="hljs-variable language_">self</span>.model = AutoModel.from_pretrained(path).cuda()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading EmbeddingModel from <span class="hljs-subst">&#123;path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">self, texts: <span class="hljs-type">List</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate embedding for text list</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        encoded_input = <span class="hljs-variable language_">self</span>.tokenizer(texts, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>        encoded_input = &#123;k: v.cuda() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()&#125;<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            model_output = <span class="hljs-variable language_">self</span>.model(**encoded_input)<br>            sentence_embeddings = model_output[<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>]<br>        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> sentence_embeddings.tolist()<br></code></pre></td></tr></table></figure><p>通过传入模型路径，新建一个 <code>EmbeddingModel</code> 对象 <code>embed_model</code>。初始化时自动加载向量模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create embedding model...&quot;</span>)<br>embed_model_path = <span class="hljs-string">&#x27;./AI-ModelScope/bge-small-zh-v1___5&#x27;</span><br>embed_model = EmbeddingModel(embed_model_path)<br></code></pre></td></tr></table></figure><p><code>EmbeddingModel</code> 类还有一个 <code>get_embeddings()</code> 函数，它可以获得输入文本的向量表示。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>为了实现向量检索，定义一个向量库索引类 <code>VectorStoreIndex</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义向量库索引类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorStoreIndex</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for VectorStoreIndex</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, doecment_path: <span class="hljs-built_in">str</span>, embed_model: EmbeddingModel</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.documents = []<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(doecment_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>):<br>            line = line.strip()<br>            <span class="hljs-variable language_">self</span>.documents.append(line)<br><br>        <span class="hljs-variable language_">self</span>.embed_model = embed_model<br>        <span class="hljs-variable language_">self</span>.vectors = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings(<span class="hljs-variable language_">self</span>.documents)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.documents)&#125;</span> documents for <span class="hljs-subst">&#123;doecment_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarity</span>(<span class="hljs-params">self, vector1: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], vector2: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        calculate cosine similarity between two vectors</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dot_product = np.dot(vector1, vector2)<br>        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> magnitude:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> dot_product / magnitude<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, k: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        question_vector = <span class="hljs-variable language_">self</span>.embed_model.get_embeddings([question])[<span class="hljs-number">0</span>]<br>        result = np.array([<span class="hljs-variable language_">self</span>.get_similarity(question_vector, vector) <span class="hljs-keyword">for</span> vector <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vectors])<br>        <span class="hljs-keyword">return</span> np.array(<span class="hljs-variable language_">self</span>.documents)[result.argsort()[-k:][::-<span class="hljs-number">1</span>]].tolist() <br></code></pre></td></tr></table></figure><p>类似地，通过传入知识库文件路径，新建一个 <code>VectorStoreIndex</code> 对象 <code>index</code>。初始化时会自动读取知识库的内容，然后传入向量模型，获得向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create index...&quot;</span>)<br>doecment_path = <span class="hljs-string">&#x27;./knowledge.txt&#x27;</span><br>index = VectorStoreIndex(doecment_path, embed_model)<br></code></pre></td></tr></table></figure><p>上文提到 <code>get_embeddings()</code> 函数支持一次性传入多条文本，但由于GPU的显存有限，输入的文本不宜太多。</p><p>所以，如果知识库很大，需要将知识库切分成多个batch，然后分批次送入向量模型。</p><p><code>VectorStoreIndex</code> 类还有一个 <code>get_similarity()</code> 函数，它用于计算两个向量之间的相似度，这里采用了余弦相似度。<code>VectorStoreIndex</code> 类的入口，即查询函数 <code>query()</code>。传入用户的提问后，首先会送入向量模型获得其向量表示，然后与知识库中的所有向量计算相似度，最后将 <code>k</code> 个最相似的文档按顺序返回，<code>k</code>默认为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">question = <span class="hljs-string">&#x27;介绍一下广州&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Question:&#x27;</span>, question)<br><br>context = index.query(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Context:&#x27;</span>, context)<br></code></pre></td></tr></table></figure><h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3><p>为了实现基于RAG的生成，我们还需要定义一个大语言模型类 <code>LLM</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义大语言模型类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLM</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    class for Yuan2.0 LLM</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat tokenizer...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Creat model...&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Loading Yuan2.0 model from <span class="hljs-subst">&#123;model_path&#125;</span>.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, context: <span class="hljs-type">List</span></span>):<br>        <span class="hljs-keyword">if</span> context:<br>            prompt = <span class="hljs-string">f&#x27;背景：<span class="hljs-subst">&#123;context&#125;</span>\n问题：<span class="hljs-subst">&#123;question&#125;</span>\n请基于背景，回答问题。&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            prompt = question<br><br>        prompt += <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>        inputs = <span class="hljs-variable language_">self</span>.tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>        outputs = <span class="hljs-variable language_">self</span>.model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">1024</span>)<br>        output = <span class="hljs-variable language_">self</span>.tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>这里我们传入 <code>Yuan2-2B-Mars</code> 的模型路径，新建一个 <code>LLM</code> 对象 <code>llm</code>。初始化时自动加载源大模型的tokenizer和模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; Create Yuan2.0 LLM...&quot;</span>)<br>model_path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br>llm = LLM(model_path)<br></code></pre></td></tr></table></figure><p><code>LLM</code> 类的入口是生成函数 <code>generate()</code>，它有两个参数：</p><ul><li><code>question</code>: 用户提问，是一个str</li><li><code>context</code>: 检索到的上下文信息，是一个List，默认是[]，代表没有使用RAG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; Without RAG:&#x27;</span>)<br>llm.generate(question, [])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&gt; With RAG:&#x27;</span>)<br>llm.generate(question, context)<br></code></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">&gt;</span> <span class="hljs-string">Without RAG:</span><br><span class="hljs-attr">广州大学（Guangzhou</span> <span class="hljs-string">University）是广东省内一所综合性大学，位于中国广东省广州市。广州大学成立于1952年，前身为广州工学院，是中华人民共和国成立后创建的第一所高等工科院校。</span><br><span class="hljs-attr">广州大学坐落在广州市海珠区，占地面积广阔，校园环境优美。学校拥有多个校区，其中主校区位于广州市番禺区，其他校区分布在广州市的其他地区。学校占地面积约4000亩，拥有现代化的教学、实验和生活设施。</span><br><span class="hljs-attr">广州大学以培养人才为宗旨，注重理论与实践相结合的教学模式。学校开设了多个学院和专业，涵盖了工学、理学、文学、法学、经济学、管理学、艺术学等多个领域。学校现有本科专业近300个，研究生专业涵盖科学、工程、管理、文学、法学、艺术等多个领域。</span><br><span class="hljs-attr">广州大学注重国际交流与合作，积极推进国际化办学。学校与许多国际知名大学建立了合作关系，开展学术交流和合作研究。此外，学校还鼓励学生参与国际交流项目，提供海外实习和留学机会，提升学生的国际视野和能力。</span><br><span class="hljs-attr">广州大学一直以来致力于为学生提供优质的教育环境和丰富的学习资源。学校拥有先进的教学设施和实验室，以及图书馆、体育场馆、艺术工作室等丰富的学生课外活动设施。</span><br><span class="hljs-attr">广州大学以其优秀的教学质量、领先的科研水平和培养优秀学生的能力而闻名。学校致力于培养具有创新精神和社会责任感的高素质人才，为地方经济发展和社会进步做出贡献。&lt;eod&gt;</span><br><span class="hljs-attr">&gt;</span> <span class="hljs-string">With RAG:</span><br><span class="hljs-attr">广州大学是一所位于广东省广州市的全日制普通高等学校，实行省市共建、以市为主的办学体制。学校的办学历史可以追溯到1927年创办的私立广州大学，后来在1951年并入华南联合大学。1984年定名为广州大学。2000年，广州大学经过教育部批准，与广州教育学院、广州师范学院、华南建设学院西院、广州高等师范专科学校合并组建新的广州大学。&lt;eod&gt;</span><br></code></pre></td></tr></table></figure><h2 id="day03-微调技术原理与实践"><a href="#day03-微调技术原理与实践" class="headerlink" title="day03-微调技术原理与实践"></a>day03-微调技术原理与实践</h2><p>模型微调也被称为指令微调（Instruction Tuning）或者有监督微调（Supervised Fine-tuning, SFT），该方法利用成对的任务输入与预期输出数据，训练模型学会以问答的形式解答问题，从而解锁其任务解决潜能。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p><p>指令微调并非无中生有地传授新知，而是更多地扮演着催化剂的角色，激活模型内在的潜在能力，而非单纯地灌输信息。</p><p>相较于预训练所需的海量数据，指令微调所需数据量显著减少，从几十万到上百万条不等的数据，均可有效激发模型的通用任务解决能力。</p><h3 id="轻量化微调"><a href="#轻量化微调" class="headerlink" title="轻量化微调"></a>轻量化微调</h3><p>由于大模型的参数量巨大， 进行全量参数微调需要消耗非常多的算力。为了解决这一问题，研究者提出了参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning），这些方法通过训练极少的模型参数，同时保证微调后的模型表现可以与全量微调相媲美。</p><p>常用的轻量化微调技术有<code>LoRA</code>、<code>Adapter</code> 和 <code>Prompt Tuning</code>。</p><p>LoRA:<a href="https://arxiv.org/pdf/2106.09685">https://arxiv.org/pdf/2106.09685</a></p><p>大模型轻量级微调（LoRA）：训练速度、显存占用分析:<a href="https://zhuanlan.zhihu.com/p/666000885">https://zhuanlan.zhihu.com/p/666000885</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#模型下载</span><br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;IEITYuan/Yuan2-2B-Mars-hf&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;.&#x27;</span>)<br><span class="hljs-comment"># 导入环境</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer<br><span class="hljs-comment"># 读取数据</span><br>df = pd.read_json(<span class="hljs-string">&#x27;./data.json&#x27;</span>)<br>ds = Dataset.from_pandas(df)<br><span class="hljs-comment"># 加载 tokenizer</span><br>path = <span class="hljs-string">&#x27;./IEITYuan/Yuan2-2B-Mars-hf&#x27;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=<span class="hljs-literal">False</span>, add_bos_token=<span class="hljs-literal">False</span>, eos_token=<span class="hljs-string">&#x27;&lt;eod&gt;&#x27;</span>)<br>tokenizer.add_tokens([<span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;predict&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_SUFFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_PREFIX&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;FIM_MIDDLE&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_before&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_msg&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;commit_after&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_start&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_text&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_code&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;jupyter_output&gt;&#x27;</span>,<span class="hljs-string">&#x27;&lt;empty_output&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br>tokenizer.pad_token = tokenizer.eos_token<br><span class="hljs-comment"># 定义数据处理函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_func</span>(<span class="hljs-params">example</span>):<br>    MAX_LENGTH = <span class="hljs-number">384</span>    <span class="hljs-comment"># Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性</span><br><br>    instruction = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;input&#x27;</span>]&#125;</span>&lt;sep&gt;&quot;</span>)<br>    response = tokenizer(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;example[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&lt;eod&gt;&quot;</span>)<br>    input_ids = instruction[<span class="hljs-string">&quot;input_ids&quot;</span>] + response[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>    attention_mask = [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(input_ids) <br>    labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(instruction[<span class="hljs-string">&quot;input_ids&quot;</span>]) + response[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-comment"># instruction 不计算loss</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(input_ids) &gt; MAX_LENGTH:  <span class="hljs-comment"># 做一个截断</span><br>        input_ids = input_ids[:MAX_LENGTH]<br>        attention_mask = attention_mask[:MAX_LENGTH]<br>        labels = labels[:MAX_LENGTH]<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids,<br>        <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>        <span class="hljs-string">&quot;labels&quot;</span>: labels<br>    &#125;<br><span class="hljs-comment"># 处理数据集</span><br>tokenized_id = ds.<span class="hljs-built_in">map</span>(process_func, remove_columns=ds.column_names)<br>tokenized_id<br><span class="hljs-comment"># 数据检查</span><br>tokenizer.decode(tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>tokenizer.decode(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x != -<span class="hljs-number">100</span>, tokenized_id[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;labels&quot;</span>])))<br><span class="hljs-comment"># 模型加载</span><br>model = AutoModelForCausalLM.from_pretrained(path, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype=torch.bfloat16, trust_remote_code=<span class="hljs-literal">True</span>)<br>model<br>model.enable_input_require_grads() <span class="hljs-comment"># 开启gradient_checkpointing时，要执行该方法</span><br><span class="hljs-comment"># 配置Lora</span><br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model<br><br>config = LoraConfig(<br>    task_type=TaskType.CAUSAL_LM, <br>    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>, <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],<br>    inference_mode=<span class="hljs-literal">False</span>, <span class="hljs-comment"># 训练模式</span><br>    r=<span class="hljs-number">8</span>, <span class="hljs-comment"># Lora 秩</span><br>    lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># Lora alaph，具体作用参见 Lora 原理</span><br>    lora_dropout=<span class="hljs-number">0.1</span><span class="hljs-comment"># Dropout 比例</span><br>)<br>config<br><span class="hljs-comment"># 构建PeftModel</span><br>model = get_peft_model(model, config)<br>model<br><span class="hljs-comment"># 设置训练参数</span><br>args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./output/Yuan2.0-2B_lora_bf16&quot;</span>,<br>    per_device_train_batch_size=<span class="hljs-number">12</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">1</span>,<br>    logging_steps=<span class="hljs-number">1</span>,<br>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br>    num_train_epochs=<span class="hljs-number">3</span>,<br>    learning_rate=<span class="hljs-number">5e-5</span>,<br>    save_on_each_node=<span class="hljs-literal">True</span>,<br>    gradient_checkpointing=<span class="hljs-literal">True</span>,<br>    bf16=<span class="hljs-literal">True</span><br>)<br><span class="hljs-comment"># 初始化Trainer</span><br>trainer = Trainer(<br>    model=model,<br>    args=args,<br>    train_dataset=tokenized_id,<br>    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="hljs-literal">True</span>),<br>)<br><span class="hljs-comment"># 模型训练</span><br>trainer.train()<br><span class="hljs-comment"># 定义生成函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">prompt</span>):<br>    prompt = prompt + <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span><br>    inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].cuda()<br>    outputs = model.generate(inputs, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">256</span>)<br>    output = tokenizer.decode(outputs[<span class="hljs-number">0</span>])<br>    <span class="hljs-built_in">print</span>(output.split(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)[-<span class="hljs-number">1</span>])<br>input_str = <span class="hljs-string">&#x27;张三，汉族，金融学硕士。&#x27;</span><br>prompt = template.replace(<span class="hljs-string">&#x27;input_str&#x27;</span>, input_str).strip()<br>generate(prompt)<br>&#123;<span class="hljs-string">&quot;姓名&quot;</span>: [<span class="hljs-string">&quot;张三&quot;</span>], <span class="hljs-string">&quot;国籍&quot;</span>: [<span class="hljs-string">&quot;汉族&quot;</span>], <span class="hljs-string">&quot;职位&quot;</span>: [<span class="hljs-string">&quot;金融学硕士&quot;</span>]&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>部署</tag>
      
      <tag>微调技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据技术学习笔记</title>
    <link href="/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="大数据技术路线总揽"><a href="#大数据技术路线总揽" class="headerlink" title="大数据技术路线总揽"></a>大数据技术路线总揽</h2><h2 id="MapReduce编程思想"><a href="#MapReduce编程思想" class="headerlink" title="MapReduce编程思想"></a>MapReduce编程思想</h2><h2 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h2><h3 id="Google-files-system"><a href="#Google-files-system" class="headerlink" title="Google files system"></a>Google files system</h3><p>GFS直接以Linux为基础存储层，并且设计模式为单master模式。</p><p>另外一方面，GFS 还是采用了 Checkpoints、操作日志（Operation Logs）、影子Master（Shadow Master）等一系列的工程手段，来尽可能地保障整个系统的“可恢复（Recoverable）”，以及读层面的“可用性（Availability）”。</p>]]></content>
    
    
    <categories>
      
      <category>数据科学与数据开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>big data</tag>
      
      <tag>分布式计算</tag>
      
      <tag>存储</tag>
      
      <tag>数仓</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习基础</title>
    <link href="/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="Machine-Learning-Notebook"><a href="#Machine-Learning-Notebook" class="headerlink" title="Machine Learning-Notebook"></a>Machine Learning-Notebook</h1><center>Andrew Ng-吴恩达&copy;</center><center>Standford ONLINE & DeepLearning.AI</center><center>Mungeryang-杨桂淼总结</center><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>“Field of study that gives computers the ability to learn without being explicitly programmed.”——Arthur Samuel(1995)</p><p>  Practical advice for applying learning algorithm</p><h2 id="Basic-conception-cookbook"><a href="#Basic-conception-cookbook" class="headerlink" title="Basic conception cookbook"></a>Basic conception cookbook</h2><p>Data set:数据集包括训练集+测试集</p><p>Training set: 用于模型训练的数据集</p><p>Test set: 用于模型测试的数据集</p><p>Cost function: 衡量模型预测结果与真实结果之间的差异或误差</p><p>Gradient: 方向导数；梯度的大小将表示函数在某一方向上的变化率</p><p>Gradient descent:  一种最优化的算法，找到函数的局部最小值</p><p>Accurate : 所有分类（无论是正类还是负类）正确分类的比例。TP + TN &#x2F; TP+FP+TN+FN</p><p>Recall(also known as sensitivity): the fraction of relevant instances that were retrieved；所有实际正例被正确分类为正例的比例。TP&#x2F;TP+FN</p><p>Precision(also called positive predictive value) : the fraction of relevant instances among the retrieved instances;模型所有正分类分类中实际正分类的比例。TP&#x2F;TP+FP</p><p>F1: F1值 &#x3D; 正确率 * 召回率 * 2 &#x2F; (正确率 + 召回率)；F1值的范围在0和1之间，值越接近1表示模型的性能越好。F1值越大表示分类器在识别正例样本时取得了更好的平衡。</p><p>A <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><h2 id="Supervised-learning-监督学习"><a href="#Supervised-learning-监督学习" class="headerlink" title="Supervised learning-监督学习"></a>Supervised learning-监督学习</h2><p> Learns from being given “<strong>right answers</strong>(labels)”</p><img src="/img/fig/1.1.png" alt="s" style="text-align: center;"/><p>In all of these applications, we will first train our model with examples of inputs <strong>x</strong> and right answers, that is the labels <strong>y</strong>. After the model has learned from these input, output, or x and y pairs, they can take a brand new input x, something it has never seen before, and try to produce the appropriate corresponding output y.  </p><img src="/img/fig/1.2.jpg" alt="s" style="text-align: center;" /><p>  The task of the supervised learning algorithm is to produce more of these right answers based on labels.  </p><p>Classification is to predict categories,Regression is to predict a number. </p><img src="/img/fig/1.3.png" alt="s" style="text-align: center;" /><h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning-无监督学习"></a>Unsupervised learning-无监督学习</h2><p>Learns from being given “<strong>no-right answers</strong>(unlabeled data)”, data only comes with inputs x, but not output labels y. Algorithm has to find structure automatically in the data and automatically figure out whether the major types of individuals.</p><img src="/img/fig/1.4.png" style="text-align: center;" /><h2 id="Linear-Regression-with-one-variable"><a href="#Linear-Regression-with-one-variable" class="headerlink" title="Linear Regression with one variable"></a>Linear Regression with one variable</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Linear regression means fitting a <code>straight line</code> to the data. -&gt;linear regression  </p><img src="/img/fig/2.1.jpg" alt="s" style="text-align: center;"/><p>Regression model is to predict numbers</p><p>Classification model is to predicts categories</p><h3 id="Terminology-in-ML"><a href="#Terminology-in-ML" class="headerlink" title="Terminology in ML"></a><strong>Terminology in ML</strong></h3><p><code>Training dataset</code>-&gt;data used to train the model</p><p>$x&#x3D;$“input”variables feature</p><p>$y&#x3D;$“output ”variables or “target”variables</p><p>$(x,y)&#x3D;$single training example</p><p>$(x^{(i)},y^{(i)})&#x3D;i^{th}$ training example not exponent</p><img src="/img/fig/2.2.jpg" alt="s" style="text-align: center;"/><p>In the linear regression, we instantly believe the function is a linear function as follow:<br>$$<br>f_{w,b}(X)&#x3D;wX+b \<br>f(X)&#x3D;wX+b<br>$$<br>when we give a “x”as a input variable, we can get a “y-hat”variable as a result.</p><h3 id="triangular-ruler-Cost-function"><a href="#triangular-ruler-Cost-function" class="headerlink" title=":triangular_ruler:Cost function"></a>:triangular_ruler:Cost function</h3><p>squared error<br>$$<br>\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2<br>$$<br><img src="/img/fig/2.3.jpg" alt="s" style="text-align: center;" /></p><p>Model: $f_{w,b}(X)&#x3D;wX+b$.$w(slope),b(intersects)$ are parameters.<br>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{w.b}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2 \<br>&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2<br>$$<br>Our goal is to minimize the parameters to fit the model:<br>$$<br>\underset{w.b}{\min}J\left( w,b \right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#define cost function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    cost_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i]) ** <span class="hljs-number">2</span><br>        cost_sum += cost<br>    total_cost = (<span class="hljs-number">1</span>/(<span class="hljs-number">2</span>*m))*cost_sum<br>    <span class="hljs-keyword">return</span> total_cost<br></code></pre></td></tr></table></figure><h3 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent-梯度下降"></a>Gradient descent-梯度下降</h3><p><strong>Simultaneous update</strong> the parameters w and b until the cost function is <code>convergence</code>:</p><p>Simultaneous update the parameters is significant, we must focus on the order about the algorithm.</p><p>Correct order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>\<br>w&#x3D;tmp_w<br>\<br>b&#x3D;tmp_b<br>$$<br>Incorrect order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right) \<br>w&#x3D;tmp_w \<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right) \<br>b&#x3D;tmp_b \<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#compute gradient</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradient</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_dw = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_dw += dj_dw_i<br>        dj_db += dj_db_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></td></tr></table></figure><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>The choice of learning rate, alpha($\alpha$) will have a huge impact on the efficiency of our implementation of Gradient descent.<br>$$<br>w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>$$<br><img src="/img/fig/2.6.jpg" alt="s" style="text-align: center;" /></p><p>If we chose the alpha is too large, the fit efficiency is not very well. We can design the algorithm to decrease the alpha($\alpha$) following by the w and b.</p><img src="/img/fig/cost1.png" alt="s" style="text-align: center;" /><h2 id="Multiple-linear-regression"><a href="#Multiple-linear-regression" class="headerlink" title="Multiple linear regression"></a>Multiple linear regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>【Model】<br>$$<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n<br>$$<br><code>Dot product(inner product)</code> of two vectors about $w$ and $b$.<br>$$<br>\vec{w}&#x3D;\left[ w_1,w_2···,w_n \right]<br>\<br>\vec{x}&#x3D;\left[ x_1,x_2···,x_n \right]<br>$$<br>$x_j&#x3D;j^{th} features$</p><p>$n &#x3D; $ numbers of features</p><p>$\vec{x}^{(i)}$ &#x3D; features of $i^{th}$ training example</p><p>$\vec{x}^{(i)}_j$ &#x3D; value of feature j in  $i^{th}$ training example</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>We contrast the two process between vectorization and without vectorization.</p><p>without vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n):<br>    f = f + w[i]*x[i]<br>f = f + b<br></code></pre></td></tr></table></figure><p>If n is infinite, the algorithm is time consuming.</p><p>with vectorization</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>f = np.dot(w,b)<br></code></pre></td></tr></table></figure><p> Without vectorization, we just only use the for loop to calculate the results one by one. In the contrast, we can use the function numpy.dot() to calculate the result. Using numpy can decrease the time complexity and improve the algorithm efficiency. Numpy can use <code>parallel process</code> hardware to carry out the data.</p><p>Dot product of two vectors:<br>$$<br>a·b&#x3D;\underset{i&#x3D;0}{\overset{n-1}{\varSigma}}a_ib_i&#x3D;\left[ \begin{array}{l}<br>    ,, a_0\<br>    ,, a_1\<br>    ,,···\<br>    a_{n-1}\<br>\end{array} \right] \left[ \begin{array}{l}<br>    ,, b_0\<br>    ,, b_1\<br>    ,,···\<br>    b_{n-1}\<br>\end{array} \right] &#x3D;\left[ a_0b_0+a_1b_1+···+a_{n-1}b_{n-1} \right]<br>$$</p><h3 id="Gradient-descent-in-Multiple-regression"><a href="#Gradient-descent-in-Multiple-regression" class="headerlink" title="Gradient descent in Multiple regression"></a>Gradient descent in Multiple regression</h3><p>Parameters include $w_{1},w_{2},···w_{n}$ and $b$</p><p>Model is $f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n+b$</p><p>Cost function is $J\left( w_{1},w_{2},···w_{n},b \right)$</p><p>Gradient descent:</p><pre><code class="hljs">repeat&#123;</code></pre><p>$$<br>w_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>}</p><p><code>for i in range(1,n)</code>:<br>$$<br>\frac{\partial}{\partial w_{1}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{1}^{\left( i \right)} \<br>\frac{\partial}{\partial w_{n}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{n}^{\left( i \right)} \<br>\frac{\partial}{\partial b}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right)<br>$$<br>Normal equation:</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations</li></ul><p>Disadvantages:</p><ul><li>Does not generalize to other learning algorithm</li><li>Slow when number of features is large(n&gt;10000)</li></ul><h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The technology of Features scaling will enable Gradient descent to run much faster.</p><p>Z-score normalization:<br>$$<br>x_i&#x3D;\frac{x_i-\mu}{\sigma}<br>$$<br>$\mu$ is the sample average value, and $\sigma$ is standard error of sample.</p><p>Aim for about $-1\leqslant x_j\leqslant 1$ for $x_{j}$.</p><p>The objective of Gradient descent in Multiple regression is:<br>$$<br>\underset{\vec{w}.b}{\min},,J\left(\vec{w},b \right)<br>$$<br><img src="/img/fig/2.7.jpg" alt="s" style="text-align: center;" /></p><p>As the calculate process, $y&#x3D;J\left(\vec{w},b \right)$ is decreased until a low score. If we can get the similar the result in the experiment, the cost function is <code>convergence</code> and we can find the min $J\left(\vec{w},b \right)$.</p><p>【<strong>Skill</strong>】</p><p>At the beginning, we can set the learning rate($\alpha$) in a small value like 0.001. As running the algorithm, we can update the parameter $\alpha$ gradually.(0.001-&gt;0.01-&gt;0.1)</p><p><strong>Feature Engineering</strong> : Using <code>intuition</code> to design new features, by transforming or combining original features.</p><blockquote><p>[Wiki]:<strong>Feature engineering</strong> or <strong>feature extraction</strong> or <strong>feature discovery</strong> is the process of extracting features (characteristics, properties, attributes) from raw data.</p></blockquote><img src="/img/fig/2.8.jpg" style="text-align: center;" /><p>Except for using linear regression, we can also use <code>polynomial regression</code> to fit data. Through finding the segment of the data distribution(scatter), we can create special features($\sqrt{x}\quad x^3$) to fit data successfully.<br>$$<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+b\quad②<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+b\quad①<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2\sqrt{x}+b\quad③<br>$$</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>It turns out that, linear regression is not the good algorithm for classification. For classification, the output is not a continuous number. In the fact, it is a class variable like <strong>0(false-negative class)</strong> or <strong>1(true-positive class)</strong>.</p><img src="/img/fig/3.1.jpg" alt="s" style="text-align: center;" /><p>Sometimes, if we want to classify the output, the method of linear regression maybe not fit. Through the figure, the result of predict can be changed when we add a sample. The original fit result is the blue line, but, the new result is the green line. The standard of classification can be changed as the sample we adding.</p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>In this chapter, we will learn a useful algorithm model——<code>Logistic Regression</code> to solve the classification problem.</p><p>With linear regression method, the model is $f_{\vec{w},b}\left(x^{i} \right) &#x3D;\vec{w}·x^{i}+b$, to predict $y$ given $x$. However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</p><p>This can be accomplished by using a <code>&quot;sigmoid function&quot;</code> which maps all input values to values between 0 and 1.</p><p>【sigmoid function】</p><img src="/img/fig/3.2.jpg" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;\frac{1}{1+e^{-Z}},0&lt;g\left( Z \right) &lt;1<br>\<br>f_{w,b}\left( X^{\left( i \right)} \right) &#x3D;g\left( w·X^{\left( i \right)}+b \right)<br>\<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;g\left( \vec{w}·\vec{x}+b \right) &#x3D;\frac{1}{1+e^{-\left( \vec{w}·\vec{x}+b \right)}}<br>$$</p><p>We can calculate the Z-score to classify the predict value by the probability.</p><p>$$<br>P\left( y&#x3D;0 \right) +P\left( y&#x3D;1 \right) &#x3D;1<br>$$</p><img src="/img/fig/3.3.jpg" style="text-align: center;"/><p>$$<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;1|\vec{X};\vec{w},b \right)<br>\<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;0|\vec{X};\vec{w},b \right)<br>$$</p><p>Probability that $y&#x3D;1$  or $y&#x3D;0$, given input $\vec{x}$, parameters $w,b$.</p><p>If $y&#x3D;f_{\vec{w},b}\left( \vec{X} \right) &gt; 0.5$ -&gt; $g(Z)&gt;0.5$ -&gt; $Z&gt;0$ -&gt; $\vec{w}·\vec{x}+b &gt; 0$ &#x3D;&#x3D; YES $Y&#x3D;1$.</p><p>Else, NO $y &#x3D; 0$.</p><h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><img src="/img/fig/3.4.jpg" alt="s" style="text-align: center;" /><p>$$<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+b \right)<br>\<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b \right)\<br>boundary1&#x3D;w_1x_1+w_2x_2+b&#x3D;0<br>\<br>boundary2&#x3D;w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b&#x3D;0<br>$$</p><h3 id="Cost-function-for-regularized-logistic-regression"><a href="#Cost-function-for-regularized-logistic-regression" class="headerlink" title="Cost function for regularized logistic regression"></a>Cost function for regularized logistic regression</h3><p>Cost function:</p><p>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\frac{1}{2}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2&#x3D;L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right)<br>$$</p><p>Our goal is to minimize the parameters to fit the model:</p><p>$$<br>\underset{w.b}{\min},,J\left( w,b \right)<br>$$</p><p><strong>Simplified</strong> loss function:</p><p>$$<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;\begin{cases}<br>    -\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;1\<br>    -\log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;0\<br>\end{cases}\ \Longrightarrow \<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;-y^{\left( i \right)}\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) -\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right)<br>$$</p><p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}  \sum_{i&#x3D;0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2<br>$$</p><p>where:</p><p>$$<br>f_{\mathbf{w},b}(\mathbf{x}^{(i)}) &#x3D; sigmod(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)<br>$$</p><p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p><p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}\sum_{i&#x3D;0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]<br>$$</p><p>As was the case in linear regression above, the difference is the regularization term, which is   $\frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2$ </p><p>Gradient descent:</p><p>repeat{</p><p>$$<br>tw_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>tb&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p><p>$$<br>w&#x3D;tw_{j}\<br>b&#x3D;tb<br>$$</p><p>}</p><img src="/img/fig/gridant1.png" style="text-align: center;" /><p>To provide the overfitting problem, we apply the regularized method to add the penalty coefficient. That is why the we add the $\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}$ at the end of the formula.</p><img src="/img/fig/3.5.png" alt="s" style="text-align: center;" /><p><strong>Regularized</strong>:</p><p>$$<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}<br>\<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}b_{j}^{2}<br>$$</p><p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$.</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;&#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\ \<br>$$</p><p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial b}  &#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})<br>$$</p><ul><li><p>m is the number of training examples in the data set      </p></li><li><p>$f_{\mathbf{w},b}(x^{(i)})$ is the model’s prediction, while $y^{(i)}$ is the target</p></li><li><p>For a  <span style="color:blue"> <strong>linear</strong> </span> regression model<br>  $f_{\mathbf{w},b}(x) &#x3D; \mathbf{w} \cdot \mathbf{x} + b$  </p></li><li><p>For a <span style="color:blue"> <strong>logistic</strong> </span> regression model<br>  $z &#x3D; \mathbf{w} \cdot \mathbf{x} + b$ \ $f_{\mathbf{w},b}(x) &#x3D; g(z)$<br>  where $g(z)$ is the sigmoid function:</p></li></ul><p>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#cpmpute cost function through itertion process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function</span>):<br>    w = copy.deepcopy(w_in)<br>    J_history = []<br>    p_history = []<br>    b = b_in <br>    w = w_in<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw,dj_db = gradient_function(x,y,w,b)<br>        <span class="hljs-comment"># update the parameters </span><br>        b = b - alpha * dj_db<br>        w = w - alpha * dj_dw<br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">100000</span>:<br>            J_history.append(cost_function(x,y,w,b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i % math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteraton <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> w,b,J_history,p_history<br></code></pre></td></tr></table></figure><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><h3 id="How-to-install-TensorFlow"><a href="#How-to-install-TensorFlow" class="headerlink" title="How to install TensorFlow"></a>How to install TensorFlow</h3><ul><li>conda install tensorflow</li><li>pip install tensorflow</li></ul><h3 id="How-the-brain-works"><a href="#How-the-brain-works" class="headerlink" title="How the brain works"></a>How the brain works</h3><p>The following picture shows the basic structure about neutral network. Like human&#96;s neutral cell, it passes information by layers, which every cell includes a logistic function.</p><img src="/img/fig/4.2.jpg" style="text-align: center;" /><h3 id="Data-format-in-Tensorflow"><a href="#Data-format-in-Tensorflow" class="headerlink" title="Data format in Tensorflow"></a>Data format in Tensorflow</h3><p><code>matrix and tensor</code></p><p>Numpy, a standard library created in 1970s, is used to calculate linear algebra in python(data analysis). Tensorflow is a machine learning framework created by Google.</p><p>In Tensorflow, the input format must <strong>a matrix</strong>. We should focus on the special characteristic in our work.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#一维数组</span><br>b = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<span class="hljs-comment">#一维矩阵</span><br>x = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">14</span>],<br>])<span class="hljs-comment">#3X3矩阵</span><br>Z = np.matmul(A_in,W) + B <span class="hljs-comment">#input matrix to simplify for loop</span><br></code></pre></td></tr></table></figure><h3 id="Build-a-Tensorflow"><a href="#Build-a-Tensorflow" class="headerlink" title="Build a Tensorflow"></a>Build a Tensorflow</h3><ol><li>build the structure of the model</li><li>compile the model</li><li>input training data</li><li>fit the model</li><li>predict the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span><br>layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_3 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1,layer_2,...layer_n])<br>----------------------------<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>])<br><span class="hljs-number">2.</span><br>model.<span class="hljs-built_in">compile</span>()<br><span class="hljs-number">3.</span><br>x = np.array([[<span class="hljs-number">0.</span>..,<span class="hljs-number">245</span>,...,<span class="hljs-number">17</span>],[<span class="hljs-number">0.</span>..,<span class="hljs-number">200</span>,...,<span class="hljs-number">284</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>])<br><span class="hljs-number">4.</span><br>model.fit<br><span class="hljs-number">5.</span><br>model.predict(x_new)<br></code></pre></td></tr></table></figure><h3 id="Implementation-of-the-preceding-communication"><a href="#Implementation-of-the-preceding-communication" class="headerlink" title="Implementation of the preceding communication"></a>Implementation of the preceding communication</h3><img src="/img/fig/4.4.png" alt="s" style="text-align: center;"/>$$\vec{w}_{1}^{\left[ 1 \right]}=\left[ \begin{array}{c}    1\\    2\\\end{array} \right] ,\vec{w}_{2}^{\left[ 1 \right]}=\left[ \begin{array}{c}    -3\\    4\\\end{array} \right] ,\vec{w}_{3}^{\left[ 1 \right]}=\left[ \begin{array}{c}    5\\    -6\\\end{array} \right] $$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([<br>    [<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,<span class="hljs-number">5</span>],<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,-<span class="hljs-number">6</span>]<br>]) <br>b = np.array([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>a_in = np.array([-<span class="hljs-number">2</span>,<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>Implement the coding of dense function with python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in,W,b,g</span>):<br>    <span class="hljs-comment">#units equals to the numbers of cols of W</span><br>    units = W.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment">#Create a matrix with the same size of units--[0,0,0]</span><br>    a_out = np.zeros(units)<br>    <span class="hljs-comment">#compete the g(z)</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x,W1,b1)<br>    a2 = dense(a1,W2,b2)<br>    a3 = desne(a2,W3,b3)<br>    a4 = dense(a3,W4,b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br>    <br>    <br></code></pre></td></tr></table></figure><h3 id="Choose-activation-function"><a href="#Choose-activation-function" class="headerlink" title="Choose activation function"></a>Choose activation function</h3><p>Three types activation function:</p><img src="/img/fig/4.3.jpg" alt="s" style="text-align: center;" /><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">activation=<span class="hljs-string">&quot;linear&quot;</span><br>activation=<span class="hljs-string">&quot;sigmoid&quot;</span><br>activation=<span class="hljs-string">&quot;relu&quot;</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>type y</th><th>0&#x2F;1</th><th>+&#x2F;-</th><th>0&#x2F;+</th></tr></thead><tbody><tr><td>sigmoid</td><td>binary classfication</td><td></td><td></td></tr><tr><td>linear</td><td></td><td>regression</td><td></td></tr><tr><td>relu</td><td></td><td></td><td>regression</td></tr></tbody></table><p>[why we need activation function?]</p><p>If we use the linear activation function in  hidden layers all the time, the predict results of neutral network equal to linear regression. that&#96;s why we should use sigmoid or relu function as the activation function to build the network.</p><h3 id="Soft-max-regression"><a href="#Soft-max-regression" class="headerlink" title="Soft-max regression"></a>Soft-max regression</h3><p>Softmax function has been used to solve the  multi-class problem. Essentially, it is still a <strong>classification problem</strong> <code>based on probability</code>. The expression of Softmax function as follow:<br>$$<br>z_j&#x3D;\vec{w}_j·\vec{x}+b_j\<br>a_j&#x3D;\frac{e^{z_j}}{\underset{k&#x3D;1}{\overset{n}{\varSigma}}e^{z_k}}&#x3D;P\left( y&#x3D;j|\vec{x} \right)<br>$$<br>a_j is the possibility of predict result.</p><p>The cost function of Soft-max function is:<br>$$<br>a_N&#x3D;\frac{e^{Z_N}}{e^{Z_1}+e^{Z_2}+\cdot \cdot \cdot +e^{Z_N}}&#x3D;P\left( y&#x3D;N|\vec{x} \right)<br>$$</p><p>$$<br>loss\left( a_1,…,a_N,y \right) &#x3D;\begin{cases}<br>    -\log a_1,,if,,y&#x3D;1\<br>    -\log a_2,,if,,y&#x3D;2\<br>    \vdots\<br>    -\log a_n,,if,,y&#x3D;n\<br>\end{cases}<br>$$<br>The result is <code>one of</code> the loss functions. </p><h3 id="Output-of-the-soft-max"><a href="#Output-of-the-soft-max" class="headerlink" title="Output of the soft-max"></a>Output of the soft-max</h3><img src="/img/fig/4.5.jpg" style="text-align: center;" /><p>The outcome of soft-max classification is multiply. Every outcome will be competed a score to predict the right answer. </p><p>Carefully, the loss function we must choose the <code>SparseCategoricalCrossentropy</code>.<br>$$<br>a_{N}^{\left[ l \right]}&#x3D;\frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>\<br>-\log a_{N}^{\left[ l \right]}\ne -\log \frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>$$<br>The difference of competing order can lead to the different outcome, which has different model accurancy.</p><h3 id="Improve-the-soft-max-model"><a href="#Improve-the-soft-max-model" class="headerlink" title="Improve the soft-max model"></a>Improve the soft-max model</h3><p>In order to improve the accuracy of calculations, we have made the following improvements to the model algorithm:</p><ul><li>In soft-max layer, we adopt the <code>linear</code> function as the activation.</li><li>In the process of compiling model, we add a parameter to improve the accurancy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#linear function as the activation function in the soft-max layer</span><br>model = sequential([<br>    Dense(units = <span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>)<br>])<br><span class="hljs-comment">#add parameter:from_logits=True </span><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br>logit = model(X)<br>f_x = tf.nn.sigmoid(logit)<br></code></pre></td></tr></table></figure><h3 id="MNIST-Adam"><a href="#MNIST-Adam" class="headerlink" title="MNIST Adam"></a>MNIST Adam</h3><h2 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h2><h3 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h3><p>Model fits the training data well(over-fit) but will fail to generalize to new examples not in the training set.</p><p>Hence, we need to <strong>partition the dataset</strong> to test the accurancy of the model.</p><p>The data set was divided into <code>test set</code> and <code>train set</code>.</p><p>【regression】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m_{train}}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{test}}\left[ \underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) -y</em>{test}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>compute train error:</p><p>$$<br>J_{train}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{train}}\left[ \underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) -y</em>{train}^{\left( i \right)} \right) ^2 \right]<br>$$</p><p>【train】</p><p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;-\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left[ y^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) \right] +\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2}<br>$$</p><p>compute test error:</p><p>$$<br>J_{test}&#x3D;-\frac{1}{m_{test}}\underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left[ y_{test}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) \right)<br>+\left( 1-y</em>{test}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{test}^{\left( i \right)} \right) \right) \right]<br>$$</p><p>compute train error:</p><p>$$<br>-\frac{1}{m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left[ y_{train}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) \right) +\left( 1-y</em>{train}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{train}^{\left( i \right)} \right) \right) \right]<br>$$</p><h3 id="Model-selection-and-cross-validation"><a href="#Model-selection-and-cross-validation" class="headerlink" title="Model selection and cross validation"></a>Model selection and cross validation</h3><p>If we want to fit a function to predict a problem or classification, we often use test error $J_{test}$ to judge the accurancy of the model. But, the J test is likely to be an <code>optimistic estimate</code> of generalization error. Because, when we choose the degree of parameter d in polynomial fit.,This fit of J test may lower than the actual estimate. The optimistic estimate can lead to a low score of J_test.</p><p>So, we need to partition the dataset as three parts to avoid optimistic estimate:</p><ul><li>training set</li><li>cross validation set</li><li>test set</li></ul><p>compute cross validation error:</p><p>$$<br>J_{cv}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{cv}}\left[ \underset{i&#x3D;1}{\overset{m_{cv}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{cv}^{\left( i \right)} \right) -y</em>{cv}^{\left( i \right)} \right) ^2 \right]<br>$$</p><h3 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h3><p>Have idea-Train model-Diagnose bias and variance</p><p> $J_{train}$ reflects bias, and  $J_{cv}$ reflects variance. A perfect model has low  $J_{train}$ and low  $J_{cv}$.</p><p>As the increasing of degree of d, $J_{train}$ will typically go down. Meanwhile, $J_{cv}$ will also go down and then it will increase.$J_{cv}$ will have a min value for different degree of d.</p><img src="/img/fig/4.6.jpg" alt="s" style="text-align: center;" /><p>How do you tell if our algorithm has a bias or variance problem?</p><ul><li>High bias(under fit): $J_{train}$ will be high($J_{train}\approx J_{cv}$)</li><li>High variance(over fit): $J_{cv}$&gt;&gt;$J_{train}$($J_{train}$ may be low)</li><li>High bias and High variance $J_{train}$ will be high and $J_{cv}$&gt;&gt;$J_{train}$</li></ul><h3 id="Regularization-and-bias-variance"><a href="#Regularization-and-bias-variance" class="headerlink" title="Regularization and bias&#x2F;variance"></a>Regularization and bias&#x2F;variance</h3><p>A large neutral network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_1 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&quot;sigmoid&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>model = Sequential([layer_1,layer_2,layer_3])<br></code></pre></td></tr></table></figure><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>When we increase the size of training set, the train error will increase. But, the error of cross validation will decrease. As the increasing of sample points. a regression function(a line or a curve) cannot fit all the point.</p><img src="/img/fig/4.7.jpg" style="text-align: center;" /><p>If a algorithm suffers from high variance, getting more training data is <strong>likely</strong> to help.</p><p>If a algorithm suffers from high bias, getting more training data <strong>will not</strong> help much.</p><p>【Debugging algorithm】</p><p>we have implemented the regularized linear regression:</p><p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p><table><thead><tr><th align="center">operation</th><th>what should do</th></tr></thead><tbody><tr><td align="center">get more training examples</td><td>fixes high variance</td></tr><tr><td align="center">try smaller sets of features</td><td>fixes high variance</td></tr><tr><td align="center">try getting additional features</td><td>fixes high bias</td></tr><tr><td align="center">try adding polynomial features</td><td>fixes high bias</td></tr><tr><td align="center">try decreasing  $\lambda$</td><td>fixes high bias</td></tr><tr><td align="center">try increasing  $\lambda$</td><td>fixes high variance</td></tr></tbody></table><p><code>Trade-off</code></p><p>Simple model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+b$) will get high bias VS complex model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+w_3x^3+w_4x^4+b$) will get high variance.</p><img src="/img/fig/4.8.jpg" style="text-align: center;" /><h3 id="Cycle-of-machine-learning"><a href="#Cycle-of-machine-learning" class="headerlink" title="Cycle of machine learning"></a>Cycle of machine learning</h3><p>The cycle of ML process:</p><img src="/img/fig/4.9.jpg" style="text-align: center;" /><img src="/img/fig/4.10.jpg" style="text-align: center;" /><p>How to apply the ML model to solve the actual problem in software engineering design?</p><img src="/img/fig/4.11.jpg" alt="s" style="text-align: center;" /><p>ML model is collected in the inference server. we use mobile app through API call to achieve these function.</p><h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><strong>Precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.</p><p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that were retrieved. </p><p>We design a <code>confusion matrix</code> to show it:</p><img src="/img/fig/4.12.jpg" alt="s" style="text-align: center;"/><p>In the trade-off  between Precision(P) and Recall(R), we use F1 score to evaluate the efficiency about the model.</p><p>the trade-off  between Precision(P) and Recall(R) has shown in the figure:</p><img src="/img/fig/4.13.jpg" style="text-align: center;" /><p>$$<br>F1&#x3D;\frac{1}{\frac{1}{2}\left( \frac{1}{P}+\frac{1}{R} \right)}&#x3D;\frac{2PR}{P+R}<br>$$</p><h2 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h2><p>The structure of a decision tree:</p><img src="/img/fig/5.1.jpg" style="text-align: center;" /><h3 id="Methods-chosen"><a href="#Methods-chosen" class="headerlink" title="Methods chosen"></a>Methods chosen</h3><p>For Signal Decision tree, we should focus on the problem is that the data features.</p><p>If the data is  discrete(just like 0 or 1), we can build the Signal Decision tree model. But,a row data may includes more than two classes, in this situation we should use <code>one-hot encoding</code>.</p><blockquote><p>one-hot encoding only fit for the decision tree model.</p></blockquote><p>If the data has continuous data(not only just like 0 or 1), we should split on a continuous variance.</p><p>For Multiple trees, we can use <strong>Random Forest</strong> and <strong>XGboost</strong> algorithm to solve.</p><img src="/img/fig/5.3.jpg" style="text-align: center;" /><h3 id="Purity-entropy"><a href="#Purity-entropy" class="headerlink" title="Purity(entropy)"></a>Purity(entropy)</h3><p>$p_{1}$ &#x3D; fraction of examples that are True.</p><img src="/img/fig/5.4.png" style="text-align: center;" /><p>$$<br>H\left( p_1 \right) &#x3D;-p_1\log \left( p_1 \right) -p_0\log \left( p_0 \right)<br>\<br>&#x3D;-p_1\log \left( p_1 \right) -\left( 1-p_1 \right) \log \left( 1-p_1 \right)<br>$$</p><img src="/img/fig/5.5.jpg" style="text-align: center;" /><p>In this figure, $w^{left}$&#x3D;2&#x2F;5、 $w^{right}$&#x3D;3&#x2F;5、$p_{1}^{left}$&#x3D;5&#x2F;10、$p_{2}^{left}$&#x3D;5&#x2F;10.</p><p>Information Purity</p><p>$$<br>Information Purity &#x3D;H\left( p_{1}^{root} \right) -\left( w^{left}H\left( p_{1}^{left} \right) +w^{right}H\left( p_{1}^{right} \right) \right)<br>$$</p><p>We should choose the <code>max</code> value of the Information Purity to <strong>recursive</strong> the decision tree model, which is called <code>Information Gain</code>.</p><p>In the process of split on a continuous variance(<strong>Regression tree</strong>), we also choose the max decreasing variance result as a good fit model.</p><img src="/img/fig/5.2.jpg" alt="s" style="text-align: center;"/><p>The purity of regression tree(equal to information gain):</p><p>$$<br>D&#x3D;V^{root}-\left( w^{left}V^{left}+w^{right}V^{right} \right)<br>$$</p><p>V instead of <code>variance</code>.</p><h3 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h3><ul><li><p>Start with all examples at the root node</p></li><li><p>Calculate information gain for all features, and pick the one with the highest information gain</p></li><li><p>Split dataset according to selected features, and create left and right branches of the tree</p></li><li><p>Keep repeating splitting process until stopping criteria is met:</p><pre><code class="hljs">      <figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">when a node <span class="hljs-keyword">is</span> <span class="hljs-number">100</span>% one <span class="hljs-keyword">class</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">splitting</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">will</span> <span class="hljs-symbol">result</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">tree</span> <span class="hljs-symbol">exceeding</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">maximum</span> <span class="hljs-symbol">depth</span><br><br><span class="hljs-symbol">Information</span> <span class="hljs-symbol">gain</span> <span class="hljs-symbol">from</span> <span class="hljs-symbol">additional</span> <span class="hljs-symbol">splits</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">less</span> <span class="hljs-symbol">than</span> <span class="hljs-symbol">threshold</span><br><br><span class="hljs-symbol">when</span> <span class="hljs-symbol">number</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">examples</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">below</span>  <span class="hljs-symbol">a</span> <span class="hljs-symbol">threshold</span><br></code></pre></td></tr></table></figure></code></pre></li></ul><h2 id="Decision-tree-VS-Neutral-network"><a href="#Decision-tree-VS-Neutral-network" class="headerlink" title="Decision tree VS Neutral network"></a>Decision tree VS Neutral network</h2><h3 id="Decision-tree-1"><a href="#Decision-tree-1" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul><li>Works well on tabular(structured) data</li><li>Not recommended for unstructured data(images,audios,text)</li><li>Small decision tree may be human interpretable</li></ul><h3 id="Neutral-network"><a href="#Neutral-network" class="headerlink" title="Neutral network"></a>Neutral network</h3><ul><li>Works well on all types of data,including tabular(structured) data and unstructured data(images,audios,text)</li><li>May be slower than decision tree</li><li>Works with transfer learning</li><li>When building a system of multiple models working together, it might be easier to string together multiple neutral network</li></ul><h2 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM-支持向量机"></a>SVM-支持向量机</h2><h2 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h2><p>L1、L2范数</p><p>正则化</p><p>归一化</p>]]></content>
    
    
    <categories>
      
      <category>人工智能基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>本科毕业论文致谢</title>
    <link href="/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
    <url>/2024/08/06/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>时光匆匆，转眼间四年的本科生活即将接近尾声。在此，请允许我以一篇致谢，表达在这四年时光里最真挚的谢意！</p><p>感谢论文指导老师杨国庆教授的悉心指导。从2021年参加数学建模比赛与老师结缘，杨老师严谨的治学态度和科学的研究方法给了我极大的帮助和影响，由衷感谢杨老师对我的关心和指导！</p><p>同时，由衷感谢教过我的河北大学国际学院信管专业全体ISEC教师：杨秀丹老师、吴树芳老师、徐杰老师、史海燕老师、郝杰老师、史江兰老师、贾金英老师、宇文姝丽老师、郭海玲老师、崔广志老师、吴利明老师、韩倩老师、陈婷老师、张鑫老师(排名不分先后)，以及其他全部科任老师。</p><p>感谢我的教练吕旭老师。大一期间将我纳入冰雪运动队，从此便与体育运动结下不解之缘。吕旭老师积极的心态与干练的作风对我产生了深远影响。从教练身上，我也更加懂得了什么是责任，什么叫标杆。</p><p>感谢我的辅导员曹永姝老师。四年的本科生活，曹老师对我照顾有加，老师的隐忍与坚持对我产生了很大的影响。她了解我的脾气与性格，尊重我的选择。</p><p>感恩在河北大学遇到的老师们，师恩天大，永记心间。</p><p>感谢我的父亲杨海龙先生、母亲刘秋菊女士。二十四年的成长之路，你们敢于放手，让我自由生长、大胆试错。每当我面临人生抉择的时候，你们都是那样义无反顾地支持我的选择。</p><p>感谢我的舅舅刘运陶先生，他对于我的成长是特别的。舅舅是我们家族中的第一位本科生、研究生，他潜移默化地培养了我的阅读习惯、学习习惯，并用他四十多年走过的弯路为我规避错误，他一直是我的榜样。</p><p>感谢我的姥姥邵长芬女士、姥爷刘增才先生。从四岁上幼儿园开始我便离开父母跟随姥姥、姥爷生活，直到我十八岁离开小镇去沧州市第一中学读高中。十四年的时光大部分都是和姥姥、姥爷度过的，姥姥、姥爷的勤劳、隐忍、坚韧、厚道对我的性格塑造影响至深。感谢我的奶奶刘绪巧女士、爷爷杨春林先生。2017年的仲夏，我二叔的车祸离世对两位老人以及全家人造成了沉痛打击，我曾一度认为两位老人会无法走出晚年丧子之痛。但是时过境迁，二老以顽强不屈、坚忍不拔的精神面貌给儿孙们以希望。</p><p>感恩我的家人们，让我有足够的勇气去面对生活中的任何挫折与苦难。</p><p>最后，感谢这四年的自己。前路漫漫，未来还会有很长的路要走，还会有更多的挑战、磨难需要去面对，还会有更多的责任需要去承担。会遇到很多人，会经历更多的事。但是无论怎样，请不要丢掉良心和理想，都不要忘记抽时间回忆回忆在河北大学这四年的美好时光。加油，祝好！</p><p>感谢百忙之中参加答辩的各位领导、老师们！</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell学习笔记</title>
    <link href="/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/08/05/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量的分类：</p><ul><li>用户变量：用户自己定义的变量</li><li>系统变量：系统已经定义的变量，在整个Linux系统中起作用</li><li>特殊变量</li></ul><p>变量的类型：</p><ul><li>字符串类型</li><li>数字类型</li></ul><p>变量定义的格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">变量名=变量值 #注意=左右两边不可以有空格<br><span class="hljs-meta prompt_">#</span><span class="language-bash">直接赋值</span><br>username=&quot;ygm&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">键盘赋值</span><br>read username<br><span class="hljs-meta prompt_">#</span><span class="language-bash">执行的命令结果赋值</span><br>str=$(pwd)<br>str=$(ll)<br>str=`ps -ef`<br></code></pre></td></tr></table></figure><p>变量的访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>echo $name<br>echo $echo&#123;name&#125;<br></code></pre></td></tr></table></figure><p>特殊变量</p><table><thead><tr><th>变量名</th><th>定义</th></tr></thead><tbody><tr><td>$#</td><td>命令行参数的个数</td></tr><tr><td>$?</td><td>前一个命令或函数的返回码</td></tr><tr><td>$n</td><td>$1表示第一个参数</td></tr><tr><td>$0</td><td>当前程序的名称</td></tr><tr><td>$*</td><td>以“参数1，参数2···”保存所有参数</td></tr></tbody></table><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>结论：推荐编程的时候使用双引号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">str=hello<br>str=`hello`<br>str=&quot;hello&quot;<br><br>str2=&#x27;I am $&#123;str&#125;&#x27;#单引号不会解释字符串里面的变量<br>str2=&quot;I am $&#123;str&#125;&quot;#双引号可以解释字符串里面的变量<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出字符串的长度</span><br>echo $&#123;#name&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">提取子字符串</span><br>echo $&#123;string:a:b&#125;#从索引(索引从0开始)为a个位置开始截取长度为b的子字符串<br></code></pre></td></tr></table></figure><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><p>算数运算在shell中要遵守严格的规范格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo `expr 2 + 3`#必须是反引号包裹、加号与数字之间留有空格<br>echo `expr 2 \* 3`#注意乘法和除法必须使用转义符号\前缀才会起作用<br>echo `expr 2 \% 4`<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此外，算数运算还可以用$(())和$[]表示-推荐方式</span><br>a=2<br>b=3<br>echo $((a+b))<br>echo $(($a+$b))<br>echo $[a+b]<br>echo $[$a+$b]<br></code></pre></td></tr></table></figure><h3 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h3><p>数字比较</p><ul><li>-eq：比较两个数是否相等，相等返回true</li><li>-ne：比较两个数是否不想等，不想等返回true</li><li>-gt：检测左边的数是否大于右边，若是返回true</li><li>-lt：检测左边的数是够小于右边，若是返回true</li><li>-ge：检测左边的数是否大于等于右边，若是返回true</li><li>-le：检测左边的数是否小于等于右边，若是返回true</li></ul><p>字符串比较：</p><ul><li>-z STRING：字符串长度为0</li><li>-n STRING：字符串长度不为0</li><li>&#x3D;：判断字符串长度是否相等</li><li>！&#x3D;：判断字符串长度是否不想等</li></ul><p>文件：</p><p>-f：存在且普通的文件</p><p>-e：文件存在</p><p>-d：存在且是目录</p><p>-h：存在且是链接</p><p>-r：存在且是只读</p><p>-w：存在且是可写</p><p>-x：存在且是可执行</p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组用小括号表示，中间元素用空格隔开，也可以直接定义数组中的每个元素的值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">array=(1 2 &quot;hello&quot; ygm)<br>array[4]=&quot;xyc&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取数组元素</span><br>echo $&#123;array[index]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">读取整个数组</span><br>echo $&#123;array[*]&#125;<br>echo $&#123;array[@]&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">获取数组长度</span><br>echo $&#123;#array[*]&#125;<br><br></code></pre></td></tr></table></figure><h3 id="shell命令"><a href="#shell命令" class="headerlink" title="shell命令"></a>shell命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">逻辑运算符&amp;&amp;和||<br><br>&amp;&amp; 表示与，|| 表示或<br>二者具有短路原则：<br>expr1 &amp;&amp; expr2：当expr1为假时，直接忽略expr2<br>expr1 || expr2：当expr1为真时，直接忽略expr2<br>表达式的exit code为0，表示真；为非零，表示假。（与C/C++中的定义相反）<br><br></code></pre></td></tr></table></figure><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><p>if判断语句范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">if condition<br>then <br>···<br>else<br>···<br>fi<br><br>if[ &quot;a&quot; -lt &quot;b&quot; ] &amp;&amp; [ &quot;a&quot; -gt 2]<br>then <br>echo $&#123;a&#125;在范围内<br>fi<br><br>if[ $a -eq 2]<br>then <br>echo $&#123;a&#125;等于2<br>elif [ $a -eq 3]<br>then<br>echo $&#123;a&#125;等于3<br>else<br>echo 其他<br>fi<br></code></pre></td></tr></table></figure><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><p>for范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">for v in var1 var2 var3<br>do<br>echo $v<br>done<br><br>for flie in `ls`<br>do <br>echo $file<br>done<br><br>for i in $(seq 1 10)<br>do<br>echo $i<br>done<br><br>for ((i = 1;i&lt;10;i++))<br>do<br>echo $i<br>done<br></code></pre></td></tr></table></figure><p>while循环范式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">while read name<br>do <br>echo $name<br>done<br><br>until [&quot;$&#123;word&#125;&quot; == &quot;yes&quot;] || [&quot;$&#123;word&#125;&quot; == &quot;YES&quot;]<br>do <br>read -p &quot;please input yse or YES to stop this program:&quot; word<br>done<br></code></pre></td></tr></table></figure><p>PS3使用方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;what is your favourite OS?&quot;<br>PS3=&quot;please enter your chose:&quot;<br>select var in &quot;linux&quot; &quot;windowns&quot; &quot;unix&quot;<br>do<br>break;<br>done<br>echo &quot;you have selected $var&quot;<br></code></pre></td></tr></table></figure><h2 id="一键安装JDK"><a href="#一键安装JDK" class="headerlink" title="一键安装JDK"></a>一键安装JDK</h2><p>使用shell脚本实现自动化部署</p><p>jdk_install.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">提示安装jdk</span><br>echo &quot;开始安装jdk&quot;<br>sleep 1<br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除自带的jdk</span><br>oldjdk=$(rpm -qa | grep jdk)<br>for old in $&#123;oldjdk&#125;<br>do<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-variable">$old</span></span><br>rpm -e --nodes $old<br>done<br><span class="hljs-meta prompt_">#</span><span class="language-bash">创建安装目录</span><br>java_path=$(/export/server)<br>if[ !-d $java_path]<br>then<br> mkdir -p $java_path<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">解压jdk安装包</span><br>tar -zxvf /export/softwore/jdk-8u241-linux-x64.tar.gz -C $java_path<br><span class="hljs-meta prompt_">#</span><span class="language-bash">设置环境变量</span><br>JAVA_HOME=&quot;/export/server/jdk1.8.0_241&quot;<br>grep &quot;JAVA_HOME&quot; /etc/profile<br>if[ #? -ne 0]<br>then<br><span class="hljs-meta prompt_">#</span><span class="language-bash">JAVA_HOME</span><br>echo &quot;---------JAVA_HOME-----------&quot;<br>echo `export JAVA_HOME=/export/server/jdk1.8.0_241` &gt;&gt; /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">PATH</span><br>echo &quot;----------PATH-----------&quot;<br>echo `export PATH=:$JAVA_HOME/bin:$PATH` &gt;&gt; /etc/profile<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">加载环境变量</span><br>sleep 1<br>source /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装完成提示</span><br>echo &quot;恭喜您jdk安装成功！&quot;<br>java -version<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>云计算</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cloud computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/08/05/hello-world/"/>
    <url>/2024/08/05/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! </p><p>值得纪念的日子，今天把之前的博客内容全部成功迁移到了新的平台上面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>  cout &lt;&lt; welcome to hexo! &lt;&lt; endl;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>归家小记</title>
    <link href="/2024/04/24/%E5%BD%92%E5%AE%B6%E5%B0%8F%E8%AE%B0/"/>
    <url>/2024/04/24/%E5%BD%92%E5%AE%B6%E5%B0%8F%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>2024.04.13-2024.04.20</p><p>发小结婚，正好毕设初稿交上了，给自己放了一周的假期，回家搂席，调整一周。</p><p>返校回去的一个多月，自己明显懒惰了许多，每天都是睡到八点半，晚上也睡的很晚，生活并不规律。回家后的一个最明显的特征就是回来以后懒觉睡的少了。晚上十一点睡觉，早晨七点起床，睡的很踏实。</p><p>回家这几天，早起的习惯逐渐调节回来了，我希望自己回学校以后还能够继续保持好习惯。早上七点二十之前务必起床，晚上十一点半前务必睡觉。</p><p>周六晚上回来后，去爷爷家做了很久，和爷爷奶奶聊了很长时间的天；周一到周五在杜生陪姥姥姥爷了很长时间，感觉很幸福、很充实！</p><p>周日下午和桂一聊了一会，孩子正处在叛逆期，净和她妈妈对着干，上周在学校和老师发生了矛盾，到家后把自己关在屋子里不出来。其实想想自己当年那会，也是这股子劲头。为了二叔，我觉着，我作为大哥的有责任去教导它，开导他。聊天的最后，我给他写了八个大字：不要抱怨、不要逃避。希望他能记住我说的这几个字吧，算是过来人的教训。</p><p>周日晚上三叔请客吃饭，喝了不少啤酒，喝的肚子挺难受的。客人走后，三叔哭了，这应该我今生第一次看见三叔在我面前哭泣。他这个人，一生太过于小心翼翼，内心的自卑使他无法正常和亲人沟通感情，这是原生家庭和成长经历带给他的。三叔一定背负着太多的压力与重担，那一刻我想也是稍微的解脱，在大侄子面前卸下了平时严肃的面孔，终于吐露了心声。跟我说的最多的，也是当年我父亲跟我说的最多的：要努力。</p><p>爷爷一会也来了，我们爷三个，难得的机会坐在了一起。没有想象中的那么敞开心扉的畅聊，更多的有的却是沉默。婶婶说了一句幽默的话：小儿子，大孙子，老爷爷的命根子。</p><p>周二参加了好朋友的婚礼，快有5、6年没有正经参加过别人的婚礼了。现场的感觉真是不一般，说实话，很感动，想结婚哈哈。</p><p>周三晚上和俩挚友聚餐，三个人中今年只有我今年幸运保送研究生了，他们两个考研全部失败了。我一点没有沾沾自喜，跟多的是感同身受。三个人、三条道路、三种人生。那晚喝了点白酒，我们聊了很长时间的天。。。</p><p>现在是周五晚上了，想着赶紧把一周归家小记赶紧写出来。因为我知道，如果我再拖着，我肯定就不会再写了…</p><p>周六返校，周日又正式开工了。回去以后少拖延，少睡觉，多写代码，多看书，提升技术！</p><p>道路是曲折的，前途是光明的。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>湾区游记</title>
    <link href="/2024/02/23/%E6%B9%BE%E5%8C%BA%E6%B8%B8%E8%AE%B0/"/>
    <url>/2024/02/23/%E6%B9%BE%E5%8C%BA%E6%B8%B8%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>自驾一路向南，澳门、珠海、佛山、广州</p><p>2024.02.16-2024.02.22</p><p><img src="/img/life/zhuhai.jpg"></p><p>和老舅一家自驾大湾区(广州、珠海、佛山、澳门)。总体收获很大，一路上经历丰富，有高速堵车、昼夜奔袭、度假休息、患病感冒。这次出行，属实是把自己的身体耐造性提高性了一大截，不过代价就是回来之后感染了 新冠嗓子疼病毒 并且身体变虚了。。。开学晚回去几天，把身体调整调整，休整好身体之后再出发。</p><p><img src="/img/life/aomen.jpeg"></p><p>大湾区气候感觉还是潮湿，特别是珠海。一路上的风景也是很美的，我国的经济特区珠海、横琴，特别行政区澳门，还有超级都市群广州、佛山都特别开眼界。以前总觉得湖北、湖南地区属于我国的经济发达地区，总之要比我们河北省好。但是经过这次的实地调查后才发现，原来湖北、湖南大部分地区都依旧还是非常贫困的山区。高速公路也是非常不好走，只有双向4车道，特别是经过湖南地区的隧道的时候，几乎每年都要堵车。围绕着长沙的几个小城市还都是不错的，湘潭、株洲市都很干净，衡阳也可以。回来之后江西境内的高速都要顺畅很多，而且总体体验要比湖北、湖南地区丝滑很多。地势也没有绵延起伏，赣州以南有很多高山，赣州以北又有很多的平原。河南是个人口大省，人口密度大到难以想象，进入高速后根据车流量就可以判断出来。这片中原大地是我国的大粮仓，一路上除华北平原外少有的大平原开阔地带。</p><p><img src="/img/life/duche.jpeg"></p><p>这一路上和老舅、舅妈聊了很多，家庭、学业、事业。。。原来老舅和舅妈大二才确定关系、原来老妈年轻的时候并没有那么有自信和光环、原来我一直有偏见的LMJ在老舅眼里被降维打击般的望眼欲穿。老舅也和自己分享了很多学业、事业上的经历，对于<strong>顺境和逆境的分析</strong>、<strong>对于名和利的取舍</strong>、<strong>对于变通的处世方法</strong>。他将他自己宝贵的经验分享给了我，自己一定要认真复盘。</p><p>大湾区之旅，一生铭记，并且未完待续….</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>毛选及相关</title>
    <link href="/2023/12/26/%E6%AF%9B%E9%80%89%E5%8F%8A%E7%9B%B8%E5%85%B3/"/>
    <url>/2023/12/26/%E6%AF%9B%E9%80%89%E5%8F%8A%E7%9B%B8%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<div class="note note-danger">            <p>今日欢呼孙大圣，只缘妖雾又重来</p>          </div><p><img src="/img/life/mao.webp"></p><h1 id="唯物辩证法的基本规律"><a href="#唯物辩证法的基本规律" class="headerlink" title="唯物辩证法的基本规律"></a>唯物辩证法的基本规律</h1><p>《大众哲学》——艾思奇</p><p>唯物主义者必须让自己养成一种习惯：遇到任何事情、任何问题，都能够认真地做系统周密的调查研究，把事物本身的规律寻找出来，然后按照规律去办事，去解决问题，去指导工作，使事情办的好，问题得以顺利解决，工作做的有成绩，不至于碰钉子，或者至少不碰大钉子。</p><p>不如意和碰钉子的原因是因为我们的主观意愿和事物的客观规律不符合！</p><p>世界上一切事物，是在我们的主观心意之外独立存在着的，它们的发展变化情况不是依赖我们的心意为转移的。世界上的事情，绝对不是我们想要怎样就怎样的，相反的，常常是我们希望这样，而偏偏事实上就不会这样。考试失利、投资失败、分手失恋、失业…</p><p>“凡事不会无缘无故的发生”，这就是“无风不起浪”这一个成语包含的最主要的哲学思想，这种思想在每个人的头脑中普遍存在着，其实每个人都相信，一切事物的发生，它的出现总有一定的原因，一定道理。一切事物不会凭空产生和消失，任何事物都有它发生、发展和没落的过程。</p><h1 id="实践论"><a href="#实践论" class="headerlink" title="实践论"></a>实践论</h1><p>用联系和运动的观点看待事物的发展</p><h1 id="矛盾论"><a href="#矛盾论" class="headerlink" title="矛盾论"></a>矛盾论</h1><p>对立统一</p><p>在研究一切问题的时候，注意到事物发展的曲折性，不要把事物的发展，看作一条直线上升，而没有任何迂回、后退、停滞的形式，也不要把任何事物的发展看作是简单的循环，而不注意它由低级到高级的上升。事物的发展是曲折的，螺旋式上升的。</p><h2 id="战略战术分析"><a href="#战略战术分析" class="headerlink" title="战略战术分析"></a>战略战术分析</h2><p>中国红色政权存在的原因</p><p>井冈山斗争</p><p>论持久战</p><p>抗日游击战争的战略问题</p><h1 id="预见力与原则"><a href="#预见力与原则" class="headerlink" title="预见力与原则"></a>预见力与原则</h1><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><p>1.独立自主</p><p>独立自主、自力更生，在此基础之上来尽可能多的争取外援。合理的看待外援也是一种辩证法。</p><p>只有坚持独立自主，自己把自己照顾好，让自己强大起来，才能帮助到父母和家人，才能真正帮助到朋友。否则都是假的。</p><p>白手起家还需要独立自主。</p><p>爱别人，首先爱自己。</p><p>让步一定是积极的而不是消极的，本质是为了更好的合作，同样，让步也是有一定界限的。</p><p>2.实事求是</p><p>实事——客观存在的一切事物</p><p>求——调查研究</p><p>是——客观事物的内部联系，即规律性</p><p>实事：研究一切问题都要从实际事物出发，而不是从理论、概念、观点和定义出发。</p><p>求是：研究事物的规律，总结事物的规律，然后用规律指导行动。</p><p>3.群众路线</p><p>群众路线就是不脱离不脱离社会实际，因为群众是构成社会实际最基础的基础。</p><h2 id="预见力"><a href="#预见力" class="headerlink" title="预见力"></a>预见力</h2><p>什么事预见力？就是将矛盾发展演变成种种趋势加以判断，然后对自己主观能动性加以判断，最后得出事物发展的最大可能性。并且把这种可能性和必然性指出来，辅助以相应的对策，最后取得胜利。</p><p>在复杂的事物也是由简单事物的叠加组成的，提高预见力与洞察力，本质上就是需要我们擅长事物的矛盾分析和条件分析。</p><h1 id="成事的科学原理"><a href="#成事的科学原理" class="headerlink" title="成事的科学原理"></a>成事的科学原理</h1><p>分析清楚事物的矛盾，通过调查研究得出干这件事需要哪些条件，然后确定现在是否时机已经成熟们是否具备这样的条件，之后就是发挥主观能动行去创造条件，最终把事情尽可能的按照自己想的那样去办成。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>张宇智慧箴言</title>
    <link href="/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/"/>
    <url>/2023/10/14/%E5%BC%A0%E5%AE%87%E6%99%BA%E6%85%A7%E7%AE%B4%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>宝剑锋从磨砺出，梅花香自苦寒来</p>          </div><h2 id="别害怕"><a href="#别害怕" class="headerlink" title="别害怕"></a>别害怕</h2><p>不要试图掌控自己的奋斗过程，完美自己的各项计划，你会失落的，因为事实上它们并不听你的，你越跟它较劲，它越不听话。<br>不过这没什么，每一个成功，都是源于一段不完美的甚至是很狼狈的奋斗。<br>你只要，摈弃杂念，放下功利，全神贯注，尽力就好，学不完又怎样，考的上就行。好孩子，听话。</p><h2 id="拼搏"><a href="#拼搏" class="headerlink" title="拼搏"></a>拼搏</h2><p>年轻的时候，闯一闯，拼一拼，不要怕什么。实在扛不住了，哭一哭，喊一喊。大不了，回家。<br>不要把感情和情绪放在社会上、网络上，真正值得你在乎的，是表面上希望你出人头地，对你百般挑剔，但在心中却只为你祈求平安健康快乐的家人。<br>孩子们，听话，你逼自己的方法，可能不太对。放下包袱，放下功利，带着爱，带着对这个世界的好奇，去拼搏一下。累了，咱们回家就是了。<br><strong>健健康康，内心阳光</strong>，这八个字，可别弄丢了，丢了，会悔恨终身的。</p><h2 id="最大的敌人是自己"><a href="#最大的敌人是自己" class="headerlink" title="最大的敌人是自己"></a>最大的敌人是自己</h2><p>笛卡尔说，征服你自己，而不是征服世界。孩子们，晚安。</p><h2 id="嗜欲深者天机浅天机浅"><a href="#嗜欲深者天机浅天机浅" class="headerlink" title="嗜欲深者天机浅天机浅"></a>嗜欲深者天机浅天机浅</h2><p>古有云，<strong>无欲则刚</strong>。心思繁杂，欲望太多，则易焦虑，徘徊，无己见，恐非做事之正道。孩子们，务必静心，放下功利，用心为之，方可成之。</p><h2 id="基本功"><a href="#基本功" class="headerlink" title="基本功"></a>基本功</h2><p>有人问基础阶段以什么标准来看自己是不是真的懂了，这个问题我前面也回答过，这里详细说几句。我在课上提到一个人，叫做费曼，他是诺贝尔物理学奖获得者，人们试图研究费曼为什么对问题的理解总是透彻深刻，深得其精髓要义，事实上，费曼自己提出过一个观点：把你所学到的理解到的概念性质和方法讲给一个比如小孩子听（这里是为了确保聆听的人对你所讲述的知之甚少），而且让他尽量听懂，这时你无法用那些复杂的专业性的表达来叙述，在寻找简单通俗易懂的语言的时候，你的大脑在不断的深化对于知识的认识，迫使自己在更深层次上理解它们，这样你一定会停滞在很多地方，而这些地方就是你没有理解的地方。通过努力，把你的知识传递给这个孩子并让他听懂，就是你真的懂了，这就是过关。<br>我前面说到，你一定要会复述我课上对知识的讲解，甚至比我讲的还要简要精炼，这是你迅速融入这个学科思维状态的最佳途径。各位，我再写一会书，你们好好思考下我上面这段话，我是真的希望你们能够懂得如何学习，不要再功利的去卷了，真学东西，学真东西，学会怎么学，不仅为了考研，更为了自己的一生。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高等数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>前路漫漫亦灿灿</title>
    <link href="/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/"/>
    <url>/2023/09/29/%E5%89%8D%E8%B7%AF%E6%BC%AB%E6%BC%AB%E4%BA%A6%E7%81%BF%E7%81%BF/</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>不要辜负这份好运气——写在顺利推免后读任正非先生<a href="https://baike.baidu.com/item/%E6%88%91%E7%9A%84%E7%88%B6%E4%BA%B2%E6%AF%8D%E4%BA%B2/6610022">《我的父亲母亲》</a>有感</p>          </div><p>2023年9月29日，下午14:50，故事开始了新的篇章。</p><p>从2023年2月15日离家返校，一直到10月1号，整整228天，家门未踏入一步。一直想带一个好的结果回家，经历磕磕绊绊，最终如愿以偿。</p><p>想说的东西、想表达的东西有好多，但却难以启齿、无从下笔。</p><p>回首这段岁月，感谢自己没有放弃自己，感谢暑假留校踏踏实实读书学习的沉淀。暑假留校两个月，自己在图书馆五楼靠窗户占了一整张桌子，五楼人烟稀少，八月份安安静静在那里度过了一个月充实的时光。早上八点半开馆准时到达，晚上在漆黑的五楼写读书笔记。如今再进入五楼，往事历历如过眼云烟般浮现在眼前。上一次有这种感觉，还是因为19年底口罩事情高三开学延期，20年初居家复习的岁月。这两段时光交织在一起，给我的反思就是，要想做成一件事情，首先要把自己调整成静音模式，踏踏实实忘我般的投入进去。想成事，不是每天炸炸呼呼，一天八条朋友圈，恨不能别人不知道我干啥。而是要沉淀、要安静、要忍耐、要坚持。</p><p>人性的弱点之一就是<strong>只看结果，不注重过程</strong>。</p><p>你考上研了，考上公了，周围都是欢呼、赞美之词。”牛逼、大佬、交给朋友吧…”诸如此类的话，会让你沉迷享受，会让你飘飘然忘乎所以。但是，当你没考上、没惊起波澜，周围人并不会关注，甚至会有讥讽、嘲笑不绝于耳。”他不行，他光玩，他就不是那块料…”。所以，自强不息、厚德载物，永远都是法宝，让自己强大起来，让自己内心强大起来，才会在面对这些情况的时候，做到多一份的从容、淡定与坦然。</p><p>带着好结果回家过节，家人们由衷地为我感到高兴、自豪。感谢家人，没有家人的理解与支持，我没有勇气走好今天的路。</p><p>我的父亲杨海龙，1976年出生于河北省沧州市一个普普通通的农村家庭。小时候学习成绩不错，凭借自己的努力一直读到了高中毕业。中考成绩很好，但是因为家里穷要继续供给我二叔、三叔读书。于是，父亲放弃了去县里读高中的机会，选择了在镇上完成学业，很多年前，他仍然自嘲:“以我当年的成绩，要是去县里上，考过河北工业绰绰有余”。高中毕业后在我二爷家工厂里工作，然后和爷爷、叔叔们创业。</p><p>我的母亲刘秋菊，1978年出生于河北省沧州市一个普普通通的农村家庭。小的时候学习成绩一般，肯定没有我父亲学习好，想考中专，结果也是为了弟弟妹妹的学业而放弃了自己的梦想。初中毕业后进入农村信用社当会计，后来在我姥爷的厂子里当会计，一干就是半辈子。母亲是一个聪明的人，积极、阳光、热爱学习，她算账、算数没出过错，爱写读书笔记，有阅读习惯，这些都润物细无声般影响着我。</p><p>两个有着相同命运路径的人走到了一起，养育了我、培养了我。特别是，今年因为面试忙于各个城市奔波，报辅导机构，发paper。以上种种，当我需要钱的时候，老父亲总是那么斩钉截铁地转给我，让我没有任何顾虑。钱是好东西，而且钱是干净的，脏的是人心。走正道，好好赚钱、多赚钱永远都是幸福的。</p><p>这些年，二叔的去世一直也是我前进的动力。我很少和外人去谈我二叔因为车祸去世的事情，我认为这是家族之殇。我作为家族中的长子长孙，在我17岁那年，他的死，使我对我的家庭、对我自己有了新的理解。挫折都是发人深省的，我庆幸自己没有被生活的重创打垮，我也庆幸，家里的老人没有因为生活的苦难而使他们丧失对生活的希望。在经历苦难之后，我看到的依然是他们积极的生活态度，没有怨天尤人、没有一蹶不振，永远都给子女树立了好的榜样。其实，这就是我遇到无数困难后，依然有勇气面对的底气来源。</p><p>今年还要感谢张宇(宇爹)，不确定的日子里一直在通过备考高数舒缓。激发了我对数学的热情，重塑了学科的理解。</p><p>没有轻舟已过万重山，唯有前路漫漫亦灿灿。逝去的已经逝去，活着的仍要前行。</p>]]></content>
    
    
    <categories>
      
      <category>动机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>致谢</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
