

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Munger Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="Machine Learning-NotebookAndrew Ng-吴恩达&copy; Standford ONLINE &amp; DeepLearning.AI Mungeryang-杨桂淼总结     Introduction“Field of study that gives computers the ability to learn without being explicitly prog">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础">
<meta property="og:url" content="http://example.com/2024/08/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="munger写字的地方">
<meta property="og:description" content="Machine Learning-NotebookAndrew Ng-吴恩达&copy; Standford ONLINE &amp; DeepLearning.AI Mungeryang-杨桂淼总结     Introduction“Field of study that gives computers the ability to learn without being explicitly prog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/fig/1.1.png">
<meta property="og:image" content="http://example.com/img/fig/1.2.jpg">
<meta property="og:image" content="http://example.com/img/fig/1.3.png">
<meta property="og:image" content="http://example.com/img/fig/1.4.png">
<meta property="og:image" content="http://example.com/img/fig/2.1.jpg">
<meta property="og:image" content="http://example.com/img/fig/2.2.jpg">
<meta property="og:image" content="http://example.com/img/fig/2.3.jpg">
<meta property="og:image" content="http://example.com/img/fig/2.6.jpg">
<meta property="og:image" content="http://example.com/img/fig/cost1.png">
<meta property="og:image" content="http://example.com/img/fig/2.7.jpg">
<meta property="og:image" content="http://example.com/img/fig/2.8.jpg">
<meta property="og:image" content="http://example.com/img/fig/3.1.jpg">
<meta property="og:image" content="http://example.com/img/fig/3.2.jpg">
<meta property="og:image" content="http://example.com/img/fig/3.3.jpg">
<meta property="og:image" content="http://example.com/img/fig/3.4.jpg">
<meta property="og:image" content="http://example.com/img/fig/gridant1.png">
<meta property="og:image" content="http://example.com/img/fig/3.5.png">
<meta property="og:image" content="http://example.com/img/fig/4.2.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.4.png">
<meta property="og:image" content="http://example.com/img/fig/4.3.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.5.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.6.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.7.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.8.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.9.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.10.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.11.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.12.jpg">
<meta property="og:image" content="http://example.com/img/fig/4.13.jpg">
<meta property="og:image" content="http://example.com/img/fig/5.1.jpg">
<meta property="og:image" content="http://example.com/img/fig/5.3.jpg">
<meta property="og:image" content="http://example.com/img/fig/5.4.png">
<meta property="og:image" content="http://example.com/img/fig/5.5.jpg">
<meta property="og:image" content="http://example.com/img/fig/5.2.jpg">
<meta property="article:published_time" content="2024-08-09T14:57:55.000Z">
<meta property="article:modified_time" content="2024-08-10T13:22:54.492Z">
<meta property="article:author" content="Munger Yang">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/fig/1.1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>机器学习基础 - munger写字的地方</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/gundongtiao.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":75,"cursorChar":"_","loop":true,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Munger Yang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about" target="_self">
                <i class="iconfont icon-addrcard"></i>
                <span>主页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>博客</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/" target="_self">
                    <i class="iconfont icon-pen"></i>
                    <span>文章</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/archives/" target="_self">
                    <i class="iconfont icon-archive-fill"></i>
                    <span>总览</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/" target="_self">
                    <i class="iconfont icon-category-fill"></i>
                    <span>分类</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/tags/" target="_self">
                    <i class="iconfont icon-tags-fill"></i>
                    <span>标签</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/picture" target="_self">
                <i class="iconfont icon-image"></i>
                <span>时刻</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/self/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>简历</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/paper.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习基础"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-08-09 22:57" pubdate>
          2024年8月9日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          27 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习基础</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Machine-Learning-Notebook"><a href="#Machine-Learning-Notebook" class="headerlink" title="Machine Learning-Notebook"></a>Machine Learning-Notebook</h1><center>Andrew Ng-吴恩达&copy;</center>
<center>Standford ONLINE & DeepLearning.AI</center>
<center>Mungeryang-杨桂淼总结</center>




<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>“Field of study that gives computers the ability to learn without being explicitly programmed.”——Arthur Samuel(1995)</p>
<p>  Practical advice for applying learning algorithm</p>
<h2 id="Basic-conception-cookbook"><a href="#Basic-conception-cookbook" class="headerlink" title="Basic conception cookbook"></a>Basic conception cookbook</h2><p>Data set:</p>
<p>Training set:</p>
<p>Test set:</p>
<p>Cost function:</p>
<p>Gradient:</p>
<p>Gradient descent:</p>
<p>Recall:</p>
<p>Precision:</p>
<h2 id="Supervised-learning-监督学习"><a href="#Supervised-learning-监督学习" class="headerlink" title="Supervised learning-监督学习"></a>Supervised learning-监督学习</h2><p> Learns from being given “<strong>right answers</strong>(labels)”</p>
<img src="/img/fig/1.1.png" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>



<p>In all of these applications, we will first train our model with examples of inputs <strong>x</strong> and right answers, that is the labels <strong>y</strong>. After the model has learned from these input, output, or x and y pairs, they can take a brand new input x, something it has never seen before, and try to produce the appropriate corresponding output y.  </p>
<img src="/img/fig/1.2.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />



<p>  The task of the supervised learning algorithm is to produce more of these right answers based on labels.  </p>
<p>Classification is to predict categories,Regression is to predict a number. </p>
<img src="/img/fig/1.3.png" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />



<h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning-无监督学习"></a>Unsupervised learning-无监督学习</h2><p>Learns from being given “<strong>no-right answers</strong>(unlabeled data)”, data only comes with inputs x, but not output labels y. Algorithm has to find structure automatically in the data and automatically figure out whether the major types of individuals.</p>
<img src="/img/fig/1.4.png" srcset="/img/loading.gif" lazyload style="text-align: center;" />



<h2 id="Linear-Regression-with-one-variable"><a href="#Linear-Regression-with-one-variable" class="headerlink" title="Linear Regression with one variable"></a>Linear Regression with one variable</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Linear regression means fitting a <code>straight line</code> to the data. -&gt;linear regression  </p>
<img src="/img/fig/2.1.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>

<p>Regression model is to predict numbers</p>
<p>Classification model is to predicts categories</p>
<h3 id="Terminology-in-ML"><a href="#Terminology-in-ML" class="headerlink" title="Terminology in ML"></a><strong>Terminology in ML</strong></h3><p><code>Training dataset</code>-&gt;data used to train the model</p>
<p>$x&#x3D;$“input”variables feature</p>
<p>$y&#x3D;$“output ”variables or “target”variables</p>
<p>$(x,y)&#x3D;$single training example</p>
<p>$(x^{(i)},y^{(i)})&#x3D;i^{th}$ training example not exponent</p>
<img src="/img/fig/2.2.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>

<p>In the linear regression, we instantly believe the function is a linear function as follow:<br>$$<br>f_{w,b}(X)&#x3D;wX+b \<br>f(X)&#x3D;wX+b<br>$$<br>when we give a “x”as a input variable, we can get a “y-hat”variable as a result.</p>
<h3 id="triangular-ruler-Cost-function"><a href="#triangular-ruler-Cost-function" class="headerlink" title=":triangular_ruler:Cost function"></a>:triangular_ruler:Cost function</h3><p>squared error<br>$$<br>\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2<br>$$<br><img src="/img/fig/2.3.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" /></p>
<p>Model: $f_{w,b}(X)&#x3D;wX+b$.	$w(slope),b(intersects)$ are parameters.<br>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{w.b}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2 \<br>&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2<br>$$<br>Our goal is to minimize the parameters to fit the model:<br>$$<br>\underset{w.b}{\min}J\left( w,b \right)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#define cost function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    cost_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i]) ** <span class="hljs-number">2</span><br>        cost_sum += cost<br>    total_cost = (<span class="hljs-number">1</span>/(<span class="hljs-number">2</span>*m))*cost_sum<br>    <span class="hljs-keyword">return</span> total_cost<br></code></pre></td></tr></table></figure>



<h3 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent-梯度下降"></a>Gradient descent-梯度下降</h3><p><strong>Simultaneous update</strong> the parameters w and b until the cost function is <code>convergence</code>:</p>
<p>Simultaneous update the parameters is significant, we must focus on the order about the algorithm.</p>
<p>Correct order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>\<br>w&#x3D;tmp_w<br>\<br>b&#x3D;tmp_b<br>$$<br>Incorrect order:<br>$$<br>tmp_w&#x3D;w-\alpha \frac{\partial}{\partial w}J\left( w,b \right) \<br>w&#x3D;tmp_w \<br>tmp_b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left( w,b \right) \<br>b&#x3D;tmp_b \<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#compute gradient</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradient</span>(<span class="hljs-params">x,y,w,b</span>):<br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_dw = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_dw += dj_dw_i<br>        dj_db += dj_db_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></td></tr></table></figure>



<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>The choice of learning rate, alpha($\alpha$) will have a huge impact on the efficiency of our implementation of Gradient descent.<br>$$<br>w-\alpha \frac{\partial}{\partial w}J\left( w,b \right)<br>\<br>b-\alpha \frac{\partial}{\partial b}J\left( w,b \right)<br>$$<br><img src="/img/fig/2.6.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" /></p>
<p>If we chose the alpha is too large, the fit efficiency is not very well. We can design the algorithm to decrease the alpha($\alpha$) following by the w and b.</p>
<img src="/img/fig/cost1.png" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />



<h2 id="Multiple-linear-regression"><a href="#Multiple-linear-regression" class="headerlink" title="Multiple linear regression"></a>Multiple linear regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>【Model】<br>$$<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n<br>$$<br><code>Dot product(inner product)</code> of two vectors about $w$ and $b$.<br>$$<br>\vec{w}&#x3D;\left[ w_1,w_2···,w_n \right]<br>\<br>\vec{x}&#x3D;\left[ x_1,x_2···,x_n \right]<br>$$<br>$x_j&#x3D;j^{th} features$</p>
<p>$n &#x3D; $ numbers of features</p>
<p>$\vec{x}^{(i)}$ &#x3D; features of $i^{th}$ training example</p>
<p>$\vec{x}^{(i)}_j$ &#x3D; value of feature j in  $i^{th}$ training example</p>
<h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>We contrast the two process between vectorization and without vectorization.</p>
<p>without vectorization</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n):<br>    f = f + w[i]*x[i]<br>f = f + b<br></code></pre></td></tr></table></figure>

<p>If n is infinite, the algorithm is time consuming.</p>
<p>with vectorization</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>f = np.dot(w,b)<br></code></pre></td></tr></table></figure>

<p> Without vectorization, we just only use the for loop to calculate the results one by one. In the contrast, we can use the function numpy.dot() to calculate the result. Using numpy can decrease the time complexity and improve the algorithm efficiency. Numpy can use <code>parallel process</code> hardware to carry out the data.</p>
<p>Dot product of two vectors:<br>$$<br>a·b&#x3D;\underset{i&#x3D;0}{\overset{n-1}{\varSigma}}a_ib_i&#x3D;\left[ \begin{array}{l}<br>    ,, a_0\<br>    ,, a_1\<br>    ,,···\<br>    a_{n-1}\<br>\end{array} \right] \left[ \begin{array}{l}<br>    ,, b_0\<br>    ,, b_1\<br>    ,,···\<br>    b_{n-1}\<br>\end{array} \right] &#x3D;\left[ a_0b_0+a_1b_1+···+a_{n-1}b_{n-1} \right]<br>$$</p>
<h3 id="Gradient-descent-in-Multiple-regression"><a href="#Gradient-descent-in-Multiple-regression" class="headerlink" title="Gradient descent in Multiple regression"></a>Gradient descent in Multiple regression</h3><p>Parameters include $w_{1},w_{2},···w_{n}$ and $b$</p>
<p>Model is $f_{\vec{w},b}\left( \vec{x} \right) &#x3D;\vec{w}·\vec{x}+b&#x3D;w_1x_1+w_2x_2+···+w_nx_n+b$</p>
<p>Cost function is $J\left( w_{1},w_{2},···w_{n},b \right)$</p>
<p>Gradient descent:</p>
<pre><code class="hljs">repeat&#123;
</code></pre>
<p>$$<br>w_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p>
<p>$$<br>b&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p>
<p>}</p>
<p><code>for i in range(1,n)</code>:<br>$$<br>\frac{\partial}{\partial w_{1}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{1}^{\left( i \right)} \<br>\frac{\partial}{\partial w_{n}}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) x_{n}^{\left( i \right)} \<br>\frac{\partial}{\partial b}J\left(\vec{w},b \right)&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right)<br>$$<br>Normal equation:</p>
<ul>
<li>Only for linear regression</li>
<li>Solve for w,b without iterations</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Does not generalize to other learning algorithm</li>
<li>Slow when number of features is large(n&gt;10000)</li>
</ul>
<h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The technology of Features scaling will enable Gradient descent to run much faster.</p>
<p>Z-score normalization:<br>$$<br>x_i&#x3D;\frac{x_i-\mu}{\sigma}<br>$$<br>$\mu$ is the sample average value, and $\sigma$ is standard error of sample.</p>
<p>Aim for about $-1\leqslant x_j\leqslant 1$ for $x_{j}$.</p>
<p>The objective of Gradient descent in Multiple regression is:<br>$$<br>\underset{\vec{w}.b}{\min},,J\left(\vec{w},b \right)<br>$$<br><img src="/img/fig/2.7.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" /></p>
<p>As the calculate process, $y&#x3D;J\left(\vec{w},b \right)$ is decreased until a low score. If we can get the similar the result in the experiment, the cost function is <code>convergence</code> and we can find the min $J\left(\vec{w},b \right)$.</p>
<p>【<strong>Skill</strong>】</p>
<p>At the beginning, we can set the learning rate($\alpha$) in a small value like 0.001. As running the algorithm, we can update the parameter $\alpha$ gradually.(0.001-&gt;0.01-&gt;0.1)</p>
<p><strong>Feature Engineering</strong> : Using <code>intuition</code> to design new features, by transforming or combining original features.</p>
<blockquote>
<p>[Wiki]:<strong>Feature engineering</strong> or <strong>feature extraction</strong> or <strong>feature discovery</strong> is the process of extracting features (characteristics, properties, attributes) from raw data.</p>
</blockquote>
<img src="/img/fig/2.8.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>Except for using linear regression, we can also use <code>polynomial regression</code> to fit data. Through finding the segment of the data distribution(scatter), we can create special features($\sqrt{x}\quad x^3$) to fit data successfully.<br>$$<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+b\quad②<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+b\quad①<br>\<br>f_{\vec{w},b}\left( x \right) &#x3D;w_1x+w_2\sqrt{x}+b\quad③<br>$$</p>
<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>It turns out that, linear regression is not the good algorithm for classification. For classification, the output is not a continuous number. In the fact, it is a class variable like <strong>0(false-negative class)</strong> or <strong>1(true-positive class)</strong>.</p>
<img src="/img/fig/3.1.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />

<p>Sometimes, if we want to classify the output, the method of linear regression maybe not fit. Through the figure, the result of predict can be changed when we add a sample. The original fit result is the blue line, but, the new result is the green line. The standard of classification can be changed as the sample we adding.</p>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>In this chapter, we will learn a useful algorithm model——<code>Logistic Regression</code> to solve the classification problem.</p>
<p>With linear regression method, the model is $f_{\vec{w},b}\left(x^{i} \right) &#x3D;\vec{w}·x^{i}+b$, to predict $y$ given $x$. However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</p>
<p>This can be accomplished by using a <code>&quot;sigmoid function&quot;</code> which maps all input values to values between 0 and 1.</p>
<p>【sigmoid function】</p>
<img src="/img/fig/3.2.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>$$<br>g\left( Z \right) &#x3D;\frac{1}{1+e^{-Z}},0&lt;g\left( Z \right) &lt;1<br>\<br>f_{w,b}\left( X^{\left( i \right)} \right) &#x3D;g\left( w·X^{\left( i \right)}+b \right)<br>\<br>f_{\vec{w},b}\left( \vec{x} \right) &#x3D;g\left( \vec{w}·\vec{x}+b \right) &#x3D;\frac{1}{1+e^{-\left( \vec{w}·\vec{x}+b \right)}}<br>$$</p>
<p>We can calculate the Z-score to classify the predict value by the probability.</p>
<p>$$<br>P\left( y&#x3D;0 \right) +P\left( y&#x3D;1 \right) &#x3D;1<br>$$</p>
<img src="/img/fig/3.3.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;"/>

<p>$$<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;1|\vec{X};\vec{w},b \right)<br>\<br>f_{\vec{w},b}\left( \vec{X} \right) &#x3D;P\left( y&#x3D;0|\vec{X};\vec{w},b \right)<br>$$</p>
<p>Probability that $y&#x3D;1$  or $y&#x3D;0$, given input $\vec{x}$, parameters $w,b$.</p>
<p>If $y&#x3D;f_{\vec{w},b}\left( \vec{X} \right) &gt; 0.5$ -&gt; $g(Z)&gt;0.5$ -&gt; $Z&gt;0$ -&gt; $\vec{w}·\vec{x}+b &gt; 0$ &#x3D;&#x3D; YES $Y&#x3D;1$.</p>
<p>Else, NO $y &#x3D; 0$.</p>
<h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><img src="/img/fig/3.4.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />



<p>$$<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+b \right)<br>\<br>g\left( Z \right) &#x3D;g\left( w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b \right)\<br>boundary1&#x3D;w_1x_1+w_2x_2+b&#x3D;0<br>\<br>boundary2&#x3D;w_1x_1+w_2x_2+w_3x_1x_2+w_4x_{4}^{2}+b&#x3D;0<br>$$</p>
<h3 id="Cost-function-for-regularized-logistic-regression"><a href="#Cost-function-for-regularized-logistic-regression" class="headerlink" title="Cost function for regularized logistic regression"></a>Cost function for regularized logistic regression</h3><p>Cost function:</p>
<p>$$<br>J_{\left( w,b \right)}&#x3D;\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\frac{1}{2}\left( wx^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2&#x3D;L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right)<br>$$</p>
<p>Our goal is to minimize the parameters to fit the model:</p>
<p>$$<br>\underset{w.b}{\min},,J\left( w,b \right)<br>$$</p>
<p><strong>Simplified</strong> loss function:</p>
<p>$$<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;\begin{cases}<br>    -\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;1\<br>    -\log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) ,if,,y^{\left( i \right)}&#x3D;0\<br>\end{cases}\ \Longrightarrow \<br>L\left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) ,y^{\left( i \right)} \right) &#x3D;-y^{\left( i \right)}\log \left( f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) -\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\vec{w},b}\left( \vec{x}^{\left( i \right)} \right) \right)<br>$$</p>
<p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p>
<p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}  \sum_{i&#x3D;0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2<br>$$</p>
<p>where:</p>
<p>$$<br>f_{\mathbf{w},b}(\mathbf{x}^{(i)}) &#x3D; sigmod(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)<br>$$</p>
<p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p>
<p>$$<br>J(\mathbf{w},b) &#x3D; \frac{1}{m}\sum_{i&#x3D;0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]<br>$$</p>
<p>As was the case in linear regression above, the difference is the regularization term, which is   $\frac{\lambda}{2m}  \sum_{j&#x3D;0}^{n-1} w_j^2$ </p>
<p>Gradient descent:</p>
<p>repeat{</p>
<p>$$<br>tw_{j}&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;w_{j}-\alpha \frac{\partial}{\partial w_{j}}J\left(\vec{w},b \right)<br>$$</p>
<p>$$<br>tb&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(w_{1},w_{2},···w_{n},b \right)&#x3D;b-\alpha \frac{\partial}{\partial b}J\left(\vec{w},b \right)<br>$$</p>
<p>$$<br>w&#x3D;tw_{j}\<br>b&#x3D;tb<br>$$</p>
<p>}</p>
<img src="/img/fig/gridant1.png" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>To provide the overfitting problem, we apply the regularized method to add the penalty coefficient. That is why the we add the $\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}$ at the end of the formula.</p>
<img src="/img/fig/3.5.png" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />

<p><strong>Regularized</strong>:</p>
<p>$$<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}<br>\<br>J_{\left( \vec{w},b \right)}&#x3D;\frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( \vec{w}·\vec{x}^{\left( i \right)}+b-y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}w_{j}^{2}+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}b_{j}^{2}<br>$$</p>
<p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$.</p>
<p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;&#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\ \<br>$$</p>
<p>$$<br>\frac{\partial J(\mathbf{w},b)}{\partial b}  &#x3D; \frac{1}{m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})<br>$$</p>
<ul>
<li><p>m is the number of training examples in the data set      </p>
</li>
<li><p>$f_{\mathbf{w},b}(x^{(i)})$ is the model’s prediction, while $y^{(i)}$ is the target</p>
</li>
<li><p>For a  <span style="color:blue"> <strong>linear</strong> </span> regression model<br>  $f_{\mathbf{w},b}(x) &#x3D; \mathbf{w} \cdot \mathbf{x} + b$  </p>
</li>
<li><p>For a <span style="color:blue"> <strong>logistic</strong> </span> regression model<br>  $z &#x3D; \mathbf{w} \cdot \mathbf{x} + b$ \ $f_{\mathbf{w},b}(x) &#x3D; g(z)$<br>  where $g(z)$ is the sigmoid function:</p>
</li>
</ul>
<p>$$<br>g(z) &#x3D; \frac{1}{1+e^{-z}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#cpmpute cost function through itertion process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function</span>):<br>    w = copy.deepcopy(w_in)<br>    J_history = []<br>    p_history = []<br>    b = b_in <br>    w = w_in<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw,dj_db = gradient_function(x,y,w,b)<br>        <span class="hljs-comment"># update the parameters </span><br>        b = b - alpha * dj_db<br>        w = w - alpha * dj_dw<br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">100000</span>:<br>            J_history.append(cost_function(x,y,w,b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i % math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteraton <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>&quot;</span>,<span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> w,b,J_history,p_history<br></code></pre></td></tr></table></figure>





<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><h3 id="How-to-install-TensorFlow"><a href="#How-to-install-TensorFlow" class="headerlink" title="How to install TensorFlow"></a>How to install TensorFlow</h3><ul>
<li>conda install tensorflow</li>
<li>pip install tensorflow</li>
</ul>
<h3 id="How-the-brain-works"><a href="#How-the-brain-works" class="headerlink" title="How the brain works"></a>How the brain works</h3><p>The following picture shows the basic structure about neutral network. Like human&#96;s neutral cell, it passes information by layers, which every cell includes a logistic function.</p>
<img src="/img/fig/4.2.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />



<h3 id="Data-format-in-Tensorflow"><a href="#Data-format-in-Tensorflow" class="headerlink" title="Data format in Tensorflow"></a>Data format in Tensorflow</h3><p><code>matrix and tensor</code></p>
<p>Numpy, a standard library created in 1970s, is used to calculate linear algebra in python(data analysis). Tensorflow is a machine learning framework created by Google.</p>
<p>In Tensorflow, the input format must <strong>a matrix</strong>. We should focus on the special characteristic in our work.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<span class="hljs-comment">#一维数组</span><br>b = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<span class="hljs-comment">#一维矩阵</span><br>x = np.array([<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>    [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">14</span>],<br>])<span class="hljs-comment">#3X3矩阵</span><br>Z = np.matmul(A_in,W) + B <span class="hljs-comment">#input matrix to simplify for loop</span><br></code></pre></td></tr></table></figure>

<h3 id="Build-a-Tensorflow"><a href="#Build-a-Tensorflow" class="headerlink" title="Build a Tensorflow"></a>Build a Tensorflow</h3><ol>
<li>build the structure of the model</li>
<li>compile the model</li>
<li>input training data</li>
<li>fit the model</li>
<li>predict the model</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span><br>layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_3 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1,layer_2,...layer_n])<br>----------------------------<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>])<br><span class="hljs-number">2.</span><br>model.<span class="hljs-built_in">compile</span>()<br><span class="hljs-number">3.</span><br>x = np.array([[<span class="hljs-number">0.</span>..,<span class="hljs-number">245</span>,...,<span class="hljs-number">17</span>],[<span class="hljs-number">0.</span>..,<span class="hljs-number">200</span>,...,<span class="hljs-number">284</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>])<br><span class="hljs-number">4.</span><br>model.fit<br><span class="hljs-number">5.</span><br>model.predict(x_new)<br></code></pre></td></tr></table></figure>

<h3 id="Implementation-of-the-preceding-communication"><a href="#Implementation-of-the-preceding-communication" class="headerlink" title="Implementation of the preceding communication"></a>Implementation of the preceding communication</h3><img src="/img/fig/4.4.png" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>
$$
\vec{w}_{1}^{\left[ 1 \right]}=\left[ \begin{array}{c}
    1\\
    2\\
\end{array} \right] ,\vec{w}_{2}^{\left[ 1 \right]}=\left[ \begin{array}{c}
    -3\\
    4\\
\end{array} \right] ,\vec{w}_{3}^{\left[ 1 \right]}=\left[ \begin{array}{c}
    5\\
    -6\\
\end{array} \right] 
$$


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([<br>    [<span class="hljs-number">1</span>,-<span class="hljs-number">3</span>,<span class="hljs-number">5</span>],<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,-<span class="hljs-number">6</span>]<br>]) <br>b = np.array([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>a_in = np.array([-<span class="hljs-number">2</span>,<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure>

<p>Implement the coding of dense function with python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in,W,b,g</span>):<br>    <span class="hljs-comment">#units equals to the numbers of cols of W</span><br>    units = W.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment">#Create a matrix with the same size of units--[0,0,0]</span><br>    a_out = np.zeros(units)<br>    <span class="hljs-comment">#compete the g(z)</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x,W1,b1)<br>    a2 = dense(a1,W2,b2)<br>    a3 = desne(a2,W3,b3)<br>    a4 = dense(a3,W4,b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br>    <br>    <br></code></pre></td></tr></table></figure>

<h3 id="Choose-activation-function"><a href="#Choose-activation-function" class="headerlink" title="Choose activation function"></a>Choose activation function</h3><p>Three types activation function:</p>
<img src="/img/fig/4.3.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs py">activation=<span class="hljs-string">&quot;linear&quot;</span><br>activation=<span class="hljs-string">&quot;sigmoid&quot;</span><br>activation=<span class="hljs-string">&quot;relu&quot;</span><br></code></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>type y</th>
<th>0&#x2F;1</th>
<th>+&#x2F;-</th>
<th>0&#x2F;+</th>
</tr>
</thead>
<tbody><tr>
<td>sigmoid</td>
<td>binary classfication</td>
<td></td>
<td></td>
</tr>
<tr>
<td>linear</td>
<td></td>
<td>regression</td>
<td></td>
</tr>
<tr>
<td>relu</td>
<td></td>
<td></td>
<td>regression</td>
</tr>
</tbody></table>
<p>[why we need activation function?]</p>
<p>If we use the linear activation function in  hidden layers all the time, the predict results of neutral network equal to linear regression. that&#96;s why we should use sigmoid or relu function as the activation function to build the network.</p>
<h3 id="Soft-max-regression"><a href="#Soft-max-regression" class="headerlink" title="Soft-max regression"></a>Soft-max regression</h3><p>Softmax function has been used to solve the  multi-class problem. Essentially, it is still a <strong>classification problem</strong> <code>based on probability</code>. The expression of Softmax function as follow:<br>$$<br>z_j&#x3D;\vec{w}_j·\vec{x}+b_j\<br>a_j&#x3D;\frac{e^{z_j}}{\underset{k&#x3D;1}{\overset{n}{\varSigma}}e^{z_k}}&#x3D;P\left( y&#x3D;j|\vec{x} \right)<br>$$<br>a_j is the possibility of predict result.</p>
<p>The cost function of Soft-max function is:<br>$$<br>a_N&#x3D;\frac{e^{Z_N}}{e^{Z_1}+e^{Z_2}+\cdot \cdot \cdot +e^{Z_N}}&#x3D;P\left( y&#x3D;N|\vec{x} \right)<br>$$</p>
<p>$$<br>loss\left( a_1,…,a_N,y \right) &#x3D;\begin{cases}<br>    -\log a_1,,if,,y&#x3D;1\<br>    -\log a_2,,if,,y&#x3D;2\<br>    \vdots\<br>    -\log a_n,,if,,y&#x3D;n\<br>\end{cases}<br>$$<br>The result is <code>one of</code> the loss functions. </p>
<h3 id="Output-of-the-soft-max"><a href="#Output-of-the-soft-max" class="headerlink" title="Output of the soft-max"></a>Output of the soft-max</h3><img src="/img/fig/4.5.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>The outcome of soft-max classification is multiply. Every outcome will be competed a score to predict the right answer. </p>
<p>Carefully, the loss function we must choose the <code>SparseCategoricalCrossentropy</code>.<br>$$<br>a_{N}^{\left[ l \right]}&#x3D;\frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>\<br>-\log a_{N}^{\left[ l \right]}\ne -\log \frac{e^{Z_{N}^{\left[ l \right]}}}{e^{Z_{1}^{\left[ l \right]}}+e^{Z_{2}^{\left[ l \right]}}+\cdot \cdot \cdot +e^{Z_{N}^{\left[ l \right]}}}<br>$$<br>The difference of competing order can lead to the different outcome, which has different model accurancy.</p>
<h3 id="Improve-the-soft-max-model"><a href="#Improve-the-soft-max-model" class="headerlink" title="Improve the soft-max model"></a>Improve the soft-max model</h3><p>In order to improve the accuracy of calculations, we have made the following improvements to the model algorithm:</p>
<ul>
<li>In soft-max layer, we adopt the <code>linear</code> function as the activation.</li>
<li>In the process of compiling model, we add a parameter to improve the accurancy.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#linear function as the activation function in the soft-max layer</span><br>model = sequential([<br>    Dense(units = <span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>    Dense(units = <span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>)<br>])<br><span class="hljs-comment">#add parameter:from_logits=True </span><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br>logit = model(X)<br>f_x = tf.nn.sigmoid(logit)<br></code></pre></td></tr></table></figure>

<h3 id="MNIST-Adam"><a href="#MNIST-Adam" class="headerlink" title="MNIST Adam"></a>MNIST Adam</h3><h2 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h2><h3 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h3><p>Model fits the training data well(over-fit) but will fail to generalize to new examples not in the training set.</p>
<p>Hence, we need to <strong>partition the dataset</strong> to test the accurancy of the model.</p>
<p>The data set was divided into <code>test set</code> and <code>train set</code>.</p>
<p>【regression】</p>
<p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p>
<p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m_{train}}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p>
<p>compute test error:</p>
<p>$$<br>J_{test}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{test}}\left[ \underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) -y</em>{test}^{\left( i \right)} \right) ^2 \right]<br>$$</p>
<p>compute train error:</p>
<p>$$<br>J_{train}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{train}}\left[ \underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) -y</em>{train}^{\left( i \right)} \right) ^2 \right]<br>$$</p>
<p>【train】</p>
<p>Fit parameters by minimizing cost function $J( \vec{w},b)$:</p>
<p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;-\frac{1}{m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left[ y^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) \right) \right] +\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2}<br>$$</p>
<p>compute test error:</p>
<p>$$<br>J_{test}&#x3D;-\frac{1}{m_{test}}\underset{i&#x3D;1}{\overset{m_{test}}{\varSigma}}\left[ y_{test}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{test}^{\left( i \right)} \right) \right)<br>+\left( 1-y</em>{test}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{test}^{\left( i \right)} \right) \right) \right]<br>$$</p>
<p>compute train error:</p>
<p>$$<br>-\frac{1}{m_{train}}\underset{i&#x3D;1}{\overset{m_{train}}{\varSigma}}\left[ y_{train}^{\left( i \right)}\log \left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{train}^{\left( i \right)} \right) \right) +\left( 1-y</em>{train}^{\left( i \right)} \right) \log \left( 1-f_{\overrightarrow{w},b}\left( \vec{x}_{train}^{\left( i \right)} \right) \right) \right]<br>$$</p>
<h3 id="Model-selection-and-cross-validation"><a href="#Model-selection-and-cross-validation" class="headerlink" title="Model selection and cross validation"></a>Model selection and cross validation</h3><p>If we want to fit a function to predict a problem or classification, we often use test error $J_{test}$ to judge the accurancy of the model. But, the J test is likely to be an <code>optimistic estimate</code> of generalization error. Because, when we choose the degree of parameter d in polynomial fit.,This fit of J test may lower than the actual estimate. The optimistic estimate can lead to a low score of J_test.</p>
<p>So, we need to partition the dataset as three parts to avoid optimistic estimate:</p>
<ul>
<li>training set</li>
<li>cross validation set</li>
<li>test set</li>
</ul>
<p>compute cross validation error:</p>
<p>$$<br>J_{cv}\left( \overrightarrow{w},b \right) &#x3D;\frac{1}{2m_{cv}}\left[ \underset{i&#x3D;1}{\overset{m_{cv}}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}<em>{cv}^{\left( i \right)} \right) -y</em>{cv}^{\left( i \right)} \right) ^2 \right]<br>$$</p>
<h3 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h3><p>Have idea-Train model-Diagnose bias and variance</p>
<p> $J_{train}$ reflects bias, and  $J_{cv}$ reflects variance. A perfect model has low  $J_{train}$ and low  $J_{cv}$.</p>
<p>As the increasing of degree of d, $J_{train}$ will typically go down. Meanwhile, $J_{cv}$ will also go down and then it will increase.$J_{cv}$ will have a min value for different degree of d.</p>
<img src="/img/fig/4.6.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />

<p>How do you tell if our algorithm has a bias or variance problem?</p>
<ul>
<li>High bias(under fit): $J_{train}$ will be high($J_{train}\approx J_{cv}$)</li>
<li>High variance(over fit): $J_{cv}$&gt;&gt;$J_{train}$($J_{train}$ may be low)</li>
<li>High bias and High variance $J_{train}$ will be high and $J_{cv}$&gt;&gt;$J_{train}$</li>
</ul>
<h3 id="Regularization-and-bias-variance"><a href="#Regularization-and-bias-variance" class="headerlink" title="Regularization and bias&#x2F;variance"></a>Regularization and bias&#x2F;variance</h3><p>A large neutral network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_2 = Dense(units=<span class="hljs-number">15</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>layer_1 = Dense(units=<span class="hljs-number">1</span>,activation=<span class="hljs-string">&quot;sigmoid&quot;</span>,kernal_regularizer=L2(<span class="hljs-number">0.01</span>))<br>model = Sequential([layer_1,layer_2,layer_3])<br></code></pre></td></tr></table></figure>



<h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>When we increase the size of training set, the train error will increase. But, the error of cross validation will decrease. As the increasing of sample points. a regression function(a line or a curve) cannot fit all the point.</p>
<img src="/img/fig/4.7.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>If a algorithm suffers from high variance, getting more training data is <strong>likely</strong> to help.</p>
<p>If a algorithm suffers from high bias, getting more training data <strong>will not</strong> help much.</p>
<p>【Debugging algorithm】</p>
<p>we have implemented the regularized linear regression:</p>
<p>$$<br>J\left( \overrightarrow{w},b \right) &#x3D;\underset{\overrightarrow{w},b}{\min}\left[ \frac{1}{2m}\underset{i&#x3D;1}{\overset{m}{\varSigma}}\left( f_{\overrightarrow{w},b}\left( \vec{x}^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\frac{\lambda}{2m}\underset{j&#x3D;1}{\overset{n}{\varSigma}}\omega _{j}^{2} \right]<br>$$</p>
<table>
<thead>
<tr>
<th align="center">operation</th>
<th>what should do</th>
</tr>
</thead>
<tbody><tr>
<td align="center">get more training examples</td>
<td>fixes high variance</td>
</tr>
<tr>
<td align="center">try smaller sets of features</td>
<td>fixes high variance</td>
</tr>
<tr>
<td align="center">try getting additional features</td>
<td>fixes high bias</td>
</tr>
<tr>
<td align="center">try adding polynomial features</td>
<td>fixes high bias</td>
</tr>
<tr>
<td align="center">try decreasing  $\lambda$</td>
<td>fixes high bias</td>
</tr>
<tr>
<td align="center">try increasing  $\lambda$</td>
<td>fixes high variance</td>
</tr>
</tbody></table>
<p><code>Trade-off</code></p>
<p>Simple model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+b$) will get high bias VS complex model($f_{\overrightarrow{w},b}\left( x \right) &#x3D;w_1x+w_2x^2+w_3x^3+w_4x^4+b$) will get high variance.</p>
<img src="/img/fig/4.8.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<h3 id="Cycle-of-machine-learning"><a href="#Cycle-of-machine-learning" class="headerlink" title="Cycle of machine learning"></a>Cycle of machine learning</h3><p>The cycle of ML process:</p>
<img src="/img/fig/4.9.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<img src="/img/fig/4.10.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<p>How to apply the ML model to solve the actual problem in software engineering design?</p>
<img src="/img/fig/4.11.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;" />

<p>ML model is collected in the inference server. we use mobile app through API call to achieve these function.</p>
<h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><strong>Precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.</p>
<p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that were retrieved. </p>
<p>We design a <code>confusion matrix</code> to show it:</p>
<img src="/img/fig/4.12.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>

<p>In the trade-off  between Precision(P) and Recall(R), we use F1 score to evaluate the efficiency about the model.</p>
<p>the trade-off  between Precision(P) and Recall(R) has shown in the figure:</p>
<img src="/img/fig/4.13.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />


<p>$$<br>F1&#x3D;\frac{1}{\frac{1}{2}\left( \frac{1}{P}+\frac{1}{R} \right)}&#x3D;\frac{2PR}{P+R}<br>$$</p>
<h2 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h2><p>The structure of a decision tree:</p>
<img src="/img/fig/5.1.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />



<h3 id="Methods-chosen"><a href="#Methods-chosen" class="headerlink" title="Methods chosen"></a>Methods chosen</h3><p>For Signal Decision tree, we should focus on the problem is that the data features.</p>
<p>If the data is  discrete(just like 0 or 1), we can build the Signal Decision tree model. But,a row data may includes more than two classes, in this situation we should use <code>one-hot encoding</code>.</p>
<blockquote>
<p>one-hot encoding only fit for the decision tree model.</p>
</blockquote>
<p>If the data has continuous data(not only just like 0 or 1), we should split on a continuous variance.</p>
<p>For Multiple trees, we can use <strong>Random Forest</strong> and <strong>XGboost</strong> algorithm to solve.</p>
<img src="/img/fig/5.3.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />

<h3 id="Purity-entropy"><a href="#Purity-entropy" class="headerlink" title="Purity(entropy)"></a>Purity(entropy)</h3><p>$p_{1}$ &#x3D; fraction of examples that are True.</p>
<img src="/img/fig/5.4.png" srcset="/img/loading.gif" lazyload style="text-align: center;" />


<p>$$<br>H\left( p_1 \right) &#x3D;-p_1\log \left( p_1 \right) -p_0\log \left( p_0 \right)<br>\<br>&#x3D;-p_1\log \left( p_1 \right) -\left( 1-p_1 \right) \log \left( 1-p_1 \right)<br>$$</p>
<img src="/img/fig/5.5.jpg" srcset="/img/loading.gif" lazyload style="text-align: center;" />



<p>In this figure, $w^{left}$&#x3D;2&#x2F;5、 $w^{right}$&#x3D;3&#x2F;5、$p_{1}^{left}$&#x3D;5&#x2F;10、$p_{2}^{left}$&#x3D;5&#x2F;10.</p>
<p>Information Purity</p>
<p>$$<br>Information Purity &#x3D;H\left( p_{1}^{root} \right) -\left( w^{left}H\left( p_{1}^{left} \right) +w^{right}H\left( p_{1}^{right} \right) \right)<br>$$</p>
<p>We should choose the <code>max</code> value of the Information Purity to <strong>recursive</strong> the decision tree model, which is called <code>Information Gain</code>.</p>
<p>In the process of split on a continuous variance(<strong>Regression tree</strong>), we also choose the max decreasing variance result as a good fit model.</p>
<img src="/img/fig/5.2.jpg" srcset="/img/loading.gif" lazyload alt="s" style="text-align: center;"/>

<p>The purity of regression tree(equal to information gain):</p>
<p>$$<br>D&#x3D;V^{root}-\left( w^{left}V^{left}+w^{right}V^{right} \right)<br>$$</p>
<p>V instead of <code>variance</code>.</p>
<h3 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h3><ul>
<li><p>Start with all examples at the root node</p>
</li>
<li><p>Calculate information gain for all features, and pick the one with the highest information gain</p>
</li>
<li><p>Split dataset according to selected features, and create left and right branches of the tree</p>
</li>
<li><p>Keep repeating splitting process until stopping criteria is met:</p>
<pre><code class="hljs">      <figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">when a node <span class="hljs-keyword">is</span> <span class="hljs-number">100</span>% one <span class="hljs-keyword">class</span><br><br>		<span class="hljs-symbol">when</span> <span class="hljs-symbol">splitting</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">will</span> <span class="hljs-symbol">result</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">tree</span> <span class="hljs-symbol">exceeding</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">maximum</span> <span class="hljs-symbol">depth</span><br><br>		<span class="hljs-symbol">Information</span> <span class="hljs-symbol">gain</span> <span class="hljs-symbol">from</span> <span class="hljs-symbol">additional</span> <span class="hljs-symbol">splits</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">less</span> <span class="hljs-symbol">than</span> <span class="hljs-symbol">threshold</span><br><br>		<span class="hljs-symbol">when</span> <span class="hljs-symbol">number</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">examples</span> <span class="hljs-symbol">in</span> <span class="hljs-symbol">a</span> <span class="hljs-symbol">node</span> <span class="hljs-symbol">is</span> <span class="hljs-symbol">below</span>  <span class="hljs-symbol">a</span> <span class="hljs-symbol">threshold</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h2 id="Decision-tree-VS-Neutral-network"><a href="#Decision-tree-VS-Neutral-network" class="headerlink" title="Decision tree VS Neutral network"></a>Decision tree VS Neutral network</h2><h3 id="Decision-tree-1"><a href="#Decision-tree-1" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul>
<li>Works well on tabular(structured) data</li>
<li>Not recommended for unstructured data(images,audios,text)</li>
<li>Small decision tree may be human interpretable</li>
</ul>
<h3 id="Neutral-network"><a href="#Neutral-network" class="headerlink" title="Neutral network"></a>Neutral network</h3><ul>
<li>Works well on all types of data,including tabular(structured) data and unstructured data(images,audios,text)</li>
<li>May be slower than decision tree</li>
<li>Works with transfer learning</li>
<li>When building a system of multiple models working together, it might be easier to string together multiple neutral network</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/" class="category-chain-item">人工智能基础</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="print-no-link">#深度学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习基础</div>
      <div>http://example.com/2024/08/09/机器学习基础/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Munger Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年8月9日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/08/08/LLM%E5%AE%9E%E8%AE%AD/" title="和鲸社区 AI夏令营-LLM实训">
                        <span class="hidden-mobile">和鲸社区 AI夏令营-LLM实训</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createCssLink('https://registry.npmmirror.com/@waline/client/2.15.8/files/dist/waline.css')
      Fluid.utils.createScript('https://registry.npmmirror.com/@waline/client/2.15.8/files/dist/waline.js', function() {
        var options = Object.assign(
          {"serverURL":"https://comment.mungeryang.top/","path":"window.location.pathname","meta":["nick","mail","link"],"requiredMeta":["nick"],"lang":"zh-CN","emoji":["https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo"],"dark":"html[data-user-color-scheme=\"dark\"]","wordLimit":0,"pageSize":10},
          {
            el: '#waline',
            path: window.location.pathname
          }
        )
        Waline.init(options);
        Fluid.utils.waitElementVisible('#waline .vcontent', () => {
          var imgSelector = '#waline .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> <a target="_blank" rel="noopener" href="https://www.aliyun.com/"> 本网站由<img src="/img/aliyun.png" srcset="/img/loading.gif" lazyload width="65px" />提供CDN加速/云存储服务</a>
    </div>
  
  
    <div class="statistics">
  
  

  
    
    
    

  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      京ICP证20240808号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=12345678"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>京公网安备20240808号</span>
        </a>
      </span>
    
  
</div>

  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/love.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
